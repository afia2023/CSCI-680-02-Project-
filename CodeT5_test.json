[
    {
        "method_code": "def _create_hits(self, hit_attrs, qid, qdesc):                # read through until the beginning of the hsp block        self._read_until(lambda line: line.startswith((\"Internal pipeline\", \">>\")))        # start parsing the hsp block        hit_list = []        while True:            if self.line.startswith(\"Internal pipeline\"):                # by this time we should've emptied the hit attr list                assert len(hit_attrs) == 0                return hit_list            assert self.line.startswith(\">>\")            hid, hdesc = self.line[len(\">> \") :].split(\"  \", 1)            hdesc = hdesc.strip()            # read through the hsp table header and move one more line            self._read_until(                lambda line: line.startswith(                    (\" ---   ------ ----- --------\", \"   [No individual domains\")                )            )            self.line = read_forward(self.handle)            # parse the hsp table for the current hit            hsp_list = []            while True:                # break out of hsp parsing if there are no hits, it's the last hsp                # or it's the start of a new hit                if (                    self.line.startswith(\"   [No targets detected that satisfy\")                    or self.line.startswith(\"   [No individual domains\")                    or self.line.startswith(\"Internal pipeline statistics summary:\")                    or self.line.startswith(\"  Alignments for each domain:\")                    or self.line.startswith(\">>\")                ):                    hit_attr = hit_attrs.pop(0)                    hit = Hit(hsp_list)                    for attr, value in hit_attr.items():                        if attr == \"description\":                            cur_val = getattr(hit, attr)                            if cur_val and value and cur_val.startswith(value):                                continue                        setattr(hit, attr, value)                    if not hit:                        hit.query_description = qdesc                    hit_list.append(hit)                    break                parsed = [x for x in self.line.strip().split(\" \") if x]                assert len(parsed) == 16                # parsed column order:                # index, is_included, bitscore, bias, evalue_cond, evalue                # hmmfrom, hmmto, query_ends, hit_ends, alifrom, alito,                # envfrom, envto, acc_avg                frag = HSPFragment(hid, qid)                # set query and hit descriptions if they are defined / nonempty string                if qdesc:                    frag.query_description = qdesc                if hdesc:                    frag.hit_description = hdesc                # HMMER3 results are always protein                frag.molecule_type = \"protein\"                # depending on whether the program is hmmsearch, hmmscan, or phmmer                # {hmm,ali}{from,to} can either be hit_{from,to} or query_{from,to}                # for hmmscan, hit is the hmm profile, query is the sequence                if self._meta.get(\"program\") == \"hmmscan\":                    # adjust 'from' and 'to' coordinates to 0-based ones                    frag.hit_start = int(parsed[6]) - 1                    frag.hit_end = int(parsed[7])                    frag.query_start = int(parsed[9]) - 1                    frag.query_end = int(parsed[10])                elif self._meta.get(\"program\") in [\"hmmsearch\", \"phmmer\"]:                    # adjust 'from' and 'to' coordinates to 0-based ones                    frag.hit_start = int(parsed[9]) - 1                    frag.hit_end = int(parsed[10])                    frag.query_start = int(parsed[6]) - 1                    frag.query_end = int(parsed[7])                # strand is always 0, since HMMER now only handles protein                frag.hit_strand = frag.query_strand = 0                hsp = HSP([frag])                hsp.domain_index = int(parsed[0])                hsp.is_included = parsed[1] == \"!\"                hsp.bitscore = float(parsed[2])                hsp.bias = float(parsed[3])                hsp.evalue_cond = float(parsed[4])                hsp.evalue = float(parsed[5])                if self._meta.get(\"program\") == \"hmmscan\":                    # adjust 'from' and 'to' coordinates to 0-based ones                    hsp.hit_endtype = parsed[8]                    hsp.query_endtype = parsed[11]                elif self._meta.get(\"program\") in [\"hmmsearch\", \"phmmer\"]:                    # adjust 'from' and 'to' coordinates to 0-based ones                    hsp.hit_endtype = parsed[11]                    hsp.query_endtype = parsed[8]                # adjust 'from' and 'to' coordinates to 0-based ones                hsp.env_start = int(parsed[12]) - 1                hsp.env_end = int(parsed[13])                hsp.env_endtype = parsed[14]                hsp.acc_avg = float(parsed[15])                hsp_list.append(hsp)                self.line = read_forward(self.handle)            # parse the hsp alignments            if self.line.startswith(\"  Alignments for each domain:\"):                self._parse_aln_block(hid, hit.hsps)",
        "documentation": "parse a hmmer hsp block beginning with the hsp table private"
    },
    {
        "method_code": "def test_running_inferred_deployment_on_related_filtered_flow_run_state_changes():        with pytest.raises(ValidationError) as exc_info:        AutomationCore(            name=\"Run inferred deployment on some flow run state changes\",            trigger=EventTrigger(                match={\"prefect.resource.id\": \"prefect.flow-run.*\"},                match_related={\"some\": \"thing\"},                for_each={\"prefect.resource.id\"},                expect={\"prefect.flow-run.*\"},                posture=Posture.Reactive,                threshold=1,                within=timedelta(seconds=0),            ),            actions=[RunDeployment(source=\"inferred\")],        )    errors = exc_info.value.errors()    assert len(errors) == 1    (error,) = errors    assert error[\"loc\"] == tuple()    assert \"Running an inferred deployment from a flow run\" in error[\"msg\"]",
        "documentation": "it never okay to run an inferred deployment from a flow run state change even if there are filter on it because it inferred so the flow run will just generate another flow run just like it"
    },
    {
        "method_code": "def track_audio_features(self, track_id=None):                try:            return self._handle_response(                requests.get, self.audio_features_url + track_id            )        except SpotifyAPIError as e:            self._log.debug(\"Spotify API error: {}\", e)            return None",
        "documentation": "fetch track audio feature by it spotify id"
    },
    {
        "method_code": "def allow_transition(        self, from_state, to_state, probability=None, pseudocount=None    ):                # check the sanity of adding these states        for state in [from_state, to_state]:            if state not in self._state_alphabet:                raise ValueError(                    f\"State {state} was not found in the sequence alphabet\"                )        # ensure that the states are not already set        if (from_state, to_state) not in self.transition_prob and (            from_state,            to_state,        ) not in self.transition_pseudo:            # set the initial probability            if probability is None:                probability = 0            self.transition_prob[(from_state, to_state)] = probability            # set the initial pseudocounts            if pseudocount is None:                pseudocount = self.DEFAULT_PSEUDO            self.transition_pseudo[(from_state, to_state)] = pseudocount        else:            raise KeyError(                f\"Transition from {from_state} to {to_state} is already allowed.\"            )",
        "documentation": "set a transition a being possible between the two state probability and pseudocount are optional argument specifying the probability and pseudo count for the transition if these are not supplied then the value are set to the default value raise keyerror if the two state already have an allowed transition"
    },
    {
        "method_code": "def autocommit(self, conn, y=True):                raise NotImplementedError(\"pgdb does not support this!\")",
        "documentation": "set autocommit on the database connection currently not implemented"
    },
    {
        "method_code": "def _check_taxlabels(self, taxon):                # According to NEXUS standard, underscores shall be treated as spaces...,        # so checking for identity is more difficult        nextaxa = {t.replace(\" \", \"_\"): t for t in self.taxlabels}        nexid = taxon.replace(\" \", \"_\")        return nextaxa.get(nexid)",
        "documentation": "check for presence of taxon in selftaxlabels private"
    },
    {
        "method_code": "def __init__(self, cmd=\"primersearch\", **kwargs):                self.parameters = [            _Option(                [\"-seqall\", \"-sequences\", \"sequences\", \"seqall\"],                \"Sequence to look for the primer pairs in.\",                is_required=True,            ),            # When this wrapper was written primersearch used -sequences            # as the argument name. Since at least EMBOSS 5.0 (and            # perhaps earlier) this has been -seqall instead.            _Option(                [\"-infile\", \"-primers\", \"primers\", \"infile\"],                \"File containing the primer pairs to search for.\",                filename=True,                is_required=True,            ),            # When this wrapper was written primersearch used -primers            # as the argument name. Since at least EMBOSS 5.0 (and            # perhaps earlier) this has been -infile instead.            _Option(                [\"-mismatchpercent\", \"mismatchpercent\"],                \"Allowed percentage mismatch (any integer value, default 0).\",                is_required=True,            ),            _Option(                [\"-snucleotide\", \"snucleotide\"], \"Sequences are nucleotide (boolean)\"            ),            _Option([\"-sprotein\", \"sprotein\"], \"Sequences are protein (boolean)\"),        ]        _EmbossCommandLine.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def min_dist(coord, surface):        d = surface - coord    d2 = np.sum(d * d, 1)    return np.sqrt(min(d2))",
        "documentation": "return minimum distance between coord and surface"
    },
    {
        "method_code": "def Paused(    cls: Type[State[R]] = State,    timeout_seconds: Optional[int] = None,    pause_expiration_time: Optional[datetime.datetime] = None,    reschedule: bool = False,    pause_key: Optional[str] = None,    **kwargs: Any,) -> State[R]:        state_details = StateDetails.model_validate(kwargs.pop(\"state_details\", {}))    if state_details.pause_timeout:        raise ValueError(\"An extra pause timeout was provided in state_details\")    if pause_expiration_time is not None and timeout_seconds is not None:        raise ValueError(            \"Cannot supply both a pause_expiration_time and timeout_seconds\"        )    if pause_expiration_time is None and timeout_seconds is None:        pass    else:        state_details.pause_timeout = pause_expiration_time or (            pendulum.now(\"UTC\") + pendulum.Duration(seconds=timeout_seconds)        )    state_details.pause_reschedule = reschedule    state_details.pause_key = pause_key    return cls(type=StateType.PAUSED, state_details=state_details, **kwargs)",
        "documentation": "convenience function for creating paused state return state a paused state"
    },
    {
        "method_code": "def _process_path(path, metadata_filenames):                metadata_path = None        if path:            if isdir(path):                for fname in metadata_filenames:                    fpath = join(path, fname)                    if isfile(fpath):                        metadata_path = fpath                        break            elif isfile(path):                # '<pkg>.egg-info' file contains metadata directly                filenames = [\".egg-info\"]                if metadata_filenames:                    filenames.extend(metadata_filenames)                assert any(path.endswith(filename) for filename in filenames)                metadata_path = path            else:                # `path` does not exist                warnings.warn(\"Metadata path not found\", MetadataWarning)        else:            warnings.warn(\"Metadata path not found\", MetadataWarning)        return metadata_path",
        "documentation": "find metadata file inside distinfo folder or check direct file"
    },
    {
        "method_code": "def __lt__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        rslt = self._cmp(other)        if rslt:            rslt = cast(tuple[AtomKey, AtomKey], rslt)            return rslt[0] < rslt[1]        return False",
        "documentation": "test less than"
    },
    {
        "method_code": "def get_cloud_client(    host: Optional[str] = None,    api_key: Optional[str] = None,    httpx_settings: Optional[dict] = None,    infer_cloud_url: bool = False,) -> \"CloudClient\":        if httpx_settings is not None:        httpx_settings = httpx_settings.copy()    if infer_cloud_url is False:        host = host or PREFECT_CLOUD_API_URL.value()    else:        configured_url = prefect.settings.PREFECT_API_URL.value()        host = re.sub(PARSE_API_URL_REGEX, \"\", configured_url)    return CloudClient(        host=host,        api_key=api_key or PREFECT_API_KEY.value(),        httpx_settings=httpx_settings,    )",
        "documentation": "need a docstring"
    },
    {
        "method_code": "def test_less_than_comparison(self):                self.assertLess(self.mutable_s[:-1], self.mutable_s)",
        "documentation": "test lt comparison method"
    },
    {
        "method_code": "def max_filename_length(path: AnyStr, limit=MAX_FILENAME_LENGTH) -> int:        if hasattr(os, \"statvfs\"):        try:            res = os.statvfs(path)        except OSError:            return limit        return min(res[9], limit)    else:        return limit",
        "documentation": "attempt to determine the maximum filename length for the filesystem containing path if the value is greater than limit then limit is used instead to prevent error when a filesystem misreports it capacity if it cannot be determined eg on window return limit"
    },
    {
        "method_code": "def __str__(self):                # Note: Before equate was handled explicitly, the old        # code would do either \"--name \" or \"--name=value \",        # or \" -name \" or \" -name value \".  This choice is now        # now made explicitly when setting up the option.        if self.value is None:            return f\"{self.names[0]} \"        if self.is_filename:            v = _escape_filename(self.value)        else:            v = str(self.value)        if self.equate:            return f\"{self.names[0]}={v} \"        else:            return f\"{self.names[0]} {v} \"",
        "documentation": "return the value of this option for the commandline includes a trailing space"
    },
    {
        "method_code": "def __init__(self, match=1, mismatch=0):                self.match = match        self.mismatch = mismatch",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def prefect_db():        with prefect_test_harness():        yield",
        "documentation": "set up test harness for temporary db during test run"
    },
    {
        "method_code": "def get_id(self):                return self.fid",
        "documentation": "get identifier for the fragment return id for the fragment rtype int"
    },
    {
        "method_code": "def get_unpacked_list(self):                atom_list = self.get_list()        undisordered_atom_list = []        for atom in atom_list:            if atom.is_disordered():                undisordered_atom_list += atom.disordered_get_list()            else:                undisordered_atom_list.append(atom)        return undisordered_atom_list",
        "documentation": "return the list of all atom unpack disorderedatoms"
    },
    {
        "method_code": "def crop_matrix(self, matrix=None, delete=(), exclude=()):                if not matrix:            matrix = self.matrix        if [t for t in delete if not self._check_taxlabels(t)]:            raise NexusError(                f\"Unknown taxa: {', '.join(set(delete).difference(self.taxlabels))}\"            )        if exclude != []:            undelete = [t for t in self.taxlabels if t in matrix and t not in delete]            if not undelete:                return {}            m = [str(matrix[k]) for k in undelete]            sitesm = [s for i, s in enumerate(zip(*m)) if i not in exclude]            if sitesm == []:                return {t: Seq(\"\") for t in undelete}            else:                m = [Seq(s) for s in (\"\".join(x) for x in zip(*sitesm))]                return dict(zip(undelete, m))        else:            return {                t: matrix[t] for t in self.taxlabels if t in matrix and t not in delete            }",
        "documentation": "return a matrix without deleted taxon and excluded character"
    },
    {
        "method_code": "def track_for_id(self, track_id):                return None",
        "documentation": "return a trackinfo object or none if no matching release wa found"
    },
    {
        "method_code": "def run_task(    task: Task[P, Union[R, Coroutine[Any, Any, R]]],    task_run_id: Optional[UUID] = None,    task_run: Optional[TaskRun] = None,    parameters: Optional[Dict[str, Any]] = None,    wait_for: Optional[Iterable[PrefectFuture]] = None,    return_type: Literal[\"state\", \"result\"] = \"result\",    dependencies: Optional[Dict[str, Set[TaskRunInput]]] = None,    context: Optional[Dict[str, Any]] = None,) -> Union[R, State, None, Coroutine[Any, Any, Union[R, State, None]]]:        kwargs = dict(        task=task,        task_run_id=task_run_id,        task_run=task_run,        parameters=parameters,        wait_for=wait_for,        return_type=return_type,        dependencies=dependencies,        context=context,    )    if task.isasync and task.isgenerator:        return run_generator_task_async(**kwargs)    elif task.isgenerator:        return run_generator_task_sync(**kwargs)    elif task.isasync:        return run_task_async(**kwargs)    else:        return run_task_sync(**kwargs)",
        "documentation": "run the provided task args task the task to run taskrunid the id of the task run if not provided a new task run will be created taskrun the task run object if not provided a new task run will be created parameter the parameter to pas to the task waitfor a list of future to wait for before running the task returntype the return type to return either state or result dependency a dictionary of task run input to use for dependency tracking context a dictionary containing the context to use for the task run only required if the task is running on in a remote environment return the result of the task run"
    },
    {
        "method_code": "def submit(        self,        task: \"Task\",        parameters: Dict[str, Any],        wait_for: Optional[Iterable[PrefectFuture]] = None,        dependencies: Optional[Dict[str, Set[TaskRunInput]]] = None,    ) -> F:                ...",
        "documentation": "submit a task to the task run engine args task the task to submit parameter the parameter to use when running the task waitfor a list of future that the task depends on return a future object that can be used to wait for the task to complete and retrieve the result"
    },
    {
        "method_code": "def __init__(self, random_access_proxy, key_function, repr, obj_repr):                # Use key_function=None for default value        self._proxy = random_access_proxy        self._key_function = key_function        self._repr = repr        self._obj_repr = obj_repr        self._cached_prev_record = (None, None)  # (key, record)        if key_function:            offset_iter = (                (key_function(key), offset, length)                for (key, offset, length) in random_access_proxy            )        else:            offset_iter = random_access_proxy        offsets = {}        for key, offset, length in offset_iter:            # Note - we don't store the length because I want to minimise the            # memory requirements. With the SQLite backend the length is kept            # and is used to speed up the get_raw method (by about 3 times).            # The length should be provided by all the current backends except            # SFF where there is an existing Roche index we can reuse (very fast            # but lacks the record lengths)            # assert length or format in [\"sff\", \"sff-trim\"], \\            #       \"%s at offset %i given length %r (%s format %s)\" \\            #       % (key, offset, length, filename, format)            if key in offsets:                self._proxy._handle.close()                raise ValueError(f\"Duplicate key '{key}'\")            else:                offsets[key] = offset        self._offsets = offsets",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def generate_track_info(track_id=\"track info\", values={}):        track = TrackInfo(        title=\"track info\",        track_id=track_id,    )    for field in TRACK_INFO_FIELDS:        setattr(track, field, \"track info\")    for field, value in values.items():        setattr(track, field, value)    return track",
        "documentation": "return trackinfo populated with mock data the trackid field is set to the corresponding argument all other string field are set to track info"
    },
    {
        "method_code": "def get_network_mode(self) -> Optional[str]:                # User's value takes precedence; this may collide with the incompatible options        # mentioned below.        if self.network_mode:            if sys.platform != \"linux\" and self.network_mode == \"host\":                warnings.warn(                    f\"{self.network_mode!r} network mode is not supported on platform \"                    f\"{sys.platform!r} and may not work as intended.\"                )            return self.network_mode        # Network mode is not compatible with networks or ports (we do not support ports        # yet though)        if self.networks:            return None        # Check for a local API connection        api_url = self.env.get(\"PREFECT_API_URL\", PREFECT_API_URL.value())        if api_url:            try:                _, netloc, _, _, _, _ = urllib.parse.urlparse(api_url)            except Exception as exc:                warnings.warn(                    f\"Failed to parse host from API URL {api_url!r} with exception: \"                    f\"{exc}\\nThe network mode will not be inferred.\"                )                return None            host = netloc.split(\":\")[0]            # If using a locally hosted API, use a host network on linux            if sys.platform == \"linux\" and (host == \"127.0.0.1\" or host == \"localhost\"):                return \"host\"        # Default to unset        return None",
        "documentation": "return the network mode to use for the container based on the configured option and the platform"
    },
    {
        "method_code": "def _abi_parse_header(header, stream):        # header structure (after ABIF marker):    # file version, tag name, tag number,    # element type code, element size, number of elements    # data size, data offset, handle (not file handle)    head_elem_size = header[4]    head_elem_num = header[5]    head_offset = header[7]    for index in range(head_elem_num):        start = head_offset + index * head_elem_size        # add directory offset to tuple        # to handle directories with data size <= 4 bytes        stream.seek(start)        dir_entry = struct.unpack(_DIRFMT, stream.read(struct.calcsize(_DIRFMT))) + (            start,        )        # only parse desired dirs        key = dir_entry[0].decode()        key += str(dir_entry[1])        tag_name = dir_entry[0].decode()        tag_number = dir_entry[1]        elem_code = dir_entry[2]        elem_num = dir_entry[4]        data_size = dir_entry[5]        data_offset = dir_entry[6]        tag_offset = dir_entry[8]        # if data size <= 4 bytes, data is stored inside tag        # so offset needs to be changed        if data_size <= 4:            data_offset = tag_offset + 20        stream.seek(data_offset)        data = stream.read(data_size)        yield tag_name, tag_number, _parse_tag_data(elem_code, elem_num, data)",
        "documentation": "return directory content private"
    },
    {
        "method_code": "def __str__(self):                lines = []        if self.plate and self.plate.id:            lines.append(f\"Plate ID: {self.plate.id}\")        if self.id:            lines.append(f\"Well ID: {self.id}\")        lines.append(\"Time points: %i\" % len(self))        lines.append(\"Minum signal %.2f at time %.2f\" % min(self, key=lambda x: x[1]))        lines.append(\"Maximum signal %.2f at time %.2f\" % max(self, key=lambda x: x[1]))        lines.append(repr(self))        return \"\\n\".join(lines)",
        "documentation": "return a human readable summary of the record string the python builtin function str work by calling the object str method eg from bio import phenotype plate phenotypereadphenotypeplatejson pmjson record platea printrecord plate id pm well id a time point minum signal at time maximum signal at time wellrecord note that long time span are shown truncated"
    },
    {
        "method_code": "def _parse_row(self):                cols = [x for x in self.line.strip().split(\" \") if x]        if len(cols) < 18:            raise ValueError(\"Less columns than expected, only %i\" % len(cols))        # if len(cols) > 19, we have extra description columns        # combine them all into one string in the 19th column        cols[18] = \" \".join(cols[18:])        # assign parsed column data into qresult, hit, and hsp dicts        qresult = {}        qresult[\"id\"] = cols[2]  # query name        qresult[\"accession\"] = cols[3]  # query accession        hit = {}        hit[\"id\"] = cols[0]  # target name        hit[\"accession\"] = cols[1]  # target accession        hit[\"evalue\"] = float(cols[4])  # evalue (full sequence)        hit[\"bitscore\"] = float(cols[5])  # score (full sequence)        hit[\"bias\"] = float(cols[6])  # bias (full sequence)        hit[\"domain_exp_num\"] = float(cols[10])  # exp        hit[\"region_num\"] = int(cols[11])  # reg        hit[\"cluster_num\"] = int(cols[12])  # clu        hit[\"overlap_num\"] = int(cols[13])  # ov        hit[\"env_num\"] = int(cols[14])  # env        hit[\"domain_obs_num\"] = int(cols[15])  # dom        hit[\"domain_reported_num\"] = int(cols[16])  # rep        hit[\"domain_included_num\"] = int(cols[17])  # inc        hit[\"description\"] = cols[18]  # description of target        hsp = {}        hsp[\"evalue\"] = float(cols[7])  # evalue (best 1 domain)        hsp[\"bitscore\"] = float(cols[8])  # score (best 1 domain)        hsp[\"bias\"] = float(cols[9])  # bias (best 1 domain)        # strand is always 0, since HMMER now only handles protein        frag = {}        frag[\"hit_strand\"] = frag[\"query_strand\"] = 0        frag[\"molecule_type\"] = \"protein\"        return {\"qresult\": qresult, \"hit\": hit, \"hsp\": hsp, \"frag\": frag}",
        "documentation": "return a dictionary of parsed row value private"
    },
    {
        "method_code": "def name(self):                separator = ':' if self.family == 6 else '.'        start_chunks = str(self.start_address.ip).split(separator)        end_chunks = str(self.end_address.ip).split(separator)        base_chunks = []        for a, b in zip(start_chunks, end_chunks):            if a == b:                base_chunks.append(a)        base_str = separator.join(base_chunks)        start_str = separator.join(start_chunks[len(base_chunks):])        end_str = separator.join(end_chunks[len(base_chunks):])        return f'{base_str}{separator}{start_str}-{end_str}/{self.start_address.prefixlen}'",
        "documentation": "return an efficient string representation of the ip range"
    },
    {
        "method_code": "def index_packages(num):        # XXX: get_index_r_X should probably be refactored to avoid loading the environment like this.    get_index = getattr(helpers, f\"get_index_r_{num}\")    index, _ = get_index(context.subdir)    return list(index.values())",
        "documentation": "get the index data of the helpersgetindexr helper"
    },
    {
        "method_code": "def snapshot(self):                exclude_fields = []        if get_config().CHANGELOG_SKIP_EMPTY_CHANGES:            exclude_fields = ['last_updated',]        self._prechange_snapshot = self.serialize_object(exclude=exclude_fields)",
        "documentation": "save a snapshot of the object current state in preparation for modification the snapshot is saved a prechangesnapshot on the instance"
    },
    {
        "method_code": "def calculate(self, sequence):                # TODO - Code itself tolerates ambiguous bases (as NaN).        if sorted(self.alphabet) != [\"A\", \"C\", \"G\", \"T\"]:            raise ValueError(                \"PSSM has wrong alphabet: %s - Use only with DNA motifs\" % self.alphabet            )        # NOTE: The C code handles mixed case input as this could be large        # (e.g. contig or chromosome), so requiring it be all upper or lower        # case would impose an overhead to allocate the extra memory.        try:            sequence = bytes(sequence)        except TypeError:  # str            try:                sequence = bytes(sequence, \"ASCII\")            except TypeError:                raise ValueError(                    \"sequence should be a Seq, MutableSeq, string, or bytes-like object\"                ) from None            except UnicodeEncodeError:                raise ValueError(                    \"sequence should contain ASCII characters only\"                ) from None        except Exception:            raise ValueError(                \"sequence should be a Seq, MutableSeq, string, or bytes-like object\"            ) from None        n = len(sequence)        m = self.length        # Create the numpy arrays here; the C module then does not rely on numpy        # Use a float32 for the scores array to save space        scores = np.empty(n - m + 1, np.float32)        logodds = np.array(            [[self[letter][i] for letter in \"ACGT\"] for i in range(m)], float        )        _pwm.calculate(sequence, logodds, scores)        if len(scores) == 1:            return scores[0]        else:            return scores",
        "documentation": "return the pwm score for a given sequence for all position note the sequence can only be a dna sequence the search is performed only on one strand if the sequence and the motif have the same length a single number is returned otherwise the result is a onedimensional numpy array"
    },
    {
        "method_code": "def add_done_callback(self, callback: Call) -> Call:                raise NotImplementedError()",
        "documentation": "schedule a call to run when the waiter is done waiting if the waiter is already done a runtimeerror error will be thrown"
    },
    {
        "method_code": "def test_ddbj_gff3(self):                handle = TogoWS.entry(\"ddbj\", \"X52960\", format=\"gff\")        data = handle.read()        handle.close()        self.assertTrue(data.startswith(\"##gff-version 3\\nX52960\\tDDBJ\\t\"), data)",
        "documentation": "biotogowsentryddbj x formatgff"
    },
    {
        "method_code": "def from_msa(cls, align):                rec = [SeqRecord(CodonSeq(str(i.seq)), id=i.id) for i in align._records]        return cls(rec)",
        "documentation": "convert a multipleseqalignment to codonalignment function to convert a multipleseqalignment to codonalignment it is the user responsibility to ensure all the requirement needed by codonalignment is met"
    },
    {
        "method_code": "def test_output_directory_with_space(self):                temp_out_dir = \"xxmotif test\"        input_file = self.copy_and_mark_for_cleanup(\"Fasta/f002\")        try:            XXmotifCommandline(outdir=temp_out_dir, seqfile=input_file)        except ValueError:            pass        else:            self.fail(\"expected ValueError\")",
        "documentation": "test an output directory containing a space"
    },
    {
        "method_code": "def commands(self):                cmd = ui.Subcommand(            \"bareasc\", help=\"unidecode version of beet list command\"        )        cmd.parser.usage += (            \"\\n\" \"Example: %prog -f '$album: $title' artist:beatles\"        )        cmd.parser.add_all_common_options()        cmd.func = self.unidecode_list        return [cmd]",
        "documentation": "add bareasc command a unidecode version of list"
    },
    {
        "method_code": "def _parse_row(self):                assert self.line        cols = [x for x in self.line.strip().split(\"\\t\") if x]        self._validate_cols(cols)        psl = {}        psl[\"qname\"] = cols[9]  # qName        psl[\"qsize\"] = int(cols[10])  # qSize        psl[\"tname\"] = cols[13]  # tName        psl[\"tsize\"] = int(cols[14])  # tSize        psl[\"matches\"] = int(cols[0])  # matches        psl[\"mismatches\"] = int(cols[1])  # misMatches        psl[\"repmatches\"] = int(cols[2])  # repMatches        psl[\"ncount\"] = int(cols[3])  # nCount        psl[\"qnuminsert\"] = int(cols[4])  # qNumInsert        psl[\"qbaseinsert\"] = int(cols[5])  # qBaseInsert        psl[\"tnuminsert\"] = int(cols[6])  # tNumInsert        psl[\"tbaseinsert\"] = int(cols[7])  # tBaseInsert        psl[\"strand\"] = cols[8]  # strand        psl[\"qstart\"] = int(cols[11])  # qStart        psl[\"qend\"] = int(cols[12])  # qEnd        psl[\"tstart\"] = int(cols[15])  # tStart        psl[\"tend\"] = int(cols[16])  # tEnd        psl[\"blockcount\"] = int(cols[17])  # blockCount        psl[\"blocksizes\"] = _list_from_csv(cols[18], int)  # blockSizes        psl[\"qstarts\"] = _list_from_csv(cols[19], int)  # qStarts        psl[\"tstarts\"] = _list_from_csv(cols[20], int)  # tStarts        if self.pslx:            psl[\"qseqs\"] = _list_from_csv(cols[21])  # query sequence            psl[\"tseqs\"] = _list_from_csv(cols[22])  # hit sequence        return psl",
        "documentation": "return a dictionary of parsed column value private"
    },
    {
        "method_code": "def get_blank_emissions(self):                return self._emission_pseudo",
        "documentation": "get the starting default emission for each sequence this return a dictionary of the default emission for each letter the dictionary is structured with key a seqletter emissionletter and value a the starting number of emission"
    },
    {
        "method_code": "def test_hmmer3text_30_multiple_middle(self):                filename = \"Hmmer/text_30_hmmscan_001.out\"        raw =   # noqa : W291        self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "documentation": "test hmmertext raw string retrieval hmmer multiple query middle texthmmscanout"
    },
    {
        "method_code": "def wrapped_future(self) -> F:                return self._wrapped_future",
        "documentation": "the underlying future object wrapped by this prefect future"
    },
    {
        "method_code": "def test_pil_file_deinterlace(self):                path = PILBackend().deinterlace(self.IMG_225x225)        from PIL import Image        with Image.open(path) as img:            assert \"progression\" not in img.info",
        "documentation": "test pil deinterlace function check if the pilbackenddeinterlace function return image that are nonprogressive"
    },
    {
        "method_code": "def stochiometry(self):                # Note: This an inefficient and ugly temporary implementation.        #       To be practical, stochiometric matrices should probably        #       be implemented by sparse matrices, which would require        #       NumPy dependencies.        #        # PS: We should implement automatic checking for NumPy here.        species = self.species()        reactions = self.reactions()        stoch = [] * len(reactions)        for i in range(len(reactions)):            stoch[i] = 0 * len(species)            for s in reactions[i].species():                stoch[species.index(s)] = reactions[i].reactants[s]        return (species, reactions, stoch)",
        "documentation": "compute the stoichiometry matrix for self return specie reaction stoch where specie ordered list of specie in this system reaction ordered list of reaction in this system stoch d array where stochij is coef of the jth specie in the ith reaction a defined by specie and reaction above"
    },
    {
        "method_code": "def test_import_objects(self):                IMPORT_DATA =         # Create the manufacturer and platform        manufacturer = Manufacturer(name='Generic', slug='generic')        manufacturer.save()        platform = Platform(name='Platform', slug='test-platform', manufacturer=manufacturer)        platform.save()        # Add all required permissions to the test user        self.add_permissions(            'dcim.view_devicetype',            'dcim.add_devicetype',            'dcim.add_consoleporttemplate',            'dcim.add_consoleserverporttemplate',            'dcim.add_powerporttemplate',            'dcim.add_poweroutlettemplate',            'dcim.add_interfacetemplate',            'dcim.add_frontporttemplate',            'dcim.add_rearporttemplate',            'dcim.add_modulebaytemplate',            'dcim.add_devicebaytemplate',            'dcim.add_inventoryitemtemplate',        )        form_data = {            'data': IMPORT_DATA,            'format': 'yaml'        }        response = self.client.post(reverse('dcim:devicetype_import'), data=form_data, follow=True)        self.assertHttpStatus(response, 200)        device_type = DeviceType.objects.get(model='TEST-1000')        self.assertEqual(device_type.manufacturer.pk, manufacturer.pk)        self.assertEqual(device_type.default_platform.pk, platform.pk)        self.assertEqual(device_type.slug, 'test-1000')        self.assertEqual(device_type.u_height, 2)        self.assertFalse(device_type.is_full_depth)        self.assertEqual(device_type.airflow, DeviceAirflowChoices.AIRFLOW_FRONT_TO_REAR)        self.assertEqual(device_type.subdevice_role, SubdeviceRoleChoices.ROLE_PARENT)        self.assertEqual(device_type.weight, 10)        self.assertEqual(device_type.weight_unit, WeightUnitChoices.UNIT_KILOGRAM)        self.assertEqual(device_type.comments, 'Test comment')        # Verify all of the components were created        self.assertEqual(device_type.consoleporttemplates.count(), 3)        cp1 = ConsolePortTemplate.objects.first()        self.assertEqual(cp1.name, 'Console Port 1')        self.assertEqual(cp1.type, ConsolePortTypeChoices.TYPE_DE9)        self.assertEqual(device_type.consoleserverporttemplates.count(), 3)        csp1 = ConsoleServerPortTemplate.objects.first()        self.assertEqual(csp1.name, 'Console Server Port 1')        self.assertEqual(csp1.type, ConsolePortTypeChoices.TYPE_RJ45)        self.assertEqual(device_type.powerporttemplates.count(), 3)        pp1 = PowerPortTemplate.objects.first()        self.assertEqual(pp1.name, 'Power Port 1')        self.assertEqual(pp1.type, PowerPortTypeChoices.TYPE_IEC_C14)        self.assertEqual(device_type.poweroutlettemplates.count(), 3)        po1 = PowerOutletTemplate.objects.first()        self.assertEqual(po1.name, 'Power Outlet 1')        self.assertEqual(po1.type, PowerOutletTypeChoices.TYPE_IEC_C13)        self.assertEqual(po1.power_port, pp1)        self.assertEqual(po1.feed_leg, PowerOutletFeedLegChoices.FEED_LEG_A)        self.assertEqual(device_type.interfacetemplates.count(), 3)        iface1 = InterfaceTemplate.objects.first()        self.assertEqual(iface1.name, 'Interface 1')        self.assertEqual(iface1.type, InterfaceTypeChoices.TYPE_1GE_FIXED)        self.assertTrue(iface1.mgmt_only)        self.assertTrue(iface1.enabled)        iface2 = InterfaceTemplate.objects.filter(name=\"Interface 2\").first()        self.assertFalse(iface2.enabled)        self.assertEqual(device_type.rearporttemplates.count(), 3)        rp1 = RearPortTemplate.objects.first()        self.assertEqual(rp1.name, 'Rear Port 1')        self.assertEqual(device_type.frontporttemplates.count(), 3)        fp1 = FrontPortTemplate.objects.first()        self.assertEqual(fp1.name, 'Front Port 1')        self.assertEqual(fp1.rear_port, rp1)        self.assertEqual(fp1.rear_port_position, 1)        self.assertEqual(device_type.modulebaytemplates.count(), 3)        mb1 = ModuleBayTemplate.objects.first()        self.assertEqual(mb1.name, 'Module Bay 1')        self.assertEqual(device_type.devicebaytemplates.count(), 3)        db1 = DeviceBayTemplate.objects.first()        self.assertEqual(db1.name, 'Device Bay 1')        self.assertEqual(device_type.inventoryitemtemplates.count(), 3)        ii1 = InventoryItemTemplate.objects.first()        self.assertEqual(ii1.name, 'Inventory Item 1')",
        "documentation": "custom import test for yamlbased import versus csv"
    },
    {
        "method_code": "def get_installed_plugins():        plugins = {}    for plugin_name in settings.PLUGINS:        plugin_name = plugin_name.rsplit('.', 1)[-1]        plugin_config = apps.get_app_config(plugin_name)        plugins[plugin_name] = getattr(plugin_config, 'version', None)    return dict(sorted(plugins.items()))",
        "documentation": "return a dictionary mapping the name of installed plugins to their version"
    },
    {
        "method_code": "def _sync_channel_to_disk(subdir_data: SubdirData):        base = Path(EXPORTED_CHANNELS_DIR) / subdir_data.channel.name    subdir_path = base / subdir_data.channel.subdir    subdir_path.mkdir(parents=True, exist_ok=True)    with open(subdir_path / \"repodata.json\", \"w\") as f:        json.dump(            _export_subdir_data_to_repodata(subdir_data), f, indent=2, sort_keys=True        )        f.flush()        os.fsync(f.fileno())",
        "documentation": "this function is only temporary and meant to patch wrong undesirable testing behaviour it should end up being replaced with the new classbased backendagnostic solver test"
    },
    {
        "method_code": "def render_pep440_post_branch(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"] or pieces[\"dirty\"]:            rendered += \".post%d\" % pieces[\"distance\"]            if pieces[\"branch\"] != \"master\":                rendered += \".dev0\"            rendered += plus_or_dot(pieces)            rendered += \"g%s\" % pieces[\"short\"]            if pieces[\"dirty\"]:                rendered += \".dirty\"    else:        # exception #1        rendered = \"0.post%d\" % pieces[\"distance\"]        if pieces[\"branch\"] != \"master\":            rendered += \".dev0\"        rendered += \"+g%s\" % pieces[\"short\"]        if pieces[\"dirty\"]:            rendered += \".dirty\"    return rendered",
        "documentation": "tagpostdistancedevghexdirty the dev mean not master branch exception no tag postdistancedevghexdirty"
    },
    {
        "method_code": "def fix_art(self, album):                if album.artpath:            self.set_permissions(files=[album.artpath])",
        "documentation": "fix the permission for album art file"
    },
    {
        "method_code": "def hit_map(self, func=None):                hits = [deepcopy(hit) for hit in self.hits]        if func is not None:            hits = [func(x) for x in hits]        obj = self.__class__(hits, self.id, self._hit_key_function)        self._transfer_attrs(obj)        return obj",
        "documentation": "create new queryresult object mapping the given function to it hit param func map function type func callable accepts hit return hit here is an example of using hitmap with a function that discard all hsps in a hit except for the first one from bio import searchio qresult nextsearchioparseblastmirnaxml blastxml printqresult program blastn query mir target refseqrna hit hsp id description girefnr homo sapiens microrna girefnr pan troglodyte microrna girefnr macaca mulatta microrna girefnr pan troglodyte microrna girefnr pan troglodyte microrna girefnr homo sapiens microrna girefnr homo sapiens microrna girefnr pan troglodyte microrna tophsp lambda hit hit mappedqresult qresulthitmaptophsp printmappedqresult program blastn query mir target refseqrna hit hsp id description girefnr homo sapiens microrna girefnr pan troglodyte microrna girefnr macaca mulatta microrna girefnr pan troglodyte microrna girefnr pan troglodyte microrna girefnr homo sapiens microrna girefnr homo sapiens microrna girefnr pan troglodyte microrna"
    },
    {
        "method_code": "def test_get_auth_handler(plugin_manager):        plugin = CustomAuthPlugin()    plugin_manager.register(plugin)    auth_handler_cls = plugin_manager.get_auth_handler(PLUGIN_NAME)    assert auth_handler_cls is CustomCondaAuth    auth_handler_cls = plugin_manager.get_auth_handler(\"DOES_NOT_EXIST\")    assert auth_handler_cls is None",
        "documentation": "return the correct auth backend class or return none"
    },
    {
        "method_code": "def find(self, sub, start=None, end=None):                return bytes(self).find(sub, start, end)",
        "documentation": "return the lowest index in data where subsection sub is found return the lowest index in data where subsection sub is found such that sub is contained within datastartend optional argument start and end are interpreted a in slice notation return on failure"
    },
    {
        "method_code": "def frame_nice(self, seq, frame, translation_table=1):                length = len(seq)        protein = self.frame(seq, frame, translation_table)        protein_length = len(protein)        protein = \"  \".join(list(protein))        protein += (((length - (abs(frame) - 1)) % 3) + 2) * \" \"        if frame < 0:            protein = protein[::-1]        res = self.header_nice(f\"Frame {frame} translation\", seq)        for i in range(0, length, 60):            subseq = seq[i : i + 60]            p = i // 3            if frame > 0:                res += \"%d/%d\\n\" % (i + 1, p + 1)                res += \" \" * (frame - 1) + protein[i : i + 60] + \"\\n\"                # seq                res += subseq.lower() + \"%5d %%\\n\" % int(self.gc(subseq)) + \"\\n\"            else:                res += \"%d/%d\\n\" % (i + 1, protein_length - len(protein[:i].split()))                # seq                res += subseq.lower() + \"%5d %%\\n\" % int(self.gc(subseq))                res += protein[i : i + 60] + \"\\n\\n\"        return res",
        "documentation": "print a pretty print single frame translation"
    },
    {
        "method_code": "def event_pattern(self) -> re.Pattern:                if self._event_pattern:            return self._event_pattern        if not self.expect:            # This preserves the trivial match for `expect`, and matches the behavior            # of expects() below            self._event_pattern = re.compile(\".+\")        else:            patterns = [                # escape each pattern, then translate wildcards ('*' -> r'.+')                re.escape(e).replace(\"\\\\*\", \".+\")                for e in self.expect | self.after            ]            self._event_pattern = re.compile(\"|\".join(patterns))        return self._event_pattern",
        "documentation": "a regular expression which may be evaluated against any event string to determine if this trigger would be interested in the event"
    },
    {
        "method_code": "def fetch_all(        self,        operation: str,        parameters: Optional[Dict[str, Any]] = None,        cursor_type: Type[SnowflakeCursor] = SnowflakeCursor,        **execute_kwargs: Any,    ) -> List[Tuple[Any]]:          # noqa        inputs = dict(            command=operation,            params=parameters,            **execute_kwargs,        )        new, cursor = self._get_cursor(inputs, cursor_type)        if new:            self.execute(                operation, parameters, cursor_type=cursor_type, **execute_kwargs            )        self.logger.debug(\"Preparing to fetch all rows.\")        return cursor.fetchall()",
        "documentation": "fetch all result from the database repeated call using the same input to any of the fetch method of this block will skip executing the operation again and instead return the next set of result from the previous execution until the resetcursors method is called args operation the sql query or other operation to be executed parameter the parameter for the operation cursortype the class of the cursor to use when creating a snowflake cursor executekwargs additional option to pas to cursorexecuteasync return a list of tuples containing the data returned by the database where each row is a tuple and each column is a value in the tuple"
    },
    {
        "method_code": "def get_filters_for_model_field(self, field):                # ForeignKey & OneToOneField        if issubclass(field.__class__, ForeignKey) or type(field) is OneToOneRel:            # Relationships to ContentType (used as part of a GFK) do not need a filter            if field.related_model is ContentType:                return [(None, None)]            # ForeignKeys to ObjectType need two filters: 'app.model' & PK            if field.related_model is ObjectType:                return [                    (field.name, ContentTypeFilter),                    (f'{field.name}_id', django_filters.ModelMultipleChoiceFilter),                ]            # ForeignKey to an MPTT-enabled model            if issubclass(field.related_model, MPTTModel) and field.model is not field.related_model:                return [(f'{field.name}_id', TreeNodeMultipleChoiceFilter)]            return [(f'{field.name}_id', django_filters.ModelMultipleChoiceFilter)]        # Many-to-many relationships (forward & backward)        elif type(field) in (ManyToManyField, ManyToManyRel):            filter_name = self.get_m2m_filter_name(field)            # ManyToManyFields to ObjectType need two filters: 'app.model' & PK            if field.related_model is ObjectType:                return [                    (filter_name, ContentTypeFilter),                    (f'{filter_name}_id', django_filters.ModelMultipleChoiceFilter),                ]            return [(f'{filter_name}_id', django_filters.ModelMultipleChoiceFilter)]        # Tag manager        if type(field) is TaggableManager:            return [('tag', TagFilter)]        # Unable to determine the correct filter class        return [(field.name, None)]",
        "documentation": "given a model field return an iterable of name class for each filter that should be defined on the model filterset class if the appropriate filter class cannot be determined it will be none"
    },
    {
        "method_code": "def get_default_settings() -> Settings:        global _DEFAULTS_CACHE    if not _DEFAULTS_CACHE:        old = os.environ        try:            os.environ = {}            settings = get_settings_from_env()        finally:            os.environ = old        _DEFAULTS_CACHE = settings    return _DEFAULTS_CACHE",
        "documentation": "return a setting object populated with default value ignoring any override from environment variable or profile this is cached since the default should not change during the lifetime of the module"
    },
    {
        "method_code": "def _get_gly_cb_vector(self, residue):                try:            n_v = residue[\"N\"].get_vector()            c_v = residue[\"C\"].get_vector()            ca_v = residue[\"CA\"].get_vector()        except Exception:            return None        # center at origin        n_v = n_v - ca_v        c_v = c_v - ca_v        # rotation around c-ca over -120 deg        rot = rotaxis(-pi * 120.0 / 180.0, c_v)        cb_at_origin_v = n_v.left_multiply(rot)        # move back to ca position        cb_v = cb_at_origin_v + ca_v        # This is for PyMol visualization        self.ca_cb_list.append((ca_v, cb_v))        return cb_at_origin_v",
        "documentation": "return a pseudo cb vector for a gly residue private the pseudocb vector is centered at the origin cb coordn coord rotated over degree along the cac axis"
    },
    {
        "method_code": "def test_ddbj(self):                fields = set(TogoWS._get_entry_fields(\"ddbj\"))        self.assertTrue(            fields.issuperset(                [                    \"entry_id\",                    \"length\",                    \"strand\",                    \"moltype\",                    \"linearity\",                    \"division\",                    \"date\",                    \"definition\",                    \"accession\",                    \"accessions\",                    \"version\",                    \"versions\",                    \"acc_version\",                    \"gi\",                    \"keywords\",                    \"organism\",                    \"common_name\",                    \"taxonomy\",                    \"comment\",                    \"seq\",                ]            ),            fields,        )",
        "documentation": "check supported field for ddbj database"
    },
    {
        "method_code": "def test_table16(self):                dna_table = unambiguous_dna_by_id[16]        nuc_table = generic_by_id[16]        amb_rna_table = ambiguous_rna_by_id[16]        self.assertEqual(generic_by_name[\"Chlorophycean Mitochondrial\"].id, 16)        self.assertIn(\"Chlorophycean Mitochondrial\", nuc_table.names)        self.assertEqual(len(dna_table.start_codons), 1)        self.assertEqual(len(dna_table.stop_codons), 2)        self.assertIn(\"URA\", amb_rna_table.stop_codons)        self.assertNotIn(\"TAG\", dna_table.stop_codons)        self.assertEqual(nuc_table.forward_table[\"UAG\"], \"L\")",
        "documentation": "check table chlorophycean mitochondrial stop codon taa and tga tra tag code for l instead of stop"
    },
    {
        "method_code": "def _pairwise(self, seq1, seq2):                score = 0        max_score = 0        if self.scoring_matrix is None:            # Score by character identity, not skipping any special letters            score = sum(                l1 == l2                for l1, l2 in zip(seq1, seq2)                if l1 not in self.skip_letters and l2 not in self.skip_letters            )            max_score = len(seq1)        else:            max_score1 = 0            max_score2 = 0            for i in range(0, len(seq1)):                l1 = seq1[i]                l2 = seq2[i]                if l1 in self.skip_letters or l2 in self.skip_letters:                    continue                try:                    max_score1 += self.scoring_matrix[l1, l1]                except IndexError:                    raise ValueError(                        f\"Bad letter '{l1}' in sequence '{seq1.id}' at position '{i}'\"                    ) from None                try:                    max_score2 += self.scoring_matrix[l2, l2]                except IndexError:                    raise ValueError(                        f\"Bad letter '{l2}' in sequence '{seq2.id}' at position '{i}'\"                    ) from None                score += self.scoring_matrix[l1, l2]            # Take the higher score if the matrix is asymmetrical            max_score = max(max_score1, max_score2)        if max_score == 0:            return 1  # max possible scaled distance        return 1 - (score / max_score)",
        "documentation": "calculate pairwise distance from two sequence private return a value between identical sequence and completely different or seq is an empty string"
    },
    {
        "method_code": "def set_timeout(self, timeout: Optional[float] = None) -> None:                if self.future.done() or self.future.running():            raise RuntimeError(\"Timeouts cannot be added when the call has started.\")        self.timeout = timeout",
        "documentation": "set the timeout for the call the timeout begin when the call start"
    },
    {
        "method_code": "def species(self):                return self.__graph.nodes()",
        "documentation": "return list of the specie in this network"
    },
    {
        "method_code": "def test_alpha_regex_mix(self):                self._setup_config(            bucket_alpha=[\"A - D\", \"E - L\"],            bucket_alpha_regex={\"A - D\": \"^[0-9a-dA-D\u2026\u00e4\u00c4]\"},        )        assert self.plugin._tmpl_bucket(\"alpha\") == \"A - D\"        assert self.plugin._tmpl_bucket(\"\u00c4rzte\") == \"A - D\"        assert self.plugin._tmpl_bucket(\"112\") == \"A - D\"        assert self.plugin._tmpl_bucket(\"\u2026and Oceans\") == \"A - D\"        assert self.plugin._tmpl_bucket(\"Eagles\") == \"E - L\"",
        "documentation": "check mixing regex and nonregex is possible"
    },
    {
        "method_code": "def read(self, source):                # Expat's parser.ParseFile function only accepts binary data;        # see also the comment below for Entrez.parse.        try:            stream = open(source, \"rb\")        except TypeError:  # not a path, assume we received a stream            if source.read(0) != b\"\":                raise StreamModeError(                    \"the XML file must be opened in binary mode.\"                ) from None            stream = source        if stream.read(0) != b\"\":            raise TypeError(\"file should be opened in binary mode\")        try:            self.parser.ParseFile(stream)        except expat.ExpatError as e:            if self.parser.StartElementHandler:                # We saw the initial <!xml declaration, so we can be sure that                # we are parsing XML data. Most likely, the XML file is                # corrupted.                raise CorruptedXMLError(e) from None            else:                # We have not seen the initial <!xml declaration, so probably                # the input data is not in XML format.                raise NotXMLError(e) from None        finally:            if stream is not source:                stream.close()        try:            record = self.record        except AttributeError:            if self.parser.StartElementHandler:                # We saw the initial <!xml declaration, and expat didn't notice                # any errors, so self.record should be defined. If not, this is                # a bug.                raise RuntimeError(                    \"Failed to parse the XML file correctly, possibly due to a bug \"                    \"in Bio.Entrez. Please contact the Biopython developers via \"                    \"the mailing list or GitHub for assistance.\"                ) from None            else:                # We did not see the initial <!xml declaration, so probably                # the input data is not in XML format.                raise NotXMLError(\"XML declaration not found\") from None        else:            del record.key            return record",
        "documentation": "set up the parser and let it read the xml result"
    },
    {
        "method_code": "def to_networkx(tree):        try:        import networkx    except ImportError:        raise MissingPythonDependencyError(            \"Install NetworkX if you want to use to_networkx.\"        ) from None    # NB (1/2010): the networkx API stabilized at v.1.0    # 1.0+: edges accept arbitrary data as kwargs, weights are floats    # 0.99: edges accept weight as a string, nothing else    # pre-0.99: edges accept no additional data    # Ubuntu Lucid LTS uses v0.99, let's support everything    if networkx.__version__ >= \"1.0\":        def add_edge(graph, n1, n2):            graph.add_edge(n1, n2, weight=n2.branch_length or 1.0)            # Copy branch color value as hex, if available            if hasattr(n2, \"color\") and n2.color is not None:                graph[n1][n2][\"color\"] = n2.color.to_hex()            elif hasattr(n1, \"color\") and n1.color is not None:                # Cascading color attributes                graph[n1][n2][\"color\"] = n1.color.to_hex()                n2.color = n1.color            # Copy branch weight value (float) if available            if hasattr(n2, \"width\") and n2.width is not None:                graph[n1][n2][\"width\"] = n2.width            elif hasattr(n1, \"width\") and n1.width is not None:                # Cascading width attributes                graph[n1][n2][\"width\"] = n1.width                n2.width = n1.width    elif networkx.__version__ >= \"0.99\":        def add_edge(graph, n1, n2):            graph.add_edge(n1, n2, (n2.branch_length or 1.0))    else:        def add_edge(graph, n1, n2):            graph.add_edge(n1, n2)    def build_subgraph(graph, top):                for clade in top:            graph.add_node(clade.root)            add_edge(graph, top.root, clade.root)            build_subgraph(graph, clade)    if tree.rooted:        G = networkx.DiGraph()    else:        G = networkx.Graph()    G.add_node(tree.root)    build_subgraph(G, tree.root)    return G",
        "documentation": "convert a tree object to a networkx graph the result is useful for graphoriented analysis and also interactive plotting with pylab matplotlib or pygraphviz though the resulting diagram is usually not ideal for displaying a phylogeny requires networkx version or later"
    },
    {
        "method_code": "def get_created_by(    prefect_automation_id: Optional[UUID] = Header(None, include_in_schema=False),    prefect_automation_name: Optional[str] = Header(None, include_in_schema=False),) -> Optional[schemas.core.CreatedBy]:        if prefect_automation_id and prefect_automation_name:        try:            display_value = b64decode(prefect_automation_name.encode()).decode()        except Exception:            display_value = None        if display_value:            return schemas.core.CreatedBy(                id=prefect_automation_id,                type=\"AUTOMATION\",                display_value=display_value,            )    return None",
        "documentation": "a dependency that return the provenance information to use when creating object during this api call"
    },
    {
        "method_code": "def _read_metadata(cls, fpath):                data = {}        if fpath and isfile(fpath):            parser = HeaderParser()            # FIXME: Is this a correct assumption for the encoding?            # This was needed due to some errors on windows            with open_utf8(fpath) as fp:                data = parser.parse(fp)        return cls._message_to_dict(data)",
        "documentation": "read the original format which is stored a rfc header"
    },
    {
        "method_code": "def __init__(self, cmd=\"fasttree\", **kwargs):                self.parameters = [            _Switch(                [\"-nt\", \"nt\"],                \"By default FastTree expects protein alignments, use -nt for nucleotides\",            ),            _Option(                [\"-n\", \"n\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-quote\", \"quote\"],                ,            ),            _Option(                [\"-pseudo\", \"pseudo\"],                ,                checker_function=_is_numeric,                equate=False,            ),            _Option(                [\"-boot\", \"boot\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-nosupport\", \"nosupport\"],                ,            ),            _Option(                [\"-intree\", \"intree\"],                ,                filename=True,                equate=False,            ),            _Option(                [\"-intree1\", \"intree1\"],                \"intree1 newickfile -- read the same starting tree for each alignment.\",                filename=True,                equate=False,            ),            _Switch(                [\"-quiet\", \"quiet\"],                ,            ),            _Switch(                [\"-nopr\", \"nopr\"],                \"-nopr -- do not write the progress indicator to stderr.\",            ),            _Option(                [\"-nni\", \"nni\"],                ,                checker_function=_is_int,                equate=False,            ),            _Option(                [\"-spr\", \"spr\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-noml\", \"noml\"],                ,            ),            _Switch(                [\"-mllen\", \"mllen\"],                ,            ),            _Switch(                [\"-nome\", \"nome\"],                ,            ),            _Option(                [\"-mlnni\", \"mlnni\"],                ,                checker_function=_is_int,                equate=False,            ),            _Option(                [\"-mlacc\", \"mlacc\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-slownni\", \"slownni\"],                ,            ),            _Switch(                [\"-wag\", \"wag\"],                ,            ),            _Switch(                [\"-gtr\", \"gtr\"],                ,            ),            _Option(                [\"-cat\", \"cat\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-nocat\", \"nocat\"],                \"Maximum likelihood model options: No CAT model (just 1 category)\",            ),            _Switch(                [\"-gamma\", \"gamma\"],                ,            ),            _Switch(                [\"-slow\", \"slow\"],                ,            ),            _Switch(                [\"-fastest\", \"fastest\"],                ,            ),            _Switch(                [\"-2nd\", \"second\"],                ,            ),            _Switch(                [\"-no2nd\", \"no2nd\"],                ,            ),            _Option(                [\"-seed\", \"seed\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch(                [\"-top\", \"top\"],                ,            ),            _Switch(                [\"-notop\", \"notop\"],                ,            ),            _Option(                [\"-topm\", \"topm\"],                ,                checker_function=_is_numeric,                equate=False,            ),            _Option(                [\"-close\", \"close\"],                ,                checker_function=_is_numeric,                equate=False,            ),            _Option(                [\"-refresh\", \"refresh\"],                ,                checker_function=_is_numeric,                equate=False,            ),            _Option(                [\"-matrix\", \"matrix\"],                ,                filename=True,                equate=False,            ),            _Switch(                [\"-nomatrix\", \"nomatrix\"],                ,            ),            _Switch(                [\"-nj\", \"nj\"],                \"Join options: regular (unweighted) neighbor-joining (default)\",            ),            _Switch(                [\"-bionj\", \"bionj\"],                ,            ),            _Option(                [\"-gtrrates\", \"gtrrates\"], \"-gtrrates ac ag at cg ct gt\", equate=False            ),            _Option([\"-gtrfreq\", \"gtrfreq\"], \"-gtrfreq A C G T\", equate=False),            _Option(                [\"-constraints\", \"constraints\"],                ,                filename=True,                equate=False,            ),            _Option(                [\"-constraintWeight\", \"constraintWeight\"],                ,                checker_function=_is_numeric,                equate=False,            ),            _Option(                [\"-log\", \"log\"],                ,                filename=True,                equate=False,            ),            _Option(                [\"-makematrix\", \"makematrix\"],                \"-makematrix [alignment]\",                filename=True,                equate=False,            ),            _Switch(                [\"-rawdist\", \"rawdist\"],                ,            ),            _Option(                [\"-sprlength\", \"sprlength\"],                ,                checker_function=_is_int,                equate=False,            ),            _Switch([\"-help\", \"help\"], \"Show the help.\"),            _Switch([\"-expert\", \"expert\"], \"Show the expert level help.\"),            _Option(                [\"-out\", \"out\"],                ,                filename=True,                equate=False,            ),            _Argument(                [\"input\"],                ,                filename=True,                is_required=True,            ),        ]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def get(self, offset):                # Most file formats with self contained records can be handled by        # parsing StringIO(self.get_raw(offset).decode())        raise NotImplementedError",
        "documentation": "return parsed object for this entry"
    },
    {
        "method_code": "def get_dn_ds_matrix(self, method=\"NG86\", codon_table=None):                from Bio.Phylo.TreeConstruction import DistanceMatrix as DM        if codon_table is None:            codon_table = CodonTable.generic_by_id[1]        names = [i.id for i in self._records]        size = len(self._records)        dn_matrix = []        ds_matrix = []        for i in range(size):            dn_matrix.append([])            ds_matrix.append([])            for j in range(i + 1):                if i != j:                    dn, ds = cal_dn_ds(                        self._records[i],                        self._records[j],                        method=method,                        codon_table=codon_table,                    )                    dn_matrix[i].append(dn)                    ds_matrix[i].append(ds)                else:                    dn_matrix[i].append(0.0)                    ds_matrix[i].append(0.0)        dn_dm = DM(names, matrix=dn_matrix)        ds_dm = DM(names, matrix=ds_matrix)        return dn_dm, ds_dm",
        "documentation": "available method include ng lwl yn and ml argument method available method include ng lwl yn and ml codontable codon table to use for forward translation"
    },
    {
        "method_code": "def test_reading_writing_alignments_pfam8(self):                path = \"Stockholm/pfam8.seed.txt\"        alignments = Align.parse(path, \"stockholm\")        alignment = next(alignments)        self.assertRaises(StopIteration, next, alignments)        self.check_alignment_pfam8(alignment)        stream = StringIO()        n = Align.write(alignment, stream, \"stockholm\")        self.assertEqual(n, 1)        stream.seek(0)        alignments = Align.parse(stream, \"stockholm\")        alignment = next(alignments)        stream.close()        self.check_alignment_pfam8(alignment)",
        "documentation": "test parsing pfam record cyclinn"
    },
    {
        "method_code": "def search(self, queryset, name, value):                return queryset",
        "documentation": "override this method to apply a generalpurpose search logic"
    },
    {
        "method_code": "def get_multilocus_f_stats(self):                return self._controller.calc_fst_all(self._fname)[0]",
        "documentation": "return the multilocus f stats explain averaging return fiscw fst fit"
    },
    {
        "method_code": "def process_album(self, album):                self._log.debug(\"generating thumbnail for {0}\", album)        if not album.artpath:            self._log.info(\"album {0} has no art\", album)            return        if self.config[\"dolphin\"]:            self.make_dolphin_cover_thumbnail(album)        size = ArtResizer.shared.get_size(album.artpath)        if not size:            self._log.warning(                \"problem getting the picture size for {0}\", album.artpath            )            return        wrote = True        if max(size) >= 256:            wrote &= self.make_cover_thumbnail(album, 256, LARGE_DIR)        wrote &= self.make_cover_thumbnail(album, 128, NORMAL_DIR)        if wrote:            self._log.info(\"wrote thumbnail for {0}\", album)        else:            self._log.info(\"nothing to do for {0}\", album)",
        "documentation": "produce thumbnail for the album folder"
    },
    {
        "method_code": "def __call__(self, *args, **keywds):                        keywds = self.decode(*args, **keywds)            return _align(**keywds)",
        "documentation": "call the alignment instance already created"
    },
    {
        "method_code": "def _fastq_sanger_convert_fastq_illumina(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # Map unexpected chars to null    trunc_char = chr(1)    mapping = \"\".join(        [chr(0) for ascii in range(33)]        + [chr(64 + q) for q in range(62 + 1)]        + [trunc_char for ascii in range(96, 127)]        + [chr(0) for ascii in range(127, 256)]    )    assert len(mapping) == 256    return _fastq_generic2(        in_file,        out_file,        mapping,        trunc_char,        \"Data loss - max PHRED quality 62 in Illumina 1.3+ FASTQ\",    )",
        "documentation": "fast sanger fastq to illumina fastq conversion private avoids creating seqrecord and seq object in order to speed up this conversion will issue a warning if the score had to be truncated at maximum possible in the illumina fastq format"
    },
    {
        "method_code": "def __call__(        self,        *args: P.args,        return_state: bool = False,        wait_for: Optional[Iterable[PrefectFuture]] = None,        **kwargs: P.kwargs,    ):                from prefect.utilities.visualization import (            get_task_viz_tracker,            track_viz_task,        )        # Convert the call args/kwargs to a parameter dict        parameters = get_call_parameters(self.fn, args, kwargs)        return_type = \"state\" if return_state else \"result\"        task_run_tracker = get_task_viz_tracker()        if task_run_tracker:            return track_viz_task(                self.isasync, self.name, parameters, self.viz_return_value            )        from prefect.task_engine import run_task        return run_task(            task=self,            parameters=parameters,            wait_for=wait_for,            return_type=return_type,        )",
        "documentation": "run the task and return the result if returnstate is true return the result is wrapped in a prefect state which provides error handling"
    },
    {
        "method_code": "def get_raw(self):                return [(t, self._signals[t]) for t in sorted(self._signals.keys())]",
        "documentation": "get a list of timesignal pair"
    },
    {
        "method_code": "def test_analysis_restrictions(self):                new_seq = Seq(\"TTCAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAA\")        rb = RestrictionBatch([EcoRI, KpnI, EcoRV])        ana = Analysis(rb, new_seq, linear=False)        # Output only the result for enzymes which cut blunt:        self.assertEqual(ana.blunt(), {EcoRV: []})        self.assertEqual(ana.full(), {KpnI: [], EcoRV: [], EcoRI: [33]})        # Output only the result for enzymes which have a site:        self.assertEqual(ana.with_sites(), {EcoRI: [33]})        # Output only the enzymes which have no site:        self.assertEqual(ana.without_site(), {KpnI: [], EcoRV: []})        self.assertEqual(ana.with_site_size([32]), {})        # Output only enzymes which produce 5' overhangs        self.assertEqual(ana.overhang5(), {EcoRI: [33]})        # Output only enzymes which produce 3' overhangs        self.assertEqual(ana.overhang3(), {KpnI: []})        # Output only enzymes which produce defined ends        self.assertEqual(ana.defined(), {KpnI: [], EcoRV: [], EcoRI: [33]})        # Output only enzymes hich cut N times        self.assertEqual(ana.with_N_sites(2), {})        # The enzymes which cut between position x and y:        with self.assertRaises(TypeError):            ana.only_between(\"t\", 20)        with self.assertRaises(TypeError):            ana.only_between(1, \"t\")        self.assertEqual(ana.only_between(1, 20), {})        self.assertEqual(ana.only_between(20, 34), {EcoRI: [33]})        # Mix start/end order:        self.assertEqual(ana.only_between(34, 20), {EcoRI: [33]})        self.assertEqual(ana.only_outside(20, 34), {})        with self.assertWarns(BiopythonWarning):            ana.with_name([\"fake\"])        self.assertEqual(ana.with_name([EcoRI]), {EcoRI: [33]})        self.assertEqual((ana._boundaries(1, 20)[:2]), (1, 20))        # Reverse order:        self.assertEqual((ana._boundaries(20, 1)[:2]), (1, 20))        # Fix negative start:        self.assertEqual((ana._boundaries(-1, 20)[:2]), (20, 33))        # Fix negative end:        self.assertEqual((ana._boundaries(1, -1)[:2]), (1, 33))        # Sites in- and outside of boundaries        new_seq = Seq(\"GAATTCAAAAAAGAATTC\")        rb = RestrictionBatch([EcoRI])        ana = Analysis(rb, new_seq)        # Cut at least inside        self.assertEqual(ana.between(1, 7), {EcoRI: [2, 14]})        # Cut at least inside and report only inside site        self.assertEqual(ana.show_only_between(1, 7), {EcoRI: [2]})        # Cut at least outside        self.assertEqual(ana.outside(1, 7), {EcoRI: [2, 14]})        # Don't cut within        self.assertEqual(ana.do_not_cut(7, 12), {EcoRI: [2, 14]})",
        "documentation": "test fancier restriction analysis"
    },
    {
        "method_code": "def __init__(self, cmd=\"needle\", **kwargs):                self.parameters = [            _Option(                [\"-asequence\", \"asequence\"],                \"First sequence to align\",                filename=True,                is_required=True,            ),            _Option(                [\"-bsequence\", \"bsequence\"],                \"Second sequence to align\",                filename=True,                is_required=True,            ),            _Option([\"-gapopen\", \"gapopen\"], \"Gap open penalty\", is_required=True),            _Option(                [\"-gapextend\", \"gapextend\"], \"Gap extension penalty\", is_required=True            ),            _Option([\"-datafile\", \"datafile\"], \"Matrix file\", filename=True),            _Option([\"-endweight\", \"endweight\"], \"Apply And gap penalties\"),            _Option(                [\"-endopen\", \"endopen\"],                \"The score taken away when an end gap is created.\",            ),            _Option(                [\"-endextend\", \"endextend\"],                \"The score added to the end gap penalty for each base or \"                \"residue in the end gap.\",            ),            _Switch(                [\"-nobrief\", \"nobrief\"], \"Display extended identity and similarity\"            ),            _Switch([\"-brief\", \"brief\"], \"Display brief identity and similarity\"),            _Option(                [\"-similarity\", \"similarity\"], \"Display percent identity and similarity\"            ),            _Option(                [\"-snucleotide\", \"snucleotide\"], \"Sequences are nucleotide (boolean)\"            ),            _Option([\"-sprotein\", \"sprotein\"], \"Sequences are protein (boolean)\"),            _Option(                [\"-aformat\", \"aformat\"],                \"Display output in a different specified output format\",            ),        ]        _EmbossCommandLine.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def test_short_bigarrow(self):                self.short_sigils(\"BIGARROW\")",
        "documentation": "feature bigarrow sigil head within bounding box"
    },
    {
        "method_code": "def _open_for_random_access(filename):        handle = open(filename, \"rb\")    magic = handle.read(2)    handle.seek(0)    if magic == b\"\\x1f\\x8b\":        # This is a gzipped file, but is it BGZF?        from . import bgzf        try:            # If it is BGZF, we support that            return bgzf.BgzfReader(mode=\"rb\", fileobj=handle)        except ValueError as e:            assert \"BGZF\" in str(e)            # Not a BGZF file after all,            handle.close()            raise ValueError(                \"Gzipped files are not suitable for indexing, \"                \"please use BGZF (blocked gzip format) instead.\"            ) from None    return handle",
        "documentation": "open a file in binary mode spot if it is bgzf format etc private this functionality is used by the bioseqio and biosearchio index and indexdb function if the file is gzipped but not bgzf a specific valueerror is raised"
    },
    {
        "method_code": "def _add_image(self):                try:            self.arm_template[\"resources\"][0][\"properties\"][\"containers\"][0][                \"properties\"            ][\"image\"] = self.image        except KeyError:            raise ValueError(\"Unable to add image due to invalid job ARM template.\")",
        "documentation": "add the image to the arm template"
    },
    {
        "method_code": "def test_print_unexpected_error_message_upload_3(    mocker: MockerFixture,    monkeypatch: MonkeyPatch,    capsys: CaptureFixture,):        post_mock = mocker.patch(        \"requests.post\",        side_effect=(            AttrDict(                headers=AttrDict(Location=\"somewhere.else\"),                status_code=302,                raise_for_status=lambda: None,            ),            AttrDict(raise_for_status=lambda: None),        ),    )    input_mock = mocker.patch(\"builtins.input\", return_value=\"y\")    isatty_mock = mocker.patch(\"os.isatty\", return_value=True)    monkeypatch.setenv(\"CONDA_REPORT_ERRORS\", \"none\")    monkeypatch.setenv(\"CONDA_ALWAYS_YES\", \"false\")    monkeypatch.setenv(\"CONDA_JSON\", \"false\")    reset_context()    assert context.report_errors is None    assert not context.json    assert not context.always_yes    ExceptionHandler()(_raise_helper, AssertionError())    stdout, stderr = capsys.readouterr()    assert username_not_in_post_mock(post_mock, username=getpass.getuser())    assert isatty_mock.call_count == 1    assert input_mock.call_count == 1    assert post_mock.call_count == 2    assert not stdout    assert \"conda version\" in stderr",
        "documentation": "test that we prompt for user confirmation before submitting error report when condareporterrorsnone condaalwaysyesfalse and condajsonfalse"
    },
    {
        "method_code": "def test_seq_features(self):                test_features = self.item.features        cds_feature = test_features[6]        self.assertEqual(cds_feature.type, \"CDS\")        self.assertEqual(            str(cds_feature.location), \"join{[103:160](+), [319:390](+), [503:579](+)}\"        )        self.assertIn(\"gene\", cds_feature.qualifiers)        self.assertIn(\"protein_id\", cds_feature.qualifiers)        self.assertIn(\"codon_start\", cds_feature.qualifiers)        self.assertEqual(cds_feature.qualifiers.get(\"gene\"), [\"kin2\"])        self.assertEqual(cds_feature.qualifiers.get(\"protein_id\"), [\"CAA44171.1\"])        self.assertEqual(cds_feature.qualifiers.get(\"codon_start\"), [\"1\"])        self.assertIn(\"db_xref\", cds_feature.qualifiers)        multi_ann = cds_feature.qualifiers[\"db_xref\"]        self.assertEqual(len(multi_ann), 2)        self.assertIn(\"GI:16354\", multi_ann)        self.assertIn(\"SWISS-PROT:P31169\", multi_ann)",
        "documentation": "check seqfeatures of a sequence"
    },
    {
        "method_code": "def __init__(self, parent):                self._id = None        self._parent = parent",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def read(handle):        motif_number = 0    record = Record()    _read_version(record, handle)    _read_alphabet(record, handle)    _read_background(record, handle)    while True:        for line in handle:            if line.startswith(\"MOTIF\"):                break        else:            return record        name = line.split()[1]        motif_number += 1        length, num_occurrences, evalue = _read_motif_statistics(handle)        counts = _read_lpm(record, handle, length, num_occurrences)        # {'A': 0.25, 'C': 0.25, 'T': 0.25, 'G': 0.25}        motif = motifs.Motif(alphabet=record.alphabet, counts=counts)        motif.background = record.background        motif.length = motif.counts.length        motif.num_occurrences = num_occurrences        motif.evalue = evalue        motif.name = name        record.append(motif)        assert len(record) == motif_number    return record",
        "documentation": "parse the text output of the meme program into a memerecord object example from biomotifs import minimal with openmotifsmemeout a f record minimalreadf for motif in record printmotifname motifevalue e you can access individual motif in the record by their index or find a motif by it name from bio import motif with openmotifsminimaltestmeme a f record motifsparsef minimal motif record printmotifname krp motif recordifxa printmotifname ifxa this function wont retrieve instance a there are none in minimal meme format"
    },
    {
        "method_code": "def test_bad_args_non_int_limit(self):                self.assertRaises(            ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=1, limit=\"lots\"        )",
        "documentation": "reject biotogowssearch with noninteger limit"
    },
    {
        "method_code": "def get_theta_list(self):                theta_list = []        ca_list = self.get_ca_list()        for i in range(len(ca_list) - 2):            atom_list = (ca_list[i], ca_list[i + 1], ca_list[i + 2])            v1, v2, v3 = (a.get_vector() for a in atom_list)            theta = calc_angle(v1, v2, v3)            theta_list.append(theta)            # Put tau in xtra dict of residue            res = ca_list[i + 1].get_parent()            res.xtra[\"THETA\"] = theta        return theta_list",
        "documentation": "list of theta angle for all consecutive calpha atom"
    },
    {
        "method_code": "def _transfer_attrs(self, obj):                # list of attribute names we don't want to transfer        for attr in self.__dict__:            if attr not in self._NON_STICKY_ATTRS:                setattr(obj, attr, self.__dict__[attr])",
        "documentation": "transfer instance attribute to the given object private this method is used to transfer attribute set externally for example using setattr to a new object created from this one for example from slicing the reason this method is necessary is because different parser will set different attribute for each queryresult hit hsp or hspfragment object depending on the attribute they found in the search output file ideally we want these attribute to stick with any new instance object created from the original one"
    },
    {
        "method_code": "def mutator_stage(func):        def coro(*args):        task = None        while True:            task = yield task            func(*(args + (task,)))    return coro",
        "documentation": "decorate a function that manipulates item in a coroutine to become a simple stage mutatorstage def setkeykey item itemkey true pipe pipeline iterx false a false setkeyx listpipepull x true a false x true"
    },
    {
        "method_code": "def call(args: List[Any], log: Logger, **kwargs: Any):        try:        return command_output(args, **kwargs)    except subprocess.CalledProcessError as e:        log.debug(e.output.decode(\"utf8\", \"ignore\"))        raise ReplayGainError(            \"{} exited with status {}\".format(args[0], e.returncode)        )    except UnicodeEncodeError:        # Due to a bug in Python 2's subprocess on Windows, Unicode        # filenames can fail to encode on that platform. See:        # https://github.com/google-code-export/beets/issues/499        raise ReplayGainError(\"argument encoding failed\")",
        "documentation": "execute the command and return it output or raise a replaygainerror on failure"
    },
    {
        "method_code": "def test_apaf(self):                global EX_APAF        orig_fname = EX_APAF        try:            EX_APAF = DUMMY            self._rewrite_and_call(                orig_fname,                (                    (                        ParseTests,                        [\"test_read_apaf\", \"test_parse_apaf\", \"test_shape_apaf\"],                    ),                    (TreeTests, [\"test_DomainArchitecture\"]),                ),            )        finally:            EX_APAF = orig_fname",
        "documentation": "roundtrip parsing and serialization of apafxml"
    },
    {
        "method_code": "def reporters(self) -> tuple[Mapping[str, str]]:                if not self._reporters:            return (                {                    \"backend\": \"json\" if self.json else \"console\",                    \"output\": \"stdout\",                    \"verbosity\": self.verbosity,                    \"quiet\": self.quiet,                },            )        return self._reporters",
        "documentation": "determine the value of reporter based on other setting and the selfreporters value itself"
    },
    {
        "method_code": "def test_writer_passes_on_plain_file_handle(self):                with open(self.temp_file, \"wb\") as handle:            bgzf.BgzfWriter(fileobj=handle)",
        "documentation": "a bgzfwriter must be able to work with plain file handle"
    },
    {
        "method_code": "def init_atom_coords(self) -> None:                # dbg = True        if not np.all(self.dAtoms_needs_update):            self.dAtoms_needs_update |= (self.hAtoms_needs_update[self.dH1ndx]) | (                self.hAtoms_needs_update[self.dH2ndx]            )            self.dcsValid &= np.logical_not(self.dAtoms_needs_update)        # dihedra full size masks:        mdFwd = self.dFwd & self.dAtoms_needs_update        mdRev = self.dRev & self.dAtoms_needs_update        # update size masks        udFwd = self.dFwd[self.dAtoms_needs_update]        udRev = self.dRev[self.dAtoms_needs_update]                if np.any(self.hAtoms_needs_update):            # hedra initial coords            # sar = supplementary angle radian: angles which add to 180            sar = np.deg2rad(180.0 - self.hedraAngle[self.hAtoms_needs_update])  # angle            sinSar = np.sin(sar)            cosSarN = np.cos(sar) * -1                        # a2 is len3 up from a2 on Z axis, X=Y=0            self.hAtoms[:, 2, 2][self.hAtoms_needs_update] = self.hedraL23[                self.hAtoms_needs_update            ]            # a0 X is sin( sar ) * len12            self.hAtoms[:, 0, 0][self.hAtoms_needs_update] = (                sinSar * self.hedraL12[self.hAtoms_needs_update]            )            # a0 Z is -(cos( sar ) * len12)            # (assume angle always obtuse, so a0 is in -Z)            self.hAtoms[:, 0, 2][self.hAtoms_needs_update] = (                cosSarN * self.hedraL12[self.hAtoms_needs_update]            )                        # same again but 'reversed' : a0 on Z axis, a1 at origin, a2 in -Z            # a0r is len12 up from a1 on Z axis, X=Y=0            self.hAtomsR[:, 0, 2][self.hAtoms_needs_update] = self.hedraL12[                self.hAtoms_needs_update            ]            # a2r X is sin( sar ) * len23            self.hAtomsR[:, 2, 0][self.hAtoms_needs_update] = (                sinSar * self.hedraL23[self.hAtoms_needs_update]            )            # a2r Z is -(cos( sar ) * len23)            self.hAtomsR[:, 2, 2][self.hAtoms_needs_update] = (                cosSarN * self.hedraL23[self.hAtoms_needs_update]            )                        self.hAtoms_needs_update[...] = False            # dihedra parts other than dihedral angle            dhlen = np.sum(self.dAtoms_needs_update)  # self.dihedraLen            # only 4th atom takes work:            # pick 4th atom based on rev flag            self.a4_pre_rotation[mdRev] = self.hAtoms[self.dH2ndx, 0][mdRev]            self.a4_pre_rotation[mdFwd] = self.hAtomsR[self.dH2ndx, 2][mdFwd]            # numpy multiply, add operations below intermediate array but out=            # not working with masking:            self.a4_pre_rotation[:, 2][self.dAtoms_needs_update] = np.multiply(                self.a4_pre_rotation[:, 2][self.dAtoms_needs_update], -1            )  # a4 to +Z            a4shift = np.empty(dhlen)            a4shift[udRev] = self.hedraL23[self.dH2ndx][mdRev]  # len23            a4shift[udFwd] = self.hedraL12[self.dH2ndx][mdFwd]  # len12            self.a4_pre_rotation[:, 2][self.dAtoms_needs_update] = np.add(                self.a4_pre_rotation[:, 2][self.dAtoms_needs_update],                a4shift,            )  # so a2 at origin                        # now build dihedra initial coords            dH1atoms = self.hAtoms[self.dH1ndx]  # fancy indexing so            dH1atomsR = self.hAtomsR[self.dH1ndx]  # these copy not view            self.dAtoms[:, :3][mdFwd] = dH1atoms[mdFwd]            self.dAtoms[:, :3][mdRev] = dH1atomsR[:, 2::-1][mdRev]                    # build rz rotation matrix for dihedral angle                rz = multi_rot_Z(self.dihedraAngleRads[self.dAtoms_needs_update])        a4rot = np.matmul(            rz,            self.a4_pre_rotation[self.dAtoms_needs_update][:].reshape(-1, 4, 1),        ).reshape(-1, 4)        self.dAtoms[:, 3][mdFwd] = a4rot[udFwd]  # [self.dFwd]        self.dAtoms[:, 3][mdRev] = a4rot[udRev]  # [self.dRev]                self.dAtoms_needs_update[...] = False        # can't start assembly if initial NCaC is not valid, so copy from        # hAtoms if needed                for iNCaC in self.initNCaCs:            invalid = True            if np.all(self.atomArrayValid[[self.atomArrayIndex[ak] for ak in iNCaC]]):                invalid = False            if invalid:                hatoms = self.hAtoms[self.hedraNdx[iNCaC]]                for i in range(3):                    andx = self.atomArrayIndex[iNCaC[i]]                    self.atomArray[andx] = hatoms[i]                    self.atomArrayValid[andx] = True",
        "documentation": "set chain level dihedra initial coords from angle and distance initializes atom coordinate in local coordinate space for hedra and dihedra will be transformed appropriately later by datadcoordspace matrix for assembly"
    },
    {
        "method_code": "def serialize_context() -> Dict[str, Any]:        flow_run_context = EngineContext.get()    task_run_context = TaskRunContext.get()    tags_context = TagsContext.get()    settings_context = SettingsContext.get()    return {        \"flow_run_context\": flow_run_context.serialize() if flow_run_context else {},        \"task_run_context\": task_run_context.serialize() if task_run_context else {},        \"tags_context\": tags_context.serialize() if tags_context else {},        \"settings_context\": settings_context.serialize() if settings_context else {},    }",
        "documentation": "serialize the current context for use in a remote execution environment"
    },
    {
        "method_code": "def _get_git_branch() -> Optional[str]:        try:        branch = subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"])        branch = branch.decode().strip()    except subprocess.CalledProcessError:        return None    return branch",
        "documentation": "return the git branch for the current directory"
    },
    {
        "method_code": "def export_phylip(self, filename=None):                if not filename:            if \".\" in self.filename and self.filename.split(\".\")[-1].lower() in [                \"paup\",                \"nexus\",                \"nex\",                \"dat\",            ]:                filename = \".\".join(self.filename.split(\".\")[:-1]) + \".phy\"            else:                filename = self.filename + \".phy\"        with open(filename, \"w\") as fh:            fh.write(\"%d %d\\n\" % (self.ntax, self.nchar))            for taxon in self.taxlabels:                fh.write(f\"{safename(taxon)} {self.matrix[taxon]!s}\\n\")        return filename",
        "documentation": "write matrix into a phylip file note that this writes a relaxed phylip format file where the name are not truncated nor checked for invalid character"
    },
    {
        "method_code": "def test_stop_codons(self):                for id in ids:            rna = unambiguous_rna_by_id[id]            amb_dna = ambiguous_dna_by_id[id]            amb_rna = ambiguous_rna_by_id[id]            amb_nuc = ambiguous_generic_by_id[id]            # R = A or G, so URR = UAA or UGA / TRA = TAA or TGA = stop codons            if (                \"UAA\" in amb_rna.stop_codons                and \"UGA\" in amb_rna.stop_codons                and id not in (28, 32)            ):                self.assertEqual(amb_dna.forward_table.get(\"TRA\", \"X\"), \"X\")                with self.assertRaises(KeyError):                    amb_dna.forward_table[\"TRA\"]                    amb_rna.forward_table[\"URA\"]                    amb_nuc.forward_table[\"URA\"]                self.assertIn(\"URA\", amb_nuc.stop_codons)                self.assertIn(\"URA\", amb_rna.stop_codons)                self.assertIn(\"TRA\", amb_nuc.stop_codons)                self.assertIn(\"TRA\", amb_dna.stop_codons)            if (                \"UAG\" in rna.stop_codons                and \"UAA\" in rna.stop_codons                and \"UGA\" in rna.stop_codons                and id not in (28, 32)            ):                with self.assertRaises(KeyError):                    amb_dna.forward_table[\"TAR\"]                    amb_rna.forward_table[\"UAR\"]                    amb_nuc.forward_table[\"UAR\"]                with self.assertRaises(TranslationError):                    amb_nuc.forward_table[\"URR\"]                self.assertIn(\"UAR\", amb_nuc.stop_codons)                self.assertIn(\"UAR\", amb_rna.stop_codons)                self.assertIn(\"TAR\", amb_nuc.stop_codons)                self.assertIn(\"TAR\", amb_dna.stop_codons)                self.assertIn(\"URA\", amb_nuc.stop_codons)                self.assertIn(\"URA\", amb_rna.stop_codons)                self.assertIn(\"TRA\", amb_nuc.stop_codons)                self.assertIn(\"TRA\", amb_dna.stop_codons)",
        "documentation": "test various ambiguous codon a stop codon stop codon should not appear in forward table this should give a keyerror if an ambiguous codon may code for both stop codon and amino acid this should raise a translationerror"
    },
    {
        "method_code": "def __iter__(self):                return iter(self._offsets)",
        "documentation": "iterate over the key"
    },
    {
        "method_code": "def test_cli_does_not_skip_wrong_tag_type(self):                if not self.has_r128_support:            # This test is a lot less interesting if the backend cannot write            # both tag types.            self.skipTest(                \"r128 tags for opus not supported on backend {}\".format(                    self.backend                )            )        album_rg = self._add_album(1)        item_rg = album_rg.items()[0]        album_r128 = self._add_album(1, ext=\"opus\")        item_r128 = album_r128.items()[0]        item_rg.r128_track_gain = 0.0        item_rg.store()        item_r128.rg_track_gain = 0.0        item_r128.rg_track_peak = 42.0        item_r128.store()        self.run_command(\"replaygain\")        item_rg.load()        item_r128.load()        assert item_rg.rg_track_gain is not None        assert item_rg.rg_track_peak is not None        # FIXME: Should the plugin null this field?        # assert item_rg.r128_track_gain is None        assert item_r128.r128_track_gain is not None",
        "documentation": "check that item that have tag of the wrong type wont be skipped"
    },
    {
        "method_code": "def __getitem__(self, arg):                return self._data[arg]",
        "documentation": "pull out child by index"
    },
    {
        "method_code": "def crc32(seq):        try:        # Assume it's a Seq object        s = bytes(seq)    except TypeError:        # Assume it's a string        s = seq.encode()    return binascii.crc32(s)",
        "documentation": "return the crc checksum for a sequence string or seq object note that the case is important crcacgtacgtacgt crcacgtacgtacgt"
    },
    {
        "method_code": "def prepare_for_flow_run(        self,        flow_run: \"FlowRun\",        deployment: Optional[\"DeploymentResponse\"] = None,        flow: Optional[\"Flow\"] = None,    ):                self._related_objects = {            \"deployment\": deployment,            \"flow\": flow,            \"flow-run\": flow_run,        }        if deployment is not None:            deployment_labels = self._base_deployment_labels(deployment)        else:            deployment_labels = {}        if flow is not None:            flow_labels = self._base_flow_labels(flow)        else:            flow_labels = {}        env = {            **self._base_environment(),            **self._base_flow_run_environment(flow_run),            **self.env,        }        self.env = {key: value for key, value in env.items() if value is not None}        self.labels = {            **self._base_flow_run_labels(flow_run),            **deployment_labels,            **flow_labels,            **self.labels,        }        self.name = self.name or flow_run.name        self.command = self.command or self._base_flow_run_command()",
        "documentation": "prepare the job configuration for a flow run this method is called by the worker before starting a flow run it should be used to set any configuration value that are dependent on the flow run args flowrun the flow run to be executed deployment the deployment that the flow run is associated with flow the flow that the flow run is associated with"
    },
    {
        "method_code": "def test_220_interface_to_interface_duplex_via_multiple_front_and_rear_ports(self):                interface1 = Interface.objects.create(device=self.device, name='Interface 1')        interface2 = Interface.objects.create(device=self.device, name='Interface 2')        interface3 = Interface.objects.create(device=self.device, name='Interface 3')        rearport1 = RearPort.objects.create(device=self.device, name='Rear Port 1', positions=1)        rearport2 = RearPort.objects.create(device=self.device, name='Rear Port 2', positions=1)        rearport3 = RearPort.objects.create(device=self.device, name='Rear Port 3', positions=1)        rearport4 = RearPort.objects.create(device=self.device, name='Rear Port 4', positions=1)        frontport1 = FrontPort.objects.create(            device=self.device, name='Front Port 1', rear_port=rearport1, rear_port_position=1        )        frontport2 = FrontPort.objects.create(            device=self.device, name='Front Port 2', rear_port=rearport2, rear_port_position=1        )        frontport3 = FrontPort.objects.create(            device=self.device, name='Front Port 3', rear_port=rearport3, rear_port_position=1        )        frontport4 = FrontPort.objects.create(            device=self.device, name='Front Port 4', rear_port=rearport4, rear_port_position=1        )        cable2 = Cable(            a_terminations=[rearport1],            b_terminations=[rearport2]        )        cable2.save()        cable4 = Cable(            a_terminations=[rearport3],            b_terminations=[rearport4]        )        cable4.save()        self.assertEqual(CablePath.objects.count(), 0)        # Create cable1        cable1 = Cable(            a_terminations=[interface1],            b_terminations=[frontport1]        )        cable1.save()        self.assertPathExists(            (                interface1, cable1, frontport1, rearport1, cable2, rearport2, frontport2            ),            is_complete=False        )        # Create cable1        cable5 = Cable(            a_terminations=[interface3],            b_terminations=[frontport3]        )        cable5.save()        self.assertPathExists(            (                interface3, cable5, frontport3, rearport3, cable4, rearport4, frontport4            ),            is_complete=False        )        self.assertEqual(CablePath.objects.count(), 2)        # Create cable 3        cable3 = Cable(            a_terminations=[frontport2, frontport4],            b_terminations=[interface2]        )        cable3.save()        self.assertPathExists(            (                interface2, cable3, (frontport2, frontport4), (rearport2, rearport4), (cable2, cable4),                (rearport1, rearport3), (frontport1, frontport3), (cable1, cable5), (interface1, interface3)            ),            is_complete=True,            is_active=True        )        self.assertPathExists(            (                interface1, cable1, frontport1, rearport1, cable2, rearport2, frontport2, cable3, interface2            ),            is_complete=True,            is_active=True        )        self.assertPathExists(            (                interface3, cable5, frontport3, rearport3, cable4, rearport4, frontport4, cable3, interface2            ),            is_complete=True,            is_active=True        )        self.assertEqual(CablePath.objects.count(), 3)        # Test SVG generation        CableTraceSVG(interface1).render()",
        "documentation": "if c fp rp c rp fp c if if c fp rp c rp fp"
    },
    {
        "method_code": "def _all_releases(items):        # Count the number of \"hits\" for each release.    relcounts = defaultdict(int)    for item in items:        if item.path not in _matches:            continue        _, release_ids = _matches[item.path]        for release_id in release_ids:            relcounts[release_id] += 1    for release_id, count in relcounts.items():        if float(count) / len(items) > COMMON_REL_THRESH:            yield release_id",
        "documentation": "given an iterable of item determines according to acoustid which release the item have in common generates release id"
    },
    {
        "method_code": "def _calc_ibd(self, fname, sub, stat=\"a\", scale=\"Log\", min_dist=0.00001):                self._run_genepop(            [\".GRA\", \".MIG\", \".ISO\"],            [6, sub],            fname,            opts={                \"MinimalDistance\": min_dist,                \"GeographicScale\": scale,                \"IsolBDstatistic\": stat,            },        )        with open(fname + \".ISO\") as f:            f.readline()            f.readline()            f.readline()            f.readline()            estimate = _read_triangle_matrix(f)            f.readline()            f.readline()            distance = _read_triangle_matrix(f)            f.readline()            match = re.match(\"a = (.+), b = (.+)\", f.readline().rstrip())            a = _gp_float(match.group(1))            b = _gp_float(match.group(2))            f.readline()            f.readline()            match = re.match(\" b=(.+)\", f.readline().rstrip())            bb = _gp_float(match.group(1))            match = re.match(r\".*\\[(.+)  ;  (.+)\\]\", f.readline().rstrip())            bblow = _gp_float(match.group(1))            bbhigh = _gp_float(match.group(2))        os.remove(fname + \".MIG\")        os.remove(fname + \".GRA\")        os.remove(fname + \".ISO\")        return estimate, distance, (a, b), (bb, bblow, bbhigh)",
        "documentation": "calculate isolation by distance statistic private"
    },
    {
        "method_code": "def convert(in_file, in_format, out_file, out_format, molecule_type=None):        if molecule_type:        if not isinstance(molecule_type, str):            raise TypeError(f\"Molecule type should be a string, not {molecule_type!r}\")        elif (            \"DNA\" in molecule_type            or \"RNA\" in molecule_type            or \"protein\" in molecule_type        ):            pass        else:            raise ValueError(f\"Unexpected molecule type, {molecule_type!r}\")    f = _converter.get((in_format, out_format))    if f:        count = f(in_file, out_file)    else:        records = parse(in_file, in_format)        if molecule_type:            # Edit the records on the fly to set molecule type            def over_ride(record):                                record.annotations[\"molecule_type\"] = molecule_type                return record            records = (over_ride(_) for _ in records)        count = write(records, out_file, out_format)    return count",
        "documentation": "convert between two sequence file format return number of record argument infile an input handle or filename informat input file format lower case string outfile an output handle or filename outformat output file format lower case string moleculetype optional molecule type to apply string containing dna rna or protein note if you provide an output filename it will be opened which will overwrite any existing file without warning the idea here is that while doing this will work from bio import seqio record seqioparseinhandle informat count seqiowriterecords outhandle outformat it is shorter to write from bio import seqio count seqioconvertinhandle informat outhandle outformat also bioseqioconvert is faster for some conversion a it can make some optimisation for example going from a filename to a handle from bio import seqio from io import stringio handle stringio seqioconvertqualityexamplefastq fastq handle fasta printhandlegetvalue easr cccttcttgtcttcagcgtttctcc easr ttggcaggccaaggccgatggatca easr gttgcttctggcgtgggtggggggg blankline note some format like seqxml require you to specify the molecule type when it cannot be determined by the parser from bio import seqio from io import bytesio handle bytesio seqioconvertqualityexamplefastq fastq handle seqxml dna"
    },
    {
        "method_code": "def test_CDS(self):                gb_record = SeqIO.read(self.gb_filename, \"genbank\")        gb_cds = list(SeqIO.parse(self.gb_filename, \"genbank-cds\"))        fasta = list(SeqIO.parse(self.faa_filename, \"fasta\"))        self.compare_records(gb_cds, fasta)        cds_features = [f for f in gb_record.features if f.type == \"CDS\"]        self.assertEqual(len(cds_features), len(fasta))        for f, r in zip(cds_features, fasta):            if r.id in self.skip_trans_test:                continue            # Get the nucleotides and translate them            nuc = f.extract(gb_record.seq)            self.assertEqual(len(nuc), len(f))            try:                pro = nuc.translate(table=self.table, cds=True)            except TranslationError as e:                msg = f\"{e}\\n{r.id!r}, {nuc!r}, {self.table!r}\\n{f}\"                self.fail(msg)            if pro[-1] == \"*\":                self.assertEqual(pro[:-1], r.seq)            else:                self.assertEqual(pro, r.seq)",
        "documentation": "checking genbank cd translation v fasta faa file"
    },
    {
        "method_code": "def evaluate(self, expr, context):                if isinstance(expr, str):            if expr[0] in \"'\\\"\":                result = expr[1:-1]            else:                if expr not in context:                    raise SyntaxError(f\"unknown variable: {expr}\")                result = context[expr]        else:            assert isinstance(expr, dict)            op = expr[\"op\"]            if op not in self.operations:                raise NotImplementedError(f\"op not implemented: {op}\")            elhs = expr[\"lhs\"]            erhs = expr[\"rhs\"]            if _is_literal(expr[\"lhs\"]) and _is_literal(expr[\"rhs\"]):                raise SyntaxError(f\"invalid comparison: {elhs} {op} {erhs}\")            lhs = self.evaluate(elhs, context)            rhs = self.evaluate(erhs, context)            result = self.operations[op](lhs, rhs)        return result",
        "documentation": "evaluate a marker expression returned by the funcparserequirement function in the specified context"
    },
    {
        "method_code": "def is_preterminal(self, node):                if self.is_terminal(node):            return False not in [self.is_terminal(n) for n in self.node(node).succ]        else:            return False",
        "documentation": "return true if all successor of a node are terminal one"
    },
    {
        "method_code": "def clear(self, *names):                for name in names:            self._changed_fields.pop(name, None)        else:            self._changed_fields = {}",
        "documentation": "clear any field that were recorded a having been changed"
    },
    {
        "method_code": "def test_to_block_document_anonymous(self, block_type_x):                # explicit true        anon_block = self.MyRegisteredBlock(x=\"x\")._to_block_document(            block_schema_id=uuid4(),            block_type_id=block_type_x.id,            is_anonymous=True,        )        assert anon_block.is_anonymous is True        # explicit false        anon_block_2 = self.MyRegisteredBlock(x=\"x\")._to_block_document(            name=\"block\",            block_schema_id=uuid4(),            block_type_id=block_type_x.id,            is_anonymous=False,        )        assert anon_block_2.is_anonymous is False        # none with no fallback        anon_block_3 = self.MyRegisteredBlock(x=\"x\")._to_block_document(            name=\"block\",            block_schema_id=uuid4(),            block_type_id=block_type_x.id,            is_anonymous=None,        )        assert anon_block_3.is_anonymous is False        # none with True fallback        anon_block_4 = self.MyRegisteredBlock(x=\"x\")        anon_block_4._is_anonymous = True        doc_4 = anon_block_4._to_block_document(            block_schema_id=uuid4(),            block_type_id=block_type_x.id,            is_anonymous=None,        )        assert doc_4.is_anonymous is True        # False with True fallback        anon_block_5 = self.MyRegisteredBlock(x=\"x\")        anon_block_5._is_anonymous = True        doc_5 = anon_block_5._to_block_document(            name=\"block\",            block_schema_id=uuid4(),            block_type_id=block_type_x.id,            is_anonymous=False,        )        assert doc_5.is_anonymous is False",
        "documentation": "test passing different value to the isanonymous argument in combination with different value of the isanonymous class fallback"
    },
    {
        "method_code": "def valid_package(self) -> bool:                return len(self.file_data) > 0",
        "documentation": "return true if package ha an environment file return true or false"
    },
    {
        "method_code": "def __repr__(self) -> str:                return f\"4-{self.id!s} {self.re_class} {self.angle!s} {self.ric!s}\"",
        "documentation": "print string for dihedron object"
    },
    {
        "method_code": "def cmd_commands(self, conn):                if self.password and not conn.authenticated:            # Not authenticated. Show limited list of commands.            for cmd in SAFE_COMMANDS:                yield \"command: \" + cmd        else:            # Authenticated. Show all commands.            for func in dir(self):                if func.startswith(\"cmd_\"):                    yield \"command: \" + func[4:]",
        "documentation": "list the command available to the user"
    },
    {
        "method_code": "def __len__(self):                return len(self.features)",
        "documentation": "return the number of feature in the set"
    },
    {
        "method_code": "def test_hmmerdomtab_30_hmmscan_004(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_004.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "documentation": "test hmmscandomtab indexing hmmer single query no alignment"
    },
    {
        "method_code": "def model_copy(        self, *, update: Optional[Dict[str, Any]] = None, deep: bool = False    ):                update = update or {}        update.setdefault(\"timestamp\", self.model_fields[\"timestamp\"].get_default())        return super().model_copy(update=update, deep=deep)",
        "documentation": "copying api model should return an object that could be inserted into the database again the timestamp is reset using the default factory"
    },
    {
        "method_code": "def __exit__(self, type, value, traceback):                self.close()",
        "documentation": "close a file with with statement"
    },
    {
        "method_code": "def distplot_to_dh_arrays(        self, distplot: np.ndarray, dihedra_signs: np.ndarray    ) -> None:                ha = self.a2ha_map.reshape(-1, 3)        self.hedraL12 = distplot[ha[:, 0], ha[:, 1]]        self.hedraL23 = distplot[ha[:, 1], ha[:, 2]]        self.hedraL13 = distplot[ha[:, 0], ha[:, 2]]        da = self.d2a_map        self.dihedraL14 = distplot[da[:, 0], da[:, 3]]        self.dihedra_signs = dihedra_signs",
        "documentation": "load dihedra distance array from distplot fill classicchain array hedral l l and dihedral distance value array from input distplot dihedrasigns array from input dihedrasigns distplot and dihedra distance array must index according to atomkey mapping in classicchain hedrandx and dihedrandx created in methicchaininitedra call methatomtointernalcoordinates or at least methinitedra to generate ahamap and damap before running this explicitly removed from methdistancetointernalcoordinates so user may populate these chain dihedra array by other method"
    },
    {
        "method_code": "def __init__(self, server, sock):                super().__init__(server, sock)        self.authenticated = False        self.notifications = set()        self.idle_subscriptions = set()",
        "documentation": "create a new connection for the accepted socket client"
    },
    {
        "method_code": "def _get_fragments_phase(frags):        return [(3 - (x % 3)) % 3 for x in _get_fragments_coord(frags)]",
        "documentation": "return the phase of the given list of letter amino acid fragment private this is an internal private function and is meant for parsing exonerates threeletter amino acid output from biosearchioexonerateiobase import getfragmentsphase getfragmentsphasethr ser ala getfragmentsphasethrse rala getfragmentsphasethrse ralaleu procys getfragmentsphasethrse ralaleup rocys getfragmentsphasethrse ralaleupr ocys"
    },
    {
        "method_code": "def parse_feature(self, feature_key, lines):        r        # Skip any blank lines        iterator = (x for x in lines if x)        try:            line = next(iterator)            feature_location = line.strip()            while feature_location[-1:] == \",\":                # Multiline location, still more to come!                line = next(iterator)                feature_location += line.strip()            if feature_location.count(\"(\") > feature_location.count(\")\"):                # Including the prev line in warning would be more explicit,                # but this way get one-and-only-one warning shown by default:                warnings.warn(                    \"Non-standard feature line wrapping (didn't break on comma)?\",                    BiopythonParserWarning,                )                while feature_location[-1:] == \",\" or feature_location.count(                    \"(\"                ) > feature_location.count(\")\"):                    line = next(iterator)                    feature_location += line.strip()            qualifiers = []            for line_number, line in enumerate(iterator):                # check for extra wrapping of the location closing parentheses                if line_number == 0 and line.startswith(\")\"):                    feature_location += line.strip()                elif line[0] == \"/\":                    # New qualifier                    i = line.find(\"=\")                    key = line[1:i]  # does not work if i==-1                    value = line[i + 1 :]  # we ignore 'value' if i==-1                    if i and value.startswith(\" \") and value.lstrip().startswith('\"'):                        warnings.warn(                            \"White space after equals in qualifier\",                            BiopythonParserWarning,                        )                        value = value.lstrip()                    if i == -1:                        # Qualifier with no key, e.g. /pseudo                        key = line[1:]                        qualifiers.append((key, None))                    elif not value:                        # ApE can output /note=                        qualifiers.append((key, \"\"))                    elif value == '\"':                        # One single quote                        if self.debug:                            print(f\"Single quote {key}:{value}\")                        # DO NOT remove the quote...                        qualifiers.append((key, value))                    elif value[0] == '\"':                        # Quoted...                        value_list = [value]                        while value_list[-1][-1] != '\"':                            value_list.append(next(iterator))                        value = \"\\n\".join(value_list)                        # DO NOT remove the quotes...                        qualifiers.append((key, value))                    else:                        # Unquoted                        # if debug : print(\"Unquoted line %s:%s\" % (key,value))                        qualifiers.append((key, value))                else:                    # Unquoted continuation                    assert len(qualifiers) > 0                    assert key == qualifiers[-1][0]                    # if debug : print(\"Unquoted Cont %s:%s\" % (key, line))                    if qualifiers[-1][1] is None:                        raise StopIteration                    qualifiers[-1] = (key, qualifiers[-1][1] + \"\\n\" + line)            return feature_key, feature_location, qualifiers        except StopIteration:            # Bummer            raise ValueError(                \"Problem with '%s' feature:\\n%s\" % (feature_key, \"\\n\".join(lines))            ) from None",
        "documentation": "parse a feature given a a list of string into a tuple expects a feature a a list of string return a tuple key location qualifier for example given this genbank feature cd complementjoin locustagneq noteconserved hypothetical methanococcus jannaschii coguncharacterized acr iprbipartite nuclear localization signal ipr protein of unknown function duf codonstart transltable producthypothetical protein proteinidnp dbxrefgi dbxrefgeneid translationmrlllelkalnsidkkqlsnyliqgfiynilknteyswlhnwkk ekyfnftlipkkdiienkryyliisspdkrfievlhnkikdldiitiglaqfqlrktk kfdpklrfpwvtitpivlregkivilkgdkyykvfvkrleelkkynlikkkepileep ieislnqikdgwkiidvkdryydfrnksfsafsnwlrdlkeqslrkynnfcgknfyfe eaifegftfyktvsiririnrgeavyigtlwkelnvyrkldkeerefykflydcglgs lnsmgfgfvntkknsar then should give input keycds and the rest of the data a a list of string linescomplementjoin lnsmgfgfvntkknsar where the leading space and trailing newlines have been removed return tuple containing key a string location string qualifier a list a follows for this example key cd string location complementjoin string qualifier list of string tuples locustag neq note conserved hypothetical methanococcus jannaschiincog codonstart transltable product hypothetical protein proteinid np dbxref gi dbxref geneid translation mrlllelkalnsidkkqlsnyliqgfiynilknteyswlhnwkknekyfnft in the above example the note and translation were edited for compactness and they would contain multiple new line character displayed above a n if a qualifier is quoted in this case everything except codonstart and transltable then the quote are not removed note that no whitespace is removed"
    },
    {
        "method_code": "def get_next_available_ip(self):                if self.address and self.address.broadcast:            start_ip = self.address.ip + 1            end_ip = self.address.broadcast - 1            if start_ip <= end_ip:                available_ips = netaddr.IPSet(netaddr.IPRange(start_ip, end_ip))                available_ips -= netaddr.IPSet([                    address.ip for address in IPAddress.objects.filter(                        vrf=self.vrf,                        address__gt=self.address,                        address__net_contained_or_equal=self.address.cidr                    ).values_list('address', flat=True)                ])                if available_ips:                    return next(iter(available_ips))",
        "documentation": "return the next available ip address within this ip network if any"
    },
    {
        "method_code": "def commands(self):                mbsubmit_cmd = ui.Subcommand(            \"mbsubmit\", help=\"Submit Tracks to MusicBrainz\"        )        def func(lib, opts, args):            items = lib.items(ui.decargs(args))            self._mbsubmit(items)        mbsubmit_cmd.func = func        return [mbsubmit_cmd]",
        "documentation": "add beet ui command for mbsubmit"
    },
    {
        "method_code": "def read(source):        handle = _open(source)    try:        record = _read(handle)        if not record:            raise ValueError(\"No SwissProt record found\")        # We should have reached the end of the record by now.        # Try to read one more line to be sure:        try:            next(handle)        except StopIteration:            return record        raise ValueError(\"More than one SwissProt record found\")    finally:        if handle is not source:            handle.close()",
        "documentation": "read one swissprot record from file argument source is a filelike object or a path to a file return a record object"
    },
    {
        "method_code": "def set_all_tracks(self, attr, value):                for track in self.tracks.values():            if hasattr(track, attr):                # If the feature has the attribute set it to the passed value                setattr(track, attr, value)",
        "documentation": "set the passed attribute of all track in the set to the passed value argument attr an attribute of the track class value the value to set that attribute setalltracksself attr value"
    },
    {
        "method_code": "def parse_records(self, handle, do_features=True):                # This is a generator function        with as_handle(handle) as handle:            while True:                record = self.parse(handle, do_features)                if record is None:                    break                if record.id is None:                    raise ValueError(                        \"Failed to parse the record's ID. Invalid ID line?\"                    )                if record.name == \"<unknown name>\":                    raise ValueError(                        \"Failed to parse the record's name. Invalid ID line?\"                    )                if record.description == \"<unknown description>\":                    raise ValueError(\"Failed to parse the record's description\")                yield record",
        "documentation": "parse record return a seqrecord object iterator each record from the idlocus line to the line becomes a seqrecord the seqrecord object include seqfeatures if dofeaturestrue this method is intended for use in bioseqio"
    },
    {
        "method_code": "def package_failed(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"package_failed\", *args)",
        "documentation": "package failed somehow"
    },
    {
        "method_code": "def test_baum_welch_training_standard(self):                known_training_seq = Trainer.TrainingSequence(self.rolls, self.states)        standard_mm = self.mm_builder.get_markov_model()        trainer = Trainer.KnownStateTrainer(standard_mm)        trained_mm = trainer.train([known_training_seq])        if VERBOSE:            print(trained_mm.transition_prob)            print(trained_mm.emission_prob)        test_rolls, test_states = generate_rolls(300)        predicted_states, prob = trained_mm.viterbi(test_rolls, dice_type_alphabet)        if VERBOSE:            print(f\"Prediction probability: {prob:f}\")            Utilities.pretty_print_prediction(test_rolls, test_states, predicted_states)",
        "documentation": "standard training with known state"
    },
    {
        "method_code": "def test_create_multiple_available_prefixes(self):                vrf = VRF.objects.create(name='VRF 1')        prefix = Prefix.objects.create(prefix=IPNetwork('192.0.2.0/28'), vrf=vrf, is_pool=True)        url = reverse('ipam-api:prefix-available-prefixes', kwargs={'pk': prefix.pk})        self.add_permissions('ipam.view_prefix', 'ipam.add_prefix')        # Try to create five /30s (only four are available)        data = [            {'prefix_length': 30, 'description': 'Prefix 1'},            {'prefix_length': 30, 'description': 'Prefix 2'},            {'prefix_length': 30, 'description': 'Prefix 3'},            {'prefix_length': 30, 'description': 'Prefix 4'},            {'prefix_length': 30, 'description': 'Prefix 5'},        ]        response = self.client.post(url, data, format='json', **self.header)        self.assertHttpStatus(response, status.HTTP_409_CONFLICT)        self.assertIn('detail', response.data)        # Verify that no prefixes were created (the entire /28 is still available)        response = self.client.get(url, **self.header)        self.assertHttpStatus(response, status.HTTP_200_OK)        self.assertEqual(response.data[0]['prefix'], '192.0.2.0/28')        # Create four /30s in a single request        response = self.client.post(url, data[:4], format='json', **self.header)        self.assertHttpStatus(response, status.HTTP_201_CREATED)        self.assertEqual(len(response.data), 4)",
        "documentation": "test the creation of available prefix within a parent prefix"
    },
    {
        "method_code": "def test_psl_34_007(self):                filename = os.path.join(\"Blat\", \"pslx_34_002.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "documentation": "test blatpslx indexing single query no hit"
    },
    {
        "method_code": "def bootstrap_trees(alignment, times, tree_constructor):        if isinstance(alignment, MultipleSeqAlignment):        length = len(alignment[0])        for i in range(times):            bootstrapped_alignment = None            for j in range(length):                col = random.randint(0, length - 1)                if bootstrapped_alignment is None:                    bootstrapped_alignment = alignment[:, col : col + 1]                else:                    bootstrapped_alignment += alignment[:, col : col + 1]            tree = tree_constructor.build_tree(bootstrapped_alignment)            yield tree    else:        n, m = alignment.shape        for i in range(times):            cols = [random.randint(0, m - 1) for j in range(m)]            tree = tree_constructor.build_tree(alignment[:, cols])            yield tree",
        "documentation": "generate bootstrap replicate tree from a multiple sequence alignment parameter alignment alignment or multipleseqalignment object multiple sequence alignment to generate replicates time int number of bootstrap time treeconstructor treeconstructor tree constructor to be used to build tree"
    },
    {
        "method_code": "def test_write_single_from_blatpsl(self):                source = os.path.join(\"Blat\", \"psl_34_004.psl\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "documentation": "test blatpsl writing from blatpsl single query pslpsl"
    },
    {
        "method_code": "def _ns(tag, namespace=NAMESPACES[\"phy\"]):        return f\"{{{namespace}}}{tag}\"",
        "documentation": "format an xml tag with the given namespace private"
    },
    {
        "method_code": "def __init__(self, value=None, unit=None, desc=None, minimum=None, maximum=None):                self.value = value        self.unit = unit        self.desc = desc        self.minimum = minimum        self.maximum = maximum",
        "documentation": "initialize value of the date object"
    },
    {
        "method_code": "def objects_from_script(path: str, text: Union[str, bytes] = None) -> Dict[str, Any]:        def run_script(run_path: str):        # Cast to an absolute path before changing directories to ensure relative paths        # are not broken        abs_run_path = os.path.abspath(run_path)        with tmpchdir(run_path):            try:                return runpy.run_path(abs_run_path)            except Exception as exc:                raise ScriptError(user_exc=exc, path=path) from exc    if text:        with NamedTemporaryFile(            mode=\"wt\" if isinstance(text, str) else \"wb\",            prefix=f\"run-{filename(path)}\",            suffix=\".py\",        ) as tmpfile:            tmpfile.write(text)            tmpfile.flush()            return run_script(tmpfile.name)    else:        if not is_local_path(path):            # Remote paths need to be local to run            with fsspec.open(path) as f:                contents = f.read()            return objects_from_script(path, contents)        else:            return run_script(path)",
        "documentation": "run a python script and return all the global variable support remote path by copying to a local temporary file warning the python documentation doe not recommend using runpy for this pattern furthermore any function and class defined by the executed code are not guaranteed to work correctly after a runpy function ha returned if that limitation is not acceptable for a given use case importlib is likely to be a more suitable choice than this module the function loadscriptasmodule us importlib instead and should be used instead for loading object from script args path the path to the script to run text optionally the text of the script skip loading the content if given return a dictionary mapping variable name to value raise scripterror if the script raise an exception during execution"
    },
    {
        "method_code": "def _get_inter_coords(coords, strand=1):        # adapted from Python's itertools guide    # if strand is -1, adjust coords to the ends and starts are chained    if strand == -1:        sorted_coords = [(max(a, b), min(a, b)) for a, b in coords]        inter_coords = list(chain(*sorted_coords))[1:-1]        return list(zip(inter_coords[1::2], inter_coords[::2]))    else:        inter_coords = list(chain(*coords))[1:-1]        return list(zip(inter_coords[::2], inter_coords[1::2]))",
        "documentation": "return list of pair covering intervening range private from the given pair of coordinate return a list of pair covering the intervening range"
    },
    {
        "method_code": "def test_version_fallback() -> None:        deprecated = DeprecationHandler(None)  # type: ignore[arg-type]    assert deprecated._version_less_than(\"0\")    assert deprecated._version_tuple is None    version: Version = deprecated._version_object  # type: ignore[assignment]    assert version.major == version.minor == version.micro == 0",
        "documentation": "test that conda can run even if deprecation cant parse the version"
    },
    {
        "method_code": "def test_bulk_create_objects(self):                        # Add object-level permission            obj_perm = ObjectPermission(                name='Test permission',                actions=['add']            )            obj_perm.save()            obj_perm.users.add(self.user)            obj_perm.object_types.add(ObjectType.objects.get_for_model(self.model))            initial_count = self._get_queryset().count()            response = self.client.post(self._get_list_url(), self.create_data, format='json', **self.header)            self.assertHttpStatus(response, status.HTTP_201_CREATED)            self.assertEqual(len(response.data), len(self.create_data))            self.assertEqual(self._get_queryset().count(), initial_count + len(self.create_data))            for i, obj in enumerate(response.data):                for field in self.create_data[i]:                    if field not in self.validation_excluded_fields:                        self.assertIn(field, obj, f\"Bulk create field '{field}' missing from object {i} in response\")            for i, obj in enumerate(response.data):                self.assertInstanceEqual(                    self._get_queryset().get(pk=obj['id']),                    self.create_data[i],                    exclude=self.validation_excluded_fields,                    api=True                )",
        "documentation": "post a set of object in a single request"
    },
    {
        "method_code": "def read(handle, format, seq_count=None):        iterator = parse(handle, format, seq_count)    try:        alignment = next(iterator)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(iterator)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    if seq_count:        if len(alignment) != seq_count:            raise RuntimeError(                \"More sequences found in alignment than specified in seq_count: %s.\"                % seq_count            )    return alignment",
        "documentation": "turn an alignment file into a single multipleseqalignment object argument handle handle to the file or the filename a a string note older version of biopython only took a handle format string describing the file format seqcount optional integer number of sequence expected in each alignment recommended for fasta format file if the handle contains no alignment or more than one alignment an exception is raised for example using a pfamstockholm file containing one alignment from bio import alignio filename clustalwproteinaln format clustal alignment alignioreadfilename format printalignment of length i alignmentgetalignmentlength alignment of length if however you want the first alignment from a file containing multiple alignment this function would raise an exception from bio import alignio filename embossneedletxt format emboss alignment alignioreadfilename format traceback most recent call last valueerror more than one record found in handle instead use from bio import alignio filename embossneedletxt format emboss alignment nextalignioparsefilename format printfirst alignment ha length i alignmentgetalignmentlength first alignment ha length you must use the bioalignioparse function if you want to read multiple record from the handle"
    },
    {
        "method_code": "def test_hmmpfam_23_missing_consensus(self):                results = parse(path.join(\"Hmmer\", \"text_23_hmmpfam_003.out\"), self.fmt)        res = next(results)        self.assertEqual(\"small_input\", res.id)        self.assertEqual(\"[none]\", res.description)        self.assertEqual(\"[none]\", res.accession)        self.assertEqual(\"hmmpfam\", res.program)        self.assertEqual(\"2.3.2\", res.version)        self.assertEqual(            \"antismash/specific_modules/lantipeptides/ClassIVLanti.hmm\", res.target        )        self.assertEqual(1, len(res))        hit = res[0]        self.assertEqual(\"ClassIVLanti\", hit.id)        self.assertEqual(\"Class-IV\", hit.description)        self.assertAlmostEqual(-79.3, hit.bitscore)        self.assertAlmostEqual(1, hit.evalue)        self.assertEqual(1, hit.domain_obs_num)        self.assertEqual(1, len(hit))        hsp = hit[0]        self.assertEqual(1, hsp.domain_index)        self.assertEqual(0, hsp.hit_start)        self.assertEqual(66, hsp.hit_end)        self.assertEqual(\"[]\", hsp.hit_endtype)        self.assertEqual(5, hsp.query_start)        self.assertEqual(20, hsp.query_end)        self.assertEqual(\"..\", hsp.query_endtype)        self.assertAlmostEqual(-79.3, hsp.bitscore)        self.assertAlmostEqual(1, hsp.evalue)        self.assertEqual(len(hsp.query.seq), len(hsp.hit.seq))        self.assertEqual(len(hsp.query.seq), len(hsp.aln_annotation[\"similarity\"]))        self.assertEqual(            \"msEEqLKAFiAKvqaDtsLqEqLKaEGADvvaiAKAaGFtitteDLnahiqakeLsdeeLEgvaGg\",            hsp.hit.seq,        )        self.assertEqual(            \"        F+                           G  +t   Ln                   \",            str(hsp.aln_annotation[\"similarity\"]),        )        self.assertEqual(            \"-------CFL---------------------------GCLVTNWVLNRS-----------------\",            hsp.query.seq,        )",
        "documentation": "test parsing hmmpfam file texthmmpfamout"
    },
    {
        "method_code": "def test_dssp_with_mmcif_file_and_nonstandard_residues(self):                if self.dssp_version < VERSION_2_2_0:            self.skipTest(\"Test requires DSSP version 2.2.0 or greater\")        pdbfile = \"PDB/1AS5.cif\"        model = self.cifparser.get_structure(\"1AS5\", pdbfile)[0]        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\")  # silence DSSP warnings            dssp = DSSP(model, pdbfile)        self.assertEqual(len(dssp), 24)",
        "documentation": "test dssp generation from mmcif with nonstandard residue"
    },
    {
        "method_code": "def test_rna(self):                self.check(            [\"UACAA\", \"UACGC\", \"UACAC\", \"UACCC\", \"AACCC\", \"AAUGC\", \"AAUGC\"], \"GAUC\"        )",
        "documentation": "test biomotifsweblogo with an rna sequence"
    },
    {
        "method_code": "def __str__(self):                return f\"<{int(self)}\"",
        "documentation": "return a representation of the beforeposition object with python counting"
    },
    {
        "method_code": "def init_replaygain(self, audiofile, item: Item):                try:            rg = self._mod_replaygain.ReplayGain(audiofile.sample_rate())        except ValueError:            raise ReplayGainError(f\"Unsupported sample rate {item.samplerate}\")            return        return rg",
        "documentation": "return an initialized classaudiotoolsreplaygainreplaygain instance which requires the sample rate of the song on which the replaygain value will be computed the item is passed in case the sample rate is invalid to log the stored item sample rate return initialized replagain object rtype classaudiotoolsreplaygainreplaygain raise excreplaygainerror if the sample rate is invalid"
    },
    {
        "method_code": "def get_solver_backend(self, name: str | None = None) -> type[Solver]:                # Some light data validation in case name isn't given.        if name is None:            name = context.solver        name = name.lower()        solvers_mapping = self.get_solvers()        # Look up the solver mapping and fail loudly if it can't        # find the requested solver.        solver_plugin = solvers_mapping.get(name, None)        if solver_plugin is None:            raise CondaValueError(                f\"You have chosen a non-default solver backend ({name}) \"                f\"but it was not recognized. Choose one of: \"                f\"{', '.join(solvers_mapping)}\"            )        return solver_plugin.backend",
        "documentation": "get the solver backend with the given name or fall back to the name provided in the context see contextsolver for more detail please use the cached version of this method called methgetcachedsolverbackend for highthroughput code path which is set up a a instancespecific lru cache"
    },
    {
        "method_code": "def _clean_json(self, data):                try:            data = json.loads(data)            # Accommodate for users entering single objects            if type(data) is not list:                data = [data]            return data        except json.decoder.JSONDecodeError as err:            raise forms.ValidationError({                self.data_field: f\"Invalid JSON data: {err}\"            })",
        "documentation": "clean jsonformatted data if only a single object is defined it will be encapsulated a a list"
    },
    {
        "method_code": "def __str__(self):                return self.to_string(plain=True)",
        "documentation": "short version of tostring give plain tree"
    },
    {
        "method_code": "def _flip(self, length):                return BeforePosition(length - int(self))",
        "documentation": "return a copy of the location after the parent is reversed private"
    },
    {
        "method_code": "def _populate_env_in_manifest(self):                transformed_env = [{\"name\": k, \"value\": v} for k, v in self.env.items()]        template_env = self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][            0        ].get(\"env\")        # If user has removed `{{ env }}` placeholder and hard coded a value for `env`,        # we need to prepend our environment variables to the list to ensure Prefect        # setting propagation.        if isinstance(template_env, list):            self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"env\"] = [                *transformed_env,                *template_env,            ]        # Current templating adds `env` as a dict when the kubernetes manifest requires        # a list of dicts. Might be able to improve this in the future with a better        # default `env` value and better typing.        else:            self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][                \"env\"            ] = transformed_env",
        "documentation": "populates environment variable in the job manifest when env is templated a a variable in the job manifest it come in a a dictionary we need to convert it to a list of dictionary to conform to the kubernetes job manifest schema this function also handle the case where the user ha removed the env placeholder and hard coded a value for env in this case we need to prepend our environment variable to the list to ensure prefect setting propagation an example reason the a user would remove the env placeholder to hardcode kubernetes secret in the base job template"
    },
    {
        "method_code": "def _set_hsp_bit_score(self):                self._hsp.bits = float(self._value)        if self._descr.bits is None:            self._descr.bits = float(self._value)",
        "documentation": "record the bit score of hsp private"
    },
    {
        "method_code": "def _open_library(config):        dbpath = util.bytestring_path(config[\"library\"].as_filename())    _ensure_db_directory_exists(dbpath)    try:        lib = library.Library(            dbpath,            config[\"directory\"].as_filename(),            get_path_formats(),            get_replacements(),        )        lib.get_item(0)  # Test database connection.    except (sqlite3.OperationalError, sqlite3.DatabaseError) as db_error:        log.debug(\"{}\", traceback.format_exc())        raise UserError(            \"database file {} cannot not be opened: {}\".format(                util.displayable_path(dbpath), db_error            )        )    log.debug(        \"library database: {0}\\n\" \"library directory: {1}\",        util.displayable_path(lib.path),        util.displayable_path(lib.directory),    )    return lib",
        "documentation": "create a new library instance from the configuration"
    },
    {
        "method_code": "def test_hmmerdomtab_30_hmmscan_002(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_002.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "documentation": "test hmmscandomtab indexing hmmer single query no hit"
    },
    {
        "method_code": "def _codons2re(codons):        reg = \"\"    for i in zip(*codons):        if len(set(i)) == 1:            reg += \"\".join(set(i))        else:            reg += \"[\" + \"\".join(set(i)) + \"]\"    return reg",
        "documentation": "generate regular expression based on a given list of codon private"
    },
    {
        "method_code": "def fetch_latest_path(self) -> tuple[Path, RepodataState]:                _, state = self.fetch_latest()        return self.cache_path_json, state",
        "documentation": "retrieve latest or latestcached repodata update cache return pathlibpath to uncompressed repodata content repodatastate"
    },
    {
        "method_code": "def classify(me, observation):        scores = calculate(me, observation)    max_score, klass = scores[0], me.classes[0]    for i in range(1, len(scores)):        if scores[i] > max_score:            max_score, klass = scores[i], me.classes[i]    return klass",
        "documentation": "classify an observation into a class"
    },
    {
        "method_code": "def _set_statistics_db_len(self):                self._blast.num_letters_in_database = int(self._value)",
        "documentation": "record the number of letter in the database private"
    },
    {
        "method_code": "def compute_album_gain(self, task: AnyRgTask) -> AnyRgTask:                items = list(task.items)        self.compute(items, task.target_level, True)        if len(self._file_tags) != len(items):            raise ReplayGainError(\"Some items in album did not receive tags\")        # Collect track gains.        track_gains = []        for item in items:            try:                gain = self._file_tags[item.path][\"TRACK_GAIN\"]                peak = self._file_tags[item.path][\"TRACK_PEAK\"]            except KeyError:                raise ReplayGainError(\"results missing for track\")            track_gains.append(Gain(gain, peak))        # Get album gain information from the last track.        last_tags = self._file_tags[items[-1].path]        try:            gain = last_tags[\"ALBUM_GAIN\"]            peak = last_tags[\"ALBUM_PEAK\"]        except KeyError:            raise ReplayGainError(\"results missing for album\")        task.album_gain = Gain(gain, peak)        task.track_gains = track_gains        return task",
        "documentation": "computes the album gain for the album belonging to task and set the albumgain attribute on the task return task"
    },
    {
        "method_code": "def test_webenv_search(self):                stream = Entrez.elink(            db=\"nucleotide\",            dbfrom=\"protein\",            id=\"22347800,48526535\",            webenv=None,            query_key=None,            cmd=\"neighbor_history\",        )        recs = Entrez.read(stream)        stream.close()        record = recs.pop()        webenv = record[\"WebEnv\"]        query_key = record[\"LinkSetDbHistory\"][0][\"QueryKey\"]        stream = Entrez.esearch(            db=\"nucleotide\",            term=None,            retstart=0,            retmax=10,            webenv=webenv,            query_key=query_key,            usehistory=\"y\",        )        search_record = Entrez.read(stream)        stream.close()        self.assertEqual(2, len(search_record[\"IdList\"]))",
        "documentation": "test entrezsearch from link webenv history"
    },
    {
        "method_code": "def exception_traceback(exc: Exception) -> str:        tb = traceback.TracebackException.from_exception(exc)    return \"\".join(list(tb.format()))",
        "documentation": "convert an exception to a printable string with a traceback"
    },
    {
        "method_code": "def get_times(self):                return sorted(self._signals.keys())",
        "documentation": "get a list of the recorded time point"
    },
    {
        "method_code": "def test_inspecting_artifact_succeeds(artifacts):        expected_output = (        f\"{artifacts[0].id}\",        f\"{artifacts[0].key}\",        f\"{artifacts[0].type}\",        f\"{artifacts[0].description}\",        f\"{artifacts[0].data}\",        f\"{artifacts[1].id}\",        f\"{artifacts[1].key}\",        f\"{artifacts[1].type}\",        f\"{artifacts[1].description}\",        f\"{artifacts[1].data}\",    )    invoke_and_assert(        [\"artifact\", \"inspect\", str(artifacts[0].key)],        expected_output_contains=expected_output,        expected_code=0,        expected_output_does_not_contain=f\"{artifacts[2].id}\",    )",
        "documentation": "we expect to see all version of the artifact"
    },
    {
        "method_code": "def classify(nb, observation):        # The class is the one with the highest probability.    probs = calculate(nb, observation, scale=False)    max_prob = max_class = None    for klass in nb.classes:        if max_prob is None or probs[klass] > max_prob:            max_prob, max_class = probs[klass], klass    return max_class",
        "documentation": "classify an observation into a class"
    },
    {
        "method_code": "def esummary(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"    variables = {}    variables.update(keywds)    request = _build_request(cgi, variables)    return _open(request)",
        "documentation": "retrieve document summary a a result handle esummary retrieves document summary from a list of primary id or from the user environment see the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksnbkchapteresummary this example discovers more about entry in the structure database from bio import entrez entrezemail yournamehereexampleorg handle entrezesummarydbstructure id record entrezreadhandle handleclose printrecordid printrecordpdbdescr crystal structure of e coli aconitase b return handle to the result by default in xml format raise urlliberrorurlerror if there a network error"
    },
    {
        "method_code": "def prepare_for_flow_run(        self,        flow_run: \"FlowRun\",        deployment: Optional[\"DeploymentResponse\"] = None,        flow: Optional[\"Flow\"] = None,    ):                super().prepare_for_flow_run(flow_run, deployment, flow)        # expectations:        # - the first resource in the template is the container group        # - the container group has a single container        container_group = self.arm_template[\"resources\"][0]        container = container_group[\"properties\"][\"containers\"][0]        # set the container's environment variables        container[\"properties\"][\"environmentVariables\"] = self._get_arm_environment()        # convert the command from a string to a list, because that's what ACI expects        if self.command:            container[\"properties\"][\"command\"] = self.command.split(\" \")        self._add_image()        # Add the entrypoint if provided. Creating an ACI container with a        # command overrides the container's built-in entrypoint. Prefect base images        # use entrypoint.sh as the entrypoint, so we need to add to the beginning of        # the command list to avoid breaking EXTRA_PIP_PACKAGES installation on        # container startup.        if self.entrypoint:            container[\"properties\"][\"command\"].insert(0, self.entrypoint)        if self.image_registry:            self._add_image_registry_credentials(self.image_registry)        if self.identities:            self._add_identities(self.identities)        if self.subnet_ids:            self._add_subnets(self.subnet_ids)        if self.dns_servers:            self._add_dns_servers(self.dns_servers)",
        "documentation": "prepares the job configuration for a flow run"
    },
    {
        "method_code": "def test_read_write_globalSpecies(self):                read1_records = list(SeqIO.parse(\"SeqXML/global_species_example.xml\", \"seqxml\"))        self._write_parse_and_compare(read1_records)",
        "documentation": "read and write global specie"
    },
    {
        "method_code": "def _fastq_sanger_convert_fastq_sanger(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # Map unexpected chars to null    mapping = \"\".join(        [chr(0) for ascii in range(33)]        + [chr(ascii) for ascii in range(33, 127)]        + [chr(0) for ascii in range(127, 256)]    )    assert len(mapping) == 256    return _fastq_generic(in_file, out_file, mapping)",
        "documentation": "fast sanger fastq to sanger fastq conversion private useful for removing line wrapping and the redundant second identifier on the plus line will check also check the quality string is valid avoids creating seqrecord and seq object in order to speed up this conversion"
    },
    {
        "method_code": "def test_post_command_invoked(post_command_plugin, conda_cli):        conda_cli(\"info\")    assert len(post_command_plugin.post_command_action.mock_calls) == 1",
        "documentation": "make sure that we successfully invoked our postcommand action"
    },
    {
        "method_code": "def validate_github_access_token(v: str, values: dict) -> str:        if v is not None:        if urllib.parse.urlparse(values[\"repository\"]).scheme != \"https\":            raise InvalidRepositoryURLError(                \"Crendentials can only be used with GitHub repositories \"                \"using the 'HTTPS' format. You must either remove the \"                \"credential if you wish to use the 'SSH' format and are not \"                \"using a private repository, or you must change the repository \"                \"URL to the 'HTTPS' format. \"            )    return v",
        "documentation": "ensure that credential are not provided with ssh formatted github url note validates accesstoken specifically so that it only fire when private repository are used"
    },
    {
        "method_code": "def clustercentroids(data, mask=None, clusterid=None, method=\"a\", transpose=False):        data = __check_data(data)    mask = __check_mask(mask, data.shape)    nrows, ncolumns = data.shape    if clusterid is None:        n = ncolumns if transpose else nrows        clusterid = np.zeros(n, dtype=\"intc\")        nclusters = 1    else:        clusterid = np.require(clusterid, dtype=\"intc\", requirements=\"C\")        nclusters = max(clusterid + 1)    if transpose:        shape = (nrows, nclusters)    else:        shape = (nclusters, ncolumns)    cdata = np.zeros(shape, dtype=\"d\")    cmask = np.zeros(shape, dtype=\"intc\")    _cluster.clustercentroids(data, mask, clusterid, method, transpose, cdata, cmask)    return cdata, cmask",
        "documentation": "calculate and return the centroid of each cluster the clustercentroids routine calculates the cluster centroid given to which cluster each item belongs the centroid is defined a either the mean or the median over all item for each dimension keyword argument data nrows x ncolumns array containing the data value mask nrows x ncolumns array of integer showing which data are missing if maski j then datai j is missing clusterid array containing the cluster number for each item the cluster number should be nonnegative method specifies whether the centroid is calculated from the arithmetic mean method a default or the median method m over each dimension transpose if false each row contains the data for one item if true each column contains the data for one item return value cdata d array containing the cluster centroid if transpose is false then the dimension of cdata are nclusters x ncolumns if transpose is true then the dimension of cdata are nrows x nclusters cmask d array of integer describing which item in cdata if any are missing"
    },
    {
        "method_code": "def hash_collection(collection) -> int:        def make_hashable(item):                if isinstance(item, dict):            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))        elif isinstance(item, list):            return tuple(make_hashable(v) for v in item)        return item    hashable_collection = visit_collection(        collection, visit_fn=make_hashable, return_data=True    )    return hash(hashable_collection)",
        "documentation": "use visitcollection to transform and hash a collection args collection any the collection to hash return int the hash of the transformed collection example python from prefectawsutilities import hashcollection hashcollectiona b"
    },
    {
        "method_code": "def get_clean_data(self, prefix):                ret = {}        change_data = getattr(self, f'{prefix}_data') or {}        for k, v in change_data.items():            if k not in self.diff_exclude_fields and not k.startswith('_'):                ret[k] = v        return ret",
        "documentation": "return only the prepostchange attribute which are relevant for calculating a diff"
    },
    {
        "method_code": "def test_cable_validates_compatible_types(self):                interface1 = Interface.objects.get(device__name='TestDevice1', name='eth0')        powerport1 = PowerPort.objects.get(device__name='TestDevice2', name='psu1')        # An interface cannot be connected to a power port, for example        cable = Cable(a_terminations=[interface1], b_terminations=[powerport1])        with self.assertRaises(ValidationError):            cable.clean()",
        "documentation": "the clean method should have a check to ensure only compatible port type can be connected by a cable"
    },
    {
        "method_code": "def __init__(self):                self.database_name = []        self.posted_date = []        self.num_letters_in_database = []        self.num_sequences_in_database = []        self.ka_params = (None, None, None)        self.gapped = 0        self.ka_params_gap = (None, None, None)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def test_accept_range_none(package_server, tmp_path):        test_content = \"test content test content test content\"    host, port = package_server.getsockname()    url = f\"http://{host}:{port}/none-accept-ranges\"    expected_sha256 = hashlib.sha256(test_content.encode(\"utf-8\")).hexdigest()    # assert range request not supported    response = CondaSession().get(url, headers={\"Range\": \"bytes=10-\"})    assert response.status_code == 200    tmp_dir = tmp_path / \"sub\"    tmp_dir.mkdir()    filename = \"test-file\"    partial_file = Path(tmp_dir / f\"{filename}.partial\")    complete_file = Path(tmp_dir / filename)    partial_file.write_text(test_content[:12])    download_inner(url, complete_file, \"md5\", expected_sha256, 38, lambda x: x)    assert complete_file.read_text() == test_content    assert not partial_file.exists()    # What if the partial file was wrong? (Since this endpoint always returns    # 200 not 206, this doesn't test complete-download, then hash mismatch.    # Another test in test_fetch.py asserts that we check the hash.)    complete_file.unlink()    partial_file.write_text(\"wrong content\")    download_inner(url, complete_file, None, expected_sha256, len(test_content), None)    assert complete_file.read_text() == test_content    assert not partial_file.exists()",
        "documentation": "ensure when acceptranges is none we are able to truncate a partially downloaded file"
    },
    {
        "method_code": "def test_str_count_overlap_NN(self):                # Testing with self._examples        for seq in self._examples:            # Using search term NN as a string            self.assertEqual(seq.count_overlap(\"NN\"), 0)            self.assertEqual(seq.count_overlap(\"N\" * 13), 0)            # Using search term NN as a Seq            self.assertEqual(seq.count_overlap(Seq(\"NN\")), 0)            self.assertEqual(seq.count_overlap(Seq(\"N\" * 13)), 0)",
        "documentation": "check our countoverlap method using nn"
    },
    {
        "method_code": "def download_finished(self, pyfile):                if hasattr(pyfile.plugin, \"check_data\") and isinstance(            pyfile.plugin.check_data, dict        ):            data = pyfile.plugin.check_data.copy()        elif hasattr(pyfile.plugin, \"api_data\") and isinstance(            pyfile.plugin.api_data, dict        ):            data = pyfile.plugin.api_data.copy()        elif hasattr(pyfile.plugin, \"info\") and isinstance(pyfile.plugin.info, dict):            data = pyfile.plugin.info.copy()            # NOTE: Don't check file size until a similarity matcher will be implemented            data.pop(\"size\", None)        else:            return        pyfile.set_status(\"processing\")        if not pyfile.plugin.last_download:            self.check_failed(pyfile, None, \"No file downloaded\")        local_file = os.fsdecode(pyfile.plugin.last_download)        if not os.path.isfile(local_file):            self.check_failed(pyfile, None, \"File does not exist\")        #: Validate file size        if \"size\" in data:            api_size = int(data[\"size\"])            file_size = os.path.getsize(local_file)            if api_size != file_size:                self.log_warning(                    self._(\"File {} has incorrect size: {} B ({} expected)\").format(                        pyfile.name, file_size, api_size                    )                )                self.check_failed(pyfile, local_file, \"Incorrect file size\")            data.pop(\"size\", None)        #: Validate checksum        if data and self.config.get(\"check_checksum\"):            data[\"hash\"] = data.get(\"hash\", {})            for key in self.algorithms:                if data.get(key) and key not in data[\"hash\"]:                    data[\"hash\"][key] = data[key]                    break            if len(data[\"hash\"]) > 0:                for key in self.algorithms:                    if key in data[\"hash\"]:                        pyfile.set_custom_status(self._(\"checksum verifying\"))                        try:                            checksum = compute_checksum(                                local_file,                                key.replace(\"-\", \"\").lower(),                                progress_notify=pyfile.set_progress,                                abort=lambda: pyfile.abort,                            )                        finally:                            pyfile.set_status(\"processing\")                        if checksum is False:                            continue                        elif checksum is not None:                            if checksum.lower() == data[\"hash\"][key].lower():                                self.log_info(                                    self._(                                        'File integrity of \"{}\" verified by {} checksum ({})'                                    ).format(pyfile.name, key.upper(), checksum.lower())                                )                                pyfile.error = self._(\"checksum verified\")                                break                            else:                                self.log_warning(                                    self._(                                        \"{} checksum for file {} does not match ({} != {})\"                                    ).format(                                        key.upper(),                                        pyfile.name,                                        checksum,                                        data[\"hash\"][key].lower(),                                    )                                )                                self.check_failed(                                    pyfile, local_file, \"Checksums do not match\"                                )                        else:                            self.log_warning(                                self._(\"Unsupported hashing algorithm\"), key.upper()                            )                else:                    self.log_warning(                        self._('Unable to validate checksum for file: \"{}\"').format(                            pyfile.name                        )                    )",
        "documentation": "compute checksum for the downloaded file and compare it with the hash provided by the downloader pyfileplugincheckdata should be a dictionary which can contain a if known the exact filesize in byte eg size b hexadecimal hash string with algorithm name a key eg md ddffadd"
    },
    {
        "method_code": "def __init__(self):                # Condition variable, used to signal waiters of a change in object        # state.        self.__condition = Condition(Lock())        # Initialize with no writers.        self.__writer = None        self.__writercount = 0        self.__upgradewritercount = 0        self.__pendingwriters = []        # Initialize with no readers.        self.__readers = {}",
        "documentation": "initialize this readwrite lock"
    },
    {
        "method_code": "def _sub_clade(clade, term_names):        term_clades = [clade.find_any(name) for name in term_names]    sub_clade = clade.common_ancestor(term_clades)    if len(term_names) != sub_clade.count_terminals():        temp_clade = BaseTree.Clade()        temp_clade.clades.extend(term_clades)        for c in sub_clade.find_clades(terminal=False, order=\"preorder\"):            if c == sub_clade.root:                continue            children = set(c.find_clades(terminal=True)) & set(term_clades)            if children:                for tc in temp_clade.find_clades(terminal=False, order=\"preorder\"):                    tc_children = set(tc.clades)                    tc_new_clades = tc_children - children                    if children.issubset(tc_children) and tc_new_clades:                        tc.clades = list(tc_new_clades)                        child_clade = BaseTree.Clade()                        child_clade.clades.extend(list(children))                        tc.clades.append(child_clade)        sub_clade = temp_clade    return sub_clade",
        "documentation": "extract a compatible subclade that only contains the given terminal name private"
    },
    {
        "method_code": "def __repr__(self):                return \"Reaction(%r, %r, %r, %r)\" % (            self.reactants,            self.catalysts,            self.reversible,            self.data,        )",
        "documentation": "return a debugging string representation of self"
    },
    {
        "method_code": "def write_header(self, stream, alignments):                try:            metadata = alignments.metadata        except AttributeError:            metadata = {\"MAF Version\": \"1\"}        track_keys = (            \"name\",            \"description\",            \"frames\",            \"mafDot\",            \"visibility\",            \"speciesOrder\",        )        for key in track_keys:            if key in metadata:                self._write_trackline(stream, metadata)                break        stream.write(\"##maf\")        for key, value in metadata.items():            if key in track_keys:                continue            if key == \"Comments\":                continue            if key == \"MAF Version\":                if value != \"1\":                    raise ValueError(\"MAF version must be 1\")                key = \"version\"            elif key == \"Scoring\":                key = \"scoring\"            elif key == \"Program\":                key = \"program\"            else:                raise ValueError(\"Unexpected key '%s' for header\" % key)            stream.write(f\" {key}={value}\")        stream.write(\"\\n\")        comments = metadata.get(\"Comments\")        if comments is not None:            for comment in comments:                stream.write(f\"# {comment}\\n\")            stream.write(\"\\n\")",
        "documentation": "write the maf header"
    },
    {
        "method_code": "def test_no_crosstalk(self):                # IC_Chain.ParallelAssembleResidues = False        self.cif_4CUP.atom_to_internal_coordinates()        cpy4cup = copy.deepcopy(self.cif_4CUP)        cic0 = self.cif_4CUP.child_list[0].child_list[0].internal_coord        cic1 = cpy4cup.child_list[0].child_list[0].internal_coord        alist = [            \"omg\",            \"phi\",            \"psi\",            \"chi1\",            \"chi2\",            \"chi3\",            \"chi4\",            \"chi5\",            \"tau\",        ]        delta = 33  # degrees to change        tdelta = delta / 10.0  # more realistic for bond angle        targPos = 1        for ang in alist:            # skip by 2's with alist along original chain changing angle spec            ricTarg = cic0.chain.child_list[targPos].internal_coord            # print(targPos + 1, ricTarg.lc, ang)            targPos += 2            try:                edr = ricTarg.pick_angle(ang)                andx = edr.ndx                if ang == \"tau\":                    cic0.hedraAngle[andx] += tdelta                    cic0.hAtoms_needs_update[andx] = True                    cic0.atomArrayValid[cic0.h2aa[andx]] = False                    cic0.hAtoms_needs_update[:] = True                    cic0.atomArrayValid[:] = False                    cic0.dAtoms_needs_update[:] = True                else:                    cic0.dihedraAngle[andx] += delta                    if cic0.dihedraAngle[andx] > 180.0:                        cic0.dihedraAngle[andx] -= 360.0                    cic0.dihedraAngleRads[andx] = np.deg2rad(cic0.dihedraAngle[andx])                    cic0.dAtoms_needs_update[andx] = True                    cic0.atomArrayValid[cic0.d2aa[andx]] = False                    # test Dihedron.bits()                    pfd = IC_Residue.picFlagsDict                    if ricTarg.rbase[2] == \"P\" and ang == \"omg\":                        self.assertEqual(edr.bits(), (pfd[\"omg\"] | pfd[\"pomg\"]))                    else:                        self.assertEqual(edr.bits(), pfd[ang])            except AttributeError:                pass  # skip if residue does not have e.g. chi5        cic0.internal_to_atom_coordinates()  # move atoms        cic0.atom_to_internal_coordinates()  # get new internal coords        # generate hdelta and ddelta difference arrays so can look for what        # changed        hdelta = cic0.hedraAngle - cic1.hedraAngle        hdelta[np.abs(hdelta) < 0.00001] = 0.0        ddelta = cic0.dihedraAngle - cic1.dihedraAngle        ddelta[np.abs(ddelta) < 0.00001] = 0.0        ddelta[ddelta < -180.0] += 360.0  # wrap around circle values        targPos = 1        for ang in alist:            # same skip along original chain looking at hdelta and ddelta            # if change is as specified, set difference to 0 then we can test            # for any remaining (spurious) changes            ricTarg = cic0.chain.child_list[targPos].internal_coord            # print(targPos + 1, ricTarg.lc, ang)            targPos += 2            try:                andx = ricTarg.pick_angle(ang).ndx                if ang == \"tau\":                    self.assertAlmostEqual(hdelta[andx], tdelta, places=4)                    hdelta[andx] = 0.0                    # some other angle has to change to accommodate tau change                    # N-Ca-Cb is artifact of choices in ic_data                    # expected change so clear relevant hdelta here                    adjAngNdx = ricTarg.pick_angle(\"N:CA:CB\").ndx                    self.assertNotAlmostEqual(hdelta[adjAngNdx], 0.0, places=1)                    hdelta[adjAngNdx] = 0.0                else:                    self.assertAlmostEqual(ddelta[andx], delta, places=4)                    ddelta[andx] = 0.0            except AttributeError:                pass  # if residue does not have e.g. chi5        hsum = hdelta.sum()        self.assertEqual(hsum, 0.0)        dsum = ddelta.sum()        self.assertEqual(dsum, 0.0)        # test hedron len12, angle, len23 setters and getters        hed = list(cic0.hedra.values())[10]        val = hed.len12 + 0.5        hed.len12 = val        self.assertEqual(hed.len12, val)        val = hed.len23 + 0.5        hed.len23 = val        self.assertEqual(hed.len23, val)        val = hed.angle + 1        hed.angle = val        self.assertEqual(hed.angle, val)        dihed = list(cic0.dihedra.values())[10]        val = dihed.angle + 196        dihed.angle = val        if val > 180.0:            val -= 360.0        if val < -180.0:            val += 360.0        self.assertEqual(dihed.angle, val)",
        "documentation": "deep copy change few internal coords test nothing else change"
    },
    {
        "method_code": "def _abi_trim(seq_record):        start = False  # flag for starting position of trimmed sequence    segment = 20  # minimum sequence length    trim_start = 0  # init start index    cutoff = 0.05  # default cutoff value for calculating base score    if len(seq_record) <= segment:        return seq_record    else:        # calculate base score        score_list = [            cutoff - (10 ** (qual / -10.0))            for qual in seq_record.letter_annotations[\"phred_quality\"]        ]        # calculate cumulative score        # if cumulative value < 0, set it to 0        # first value is set to 0, because of the assumption that        # the first base will always be trimmed out        cummul_score = [0]        for i in range(1, len(score_list)):            score = cummul_score[-1] + score_list[i]            if score < 0:                cummul_score.append(0)            else:                cummul_score.append(score)                if not start:                    # trim_start = value when cumulative score is first > 0                    trim_start = i                    start = True        # trim_finish = index of highest cumulative score,        # marking the end of sequence segment with highest cumulative score        trim_finish = cummul_score.index(max(cummul_score))        return seq_record[trim_start:trim_finish]",
        "documentation": "trim the sequence using richard mott modified trimming algorithm private argument seqrecord seqrecord object to be trimmed trimmed base are determined from their segment score which is a cumulative sum of each base score base score are calculated from their quality value more about the trimming algorithm httpwwwphraporgphredphrapphredhtml httpresourcesqiagenbioinformaticscommanualsclcgenomicsworkbenchqualitytrimminghtml"
    },
    {
        "method_code": "def test_clade_to_phylogeny(self):                clade = self.phyloxml.phylogenies[0].clade[0]        tree = clade.to_phylogeny(rooted=True)        self.assertIsInstance(tree, PX.Phylogeny)",
        "documentation": "convert a clade object to a new phylogeny"
    },
    {
        "method_code": "def test_Phylogeny(self):                trees = list(PhyloXMLIO.parse(EX_PHYLO))        # Monitor lizards        self.assertEqual(trees[9].name, \"monitor lizards\")        self.assertEqual(trees[9].description, \"a pylogeny of some monitor lizards\")        self.assertTrue(trees[9].rooted)        # Network (unrooted)        self.assertEqual(            trees[6].name, \"network, node B is connected to TWO nodes: AB and C\"        )        self.assertFalse(trees[6].rooted)",
        "documentation": "instantiation of phylogeny object"
    },
    {
        "method_code": "def _construct_block_schema_spec_definitions(    root_block_schema,    block_schemas_with_references: List[        Tuple[BlockSchema, Optional[str], Optional[UUID]]    ],):        definitions = {}    for _, block_schema_references in root_block_schema.fields[        \"block_schema_references\"    ].items():        block_schema_references = (            block_schema_references            if isinstance(block_schema_references, list)            else [block_schema_references]        )        for block_schema_reference in block_schema_references:            child_block_schema = _find_block_schema_via_checksum(                block_schemas_with_references,                block_schema_reference[\"block_schema_checksum\"],            )            if child_block_schema is not None:                child_block_schema = _construct_full_block_schema(                    block_schemas_with_references=block_schemas_with_references,                    root_block_schema=child_block_schema,                )                definitions = _add_block_schemas_fields_to_definitions(                    definitions, child_block_schema                )    return definitions",
        "documentation": "construct field definition for a block schema based on the nested block schema a defined in the blockschemaswithreferences list"
    },
    {
        "method_code": "def _build_m3u_session_filename(basename):        date = datetime.datetime.now().strftime(\"%Y%m%d_%Hh%M\")    basename = re.sub(r\"(\\.m3u|\\.M3U)\", \"\", basename)    path = normpath(        os.path.join(            config[\"importfeeds\"][\"dir\"].as_filename(), f\"{basename}_{date}.m3u\"        )    )    return path",
        "documentation": "build unique mu filename by putting current date between given basename and file ending"
    },
    {
        "method_code": "def __iter__(self):                return iter(self.selected_child)",
        "documentation": "return the number of child"
    },
    {
        "method_code": "def Cancelling(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.CANCELLING, **kwargs)",
        "documentation": "convenience function for creating cancelling state return state a cancelling state"
    },
    {
        "method_code": "def test_no_matching_results(self):                # https://github.com/beetbox/beets/issues/4406        # expected return value None        lyrics = tekstowo.fetch(\"Kelly Bailey\", \"Black Mesa Inbound\")        assert lyrics is None",
        "documentation": "ensure we fetch nothing if there are search result returned but no match"
    },
    {
        "method_code": "def test_prefix(self):                result = self.lib.items(self.num_limit_prefix)        assert len(result) == self.num_limit",
        "documentation": "return the expected number with the query prefix"
    },
    {
        "method_code": "def render_pep440_post(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"] or pieces[\"dirty\"]:            rendered += \".post%d\" % pieces[\"distance\"]            if pieces[\"dirty\"]:                rendered += \".dev0\"            rendered += plus_or_dot(pieces)            rendered += \"g%s\" % pieces[\"short\"]    else:        # exception #1        rendered = \"0.post%d\" % pieces[\"distance\"]        if pieces[\"dirty\"]:            rendered += \".dev0\"        rendered += \"+g%s\" % pieces[\"short\"]    return rendered",
        "documentation": "tagpostdistancedevghex the dev mean dirty note that dev sort backwards a dirty tree will appear older than the corresponding clean one but you shouldnt be releasing software with dirty anyways exception no tag postdistancedev"
    },
    {
        "method_code": "def _playlist_or_paths(self, paths):                if config[\"play\"][\"raw\"]:            return paths        else:            return [self._create_tmp_playlist(paths)]",
        "documentation": "return either the raw path of item or a playlist of the item"
    },
    {
        "method_code": "def test_conflicting_lengths(self):                # Change the sequence length as indicated in the sequence packet        h = self.munge_buffer(0x1C, [0x00, 0x00, 0x20, 0x15])        with self.assertRaisesRegex(ValueError, \"Conflicting sequence length values\"):            SeqIO.read(h, \"gck\")        h.close()        # Change the sequence length as indicated in the features packet        h = self.munge_buffer(0x36, [0x00, 0x00, 0x20, 0x15])        with self.assertRaisesRegex(ValueError, \"Conflicting sequence length values\"):            SeqIO.read(h, \"gck\")        h.close()        # Change the number of features        h = self.munge_buffer(0x3B, 0x30)        with self.assertRaisesRegex(            ValueError, \"Features packet size inconsistent with number of features\"        ):            SeqIO.read(h, \"gck\")        h.close()        # Change the number of restriction sites        h = self.munge_buffer(0x137, 0x30)        with self.assertRaisesRegex(            ValueError, \"Sites packet size inconsistent with number of sites\"        ):            SeqIO.read(h, \"gck\")        h.close()",
        "documentation": "read a file with incorrect length"
    },
    {
        "method_code": "def stable_hash(*args: Union[str, bytes], hash_algo=_md5) -> str:        h = hash_algo()    for a in args:        if isinstance(a, str):            a = a.encode()        h.update(a)    return h.hexdigest()",
        "documentation": "given some argument produce a stable bit hash of their content support byte and string string will be utf encoded args args item to include in the hash hashalgo hash algorithm from hashlib to use return a hex hash"
    },
    {
        "method_code": "def close(self):                return self.conn.close()",
        "documentation": "close the connection no further activity possible"
    },
    {
        "method_code": "def write(self, file, encoding=DEFAULT_ENCODING, indent=True):                if indent:            _indent(self._tree.getroot())        self._tree.write(file, encoding)        return len(self._tree.getroot())",
        "documentation": "write phyloxml to a file"
    },
    {
        "method_code": "def test_post_command_not_invoked(post_command_plugin, conda_cli):        conda_cli(\"config\")    assert len(post_command_plugin.post_command_action.mock_calls) == 0",
        "documentation": "make sure that we successfully did not invoke our postcommand action"
    },
    {
        "method_code": "def temporary_database_config(tmp_database_config: BaseDatabaseConfiguration):        starting_config = MODELS_DEPENDENCIES[\"database_config\"]    try:        MODELS_DEPENDENCIES[\"database_config\"] = tmp_database_config        yield    finally:        MODELS_DEPENDENCIES[\"database_config\"] = starting_config",
        "documentation": "temporarily override the prefect rest api database configuration when the context is closed the existing database configuration will be restored args tmpdatabaseconfig prefect rest api database configuration to inject"
    },
    {
        "method_code": "def get_sigatm(self):                return self.sigatm_array",
        "documentation": "return standard deviation of atomic parameter"
    },
    {
        "method_code": "def _assign_atom_mass(self):                try:            return IUPACData.atom_weights[self.element.capitalize()]        except (AttributeError, KeyError):            return float(\"NaN\")",
        "documentation": "return atom weight private"
    },
    {
        "method_code": "def createAnalysis(self, seq_str, batch_ary):                rb = Restriction.RestrictionBatch(batch_ary)        seq = Seq(seq_str)        return Restriction.Analysis(rb, seq)",
        "documentation": "restrictionanalysis creation helper method"
    },
    {
        "method_code": "def write_record(self, record: SeqRecord) -> None:                self._record_written = True        # TODO - Is an empty sequence allowed in FASTQ format?        seq = record.seq        if seq is None:            raise ValueError(f\"No sequence for record {record.id}\")        qualities_str = _get_illumina_quality_str(record)        if len(qualities_str) != len(seq):            raise ValueError(                \"Record %s has sequence length %i but %i quality scores\"                % (record.id, len(seq), len(qualities_str))            )        # FASTQ files can include a description, just like FASTA files        # (at least, this is what the NCBI Short Read Archive does)        id_ = self.clean(record.id) if record.id else \"\"        description = self.clean(record.description)        if description and description.split(None, 1)[0] == id_:            # The description includes the id at the start            title = description        elif description:            title = f\"{id_} {description}\"        else:            title = id_        self.handle.write(f\"@{title}\\n{seq}\\n+\\n{qualities_str}\\n\")",
        "documentation": "write a single fastq record to the file"
    },
    {
        "method_code": "def test_get_channel_notice_response_timeout_error(    notices_cache_dir, notices_mock_fetch_get_session):        with patch(\"conda.notices.fetch.logger\") as mock_logger:        notices_mock_fetch_get_session().get.side_effect = requests.exceptions.Timeout        channel_notice_set = retrieve_notices()        display_notices(channel_notice_set)        for mock_call in mock_logger.mock_calls:            assert \"Request timed out for channel\" in str(mock_call)",
        "documentation": "test the timeout error case for the getchannelnoticeresponse function"
    },
    {
        "method_code": "def supplier_list(cls):                return []",
        "documentation": "return a list of supplier of the enzyme"
    },
    {
        "method_code": "def test_genic_diff_pair(        self, fname, dememorization=10000, batches=20, iterations=5000    ):                raise NotImplementedError",
        "documentation": "provision for genic differentiation for all population pair"
    },
    {
        "method_code": "def _build_query_with_filter(self, name, filter_string):                        type_class = get_graphql_type_for_model(self.model)            # Compile list of fields to include            fields_string = ''            file_fields = (strawberry_django.fields.types.DjangoFileType, strawberry_django.fields.types.DjangoImageType)            for field in type_class.__strawberry_definition__.fields:                if (                    field.type in file_fields or (                        type(field.type) is StrawberryOptional and field.type.of_type in file_fields                    )                ):                    # image / file fields nullable or not...                    fields_string += f'{field.name} {{ name }}\\n'                elif type(field.type) is StrawberryList and type(field.type.of_type) is LazyType:                    # List of related objects (queryset)                    fields_string += f'{field.name} {{ id }}\\n'                elif type(field.type) is StrawberryList and type(field.type.of_type) is StrawberryUnion:                    # this would require a fragment query                    continue                elif type(field.type) is StrawberryUnion:                    # this would require a fragment query                    continue                elif type(field.type) is StrawberryOptional and type(field.type.of_type) is StrawberryUnion:                    # this would require a fragment query                    continue                elif type(field.type) is StrawberryOptional and type(field.type.of_type) is LazyType:                    fields_string += f'{field.name} {{ id }}\\n'                elif hasattr(field, 'is_relation') and field.is_relation:                    # Note: StrawberryField types do not have is_relation                    fields_string += f'{field.name} {{ id }}\\n'                elif inspect.isclass(field.type) and issubclass(field.type, IPAddressFamilyType):                    fields_string += f'{field.name} {{ value, label }}\\n'                else:                    fields_string += f'{field.name}\\n'            query = f            return query",
        "documentation": "called by either buildquery or buildfilteredquery construct the actual query given a name and filter string"
    },
    {
        "method_code": "def test_30_hmmscan_010(self):                hmmer_file = get_file(\"text_30_hmmscan_010.out\")        qresults = list(parse(hmmer_file, FMT))        # test the Hit object without HSPs        hit = qresults[0][-1]        self.assertFalse(hit)        self.assertEqual(\"NRPS-COM_Cterm\", hit.id)        self.assertEqual(\"\", hit.description)        self.assertEqual(\"bpsA\", hit.query_id)        self.assertEqual(\"<unknown description>\", hit.query_description)        self.assertEqual(4.4e-11, hit.evalue)        self.assertEqual(33.6, hit.bitscore)        self.assertEqual(10.2, hit.bias)        self.assertEqual(2.9, hit.domain_exp_num)        self.assertEqual(0, hit.domain_obs_num)        self.assertEqual(0, len(hit))",
        "documentation": "parsing hmmscan texthmmscan"
    },
    {
        "method_code": "def setUp(self):                output_filename = os.path.join(\"Graphics\", \"square_test.pdf\")        self.c = Canvas(output_filename, pagesize=(500, 500))",
        "documentation": "set up canvas for drawing"
    },
    {
        "method_code": "def enqueue_event(queue, instance, user, request_id, event_type):        # Determine whether this type of object supports event rules    app_label = instance._meta.app_label    model_name = instance._meta.model_name    if model_name not in registry['model_features']['event_rules'].get(app_label, []):        return    assert instance.pk is not None    key = f'{app_label}.{model_name}:{instance.pk}'    if key in queue:        queue[key]['data'] = serialize_for_event(instance)        queue[key]['snapshots']['postchange'] = get_snapshots(instance, event_type)['postchange']        # If the object is being deleted, update any prior \"update\" event to \"delete\"        if event_type == OBJECT_DELETED:            queue[key]['event_type'] = event_type    else:        queue[key] = {            'object_type': ContentType.objects.get_for_model(instance),            'object_id': instance.pk,            'event_type': event_type,            'data': serialize_for_event(instance),            'snapshots': get_snapshots(instance, event_type),            'username': user.username,            'request_id': request_id        }",
        "documentation": "enqueue a serialized representation of a createdupdateddeleted object for the processing of event once the request ha completed"
    },
    {
        "method_code": "def order_package(self, package_id, position):                self.pyload.files.reorder_package(package_id, position)",
        "documentation": "give a package a new position param packageid package id param position"
    },
    {
        "method_code": "def train(    training_set,    results,    feature_fns,    update_fn=None,    max_iis_iterations=10000,    iis_converge=1.0e-5,    max_newton_iterations=100,    newton_converge=1.0e-10,):        if not training_set:        raise ValueError(\"No data in the training set.\")    if len(training_set) != len(results):        raise ValueError(\"training_set and results should be parallel lists.\")    # Rename variables for convenience.    xs, ys = training_set, results    # Get a list of all the classes that need to be trained.    classes = sorted(set(results))    # Cache values for all features.    features = [_eval_feature_fn(fn, training_set, classes) for fn in feature_fns]    # Cache values for f#.    f_sharp = _calc_f_sharp(len(training_set), len(classes), features)    # Pre-calculate the empirical expectations of the features.    e_empirical = _calc_empirical_expects(xs, ys, classes, features)    # Now train the alpha parameters to weigh each feature.    alphas = [0.0] * len(features)    iters = 0    while iters < max_iis_iterations:        nalphas = _train_iis(            xs,            classes,            features,            f_sharp,            alphas,            e_empirical,            max_newton_iterations,            newton_converge,        )        diff = [np.fabs(x - y) for x, y in zip(alphas, nalphas)]        diff = reduce(np.add, diff, 0)        alphas = nalphas        me = MaxEntropy()        me.alphas, me.classes, me.feature_fns = alphas, classes, feature_fns        if update_fn is not None:            update_fn(me)        if diff < iis_converge:  # converged            break    else:        raise RuntimeError(\"IIS did not converge\")    return me",
        "documentation": "train a maximum entropy classifier return maxentropy object train a maximum entropy classifier on a training set trainingset is a list of observation result is a list of the class assignment for each observation featurefns is a list of the feature these are callback function that take an observation and class and return a or updatefn is a callback function that is called at each training iteration it is passed a maxentropy object that encapsulates the current state of the training the maximum number of iteration and the convergence criterion for ii are given by maxiisiterations and iisconverge respectively while maxnewtoniterations and newtonconverge are the maximum number of iteration and the convergence criterion for newton method"
    },
    {
        "method_code": "def unescape(text):        if isinstance(text, bytes):        text = text.decode(\"utf-8\", \"ignore\")    out = text.replace(\"&nbsp;\", \" \")    def replchar(m):        num = m.group(1)        return unichar(int(num))    out = re.sub(\"&#(\\\\d+);\", replchar, out)    return out",
        "documentation": "resolve xxx html entity and some others"
    },
    {
        "method_code": "def test_setitem_wrong_query_id(self):                # item assignment should fail if the hit object does not have the same        # query id        self.assertRaises(ValueError, self.qresult.__setitem__, \"hit4\", hit12)",
        "documentation": "test queryresultsetitem wrong query id"
    },
    {
        "method_code": "def test_cappbuilder_real(self):                ppb = CaPPBuilder()        pp = ppb.build_peptides(self.structure)        pp0_seq = pp[0].get_sequence()        pp1_seq = pp[1].get_sequence()        pp2_seq = pp[2].get_sequence()        self.assertEqual(pp0_seq, \"DIRQGPKEPFRDYVDRFYKTLRAEQASQEVKNW\")        self.assertEqual(pp1_seq, \"TETLLVQNANPDCKTILKALGPGATLEE\")        self.assertEqual(pp2_seq, \"TACQG\")        self.assertEqual(            [ca.serial_number for ca in pp[0].get_ca_list()],            [                10,                18,                26,                37,                46,                50,                57,                66,                75,                82,                93,                104,                112,                124,                131,                139,                150,                161,                173,                182,                189,                197,                208,                213,                222,                231,                236,                242,                251,                260,                267,                276,                284,            ],        )",
        "documentation": "test cappbuilder on real pdb file"
    },
    {
        "method_code": "def _get_kappa_t(pi, TV, t=False):        pi[\"Y\"] = pi[\"T\"] + pi[\"C\"]    pi[\"R\"] = pi[\"A\"] + pi[\"G\"]    A = (        2 * (pi[\"T\"] * pi[\"C\"] + pi[\"A\"] * pi[\"G\"])        + 2        * (            pi[\"T\"] * pi[\"C\"] * pi[\"R\"] / pi[\"Y\"]            + pi[\"A\"] * pi[\"G\"] * pi[\"Y\"] / pi[\"R\"]        )        * (1 - TV[1] / (2 * pi[\"Y\"] * pi[\"R\"]))        - TV[0]    ) / (2 * (pi[\"T\"] * pi[\"C\"] / pi[\"Y\"] + pi[\"A\"] * pi[\"G\"] / pi[\"R\"]))    B = 1 - TV[1] / (2 * pi[\"Y\"] * pi[\"R\"])    a = -0.5 * log(A)  # this seems to be an error in YANG's original paper    b = -0.5 * log(B)    kappaF84 = a / b - 1    if t is False:        kappaHKY85 = 1 + (            pi[\"T\"] * pi[\"C\"] / pi[\"Y\"] + pi[\"A\"] * pi[\"G\"] / pi[\"R\"]        ) * kappaF84 / (pi[\"T\"] * pi[\"C\"] + pi[\"A\"] * pi[\"G\"])        return kappaHKY85    else:        t = (            4 * pi[\"T\"] * pi[\"C\"] * (1 + kappaF84 / pi[\"Y\"])            + 4 * pi[\"A\"] * pi[\"G\"] * (1 + kappaF84 / pi[\"R\"])            + 4 * pi[\"Y\"] * pi[\"R\"]        ) * b        return t",
        "documentation": "calculate kappa private the following formula and variable name are according to pmid"
    },
    {
        "method_code": "def disable_sqlite_foreign_keys(context):        if dialect.name == \"sqlite\":        context.execute(\"COMMIT\")        context.execute(\"PRAGMA foreign_keys=OFF\")        context.execute(\"BEGIN IMMEDIATE\")    yield    if dialect.name == \"sqlite\":        context.execute(\"END\")        context.execute(\"PRAGMA foreign_keys=ON\")        context.execute(\"BEGIN IMMEDIATE\")",
        "documentation": "disable foreign key constraint on sqlite"
    },
    {
        "method_code": "def test_output013(self):                m10_file = get_file(\"output013.m10\")        qresults = list(parse(m10_file, FMT))        self.assertEqual(3, len(qresults))        # check common attributes        for qresult in qresults:            for hit in qresult:                self.assertEqual(qresult.id, hit.query_id)                for hsp in hit:                    self.assertEqual(hit.id, hsp.hit_id)                    self.assertEqual(qresult.id, hsp.query_id)        # test first qresult        qresult = qresults[0]        self.assertEqual(\"random_s00\", qresult.id)        self.assertEqual(\"fasta\", qresult.program)        self.assertEqual(\"36.3.5c\", qresult.version)        self.assertEqual(\"protlib.fasta\", qresult.target)        self.assertEqual(16, qresult.seq_len)        self.assertEqual(\"\", qresult.description)        self.assertEqual(0, len(qresult))        # test second qresult        qresult = qresults[1]        self.assertEqual(\"sp|Q9Y2H6|68-133\", qresult.id)        self.assertEqual(\"fasta\", qresult.program)        self.assertEqual(\"36.3.5c\", qresult.version)        self.assertEqual(\"protlib.fasta\", qresult.target)        self.assertEqual(66, qresult.seq_len)        self.assertEqual(\"\", qresult.description)        self.assertEqual(1, len(qresult))        # second qresult, first hit        hit = qresult[0]        self.assertEqual(\"gi|291391832|ref|XP_002712264.1|\", hit.id)        self.assertEqual(\"PREDICTED: titin [Oryctolagus cuniculus]\", hit.description)        self.assertEqual(33406, hit.seq_len)        self.assertEqual(1, len(hit))        # second qresult, first hit, first hsp        hsp = qresult[0].hsps[0]        self.assertEqual(98, hsp.initn_score)        self.assertEqual(98, hsp.init1_score)        self.assertEqual(109, hsp.opt_score)        self.assertEqual(95.1, hsp.z_score)        self.assertEqual(30.2, hsp.bitscore)        self.assertEqual(0.43, hsp.evalue)        self.assertEqual(109, hsp.sw_score)        self.assertAlmostEqual(26.8, hsp.ident_pct)        self.assertAlmostEqual(54.9, hsp.pos_pct)        self.assertEqual(71, hsp.aln_span)        self.assertEqual(0, hsp.query_start)        self.assertEqual(66, hsp.query_end)        self.assertEqual(            \"PNGSVPPIY-----VPPGYAPQVIEDNGVRRVVVVPQAPEFHPGSHTVLHRSPHPPLPGFIPVPTMMPPPP\",            hsp.query.seq,        )        self.assertEqual(10704, hsp.hit_start)        self.assertEqual(10775, hsp.hit_end)        self.assertEqual(            \"PEKKVPPAVPKKPEAPPAKVPEAPKEVVPEKKIAVPKKPEVPPAKVPEVPKKPVIEEKPVIPVPKKVESPP\",            hsp.hit.seq,        )        self.assertEqual(0, hsp.query_strand)        # test third qresult        qresult = qresults[2]        self.assertEqual(\"sp|Q9Y2H6|265-345\", qresult.id)        self.assertEqual(\"fasta\", qresult.program)        self.assertEqual(\"36.3.5c\", qresult.version)        self.assertEqual(\"protlib.fasta\", qresult.target)        self.assertEqual(81, qresult.seq_len)        self.assertEqual(\"\", qresult.description)        self.assertEqual(4, len(qresult))        # third qresult, first hit        hit = qresult[0]        self.assertEqual(\"gi|260806189|ref|XP_002597967.1|\", hit.id)        self.assertEqual(            \"hypothetical protein BRAFLDRAFT_79792 [Branchiostoma floridae]\",            hit.description,        )        self.assertEqual(23830, hit.seq_len)        self.assertEqual(1, len(hit))        # third qresult, first hit, first hsp        hsp = qresult[0].hsps[0]        self.assertEqual(220, hsp.initn_score)        self.assertEqual(62, hsp.init1_score)        self.assertEqual(92, hsp.opt_score)        self.assertEqual(97.4, hsp.z_score)        self.assertEqual(30.5, hsp.bitscore)        self.assertEqual(0.32, hsp.evalue)        self.assertEqual(92, hsp.sw_score)        self.assertAlmostEqual(31.6, hsp.ident_pct)        self.assertAlmostEqual(60.8, hsp.pos_pct)        self.assertEqual(79, hsp.aln_span)        self.assertEqual(1, hsp.query_start)        self.assertEqual(79, hsp.query_end)        self.assertEqual(            \"LSNIVKPVASDIQARTVVLTWSPPSSLINGETDESSVPELYGYEVLISSTGKDGKYKSVYVG-EETNITLNDLKPAMDY\",            hsp.query.seq,        )        self.assertEqual(22430, hsp.hit_start)        self.assertEqual(22499, hsp.hit_end)        self.assertEqual(            \"VSNI-RPAASDISPHTLTLTWDTP------EDDGGSLITSYVVEMFDVS---DGKWQTLTTTCRRPPYPVKGLNPSATY\",            hsp.hit.seq,        )        self.assertEqual(0, hsp.query_strand)        # third qresult, second hit        hit = qresult[1]        self.assertEqual(\"gi|348553521|ref|XP_003462575.1|\", hit.id)        self.assertEqual(            \"PREDICTED: receptor-type tyrosine-protein phosphatase F isoform 1 [Cavia porcellus]\",            hit.description,        )        self.assertEqual(1899, hit.seq_len)        self.assertEqual(1, len(hit))        # third qresult, second hit, first hsp        hsp = qresult[1].hsps[0]        self.assertEqual(104, hsp.initn_score)        self.assertEqual(75, hsp.init1_score)        self.assertEqual(75, hsp.opt_score)        self.assertEqual(96.6, hsp.z_score)        self.assertEqual(26.7, hsp.bitscore)        self.assertEqual(0.36, hsp.evalue)        self.assertEqual(75, hsp.sw_score)        self.assertAlmostEqual(32.4, hsp.ident_pct)        self.assertAlmostEqual(64.9, hsp.pos_pct)        self.assertEqual(37, hsp.aln_span)        self.assertEqual(43, hsp.query_start)        self.assertEqual(80, hsp.query_end)        self.assertEqual(\"YEVLISSTGKDGKYKSVYVGEETNITLNDLKPAMDYH\", hsp.query.seq)        self.assertEqual(542, hsp.hit_start)        self.assertEqual(579, hsp.hit_end)        self.assertEqual(\"YELVYWAAEEEGQQRKVTFDPTSSYTLEDLKPDTLYH\", hsp.hit.seq)        self.assertEqual(0, hsp.query_strand)        # third qresult, third hit        hit = qresult[2]        self.assertEqual(\"gi|348553523|ref|XP_003462576.1|\", hit.id)        self.assertEqual(            \"PREDICTED: receptor-type tyrosine-protein phosphatase F isoform 2 [Cavia porcellus]\",            hit.description,        )        self.assertEqual(1908, hit.seq_len)        self.assertEqual(1, len(hit))        # third qresult, third hit, first hsp        hsp = qresult[2].hsps[0]        self.assertEqual(104, hsp.initn_score)        self.assertEqual(75, hsp.init1_score)        self.assertEqual(75, hsp.opt_score)        self.assertEqual(96.6, hsp.z_score)        self.assertEqual(26.7, hsp.bitscore)        self.assertEqual(0.36, hsp.evalue)        self.assertEqual(75, hsp.sw_score)        self.assertAlmostEqual(32.4, hsp.ident_pct)        self.assertAlmostEqual(64.9, hsp.pos_pct)        self.assertEqual(37, hsp.aln_span)        self.assertEqual(43, hsp.query_start)        self.assertEqual(80, hsp.query_end)        self.assertEqual(\"YEVLISSTGKDGKYKSVYVGEETNITLNDLKPAMDYH\", hsp.query.seq)        self.assertEqual(542, hsp.hit_start)        self.assertEqual(579, hsp.hit_end)        self.assertEqual(\"YELVYWAAEEEGQQRKVTFDPTSSYTLEDLKPDTLYH\", hsp.hit.seq)        self.assertEqual(0, hsp.query_strand)        # third qresult, fourth hit        hit = qresult[3]        self.assertEqual(\"gi|221124183|ref|XP_002154464.1|\", hit.id)        self.assertEqual(            \"PREDICTED: similar to FAD104 [Hydra magnipapillata]\", hit.description        )        self.assertEqual(860, hit.seq_len)        self.assertEqual(1, len(hit))        # third qresult, fourth hit, first hsp        hsp = qresult[3].hsps[0]        self.assertEqual(85, hsp.initn_score)        self.assertEqual(66, hsp.init1_score)        self.assertEqual(70, hsp.opt_score)        self.assertEqual(95.1, hsp.z_score)        self.assertEqual(25.3, hsp.bitscore)        self.assertEqual(0.43, hsp.evalue)        self.assertEqual(70, hsp.sw_score)        self.assertAlmostEqual(27.1, hsp.ident_pct)        self.assertAlmostEqual(58.6, hsp.pos_pct)        self.assertEqual(70, hsp.aln_span)        self.assertEqual(9, hsp.query_start)        self.assertEqual(79, hsp.query_end)        self.assertEqual(            \"ASDIQARTVVLTWSPPSSLINGETDESSVPELYGYEVLISSTGKDGKYKSVYVGEETNITLNDLKPAMDY\",            hsp.query.seq,        )        self.assertEqual(615, hsp.hit_start)        self.assertEqual(673, hsp.hit_end)        self.assertEqual(            \"ASSISYHSIKLKWGHQSS-------KKSI-----LNHTLQMQNKSGSFNTVYSGMDTSFTLSKLKELTPY\",            hsp.hit.seq,        )        self.assertEqual(0, hsp.query_strand)",
        "documentation": "test parsing fasta output outputm"
    },
    {
        "method_code": "def draw_tick(self, tickpos, ctr, ticklen, track, draw_label):                # Calculate tick coordinates        tickangle, tickcos, ticksin = self.canvas_angle(tickpos)        x0, y0 = self.xcenter + ctr * ticksin, self.ycenter + ctr * tickcos        x1, y1 = (            self.xcenter + (ctr + ticklen) * ticksin,            self.ycenter + (ctr + ticklen) * tickcos,        )        # Calculate height of text label so it can be offset on lower half        # of diagram        # LP: not used, as not all fonts have ascent_descent data in reportlab.pdfbase._fontdata        # label_offset = _fontdata.ascent_descent[track.scale_font][0]*\\        #               track.scale_fontsize/1000.        tick = Line(x0, y0, x1, y1, strokeColor=track.scale_color)        if draw_label:            # Put tick position on as label            if track.scale_format == \"SInt\":                if tickpos >= 1000000:                    tickstring = str(tickpos // 1000000) + \" Mbp\"                elif tickpos >= 1000:                    tickstring = str(tickpos // 1000) + \" Kbp\"                else:                    tickstring = str(tickpos)            else:                tickstring = str(tickpos)            label = String(                0,                0,                tickstring,  # Make label string                fontName=track.scale_font,                fontSize=track.scale_fontsize,                fillColor=track.scale_color,            )            if tickangle > pi:                label.textAnchor = \"end\"            # LP: This label_offset depends on ascent_descent data, which is not available for all            # fonts, so has been deprecated.            # if 0.5*pi < tickangle < 1.5*pi:            #    y1 -= label_offset            labelgroup = Group(label)            labelgroup.transform = (1, 0, 0, 1, x1, y1)        else:            labelgroup = None        return tick, labelgroup",
        "documentation": "return drawing element for a tick on the scale argument tickpos int position of the tick on the sequence ctr float y coord of the center of the track ticklen how long to draw the tick track track the track the tick is drawn on drawlabel boolean write the tick label"
    },
    {
        "method_code": "def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ):                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"Fastq\")        self._data = FastqGeneralIterator(self.stream)",
        "documentation": "iterate over fastq record a seqrecord object argument source input stream opened in text mode or a path to a file alphabet optional alphabet no longer used leave a none for each sequence in a sanger style fastq file there is a matching string encoding the phred quality integer between and about using ascii value with an offset of for example consider a file containing three short read easr cccttcttgtcttcagcgtttctcc easr ttggcaggccaaggccgatggatca easr gttgcttctggcgtgggtggggggg for each sequence eg cccttcttgtcttcagcgtttctcc there is a matching string encoding the phred quality using a ascii value with an offset of eg using this module directly you might run with openqualityexamplefastq a handle for record in fastqphrediteratorhandle print s recordid recordseq easr cccttcttgtcttcagcgtttctcc easr ttggcaggccaaggccgatggatca easr gttgcttctggcgtgggtggggggg typically however you would call this via bioseqio instead with fastq or fastqsanger a the format from bio import seqio with openqualityexamplefastq a handle for record in seqioparsehandle fastq print s recordid recordseq easr cccttcttgtcttcagcgtttctcc easr ttggcaggccaaggccgatggatca easr gttgcttctggcgtgggtggggggg if you want to look at the quality they are record in each record perletterannotation dictionary a a simple list of integer printrecordletterannotationsphredquality to modify the record returned by the parser you can use a generator function for example to store the mean phred quality in the record description use from statistic import mean def modifyrecordsrecords for record in record recorddescription meanrecordletterannotationsphredquality yield record with openqualityexamplefastq a handle for record in modifyrecordsfastqphrediteratorhandle printrecordid recorddescription easr easr easr"
    },
    {
        "method_code": "def test_writing_psl_34_003(self):                path = \"Blat/psl_34_003.psl\"        with open(path) as stream:            original_data = stream.read()        alignments = Align.parse(path, \"psl\")        stream = StringIO()        n = Align.write(alignments, stream, \"psl\")        self.assertEqual(n, 3)        stream.seek(0)        written_data = stream.read()        stream.close()        self.assertEqual(original_data, written_data)",
        "documentation": "test writing the alignment in pslpsl"
    },
    {
        "method_code": "def import_from(dotted_path: str):        try:        module_path, object_name = dotted_path.rsplit(\":\", 1)        module = importlib.import_module(module_path)        return getattr(module, object_name)    except ValueError as exc:        if \"not enough values to unpack\" in str(exc):            raise ValueError(                \"Invalid dotted path format. Did you mean 'module.submodule:object_name' instead of 'module.submodule.object_name'?\"            ) from exc    except Exception as exc:        raise exc",
        "documentation": "import an object from a dotted path this function dynamically import an object such a a class function or variable from a specified module using a dotted path the dotted path should be in the format modulesubmoduleobjectname where modulesubmodule is the full module path and objectname is the name of the object to be imported from that module args dottedpath str a string representing the module and object to import separated by a colon for example pathtomoduleobjectname return object the imported object specified by the dotted path raise modulenotfounderror if the module specified in the dotted path cannot be found attributeerror if the object specified in the dotted path doe not exist in the module valueerror if the dotted path is not in the correct format this can happen if the dotted path is missing the colon separator raising this error will provide a helpful message to the developer who add new object to movedinv or removedinv without following the correct format example to import the myclass class from the mypackagemymodule module you would use python myclass importfrommypackagemymodulemyclass equivalent to python from mypackagemymodule import myclass"
    },
    {
        "method_code": "def task_run_logger(    task_run: \"TaskRun\",    task: \"Task\" = None,    flow_run: \"FlowRun\" = None,    flow: \"Flow\" = None,    **kwargs: str,):        if not flow_run or not flow:        flow_run_context = prefect.context.FlowRunContext.get()        if flow_run_context:            flow_run = flow_run or flow_run_context.flow_run            flow = flow or flow_run_context.flow    return PrefectLogAdapter(        get_logger(\"prefect.task_runs\"),        extra={            **{                \"task_run_id\": str(task_run.id),                \"flow_run_id\": str(task_run.flow_run_id),                \"task_run_name\": task_run.name,                \"task_name\": task.name if task else \"<unknown>\",                \"flow_run_name\": flow_run.name if flow_run else \"<unknown>\",                \"flow_name\": flow.name if flow else \"<unknown>\",            },            **kwargs,        },    )",
        "documentation": "create a task run logger with the run metadata attached additional keyword argument can be provided to attach custom data to the log record if the task run context is available see getrunlogger instead if only the flow run context is available it will be used for default value of flowrun and flow"
    },
    {
        "method_code": "def _make_random_points(self, num_two_d_lists):                plot_info = []        random.seed(num_two_d_lists)  # for reproducibility        for two_d_list in range(num_two_d_lists):            cur_list = []            num_points = random.randrange(self.min_num_points, self.max_num_points)            for point in range(num_points):                x_point = random.randrange(self.min_point_num, self.max_point_num)                y_point = random.randrange(self.min_point_num, self.max_point_num)                cur_list.append((x_point, y_point))            plot_info.append(cur_list)        return plot_info",
        "documentation": "make a bunch of random point for testing plot"
    },
    {
        "method_code": "def test_clean_logfiles(    clear_cache,    test_recipes_channel: Path,    conda_cli: CondaCLIFixture,    tmp_env: TmpEnvFixture,    tmp_pkgs_dir: Path,):        pkg = \"small-executable\"    # logfiles don't exist ahead of time    assert not _get_logfiles(tmp_pkgs_dir)    with tmp_env(pkg):        # mimic logfiles being created        logs_dir = Path(tmp_pkgs_dir, CONDA_LOGS_DIR)        logs_dir.mkdir(parents=True, exist_ok=True)        path = logs_dir / f\"{datetime.utcnow():%Y%m%d-%H%M%S-%f}.log\"        path.touch()        # logfiles exist        assert path in _get_logfiles(tmp_pkgs_dir)        # --json flag is regression test for #5451        stdout, _, _ = conda_cli(\"clean\", \"--logfiles\", \"--yes\", \"--json\")        json.loads(stdout)  # assert valid json        # logfiles removed        assert not _get_logfiles(tmp_pkgs_dir)    # logfiles still removed    assert not _get_logfiles(tmp_pkgs_dir)",
        "documentation": "logfiles are found in pkgsdirlogs since these log file were uniquely created during the experimental phase of the condalibmambasolver"
    },
    {
        "method_code": "def write_alignments(self, alignments, output, reductions, extra_indices):                itemsPerSlot = self.itemsPerSlot        chromId = -1        itemIx = 0        sectionStartIx = 0        sectionEndIx = 0        currentChrom = None        regions = []        if self.compress is True:            buffer = _ZippedStream()        else:            buffer = BytesIO()        maxBlockSize = 0        # Supplemental Table 12: Binary BED-data format        # chromId     4 bytes, unsigned        # chromStart  4 bytes, unsigned        # chromEnd    4 bytes, unsigned        # rest        zero-terminated string in tab-separated format        formatter = struct.Struct(\"=III\")        done = False        region = None        alignments = iter(alignments)        while True:            try:                alignment = next(alignments)            except StopIteration:                itemIx = itemsPerSlot                done = True            else:                chrom, start, end, rest = self._extract_fields(alignment)                if chrom != currentChrom:                    if currentChrom is not None:                        itemIx = itemsPerSlot                    currentChrom = chrom                    chromId += 1                    reductions[\"end\"] = 0            if itemIx == itemsPerSlot:                blockStartOffset = output.tell()                size = buffer.tell()                if size > maxBlockSize:                    maxBlockSize = size                data = buffer.getvalue()                output.write(data)                buffer.seek(0)                buffer.truncate(0)                if extra_indices:                    blockEndOffset = output.tell()                    blockSize = blockEndOffset - blockStartOffset                    for extra_index in extra_indices:                        extra_index.addOffsetSize(                            blockStartOffset,                            blockSize,                            sectionStartIx,                            sectionEndIx,                        )                    sectionStartIx = sectionEndIx                region.offset = blockStartOffset                if done is True:                    break                itemIx = 0            if itemIx == 0:                region = _Region(chromId, start, end)                regions.append(region)            elif end > region.end:                region.end = end            itemIx += 1            for row in reductions:                if start >= row[\"end\"]:                    row[\"size\"] += 1                    row[\"end\"] = start + row[\"scale\"]                while end > row[\"end\"]:                    row[\"size\"] += 1                    row[\"end\"] += row[\"scale\"]            if extra_indices:                for extra_index in extra_indices:                    extra_index.addKeysFromRow(alignment, sectionEndIx)                sectionEndIx += 1            data = formatter.pack(chromId, start, end)            buffer.write(data + rest + b\"\\0\")        return maxBlockSize, regions",
        "documentation": "write alignment to the output file and return the number of alignment alignment a list or iterator returning alignment object stream output file stream"
    },
    {
        "method_code": "def test_read1(self):                filename = os.path.join(\"Prosite\", \"ps00107.txt\")        with open(filename) as handle:            record = Prosite.read(handle)        self.assertEqual(record.name, \"PROTEIN_KINASE_ATP\")        self.assertEqual(record.type, \"PATTERN\")        self.assertEqual(record.accession, \"PS00107\")        self.assertEqual(record.created, \"APR-1990\")        self.assertEqual(record.data_update, \"NOV-1995\")        self.assertEqual(record.info_update, \"MAR-2006\")        self.assertEqual(record.pdoc, \"PDOC00100\")        self.assertEqual(            record.description, \"Protein kinases ATP-binding region signature.\"        )        self.assertEqual(            record.pattern,            \"[LIV]-G-{P}-G-{P}-[FYWMGSTNH]-[SGA]-{PW}-[LIVCAT]-{PD}-x-[GSTACLIVMFY]-x(5,18)-[LIVMFYWCSTAR]-[AIVP]-[LIVMFAGCKR]-K.\",        )        self.assertEqual(record.matrix, [])        self.assertEqual(record.rules, [])        self.assertEqual(record.nr_sp_release, \"49.3\")        self.assertEqual(record.nr_sp_seqs, 212425)        self.assertEqual(record.cc_taxo_range, \"??EPV\")        self.assertEqual(record.cc_max_repeat, \"2\")        self.assertEqual(record.cc_site, [])        self.read1_positive1(record)        self.read1_positive2(record)        self.read1_positive3(record)        self.read1_positive4(record)        self.read1_false_neg(record)        self.read1_false_pos(record)        self.read1_potential(record)",
        "documentation": "parsing prosite record pstxt"
    },
    {
        "method_code": "def test_P68308(self):                filename = \"P68308.txt\"        # test the record parser        datafile = os.path.join(\"SwissProt\", filename)        with open(datafile) as test_handle:            seq_record = SeqIO.read(test_handle, \"swiss\")        self.assertIsInstance(seq_record, SeqRecord)        self.assertEqual(seq_record.id, \"P68308\")        self.assertEqual(seq_record.name, \"NU3M_BALPH\")        self.assertEqual(            seq_record.description,            \"RecName: Full=NADH-ubiquinone oxidoreductase chain 3 {ECO:0000250|UniProtKB:P03897}; EC=7.1.1.2 {ECO:0000250|UniProtKB:P03897}; AltName: Full=NADH dehydrogenase subunit 3;\",        )        self.assertEqual(            repr(seq_record.seq),            \"Seq('MNLLLTLLTNTTLALLLVFIAFWLPQLNVYAEKTSPYECGFDPMGSARLPFSMK...WAE')\",        )        with open(datafile) as test_handle:            record = SwissProt.read(test_handle)        # test a couple of things on the record -- this is not exhaustive        self.assertEqual(record.entry_name, \"NU3M_BALPH\")        self.assertEqual(record.accessions, [\"P68308\", \"P24973\"])        self.assertEqual(            record.organism_classification,            [                \"Eukaryota\",                \"Metazoa\",                \"Chordata\",                \"Craniata\",                \"Vertebrata\",                \"Euteleostomi\",                \"Mammalia\",                \"Eutheria\",                \"Laurasiatheria\",                \"Artiodactyla\",                \"Whippomorpha\",                \"Cetacea\",                \"Mysticeti\",                \"Balaenopteridae\",                \"Balaenoptera\",            ],        )        self.assertEqual(record.seqinfo, (115, 13022, \"405197D2F5D0AC4B\"))        self.assertEqual(len(record.features), 4)        feature = record.features[0]        self.assertEqual(feature.type, \"CHAIN\")        self.assertEqual(feature.location.start, 0)        self.assertEqual(feature.location.end, 115)        self.assertEqual(            feature.qualifiers[\"note\"], \"NADH-ubiquinone oxidoreductase chain 3\"        )        feature = record.features[1]        self.assertEqual(feature.type, \"TRANSMEM\")        self.assertEqual(feature.location.start, 2)        self.assertEqual(feature.location.end, 23)        self.assertEqual(feature.qualifiers[\"note\"], \"Helical\")        self.assertEqual(feature.qualifiers[\"evidence\"], \"ECO:0000255\")        feature = record.features[2]        self.assertEqual(feature.type, \"TRANSMEM\")        self.assertEqual(feature.location.start, 54)        self.assertEqual(feature.location.end, 75)        self.assertEqual(feature.qualifiers[\"note\"], \"Helical\")        self.assertEqual(feature.qualifiers[\"evidence\"], \"ECO:0000255\")        feature = record.features[3]        self.assertEqual(feature.type, \"TRANSMEM\")        self.assertEqual(feature.location.start, 83)        self.assertEqual(feature.location.end, 104)        self.assertEqual(feature.qualifiers[\"note\"], \"Helical\")        self.assertEqual(feature.qualifiers[\"evidence\"], \"ECO:0000255\")        self.assertIsNone(feature.id)        self.assertEqual(len(record.references), 1)        reference = record.references[0]        self.assertEqual(reference.authors, \"Arnason U., Gullberg A., Widegren B.\")        self.assertEqual(            reference.title,            \"The complete nucleotide sequence of the mitochondrial DNA of the fin whale, Balaenoptera physalus.\",        )        self.assertEqual(len(reference.references), 2)        self.assertEqual(reference.references[0], (\"PubMed\", \"1779436\"))        self.assertEqual(reference.references[1], (\"DOI\", \"10.1007/bf02102808\"))        # Check the two parsers agree on the essentials        self.assertEqual(seq_record.seq, record.sequence)        self.assertEqual(seq_record.description, record.description)        self.assertEqual(seq_record.name, record.entry_name)        self.assertIn(seq_record.id, record.accessions)        # Now try using the iterator - note that all these        # test cases have only one record.        # With the SequenceParser        with open(datafile) as test_handle:            records = list(SeqIO.parse(test_handle, \"swiss\"))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SeqRecord)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].seq, seq_record.seq)        self.assertEqual(records[0].description, seq_record.description)        self.assertEqual(records[0].name, seq_record.name)        self.assertEqual(records[0].id, seq_record.id)        # With the RecordParser        with open(datafile) as test_handle:            records = list(SwissProt.parse(test_handle))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SwissProt.Record)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].sequence, record.sequence)        self.assertEqual(records[0].description, record.description)        self.assertEqual(records[0].entry_name, record.entry_name)        self.assertEqual(records[0].accessions, record.accessions)",
        "documentation": "parsing swissprot file ptxt"
    },
    {
        "method_code": "def push_back(self, line):                self.buf.append(line)",
        "documentation": "unread a line that should not be parsed yet"
    },
    {
        "method_code": "def load_from_flow_run_input(cls, flow_run_input: \"FlowRunInput\"):                instance = cls(**flow_run_input.decoded_value)        instance._metadata = RunInputMetadata(            key=flow_run_input.key,            sender=flow_run_input.sender,            receiver=flow_run_input.flow_run_id,        )        return instance",
        "documentation": "load the run input from a flowruninput object args flowruninput flowruninput the flow run input to load the input for"
    },
    {
        "method_code": "def __call__(self, charA, charB):                if charA == charB:            return self.match        return self.mismatch",
        "documentation": "call a match function instance already created"
    },
    {
        "method_code": "def __len__(self):                return int(self._end) - int(self._start)",
        "documentation": "return the length of the region described by the simplelocation object note that extra care may be needed for fuzzy location eg from bioseqfeature import simplelocation from bioseqfeature import beforeposition afterposition loc simplelocationbeforeposition afterposition lenloc"
    },
    {
        "method_code": "def get_first_available_ip(self):                available_ips = self.get_available_ips()        if not available_ips:            return None        return '{}/{}'.format(next(available_ips.__iter__()), self.prefix.prefixlen)",
        "documentation": "return the first available ip within the prefix or none"
    },
    {
        "method_code": "def sink(self, species):                return self.__graph.children(species)",
        "documentation": "return list of unique sink for specie"
    },
    {
        "method_code": "def end(self):                return self._end",
        "documentation": "end location right most maximum value regardless of strand read only return an integer like position object possibly a fuzzy position"
    },
    {
        "method_code": "def test_delitem_string_ok(self):                # delitem should work with string index        del self.qresult[\"hit1\"]        self.assertEqual(2, len(self.qresult))        self.assertTrue([hit21, hit31], list(self.qresult.hits))",
        "documentation": "test queryresultgetitem with string"
    },
    {
        "method_code": "def breadth_first_search_by_name(self, root_spec, target_spec):                queue = []        queue.append([root_spec])        visited = []        while queue:            path = queue.pop(0)            node = path[-1]            if node in visited:                continue            visited.append(node)            if node == target_spec:                return path            children = []            specs = self.specs_by_name.get(node.name)            if specs is None:                continue            for _, deps in specs.items():                children.extend(list(deps))            for adj in children:                if adj.name == target_spec.name and adj.version != target_spec.version:                    pass                else:                    new_path = list(path)                    new_path.append(adj)                    queue.append(new_path)",
        "documentation": "return shorted path from rootspec to specname"
    },
    {
        "method_code": "def get_qresult_id(self, pos):                handle = self._handle        handle.seek(pos)        sentinel = b\"Query:\"        while True:            line = handle.readline().strip()            if line.startswith(sentinel):                break            if not line:                raise StopIteration        qid, desc = _parse_hit_or_query_line(line.decode())        return qid",
        "documentation": "return the query id from the nearest query line"
    },
    {
        "method_code": "def _is_latest_version(self, int_id):                cur = self.dbh.cursor()        cur.execute(            \"select count(*) from MATRIX where \"            \"BASE_ID = (select BASE_ID from MATRIX where ID = %s) \"            \"and VERSION > (select VERSION from MATRIX where ID = %s)\",            (int_id, int_id),        )        row = cur.fetchone()        count = row[0]        if count == 0:            # no matrices with higher version ID and same base id            return True        return False",
        "documentation": "check if the internal id represents the latest jaspar matrix private doe this internal id represent the latest version of the jaspar matrix collapse on base id"
    },
    {
        "method_code": "def test_hmmertext_text_21_hmmpfam_001(self):                filename = os.path.join(\"Hmmer\", \"text_21_hmmpfam_001.out\")        self.check_index(filename, self.fmt)",
        "documentation": "test hmmertext indexing hmmer"
    },
    {
        "method_code": "def _ng86(codons1, codons2, k, codon_table):        S_sites1, N_sites1 = _count_site_NG86(codons1, codon_table=codon_table, k=k)    S_sites2, N_sites2 = _count_site_NG86(codons2, codon_table=codon_table, k=k)    S_sites = (S_sites1 + S_sites2) / 2.0    N_sites = (N_sites1 + N_sites2) / 2.0    SN = [0, 0]    for codon1, codon2 in zip(codons1, codons2):        SN = [            m + n            for m, n in zip(                SN, _count_diff_NG86(codon1, codon2, codon_table=codon_table)            )        ]    ps = SN[0] / S_sites    pn = SN[1] / N_sites    if ps < 3 / 4:        dS = abs(-3.0 / 4 * log(1 - 4.0 / 3 * ps))    else:        dS = -1    if pn < 3 / 4:        dN = abs(-3.0 / 4 * log(1 - 4.0 / 3 * pn))    else:        dN = -1    return dN, dS",
        "documentation": "ng method main function private"
    },
    {
        "method_code": "def remove_locus_by_name(self, name, fname):                for i, locus in enumerate(self.loci_list):            if locus == name:                self.remove_locus_by_position(i, fname)                return",
        "documentation": "remove a locus by name argument name name fname file to be created with locus removed"
    },
    {
        "method_code": "def range(self):                lows, highs = [], []  # Holds set of low and high values from sets        if self.start is not None:            lows.append(self.start)        if self.end is not None:            highs.append(self.end)        for set in self._sets.values():            low, high = set.range()  # Get each set range            lows.append(low)            highs.append(high)        if lows:            low = min(lows)        else:            low = None        if highs:            high = max(highs)        else:            high = None        return low, high",
        "documentation": "return the lowest and highest base or mark number a a tuple"
    },
    {
        "method_code": "def test_convert_phyloxml_filename(self):                trees = Phylo.parse(\"PhyloXML/phyloxml_examples.xml\", \"phyloxml\")        out_handle = tempfile.NamedTemporaryFile(mode=\"w\", delete=False)        out_handle.close()        tmp_filename = out_handle.name        try:            count = Phylo.write(trees, tmp_filename, \"phyloxml\")        finally:            os.remove(tmp_filename)        self.assertEqual(13, count)",
        "documentation": "write phyloxml to a given filename"
    },
    {
        "method_code": "def upload(        self,        path,        url,        get={},        ref=True,        cookies=True,        just_header=False,        decode=True,        redirect=True,        req=None,    ):        # TODO: This should really go to HTTPRequest.py                if self.pyload.debug:            self.log_debug(                \"UPLOAD URL \" + url,                *[                    \"{}={}\".format(key, value)                    for key, value in locals().items()                    if key not in (\"self\", \"url\", \"_[1]\")                ],            )        with open(os.fsencode(path), mode=\"rb\") as fp:            url = fixurl(url, unquote=True)  #: Recheck in 0.6.x            if req is False:                req = get_request()            elif not req:                req = self.req            if isinstance(cookies, list):                set_cookies(req.cj, cookies)            # NOTE: req can be a HTTPRequest or a Browser object            http_req = req.http if hasattr(req, \"http\") else req            if not redirect:                http_req.c.setopt(pycurl.FOLLOWLOCATION, 0)            elif isinstance(redirect, int):                http_req.c.setopt(pycurl.MAXREDIRS, redirect)            if isinstance(ref, str):                http_req.last_url = ref            http_req.set_request_context(url, get, {}, bool(ref), bool(cookies), False)            http_req.c.setopt(pycurl.HTTPHEADER, http_req.request_headers)            http_req.response_header = b\"\"            http_req.c.setopt(pycurl.UPLOAD, 1)            http_req.c.setopt(pycurl.READFUNCTION, fp.read)            http_req.c.setopt(pycurl.INFILESIZE, os.path.getsize(path))            if just_header:                http_req.c.setopt(pycurl.FOLLOWLOCATION, 0)                http_req.c.setopt(pycurl.NOBODY, 1)                http_req.c.perform()                http_req.c.setopt(pycurl.FOLLOWLOCATION, 1)                http_req.c.setopt(pycurl.NOBODY, 0)            else:                http_req.c.perform()            http_req.c.setopt(pycurl.UPLOAD, 0)            http_req.c.setopt(pycurl.INFILESIZE, 0)            http_req.c.setopt(pycurl.POSTFIELDS, \"\")            http_req.last_effective_url = http_req.c.getinfo(pycurl.EFFECTIVE_URL)            http_req.add_cookies()            http_req.code = http_req.verify_header()            html = http_req.response_header if just_header else http_req.get_response()            http_req.rep.close()            http_req.rep = None            if decode is True:                html = http_req.decode_response(html)            if not redirect:                http_req.c.setopt(pycurl.FOLLOWLOCATION, 1)            elif isinstance(redirect, int):                maxredirs = (                    self.pyload.api.get_config_value(                        \"UserAgentSwitcher\", \"maxredirs\", \"plugin\"                    )                    or 5                )                # NOTE: req can be a HTTPRequest or a Browser object                http_req.c.setopt(pycurl.MAXREDIRS, maxredirs)            if decode:                html = purge.unescape(html)            self.last_html = html            if self.pyload.debug:                self.dump_html()            # TODO: Move to network in 0.6.x            header = {\"code\": req.code, \"url\": req.last_effective_url}            # NOTE: req can be a HTTPRequest or a Browser object            header.update(parse_html_header(http_req.response_header))            self.last_header = header            if just_header:                return header            else:                return html",
        "documentation": "uploads a file at url and return response content param url param get param ref param cooky param justheader if true only the header will be retrieved and returned a dict param decode wether to decode the output according to http header should be true in most case return response content"
    },
    {
        "method_code": "def test_rename_with_force(conda_cli: CondaCLIFixture, env_one: str, env_two: str):        # Do a force rename    conda_cli(\"rename\", \"--name\", env_one, env_two, \"--yes\")    assert locate_prefix_by_name(env_two)    with pytest.raises(EnvironmentNameNotFound):        locate_prefix_by_name(env_one)",
        "documentation": "run a test where we specify the yes flag to remove an existing directory without this flag it would return with an error message"
    },
    {
        "method_code": "def _parse_result_row(self):                fields = self.fields        columns = self.line.strip().split(\"\\t\")        if len(fields) != len(columns):            raise ValueError(                \"Expected %i columns, found: %i\" % (len(fields), len(columns))            )        qresult, hit, hsp, frag = {}, {}, {}, {}        for idx, value in enumerate(columns):            sname = fields[idx]            # flag to check if any of the _COLUMNs contain sname            in_mapping = False            # iterate over each dict, mapping pair to determine            # attribute name and value of each column            for parsed_dict, mapping in (                (qresult, _COLUMN_QRESULT),                (hit, _COLUMN_HIT),                (hsp, _COLUMN_HSP),                (frag, _COLUMN_FRAG),            ):                # process parsed value according to mapping                if sname in mapping:                    attr_name, caster = mapping[sname]                    if caster is not str:                        value = caster(value)                    parsed_dict[attr_name] = value                    in_mapping = True            # make sure that any unhandled field is not supported            if not in_mapping:                assert sname not in _SUPPORTED_FIELDS        return {\"qresult\": qresult, \"hit\": hit, \"hsp\": hsp, \"frag\": frag}",
        "documentation": "return a dictionary of parsed row value private"
    },
    {
        "method_code": "def __init__(self, score_dict, symmetric=1):                if isinstance(score_dict, substitution_matrices.Array):            score_dict = dict(score_dict)  # Access to dict is much faster        self.score_dict = score_dict        self.symmetric = symmetric",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def _parse_comments(self):                comments = {}        while True:            # parse program and version            # example: # BLASTX 2.2.26+            if \"BLAST\" in self.line and \"processed\" not in self.line:                program_line = self.line[len(\" #\") :].split(\" \")                comments[\"program\"] = program_line[0].lower()                comments[\"version\"] = program_line[1]            # parse query id and description (if available)            # example: # Query: gi|356995852 Mus musculus POU domain            elif \"Query\" in self.line:                query_line = self.line[len(\"# Query: \") :].split(\" \", 1)                comments[\"id\"] = query_line[0]                if len(query_line) == 2:                    comments[\"description\"] = query_line[1]            # parse target database            # example: # Database: db/minirefseq_protein            elif \"Database\" in self.line:                comments[\"target\"] = self.line[len(\"# Database: \") :]            # parse RID (from remote searches)            elif \"RID\" in self.line:                comments[\"rid\"] = self.line[len(\"# RID: \") :]            # parse column order, required for parsing the result lines            # example: # Fields: query id, query gi, query acc., query length            elif \"Fields\" in self.line:                comments[\"fields\"] = self._parse_fields_line()            # if the line has these strings, it's either the end of a comment            # or the end of a file, so we return all the comments we've parsed            elif \" hits found\" in self.line or \"processed\" in self.line:                self.line = self.handle.readline().strip()                return comments            self.line = self.handle.readline()            if not self.line:                return comments            else:                self.line = self.line.strip()",
        "documentation": "return a dictionary containing tab file comment private"
    },
    {
        "method_code": "def test_topology_embl(self):                # This is a bit low level, but can test parsing the ID line only        tests = [            # Modern examples with sequence version            (                \"ID   X56734; SV 1; linear; mRNA; STD; PLN; 1859 BP.\",                \"linear\",                \"mRNA\",                \"PLN\",            ),            (                \"ID   CD789012; SV 4; linear; genomic DNA; HTG; MAM; 500 BP.\",                \"linear\",                \"genomic DNA\",                \"MAM\",            ),            # Example to match GenBank example used above:            (                \"ID   U49845; SV 1; linear; genomic DNA; STD; FUN; 5028 BP.\",                \"linear\",                \"genomic DNA\",                \"FUN\",            ),            # Old examples:            (                \"ID   BSUB9999   standard; circular DNA; PRO; 4214630 BP.\",                \"circular\",                \"DNA\",                \"PRO\",            ),            (\"ID   SC10H5 standard; DNA; PRO; 4870 BP.\", None, \"DNA\", \"PRO\"),            # Patent example from 2016-06-10            # ftp://ftp.ebi.ac.uk/pub/databases/embl/patent/            (                \"ID   A01679; SV 1; linear; unassigned DNA; PAT; MUS; 12 BP.\",                \"linear\",                \"unassigned DNA\",                \"MUS\",            ),            # Old patent examples            (\"ID   NRP_AX000635; PRT; NR1; 15 SQ\", None, None, \"NR1\"),            (\"ID   NRP0000016E; PRT; NR2; 5 SQ\", None, None, \"NR2\"),            # KIPO patent examples            (\"ID   DI500001       STANDARD;      PRT;   111 AA.\", None, None, None),            (\"ID   DI644510   standard; PRT;  1852 AA.\", None, None, None),        ]        for line, topo, mol_type, div in tests:            scanner = GenBank.Scanner.EmblScanner()            consumer = GenBank._FeatureConsumer(1, GenBank.FeatureValueCleaner)            scanner._feed_first_line(consumer, line)            t = consumer.data.annotations.get(\"topology\", None)            self.assertEqual(                t, topo, f\"Wrong topology {t!r} not {topo!r} from {line!r}\"            )            mt = consumer.data.annotations.get(\"molecule_type\", None)            self.assertEqual(                mt,                mol_type,                f\"Wrong molecule_type {mt!r} not {mol_type!r} from {line!r}\",            )            d = consumer.data.annotations.get(\"data_file_division\", None)            self.assertEqual(d, div, f\"Wrong division {d!r} not {div!r} from {line!r}\")",
        "documentation": "check embl id line parsing"
    },
    {
        "method_code": "def other(self, elem, namespace, localtag):                return PX.Other(            localtag,            namespace,            elem.attrib,            value=elem.text and elem.text.strip() or None,            children=[                self.other(child, *_split_namespace(child.tag)) for child in elem            ],        )",
        "documentation": "create an other object a nonphyloxml element"
    },
    {
        "method_code": "def mpi_to_int(self, s):                return int(\"\".join(\"{:02x}\".format(s[2:][x]) for x in range(len(s[2:]))), 16)",
        "documentation": "convert gcrympifmtpgp bignum format to integer"
    },
    {
        "method_code": "def test_prefix_import_with_vlan_group(self):                IMPORT_DATA =         vlan_group = VLANGroup.objects.create(name='Group 1', slug='group-1', scope=Site.objects.get(name=\"Site 1\"))        VLAN.objects.create(vid=102, name='VLAN102', group=vlan_group)        # Add all required permissions to the test user        self.add_permissions('ipam.view_prefix', 'ipam.add_prefix')        form_data = {            'data': IMPORT_DATA,            'format': 'yaml'        }        response = self.client.post(reverse('ipam:prefix_import'), data=form_data, follow=True)        self.assertHttpStatus(response, 200)        prefix = Prefix.objects.get(prefix='10.1.2.0/24')        self.assertEqual(prefix.status, PrefixStatusChoices.STATUS_ACTIVE)        self.assertEqual(prefix.vlan.vid, 102)        self.assertEqual(prefix.site.name, \"Site 1\")",
        "documentation": "this test cover a unique import edge case where vlan group is specified during the import"
    },
    {
        "method_code": "def read_deployment_by_name(        self,        name: str,    ) -> DeploymentResponse:                try:            response = self._client.get(f\"/deployments/name/{name}\")        except httpx.HTTPStatusError as e:            if e.response.status_code == status.HTTP_404_NOT_FOUND:                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e            else:                raise        return DeploymentResponse.model_validate(response.json())",
        "documentation": "query the prefect api for a deployment by name args name a deployed flow name flownamedeploymentname raise prefectexceptionsobjectnotfound if request return httpxrequesterror if request fails return a deployment model representation of the deployment"
    },
    {
        "method_code": "def write(trees, handle, plain=False, **kwargs):        return Writer(trees).write(handle, plain=plain, **kwargs)",
        "documentation": "write a tree in newick format to the given file handle return number of tree written"
    },
    {
        "method_code": "def standard_test_procedure(self, cline):                # Overwrite existing files.        cline.force = True        # Mark output files for later cleanup.        self.add_file_to_clean(cline.outfile)        if cline.guidetree_out:            self.add_file_to_clean(cline.guidetree_out)        input_records = SeqIO.to_dict(SeqIO.parse(cline.infile, \"fasta\"))        self.assertEqual(str(eval(repr(cline))), str(cline))        output, error = cline()        self.assertTrue(not output or output.strip().startswith(\"CLUSTAL\"))        # Test if ClustalOmega executed successfully.        self.assertTrue(            error.strip() == \"\"            or error.startswith(                (                    \"WARNING: Sequence type is DNA.\",                    \"WARNING: DNA alignment is still experimental.\",                )            )        )        # TODO - Try and parse this with Bio.Nexus?        if cline.guidetree_out:            self.assertTrue(os.path.isfile(cline.guidetree_out))",
        "documentation": "shared test procedure used by all test"
    },
    {
        "method_code": "def _func_names(cls) -> list[str]:                return [s for s in dir(cls) if s.startswith(cls._prefix)]",
        "documentation": "name of tmpl function in this class"
    },
    {
        "method_code": "def close(self):                if self.fp:            self.fp.close()        self.c.close()        if hasattr(self, \"p\"):            del self.p",
        "documentation": "close everything unusable after this"
    },
    {
        "method_code": "def __repr__(self):                return \"Interaction(\" + repr(self.data) + \")\"",
        "documentation": "return a debugging string representation of self"
    },
    {
        "method_code": "def activate(self):                pass",
        "documentation": "called when addon wa activated"
    },
    {
        "method_code": "def _parse_notice_level(level: str | None) -> NoticeLevel:                try:            return NoticeLevel(level)        except ValueError:            # If we get an invalid value, rather than fail, we simply use a reasonable default            return NoticeLevel(NoticeLevel.INFO)",
        "documentation": "we use this to validate notice level and provide reasonable default if any are invalid"
    },
    {
        "method_code": "def htmx_table(context, viewname, return_url=None, **kwargs):        url_params = dict_to_querydict(kwargs)    url_params['return_url'] = return_url or context['request'].path    return {        'viewname': viewname,        'url_params': url_params,    }",
        "documentation": "embed an object list table retrieved using htmx any extra keyword argument are passed a url query parameter args context the current request context viewname the name of the view to use for the htmx request eg dcimsitelist returnurl the url to pas a the returnurl if not provided the current request path will be used"
    },
    {
        "method_code": "def draw_to_file(self, output_file, title):                width, height = self.page_size        cur_drawing = Drawing(width, height)        self._draw_title(cur_drawing, title, width, height)        start_x = inch * 0.5        end_x = width - inch * 0.5        end_y = height - 1.5 * inch        start_y = 0.5 * inch        self._draw_scatter_plot(cur_drawing, start_x, start_y, end_x, end_y)        return _write(cur_drawing, output_file, self.output_format)",
        "documentation": "write the comparative plot to a file argument outputfile the name of the file to output the information to or a handle to write to title a title to display on the graphic"
    },
    {
        "method_code": "def make_dssp_dict(filename):        with open(filename) as handle:        return _make_dssp_dict(handle)",
        "documentation": "dssp dictionary mapping identifier to property return a dssp dictionary that map chainid resid to aa s and accessibility from a dssp file parameter filename string the dssp output file"
    },
    {
        "method_code": "def __init__(self, field, pattern, fast=True, case_sensitive=None):                super().__init__(field, pattern, fast)        path = util.normpath(pattern)        # By default, the case sensitivity depends on the filesystem        # that the query path is located on.        if case_sensitive is None:            case_sensitive = util.case_sensitive(path)        self.case_sensitive = case_sensitive        # Use a normalized-case pattern for case-insensitive matches.        if not case_sensitive:            # We need to lowercase the entire path, not just the pattern.            # In particular, on Windows, the drive letter is otherwise not            # lowercased.            # This also ensures that the `match()` method below and the SQL            # from `col_clause()` do the same thing.            path = path.lower()        # Match the path as a single file.        self.file_path = path        # As a directory (prefix).        self.dir_path = os.path.join(path, b\"\")",
        "documentation": "create a path query pattern must be a path either to a file or a directory casesensitive can be a bool or none indicating that the behavior should depend on the filesystem"
    },
    {
        "method_code": "def _read(handle, length):        data = handle.read(length)    if len(data) < length:        raise ValueError(\"Cannot read %d bytes from handle\" % length)    return data",
        "documentation": "read the specified number of byte from the given handle"
    },
    {
        "method_code": "def list_biodatabase_names(self):                return self.execute_and_fetch_col0(\"SELECT name FROM biodatabase\")",
        "documentation": "return a list of all of the subdatabases"
    },
    {
        "method_code": "def test_trusted_root_new_key_mgr_online(    av_data_dir: Path,    initial_trust_root: dict,    mock_fetch_channel_signing_data: Callable,):        # load key_mgr.json    key_mgr = json.loads((TESTDATA / \"key_mgr.json\").read_text())    # return key_mgr.json then HTTPError(404)    mock_fetch_channel_signing_data(key_mgr, HTTP404)    sig_ver = _SignatureVerification()    # fetches key_mgr.json (valid) then 2.root.json (non-existant)    assert sig_ver.key_mgr == key_mgr    assert sig_ver._fetch_channel_signing_data.call_count == 2",
        "documentation": "we have a new keymgr online that can be verified against our trusted root we should accept the new keymgr"
    },
    {
        "method_code": "def gompertz(x, A, u, d, v, y0):        y = (A * np.exp(-np.exp((((u * np.e) / A) * (d - x)) + 1))) + y0    return y",
        "documentation": "gompertz growth model proposed in zwietering et al pmid"
    },
    {
        "method_code": "def _export_subdir_data_to_repodata(subdir_data: SubdirData):        state = subdir_data._internal_state    subdir = subdir_data.channel.subdir    packages = {}    packages_conda = {}    for pkg in subdir_data.iter_records():        if pkg.timestamp:            # ensure timestamp is dumped as int in milliseconds            # (pkg.timestamp is a kept as a float in seconds)            pkg.__fields__[\"timestamp\"]._in_dump = True        data = pkg.dump()        if subdir == \"noarch\" and getattr(pkg, \"noarch\", None):            data[\"subdir\"] = \"noarch\"            data[\"platform\"] = data[\"arch\"] = None        if \"features\" in data:            # Features are deprecated, so they are not implemented            # in modern solvers like mamba. Mamba does implement            # track_features minimization, so we are exposing the            # features as track_features, which seems to make the            # tests pass            data[\"track_features\"] = data[\"features\"]            del data[\"features\"]        if pkg.fn.endswith(\".conda\"):            packages_conda[pkg.fn] = data        else:            packages[pkg.fn] = data    return {        \"_cache_control\": state[\"_cache_control\"],        \"_etag\": state[\"_etag\"],        \"_mod\": state[\"_mod\"],        \"_url\": state[\"_url\"],        \"_add_pip\": state[\"_add_pip\"],        \"info\": {            \"subdir\": subdir,        },        \"packages\": packages,        \"packages.conda\": packages_conda,    }",
        "documentation": "this function is only temporary and meant to patch wrong undesirable testing behaviour it should end up being replaced with the new classbased backendagnostic solver test"
    },
    {
        "method_code": "def testError(self):                corruptRec = \"49268\\tsp\\tb.1.2.1\\t-\\n\"        self.assertRaises(ValueError, Des.Record, corruptRec)",
        "documentation": "test if a corrupt record raise the appropriate exception"
    },
    {
        "method_code": "def _expand_channels(data):        data[\"channels\"] = [        os.path.expandvars(channel) for channel in data.get(\"channels\", [])    ]",
        "documentation": "expands environment variable for the channel found in the yaml data"
    },
    {
        "method_code": "def test_read(self):                with open(\"Cellosaurus/cell_lines_1.txt\") as handle:            record = cellosaurus.read(handle)        self.assertEqual(record[\"ID\"], \"#15310-LN\")        self.assertEqual(record[\"AC\"], \"CVCL_E548\")        self.assertEqual(            record[\"SY\"],            \"15310-LN; TER461; TER-461; Ter 461; TER479; TER-479; Ter 479; Extract 519\",        )        self.assertEqual(record[\"DR\"][0], (\"dbMHC\", \"48439\"))        self.assertEqual(record[\"DR\"][1], (\"ECACC\", \"94050311\"))        self.assertEqual(record[\"DR\"][2], (\"IHW\", \"IHW09326\"))        self.assertEqual(record[\"DR\"][3], (\"IPD-IMGT/HLA\", \"10074\"))        self.assertEqual(record[\"DR\"][4], (\"Wikidata\", \"Q54398957\"))        self.assertEqual(            record[\"WW\"][0], \"http://pathology.ucla.edu/workfiles/360cx.pdf\"        )        self.assertEqual(            record[\"WW\"][1], \"http://pathology.ucla.edu/workfiles/370cx.pdf\"        )        self.assertEqual(            record[\"CC\"][0],            \"Part of: 12th International Histocompatibility Workshop (12IHW) \"            \"cell line panel.\",        )        self.assertEqual(record[\"CC\"][1], \"Population: Caucasian; French Canadian.\")        self.assertEqual(            record[\"CC\"][2],            \"HLA typing: A*03,25; B*37:01:01:01,47:01:01:03; C*06; DPA1*01; DPB1*04:01:01; DQA1*01:01:01:01,\"            \"01:03:01; DQB1*05:01:01;06:03:01:02; DRB1*01:01:01,14:17; DRB3*01:01 (IPD-IMGT/HLA=10074).\",        )        self.assertEqual(            record[\"CC\"][3],            \"Transformant: NCBI_TaxID; 10376; Epstein-Barr virus (EBV).\",        )        self.assertEqual(            record[\"CC\"][4],            \"Derived from site: In situ; Peripheral blood; UBERON=UBERON_0000178.\",        )        self.assertEqual(record[\"CC\"][5], \"Cell type: B-cell; CL=CL_0000236.\")        self.assertEqual(record[\"OX\"][0], \"NCBI_TaxID=9606; ! Homo sapiens (Human)\")        self.assertEqual(record[\"SX\"], \"Female\")        self.assertEqual(record[\"AG\"], \"Age unspecified\")        self.assertEqual(record[\"CA\"], \"Transformed cell line\")        self.assertEqual(            record[\"DT\"], \"Created: 22-10-12; Last updated: 30-01-24; Version: 18\"        )",
        "documentation": "test read function"
    },
    {
        "method_code": "def add_prefix(self, key):                return \"id_\" + key",
        "documentation": "sample keyfunction for testing index code"
    },
    {
        "method_code": "def test_Confidence(self):                with open(EX_MADE) as handle:            tree = next(PhyloXMLIO.parse(handle))        self.assertEqual(tree.name, \"testing confidence\")        for conf, type, val in zip(            tree.confidences, (\"bootstrap\", \"probability\"), (89.0, 0.71)        ):            self.assertIsInstance(conf, PX.Confidence)            self.assertEqual(conf.type, type)            self.assertAlmostEqual(conf.value, val)        self.assertEqual(tree.clade.name, \"b\")        self.assertAlmostEqual(tree.clade.width, 0.2)        for conf, val in zip(tree.clade[0].confidences, (0.9, 0.71)):            self.assertIsInstance(conf, PX.Confidence)            self.assertEqual(conf.type, \"probability\")            self.assertAlmostEqual(conf.value, val)",
        "documentation": "instantiation of confidence object"
    },
    {
        "method_code": "def matches(s):        if s.startswith(\"cdao:\"):        return (s, cdao_to_obo(s))    else:        return (s,)",
        "documentation": "check for match in both cdao and obo namespaces"
    },
    {
        "method_code": "def __init__(self, host, port, password, ctrl_port, log, ctrl_host=None):                self.host, self.port, self.password = host, port, password        self.ctrl_host, self.ctrl_port = ctrl_host or host, ctrl_port        self.ctrl_sock = None        self._log = log        # Default server values.        self.random = False        self.repeat = False        self.consume = False        self.single = False        self.volume = VOLUME_MAX        self.crossfade = 0        self.mixrampdb = 0.0        self.mixrampdelay = float(\"nan\")        self.replay_gain_mode = \"off\"        self.playlist = []        self.playlist_version = 0        self.current_index = -1        self.paused = False        self.error = None        # Current connections        self.connections = set()        # Object for random numbers generation        self.random_obj = random.Random()",
        "documentation": "create a new server bound to address host and listening on port port if password is given it is required to do anything significant on the server a separate control socket is established listening to ctrlhost on port ctrlport which is used to forward notification from the player and can be sent debug command eg using netcat"
    },
    {
        "method_code": "def test_ambiguous_location(self):                id = \"P97881\"        seqiter = SeqIO.parse(f\"SwissProt/{id}.xml\", \"uniprot-xml\")        self.assertEqual(self.db.load(seqiter), 1)        dbrecord = self.db.lookup(primary_id=id)        for feature in dbrecord.features:            if feature.type == \"signal peptide\":                self.assertIsInstance(feature.location.end, UnknownPosition)            elif feature.type == \"chain\":                self.assertIsInstance(feature.location.start, UnknownPosition)            else:                self.assertIsInstance(feature.location.start, ExactPosition)",
        "documentation": "loaded uniprotxml with ambiguous location in biosql"
    },
    {
        "method_code": "def __getitem__(self, id):                return self.selected_child[id]",
        "documentation": "return the child with the given id"
    },
    {
        "method_code": "def versions_dir(self) -> Path:                return (            Path(prefect.server.database.__file__).parent            / \"migrations\"            / \"versions\"            / \"postgresql\"        )",
        "documentation": "directory containing migration"
    },
    {
        "method_code": "def dict(self, *args, **kwargs) -> Dict:                # Support serialization of the 'URL' type        d = super().dict(*args, **kwargs)        d[\"rendered_url\"] = SecretStr(            self.rendered_url.render_as_string(hide_password=False)        )        return d",
        "documentation": "convert to a dictionary"
    },
    {
        "method_code": "def querystring(request, **kwargs):        querydict = request.GET.copy()    for k, v in kwargs.items():        if v is not None:            querydict[k] = str(v)        elif k in querydict:            querydict.pop(k)    querystring = querydict.urlencode(safe='/')    if querystring:        return '?' + querystring    else:        return ''",
        "documentation": "append or update the page number in a querystring"
    },
    {
        "method_code": "def is_3overhang(cls):                return False",
        "documentation": "return if the enzyme produce overhanging end true if the enzyme produce overhang sticky end related method reisoverhang reisblunt reisunknown"
    },
    {
        "method_code": "def test_succeeded_responds_false(self, conditions):                execution = Execution(            name=\"Test\",            namespace=\"test-namespace\",            metadata={},            spec={},            status={\"conditions\": conditions},            log_uri=\"\",        )        assert not execution.succeeded()",
        "documentation": "desired behavior succeeded should return false if executionstatus lack a list element that is a dict with a key type and a value of completed and a key status and a value of true this could be a situation where there is no element containing the key or the element with the key ha a status that is not true"
    },
    {
        "method_code": "def __init__(self, prot_sequence, monoisotopic=False):                self.sequence = prot_sequence.upper()        self.amino_acids_content = None        self.length = len(self.sequence)        self.monoisotopic = monoisotopic",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def add_item_fixtures(self, ext=\"mp3\", count=1):                # TODO base this on `add_item()`        items = []        path = os.path.join(_common.RSRC, util.bytestring_path(\"full.\" + ext))        for i in range(count):            item = Item.from_path(path)            item.album = f\"\\u00e4lbum {i}\"  # Check unicode paths            item.title = f\"t\\u00eftle {i}\"            # mtime needs to be set last since other assignments reset it.            item.mtime = 12345            item.add(self.lib)            item.move(operation=MoveOperation.COPY)            item.store()            items.append(item)        return items",
        "documentation": "add a number of item with file to the database"
    },
    {
        "method_code": "def test_bulk_update_process_eventrule(self):                sites = (            Site(name='Site 1', slug='site-1'),            Site(name='Site 2', slug='site-2'),            Site(name='Site 3', slug='site-3'),        )        Site.objects.bulk_create(sites)        for site in sites:            site.tags.set(Tag.objects.filter(name__in=['Foo', 'Bar']))        # Update three objects via the REST API        data = [            {                'id': sites[0].pk,                'name': 'Site X',                'tags': [                    {'name': 'Baz'}                ]            },            {                'id': sites[1].pk,                'name': 'Site Y',                'tags': [                    {'name': 'Baz'}                ]            },            {                'id': sites[2].pk,                'name': 'Site Z',                'tags': [                    {'name': 'Baz'}                ]            },        ]        url = reverse('dcim-api:site-list')        self.add_permissions('dcim.change_site')        response = self.client.patch(url, data, format='json', **self.header)        self.assertHttpStatus(response, status.HTTP_200_OK)        # Verify that a background task was queued for each updated object        self.assertEqual(self.queue.count, 3)        for i, job in enumerate(self.queue.jobs):            self.assertEqual(job.kwargs['event_rule'], EventRule.objects.get(name='Event Rule 2'))            self.assertEqual(job.kwargs['event_type'], OBJECT_UPDATED)            self.assertEqual(job.kwargs['model_name'], 'site')            self.assertEqual(job.kwargs['data']['id'], data[i]['id'])            self.assertEqual(len(job.kwargs['data']['tags']), len(response.data[i]['tags']))            self.assertEqual(job.kwargs['snapshots']['prechange']['name'], sites[i].name)            self.assertEqual(job.kwargs['snapshots']['prechange']['tags'], ['Bar', 'Foo'])            self.assertEqual(job.kwargs['snapshots']['postchange']['name'], response.data[i]['name'])            self.assertEqual(job.kwargs['snapshots']['postchange']['tags'], ['Baz'])",
        "documentation": "check that bulk updating multiple object with an applicable eventrule queue a background task for each updated object"
    },
    {
        "method_code": "def get_jobs(cls, instance=None):                jobs = Job.objects.filter(name=cls.name)        if instance:            object_type = ObjectType.objects.get_for_model(instance, for_concrete_model=False)            jobs = jobs.filter(                object_type=object_type,                object_id=instance.pk,            )        return jobs",
        "documentation": "get all job of this jobrunner related to a specific instance"
    },
    {
        "method_code": "def test_write(self):                for filename in nexml_files:            count = tree_counts.get(filename, 1)            if count > 0:                path = os.path.join(\"NeXML\", filename)                self.check(path)",
        "documentation": "test for serialization of object to nexml format"
    },
    {
        "method_code": "def test_get_prefixes_keyed(self):                q0 = \"-title:qux\"        q1 = \"^title:qux\"        results0 = self.lib.items(q0)        results1 = self.lib.items(q1)        self.assert_items_matched(results0, [\"foo bar\", \"beets 4 eva\"])        self.assert_items_matched(results1, [\"foo bar\", \"beets 4 eva\"])",
        "documentation": "test both negation prefix on a keyed query"
    },
    {
        "method_code": "def flow_unique_upsert_columns(self):                return self.orm.flow_unique_upsert_columns",
        "documentation": "unique column for upserting a flow"
    },
    {
        "method_code": "def write_alignment(self, alignment):                count = len(alignment)        self._length_of_sequences = alignment.get_alignment_length()        self._ids_written = []        if count == 0:            raise ValueError(\"Must have at least one sequence\")        if self._length_of_sequences == 0:            raise ValueError(\"Non-empty sequences are required\")        self.handle.write(\"# STOCKHOLM 1.0\\n\")        self.handle.write(\"#=GF SQ %i\\n\" % count)        for record in alignment:            self._write_record(record)        # This shouldn't be None... but just in case,        if alignment.column_annotations:            for k, v in sorted(alignment.column_annotations.items()):                if k in self.pfam_gc_mapping:                    self.handle.write(f\"#=GC {self.pfam_gc_mapping[k]} {v}\\n\")                elif k in self.pfam_gr_mapping:                    self.handle.write(f\"#=GC {self.pfam_gr_mapping[k]}_cons {v}\\n\")                else:                    # It doesn't follow the PFAM standards, but should we record                    # this data anyway?                    pass        self.handle.write(\"//\\n\")",
        "documentation": "use this to write another single alignment to an open file note that sequence and their annotation are recorded together rather than having a block of annotation followed by a block of aligned sequence"
    },
    {
        "method_code": "def display(object: Dict[str, Any], nesting: int = 0):        for key, value in object.items():        key += \":\"        if isinstance(value, dict):            app.console.print(\" \" * nesting + key)            display(value, nesting + 2)        else:            prefix = \" \" * nesting            app.console.print(f\"{prefix}{key.ljust(20 - len(prefix))} {value}\")",
        "documentation": "recursive display of a dictionary with nesting"
    },
    {
        "method_code": "def _set_parameters_sc_mismatch(self):                self._parameters.sc_mismatch = int(self._value)",
        "documentation": "mismatch penalty for nucleotidenucleotide comparison r private"
    },
    {
        "method_code": "def _draw_sigil_jaggy(        self, bottom, center, top, x1, x2, strand, color, border=None, **kwargs    ):                if strand == 1:            y1 = center            y2 = top            teeth = 2        elif strand == -1:            y1 = bottom            y2 = center            teeth = 2        else:            y1 = bottom            y2 = top            teeth = 4        xmin = min(x1, x2)        xmax = max(x1, x2)        height = y2 - y1        boxwidth = x2 - x1        tooth_length = min(height / teeth, boxwidth * 0.5)        headlength = tooth_length        taillength = tooth_length        strokecolor, color = _stroke_and_fill_colors(color, border)        points = []        for i in range(teeth):            points.extend(                (                    xmin,                    y1 + i * height / teeth,                    xmin + taillength,                    y1 + (i + 1) * height / teeth,                )            )        for i in range(teeth):            points.extend(                (                    xmax,                    y1 + (teeth - i) * height / teeth,                    xmax - headlength,                    y1 + (teeth - i - 1) * height / teeth,                )            )        return Polygon(            deduplicate(points),            strokeColor=strokecolor,            strokeWidth=1,            strokeLineJoin=1,  # 1=round            fillColor=color,            **kwargs,        )",
        "documentation": "draw jaggy sigil private although we may in future expose the headtail jaggy length for now both the left and right edge are drawn jagged"
    },
    {
        "method_code": "def test_transitions_to(self):                self.mm_builder.allow_transition(\"1\", \"1\", 0.5)        self.mm_builder.allow_transition(\"1\", \"2\", 0.5)        self.mm_builder.allow_transition(\"2\", \"1\", 1.0)        self.mm_builder.set_initial_probabilities({})        self.mm = self.mm_builder.get_markov_model()        state_1 = self.mm.transitions_to(\"1\")        expected_state_1 = [\"1\", \"2\"]        state_1.sort()        expected_state_1.sort()        self.assertEqual(state_1, expected_state_1)        state_2 = self.mm.transitions_to(\"2\")        expected_state_2 = [\"1\"]        state_2.sort()        expected_state_2.sort()        self.assertEqual(state_2, expected_state_2)        fake_state = self.mm.transitions_to(\"Fake\")        expected_fake_state = []        self.assertEqual(fake_state, expected_fake_state)",
        "documentation": "testing the calculation of transitionsto"
    },
    {
        "method_code": "def clean(self, value):                if self.null_option is not None and value == settings.FILTERS_NULL_CHOICE_VALUE:            return None        return super().clean(value)",
        "documentation": "when null option is enabled and none is sent a part of a form to be submitted it is sent a the string null this will check for that condition and gracefully handle the conversion to a nonetype"
    },
    {
        "method_code": "def _get_models(self, names):                models = []        if names:            # Collect all NaturalOrderingFields present on the specified models            for name in names:                try:                    app_label, model_name = name.split('.')                except ValueError:                    raise CommandError(                        f\"Invalid format: {name}. Models must be specified in the form app_label.ModelName.\"                    )                try:                    app_config = apps.get_app_config(app_label)                except LookupError as e:                    raise CommandError(str(e))                try:                    model = app_config.get_model(model_name)                except LookupError:                    raise CommandError(f\"Unknown model: {app_label}.{model_name}\")                fields = [                    field for field in model._meta.concrete_fields if type(field) is NaturalOrderingField                ]                if not fields:                    raise CommandError(                        f\"Invalid model: {app_label}.{model_name} does not employ natural ordering\"                    )                models.append(                    (model, fields)                )        else:            # Find *all* models with NaturalOrderingFields            for app_config in apps.get_app_configs():                for model in app_config.models.values():                    fields = [                        field for field in model._meta.concrete_fields if type(field) is NaturalOrderingField                    ]                    if fields:                        models.append(                            (model, fields)                        )        return models",
        "documentation": "compile a list of model to be renaturalized if no name are specified all model which have one or more naturalorderingfields will be included"
    },
    {
        "method_code": "def test_atom_read_noheader(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            warnings.simplefilter(\"ignore\", BiopythonParserWarning)            chain = SeqIO.read(\"PDB/a_structure.pdb\", \"pdb-atom\")        self.assertEqual(chain.id, \"????:A\")        self.assertEqual(chain.annotations[\"chain\"], \"A\")        self.assertEqual(chain.seq, \"Q\")",
        "documentation": "read a singlechain pdb without a header by atom entry"
    },
    {
        "method_code": "def test_reference_in_compound_location_sequence(self):                parent_sequence = Seq.Seq(\"aaccaaccaaccaaccaa\")        another_sequence = Seq.Seq(\"ttggttggttggttggtt\")        location = SimpleLocation(2, 6) + SimpleLocation(5, 8, ref=\"ANOTHER.7\")        sequence = location.extract(            parent_sequence, references={\"ANOTHER.7\": another_sequence}        )        self.assertEqual(type(sequence), Seq.Seq)        self.assertEqual(sequence, \"ccaatgg\")",
        "documentation": "test compound location with reference to another sequence"
    },
    {
        "method_code": "def overhang3(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if k.is_3overhang()}",
        "documentation": "return only cut that have overhang"
    },
    {
        "method_code": "def test_trusted_root_no_new_key_mgr_online_key_mgr_is_on_disk(    av_data_dir: Path,    initial_trust_root: dict,    key_mgr: dict,    mock_fetch_channel_signing_data: Callable,):        # return HTTPError(404)    mock_fetch_channel_signing_data(HTTP404)    sig_ver = _SignatureVerification()    # fetches key_mgr.json (non-existent), fallback to disk (exists)    assert sig_ver.key_mgr == key_mgr    assert sig_ver._fetch_channel_signing_data.call_count == 1",
        "documentation": "if we dont have a new keymgr online we use the one from disk"
    },
    {
        "method_code": "def test_higher_resolution(self):                m = copy.deepcopy(self.model)  # modifies atom.sasa        sasa = ShrakeRupley(n_points=960)        sasa.compute(m)        result = [a.sasa for a in m.get_atoms()][:5]        expected = [51.90, 31.45, 12.45, 12.72, 3.02]        for a, b in zip(result, expected):            self.assertAlmostEqual(a, b, places=2)",
        "documentation": "run shrakerupley with point per sphere"
    },
    {
        "method_code": "def molecular_weight(self):                return molecular_weight(            self.sequence, seq_type=\"protein\", monoisotopic=self.monoisotopic        )",
        "documentation": "calculate mw from protein sequence"
    },
    {
        "method_code": "def __init__(self):                self._sub_components = []",
        "documentation": "initialize a chromosome component attribute subcomponents any component which are contained under this parent component this attribute should be accessed through the add and remove function"
    },
    {
        "method_code": "def test_start_scan_failed_bad_credentials(self):                responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=200,            body=self.FAILED_BODY,        )        self.subsonicupdate.start_scan()",
        "documentation": "test failed path based on bad credential"
    },
    {
        "method_code": "def prompt_select_from_list(    console, prompt: str, options: Union[List[str], List[Tuple[Hashable, str]]]) -> str:        current_idx = 0    selected_option = None    def build_table() -> Table:                table = Table(box=False, header_style=None, padding=(0, 0))        table.add_column(            f\"? [bold]{prompt}[/] [bright_blue][Use arrows to move; enter to select]\",            justify=\"left\",            no_wrap=True,        )        for i, option in enumerate(options):            if isinstance(option, tuple):                option = option[1]            if i == current_idx:                # Use blue for selected options                table.add_row(\"[bold][blue]> \" + option)            else:                table.add_row(\"  \" + option)        return table    with Live(build_table(), auto_refresh=False, console=console) as live:        while selected_option is None:            key = readchar.readkey()            if key == readchar.key.UP:                current_idx = current_idx - 1                # wrap to bottom if at the top                if current_idx < 0:                    current_idx = len(options) - 1            elif key == readchar.key.DOWN:                current_idx = current_idx + 1                # wrap to top if at the bottom                if current_idx >= len(options):                    current_idx = 0            elif key == readchar.key.CTRL_C:                # gracefully exit with no message                exit_with_error(\"\")            elif key == readchar.key.ENTER or key == readchar.key.CR:                selected_option = options[current_idx]                if isinstance(selected_option, tuple):                    selected_option = selected_option[0]            live.update(build_table(), refresh=True)        return selected_option",
        "documentation": "given a list of option display the value to user in a table and prompt them to select one args option a list of option to present to the user a list of tuples can be passed a key value pair if a value is chosen the key will be returned return str the selected option"
    },
    {
        "method_code": "def get_selection_or_sequence(self):                seq = self.get_selection()        if not len(seq):            seq = self.sequence_id.get(1.0, \"end\")        seq = re.sub(\"[^A-Z]\", \"\", seq)        return str(seq)",
        "documentation": "return selected sequence or whole sequence if nothing selected whitespaces digit etc are removed"
    },
    {
        "method_code": "def create_seqinfo(self, parent):                # all the sequence information in the top labels        self.seq_info1 = ttk.Frame(parent, relief=\"ridge\", borderwidth=5, height=30)        self.seq_info1.pack(fill=\"both\", expand=1, side=\"top\")        self.position_ids = {}        d = self.position_ids        d[\"id\"] = ttk.Label(self.seq_info1, width=10)        d[\"from_id\"] = ttk.Label(self.seq_info1, width=10)        d[\"to_id\"] = ttk.Label(self.seq_info1, width=10)        d[\"length_id\"] = ttk.Label(self.seq_info1, width=10)        d[\"label\"] = ttk.Label(self.seq_info1, width=10)        for i in [\"id\", \"from_id\", \"to_id\", \"length_id\", \"label\"]:            d[i].pack(side=\"left\", fill=\"both\", expand=1)        self.seq_info2 = ttk.Frame(parent, relief=\"ridge\", borderwidth=5, height=30)        self.seq_info2.pack(fill=\"both\", expand=1, side=\"top\")        self.statistics_ids = {}        d = self.statistics_ids        d[\"length_id\"] = ttk.Label(self.seq_info2, width=10)        d[\"length_id\"].pack(side=\"left\", fill=\"both\", expand=1)        for nt in [\"A\", \"C\", \"G\", \"T\"]:            d[nt] = ttk.Label(self.seq_info2, width=10)            d[nt].pack(side=\"left\", fill=\"both\", expand=1)",
        "documentation": "set up two info line at top of main window"
    },
    {
        "method_code": "def test_ear_cutting(self):                self.assertFalse(EarI.is_palindromic())        self.assertFalse(EarI.is_defined())        self.assertTrue(EarI.is_ambiguous())        self.assertFalse(EarI.is_unknown())        self.assertEqual(EarI.elucidate(), \"CTCTTCN^NNN_N\")",
        "documentation": "test basic cutting with eari ambiguous overhang"
    },
    {
        "method_code": "def _get_container(containers: List[dict], name: str) -> Optional[dict]:        for container in containers:        if container.get(\"name\") == name:            return container    return None",
        "documentation": "extract a container from a list of container or container definition if not found none is returned"
    },
    {
        "method_code": "def expand(self):                # This is similar to conda.auxlib.type_coercion.typify_data_structure        # It could be DRY-er but that would break SRP.        if isinstance(self.value, Mapping):            new_value = type(self.value)((k, v.expand()) for k, v in self.value.items())        elif isiterable(self.value):            new_value = type(self.value)(v.expand() for v in self.value)        elif isinstance(self.value, ConfigurationObject):            for attr_name, attr_value in vars(self.value).items():                if isinstance(attr_value, LoadedParameter):                    self.value.__setattr__(attr_name, attr_value.expand())            return self.value        else:            new_value = expand_environment_variables(self.value)        self.value = new_value        return self",
        "documentation": "recursively expands any environment value in the loaded parameter return loadedparameter"
    },
    {
        "method_code": "def _gerund(self):                if \" \" in self.verb:            return self.verb        gerund = self.verb[:-1] if self.verb.endswith(\"e\") else self.verb        gerund += \"ing\"        return gerund",
        "documentation": "generate a likely gerund form of the english verb"
    },
    {
        "method_code": "def _get_scheduled_flow_runs_join(        self,        work_queue_query,        limit_per_queue: Optional[int],        scheduled_before: Optional[datetime.datetime],    ):                # precompute for readability        scheduled_before_clause = (            orm_models.FlowRun.next_scheduled_start_time <= scheduled_before            if scheduled_before is not None            else True        )        # get scheduled flow runs with lateral join where the limit is the        # available slots per queue        scheduled_flow_runs = (            sa.select(orm_models.FlowRun)            .where(                self._flow_run_work_queue_join_clause(                    orm_models.FlowRun, orm_models.WorkQueue                ),                orm_models.FlowRun.state_type == \"SCHEDULED\",                scheduled_before_clause,            )            .with_for_update(skip_locked=True)            # priority given to runs with earlier next_scheduled_start_time            .order_by(orm_models.FlowRun.next_scheduled_start_time)            # if null, no limit will be applied            .limit(sa.func.least(limit_per_queue, work_queue_query.c.available_slots))            .lateral(\"scheduled_flow_runs\")        )        # Perform a cross-join        join_criteria = sa.literal(True)        return scheduled_flow_runs, join_criteria",
        "documentation": "used by selfgetscheduledflowrunsfromworkqueue allowing just this function to be changed on a perdialect basis"
    },
    {
        "method_code": "def test_subdir_data_dict_state(platform=OVERRIDE_PLATFORM):        local_channel = Channel(join(CHANNEL_DIR_V1, platform))    sd = SubdirData(channel=local_channel)    sd._read_pickled({})",
        "documentation": "subdirdata can accept a dict instead of a repodatastate for compatibility"
    },
    {
        "method_code": "def run(self, data, request=None, commit=True, **kwargs):                script = ScriptModel.objects.get(pk=self.job.object_id).python_class()        # Add files to form data        if request:            files = request.FILES            for field_name, fileobj in files.items():                data[field_name] = fileobj        # Add the current request as a property of the script        script.request = request        # Execute the script. If commit is True, wrap it with the event_tracking context manager to ensure we process        # change logging, event rules, etc.        with event_tracking(request) if commit else nullcontext():            self.run_script(script, request, data, commit)",
        "documentation": "run the script args job the job associated with this execution data a dictionary of data to be passed to the script upon execution request the wsgi request associated with this execution if any commit passed through to scriptrun"
    },
    {
        "method_code": "def get_solvers(self) -> dict[str, CondaSolver]:                return {            solver_plugin.name.lower(): solver_plugin            for solver_plugin in self.get_hook_results(\"solvers\")        }",
        "documentation": "return a mapping from solver name to solver class"
    },
    {
        "method_code": "def do_call(args: argparse.Namespace, parser: ArgumentParser):        # let's see if during the parsing phase it was discovered that the    # called command was in fact a plugin subcommand    if plugin_subcommand := getattr(args, \"_plugin_subcommand\", None):        # pass on the rest of the plugin specific args or fall back to        # the whole discovered arguments        context.plugin_manager.invoke_pre_commands(plugin_subcommand.name)        result = plugin_subcommand.action(getattr(args, \"_args\", args))        context.plugin_manager.invoke_post_commands(plugin_subcommand.name)    elif name := getattr(args, \"_executable\", None):        # run the subcommand from executables; legacy path        deprecated.topic(            \"23.3\",            \"25.3\",            topic=\"Loading conda subcommands via executables\",            addendum=\"Use the plugin system instead.\",        )        executable = find_executable(f\"conda-{name}\")        if not executable:            from ..exceptions import CommandNotFoundError            raise CommandNotFoundError(name)        return _exec([executable, *args._args], os.environ)    else:        # let's call the subcommand the old-fashioned way via the assigned func..        module_name, func_name = args.func.rsplit(\".\", 1)        # func_name should always be 'execute'        module = import_module(module_name)        command = module_name.split(\".\")[-1].replace(\"main_\", \"\")        context.plugin_manager.invoke_pre_commands(command)        result = getattr(module, func_name)(args, parser)        context.plugin_manager.invoke_post_commands(command)    return result",
        "documentation": "serf a the primary entry point for command referred to in this file and for all registered plugin subcommands"
    },
    {
        "method_code": "def check_alignment_pfam6(self, alignment):                self.assertEqual(alignment.annotations[\"identifier\"], \"COX2_TM\")        self.assertEqual(alignment.annotations[\"accession\"], \"PF02790.17\")        self.assertEqual(            alignment.annotations[\"definition\"],            \"Cytochrome C oxidase subunit II, transmembrane domain\",        )        self.assertEqual(len(alignment.annotations[\"author\"]), 2)        self.assertEqual(            alignment.annotations[\"author\"][0], \"Sonnhammer ELL;0000-0002-9015-5588\"        )        self.assertEqual(            alignment.annotations[\"author\"][1], \"Griffiths-Jones SR;0000-0001-6043-807X\"        )        self.assertEqual(alignment.annotations[\"source of seed\"], \"Prosite\")        self.assertEqual(alignment.annotations[\"gathering method\"], \"22.80 18.00;\")        self.assertEqual(alignment.annotations[\"trusted cutoff\"], \"22.80 21.40;\")        self.assertEqual(alignment.annotations[\"noise cutoff\"], \"22.70 17.90;\")        self.assertEqual(            alignment.annotations[\"build method\"], \"hmmbuild HMM.ann SEED.ann\"        )        self.assertEqual(            alignment.annotations[\"search method\"],            \"hmmsearch -Z 57096847 -E 1000 --cpu 4 HMM pfamseq\",        )        self.assertEqual(alignment.annotations[\"type\"], \"Family\")        self.assertEqual(            alignment.annotations[\"wikipedia\"], [\"Cytochrome_c_oxidase_subunit_II\"]        )        self.assertEqual(len(alignment.annotations[\"references\"]), 1)        self.assertEqual(alignment.annotations[\"references\"][0][\"number\"], 1)        self.assertEqual(alignment.annotations[\"references\"][0][\"medline\"], \"8638158\")        self.assertEqual(            alignment.annotations[\"references\"][0][\"title\"],            \"The whole structure of the 13-subunit oxidized cytochrome c oxidase at 2.8 A.\",        )        self.assertEqual(            alignment.annotations[\"references\"][0][\"author\"],            \"Tsukihara T, Aoyama H, Yamashita E, Tomizaki T, Yamaguchi H, Shinzawa-Itoh K, Nakashima R, Yaono R, Yoshikawa S;\",        )        self.assertEqual(            alignment.annotations[\"references\"][0][\"location\"],            \"Science 1996;272:1136-1144.\",        )        self.assertEqual(len(alignment.annotations[\"database references\"]), 5)        self.assertEqual(            alignment.annotations[\"database references\"][0][\"reference\"],            \"INTERPRO; IPR011759;\",        )        self.assertEqual(            alignment.annotations[\"database references\"][1][\"reference\"],            \"PROSITE; PDOC00075;\",        )        self.assertEqual(            alignment.annotations[\"database references\"][2][\"reference\"],            \"SCOP; 1occ; fa;\",        )        self.assertEqual(            alignment.annotations[\"database references\"][2][\"comment\"],            \"This family corresponds to chains b and o.\",        )        self.assertEqual(            alignment.annotations[\"database references\"][3][\"reference\"], \"TC; 3.D.4;\"        )        self.assertEqual(            alignment.annotations[\"database references\"][4][\"reference\"],            \"SO; 0100021; polypeptide_conserved_region;\",        )        self.assertEqual(            alignment.annotations[\"comment\"],            \"The N-terminal domain of cytochrome C oxidase contains two transmembrane alpha-helices.\",        )        self.assertEqual(len(alignment.sequences), 11)        self.assertEqual(alignment.sequences[0].id, \"COX2_SCHPO/11-99\")        self.assertEqual(alignment.sequences[1].id, \"COX2_CANGA/17-103\")        self.assertEqual(alignment.sequences[2].id, \"COX2_NEUCR/14-102\")        self.assertEqual(alignment.sequences[3].id, \"H9D0Q0_EMENI/15-102\")        self.assertEqual(alignment.sequences[4].id, \"COX2_ARATH/17-103\")        self.assertEqual(alignment.sequences[5].id, \"COX2_ANOGA/1-83\")        self.assertEqual(alignment.sequences[6].id, \"COX2_CHICK/1-82\")        self.assertEqual(alignment.sequences[7].id, \"COX2_SHEEP/1-83\")        self.assertEqual(alignment.sequences[8].id, \"COX2_STRPU/1-83\")        self.assertEqual(alignment.sequences[9].id, \"COX2_SYNY3/19-111\")        self.assertEqual(alignment.sequences[10].id, \"A1BA41_PARDP/42-128\")        self.assertEqual(alignment.sequences[0].annotations[\"accession\"], \"P21534.4\")        self.assertEqual(alignment.sequences[1].annotations[\"accession\"], \"P43373.2\")        self.assertEqual(alignment.sequences[2].annotations[\"accession\"], \"P00411.2\")        self.assertEqual(alignment.sequences[3].annotations[\"accession\"], \"H9D0Q0.1\")        self.assertEqual(alignment.sequences[4].annotations[\"accession\"], \"P93285.2\")        self.assertEqual(alignment.sequences[5].annotations[\"accession\"], \"P34840.1\")        self.assertEqual(alignment.sequences[6].annotations[\"accession\"], \"P18944.1\")        self.assertEqual(alignment.sequences[7].annotations[\"accession\"], \"O78750.1\")        self.assertEqual(alignment.sequences[8].annotations[\"accession\"], \"P15545.1\")        self.assertEqual(alignment.sequences[9].annotations[\"accession\"], \"Q06474.2\")        self.assertEqual(alignment.sequences[10].annotations[\"accession\"], \"A1BA41.1\")        self.assertEqual(            alignment.sequences[0].seq,            \"APSSWALYFQDGASPSYLGVTHLNDYLMFYLTFIFIGVIYAICKAVIEYNYNSHPIAAKYTTHGSIVEFIWTLIPALILILVALPSFKL\",        )        self.assertEqual(            alignment.sequences[1].seq,            \"VPTPYGIYFQDSATPNQEGILELHDNIMFYLFIILGLVSWMLFTIVKTYSKNPMAYKYIKHGQTIEIIWTMFPAVILLIIAFPSFIL\",        )        self.assertEqual(            alignment.sequences[2].seq,            \"APSPWGIYFQDSATPQMEGLVELHDNIMYYLVVILFGVGWILLSIIRNYISTKSPISHKYLNHGTLIELIWTITPAVILILIAFPSFKL\",        )        self.assertEqual(            alignment.sequences[3].seq,            \"PTPWGIFFQDSASPQMEGIEELHNNIMFYLAIILFTVTWMMITIIRNFVAKKSPIAHKYMNHGTLIELIWTITPAFILILIAFPSFKL\",        )        self.assertEqual(            alignment.sequences[4].seq,            \"AEPWQLGFQDAATPIMQGIIDLHHDIFFFLILILVFVLWILVRALWHFHYKKNAIPQRIVHGTTIEILWTIFPSIILMFIAIPSFAL\",        )        self.assertEqual(            alignment.sequences[5].seq,            \"MATWANLGLQDSSSPLMEQLNFFHDHTLLILTMITILVGYIMGMLSFNKFTNRFLLHGQTIEIIWTVLPAIILMFIAFPSLRL\",        )        self.assertEqual(            alignment.sequences[6].seq,            \"MANHSQLGFQDASSPIMEELVEFHDHALMVALAICSLVLYLLTLMLMEKLSSNTVDAQEVELIWTILPAIVLVLLALPSLQI\",        )        self.assertEqual(            alignment.sequences[7].seq,            \"MAYPMQLGFQDATSPIMEELLHFHDHTLMIVFLISSLVLYIISLMLTTKLTHTSTMDAQEVETIWTILPAIILIMIALPSLRI\",        )        self.assertEqual(            alignment.sequences[8].seq,            \"MGTWAQFGLQDASSPLMEELTYFHDYALIVLTLITILVFYGLVSLLVSSNTNRFFFEGQELETIWTVIPALILILIALPSLQL\",        )        self.assertEqual(            alignment.sequences[9].seq,            \"VSLWYGQNHGLMPVAASADAEKVDGIFNYMMTIATGLFLLVEGVLVYCLIRFRRRKDDQTDGPPIEGNVPLEILWTAIPTVIVFTLAVYSFEV\",        )        self.assertEqual(            alignment.sequences[10].seq,            \"PVNGGMNFQPASSPLAHDQQWLDHFVLYIITAVTIFVCLLLLICIVRFNRRANPVPARFTHNTPIEVIWTLVPVLILVAIGAFSLPI\",        )        self.assertEqual(            alignment[0],            \"APSSWALY---FQDGASPSYLGVTHLNDYLMFYLTFIFIGVIYAICKAVIEYNYNSHPIAAKYTTHGSI-VEFIWTLIPALILILVALPSFKL\",        )        self.assertEqual(            alignment[1],            \"VPTPYGIY---FQDSATPNQEGILELHDNIMFYLFIILGLVSWMLFTIVKTY--SKNPMAYKYIKHGQT-IEIIWTMFPAVILLIIAFPSFIL\",        )        self.assertEqual(            alignment[2],            \"APSPWGIY---FQDSATPQMEGLVELHDNIMYYLVVILFGVGWILLSIIRNYISTKSPISHKYLNHGTL-IELIWTITPAVILILIAFPSFKL\",        )        self.assertEqual(            alignment[3],            \"-PTPWGIF---FQDSASPQMEGIEELHNNIMFYLAIILFTVTWMMITIIRNFVAKKSPIAHKYMNHGTL-IELIWTITPAFILILIAFPSFKL\",        )        self.assertEqual(            alignment[4],            \"-AEPWQLG---FQDAATPIMQGIIDLHHDIFFFLILILVFVLWILVRALWHFHYKKNAIPQR-IVHGTT-IEILWTIFPSIILMFIAIPSFAL\",        )        self.assertEqual(            alignment[5],            \"MATWANLG---LQDSSSPLMEQLNFFHDHTLLILTMITILVGYIMGMLSFN------KFTNRFLLHGQT-IEIIWTVLPAIILMFIAFPSLRL\",        )        self.assertEqual(            alignment[6],            \"MANHSQLG---FQDASSPIMEELVEFHDHALMVALAICSLVLYLLTLMLME------KLS-SNTVDAQE-VELIWTILPAIVLVLLALPSLQI\",        )        self.assertEqual(            alignment[7],            \"MAYPMQLG---FQDATSPIMEELLHFHDHTLMIVFLISSLVLYIISLMLTT------KLTHTSTMDAQE-VETIWTILPAIILIMIALPSLRI\",        )        self.assertEqual(            alignment[8],            \"MGTWAQFG---LQDASSPLMEELTYFHDYALIVLTLITILVFYGLVSLLVS------SNTNRFFFEGQE-LETIWTVIPALILILIALPSLQL\",        )        self.assertEqual(            alignment[9],            \"VSLWYGQNHGLMPVAASADAEKVDGIFNYMMTIATGLFLLVEGVLVYCLIRFRRRKDDQTDGPPIEGNVPLEILWTAIPTVIVFTLAVYSFEV\",        )        self.assertEqual(            alignment[10],            \"-PVNGGMN---FQPASSPLAHDQQWLDHFVLYIITAVTIFVCLLLLICIVRFNRRANPVPAR-FTHNTP-IEVIWTLVPVLILVAIGAFSLPI\",        )        self.assertEqual(            alignment.column_annotations[\"consensus sequence\"],            \"hssshsls...FQDuuSP.MEtlhclHDahhhhLshIhhhVhalLshhlhpa..ptpslsp+.hhHGph.lElIWTllPAlILlhIAhPShpL\",        )        self.assertTrue(            np.array_equal(                alignment.coordinates,                np.array(                    [                        [0, 1, 8, 8, 48, 49, 51, 54, 57, 58, 59, 60, 66, 66, 89],                        [0, 1, 8, 8, 48, 49, 49, 52, 55, 56, 57, 58, 64, 64, 87],                        [0, 1, 8, 8, 48, 49, 51, 54, 57, 58, 59, 60, 66, 66, 89],                        [0, 0, 7, 7, 47, 48, 50, 53, 56, 57, 58, 59, 65, 65, 88],                        [0, 0, 7, 7, 47, 48, 50, 53, 56, 57, 58, 58, 64, 64, 87],                        [0, 1, 8, 8, 48, 48, 48, 48, 51, 52, 53, 54, 60, 60, 83],                        [0, 1, 8, 8, 48, 48, 48, 48, 51, 51, 52, 53, 59, 59, 82],                        [0, 1, 8, 8, 48, 48, 48, 48, 51, 52, 53, 54, 60, 60, 83],                        [0, 1, 8, 8, 48, 48, 48, 48, 51, 52, 53, 54, 60, 60, 83],                        [0, 1, 8, 11, 51, 52, 54, 57, 60, 61, 62, 63, 69, 70, 93],                        [0, 0, 7, 7, 47, 48, 50, 53, 56, 57, 58, 58, 64, 64, 87],                    ],                ),            )        )        self.assertEqual(            str(alignment),            ,        )        self.assertEqual(            format(alignment, \"stockholm\"),            ,        )",
        "documentation": "check the alignment obtained by parsing pfam record coxtm"
    },
    {
        "method_code": "def check_protected_dirs(prefix: str | Path, json: bool = False) -> None:        if is_conda_environment(Path(prefix).parent):        raise CondaEnvException(            f\"The specified prefix '{prefix}' \"            \"appears to be a top level directory within an existing conda environment \"            \"(i.e., {history_file} exists). Creating an environment in this location \"            \"has the potential to irreversibly corrupt your conda installation and/or \"            \"other conda environments, please choose a different location for your \"            \"new conda environment. Aborting.\",            json,        )",
        "documentation": "ensure that the new prefix doe not contain protected directory"
    },
    {
        "method_code": "def test_reading_psl_34_003(self):                path = \"Blat/psl_34_003.bb\"        alignments = Align.parse(path, \"bigbed\")        self.check_alignments_psl_34_003(alignments)",
        "documentation": "test reading pslbb"
    },
    {
        "method_code": "def members(self):                return self.users.union(            User.objects.filter(groups__in=self.groups.all())        ).order_by('username')",
        "documentation": "return all user who belong to this notification group"
    },
    {
        "method_code": "def submit(        self,        task: \"Task\",        parameters: Dict[str, Any],        wait_for: Optional[Iterable[PrefectFuture]] = None,        dependencies: Optional[Dict[str, Set[TaskRunInput]]] = None,    ):                if not self._started or self._executor is None:            raise RuntimeError(\"Task runner is not started\")        from prefect.context import FlowRunContext        from prefect.task_engine import run_task_async, run_task_sync        task_run_id = uuid.uuid4()        cancel_event = threading.Event()        self._cancel_events[task_run_id] = cancel_event        context = copy_context()        flow_run_ctx = FlowRunContext.get()        if flow_run_ctx:            get_run_logger(flow_run_ctx).info(                f\"Submitting task {task.name} to thread pool executor...\"            )        else:            self.logger.info(f\"Submitting task {task.name} to thread pool executor...\")        submit_kwargs = dict(            task=task,            task_run_id=task_run_id,            parameters=parameters,            wait_for=wait_for,            return_type=\"state\",            dependencies=dependencies,            context=dict(cancel_event=cancel_event),        )        if task.isasync:            # TODO: Explore possibly using a long-lived thread with an event loop            # for better performance            future = self._executor.submit(                context.run,                asyncio.run,                run_task_async(**submit_kwargs),            )        else:            future = self._executor.submit(                context.run,                run_task_sync,                **submit_kwargs,            )        prefect_future = PrefectConcurrentFuture(            task_run_id=task_run_id, wrapped_future=future        )        return prefect_future",
        "documentation": "submit a task to the task run engine running in a separate thread args task the task to submit parameter the parameter to use when running the task waitfor a list of future that the task depends on return a future object that can be used to wait for the task to complete and retrieve the result"
    },
    {
        "method_code": "def lower_clamp_multiple(k):        if k >= 50:        # return 0 for large values of `k` to prevent numerical overflow        return 0.0    return math.log(max(2**k / (2**k - 1), 1e-10), 2)",
        "documentation": "computes a lower clamp multiple that can be used to bound a random variate drawn from an exponential distribution given an upper clamp multiple k and corresponding upper bound k averageinterval this function computes a lower clamp multiple c corresponding to a lower bound c averageinterval where the probability mass between the lower bound and the median is equal to the probability mass between the median and the upper bound"
    },
    {
        "method_code": "def get_rotran(self):                if self.rot is None:            raise Exception(\"Nothing superimposed yet.\")        return self.rot, self.tran",
        "documentation": "right multiplying rotation matrix and translation"
    },
    {
        "method_code": "def test_O23729(self):                filename = \"O23729.txt\"        # test the record parser        datafile = os.path.join(\"SwissProt\", filename)        with open(datafile) as test_handle:            seq_record = SeqIO.read(test_handle, \"swiss\")        self.assertIsInstance(seq_record, SeqRecord)        self.assertEqual(seq_record.id, \"O23729\")        self.assertEqual(seq_record.name, \"CHS3_BROFI\")        self.assertEqual(            seq_record.description,            \"RecName: Full=Chalcone synthase 3; EC=2.3.1.74; AltName: Full=Naringenin-chalcone synthase 3;\",        )        self.assertEqual(            repr(seq_record.seq),            \"Seq('MAPAMEEIRQAQRAEGPAAVLAIGTSTPPNALYQADYPDYYFRITKSEHLTELK...GAE')\",        )        with open(datafile) as test_handle:            record = SwissProt.read(test_handle)        # test a couple of things on the record -- this is not exhaustive        self.assertEqual(record.entry_name, \"CHS3_BROFI\")        self.assertEqual(record.accessions, [\"O23729\"])        self.assertEqual(            record.organism_classification,            [                \"Eukaryota\",                \"Viridiplantae\",                \"Streptophyta\",                \"Embryophyta\",                \"Tracheophyta\",                \"Spermatophyta\",                \"Magnoliopsida\",                \"Liliopsida\",                \"Asparagales\",                \"Orchidaceae\",                \"Epidendroideae\",                \"Vandeae\",                \"Adrorhizinae\",                \"Bromheadia\",            ],        )        self.assertEqual(record.seqinfo, (394, 42942, \"2F8D14AF4870BBB2\"))        self.assertEqual(len(record.features), 2)        feature = record.features[0]        self.assertEqual(feature.type, \"CHAIN\")        self.assertEqual(feature.location.start, 0)        self.assertEqual(feature.location.end, 394)        self.assertEqual(feature.qualifiers[\"note\"], \"Chalcone synthase 3\")        self.assertEqual(feature.id, \"PRO_0000215956\")        feature = record.features[1]        self.assertEqual(feature.type, \"ACT_SITE\")        self.assertEqual(feature.location.start, 164)        self.assertEqual(feature.location.end, 165)        self.assertEqual(            feature.qualifiers[\"evidence\"], \"ECO:0000255|PROSITE-ProRule:PRU10023\"        )        self.assertIsNone(feature.id)        self.assertEqual(len(record.references), 1)        self.assertEqual(            record.references[0].authors, \"Liew C.F., Lim S.H., Loh C.S., Goh C.J.\"        )        self.assertEqual(            record.references[0].title,            \"Molecular cloning and sequence analysis of chalcone synthase cDNAs of Bromheadia finlaysoniana.\",        )        self.assertEqual(len(record.references[0].references), 0)        # Check the two parsers agree on the essentials        self.assertEqual(seq_record.seq, record.sequence)        self.assertEqual(seq_record.description, record.description)        self.assertEqual(seq_record.name, record.entry_name)        self.assertIn(seq_record.id, record.accessions)        # Now try using the iterator - note that all these        # test cases have only one record.        # With the SequenceParser        with open(datafile) as test_handle:            records = list(SeqIO.parse(test_handle, \"swiss\"))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SeqRecord)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].seq, seq_record.seq)        self.assertEqual(records[0].description, seq_record.description)        self.assertEqual(records[0].name, seq_record.name)        self.assertEqual(records[0].id, seq_record.id)        # With the RecordParser        with open(datafile) as test_handle:            records = list(SwissProt.parse(test_handle))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SwissProt.Record)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].sequence, record.sequence)        self.assertEqual(records[0].description, record.description)        self.assertEqual(records[0].entry_name, record.entry_name)        self.assertEqual(records[0].accessions, record.accessions)",
        "documentation": "parsing swissprot file otxt"
    },
    {
        "method_code": "def _make_table(self, table: str, fields: Mapping[str, types.Type]):                # Get current schema.        with self.transaction() as tx:            rows = tx.query(\"PRAGMA table_info(%s)\" % table)        current_fields = {row[1] for row in rows}        field_names = set(fields.keys())        if current_fields.issuperset(field_names):            # Table exists and has all the required columns.            return        if not current_fields:            # No table exists.            columns = []            for name, typ in fields.items():                columns.append(f\"{name} {typ.sql}\")            setup_sql = \"CREATE TABLE {} ({});\\n\".format(                table, \", \".join(columns)            )        else:            # Table exists does not match the field set.            setup_sql = \"\"            for name, typ in fields.items():                if name in current_fields:                    continue                setup_sql += \"ALTER TABLE {} ADD COLUMN {} {};\\n\".format(                    table, name, typ.sql                )        with self.transaction() as tx:            tx.script(setup_sql)",
        "documentation": "set up the schema of the database field is a mapping from field name to type column are added if necessary"
    },
    {
        "method_code": "def test_parse2(self):                filename = os.path.join(\"SwissProt\", \"keywlist2.txt\")        with open(filename) as handle:            records = KeyWList.parse(handle)            # Testing the first record            record = next(records)            self.assertEqual(record[\"ID\"], \"2Fe-2S.\")            self.assertEqual(record[\"AC\"], \"KW-0001\")            self.assertEqual(                record[\"DE\"],                \"Protein which contains at least one 2Fe-2S iron-sulfur cluster: 2 iron atoms complexed to 2 inorganic sulfides and 4 sulfur atoms of cysteines from the protein.\",            )            self.assertEqual(                record[\"SY\"],                \"Fe2S2; [2Fe-2S] cluster; [Fe2S2] cluster; Fe2/S2 (inorganic) cluster; Di-mu-sulfido-diiron; 2 iron, 2 sulfur cluster binding.\",            )            self.assertEqual(len(record[\"GO\"]), 1)            self.assertEqual(                record[\"GO\"], [\"GO:0051537; 2 iron, 2 sulfur cluster binding\"]            )            self.assertEqual(len(record[\"HI\"]), 2)            self.assertEqual(record[\"HI\"][0], \"Ligand: Iron; Iron-sulfur; 2Fe-2S.\")            self.assertEqual(record[\"HI\"][1], \"Ligand: Metal-binding; 2Fe-2S.\")            self.assertEqual(record[\"CA\"], \"Ligand.\")            # Testing the second record            record = next(records)            self.assertEqual(record[\"ID\"], \"3D-structure.\")            self.assertEqual(record[\"AC\"], \"KW-0002\")            self.assertEqual(                record[\"DE\"],                \"Protein, or part of a protein, whose three-dimensional structure has been resolved experimentally (for example by X-ray crystallography or NMR spectroscopy) and whose coordinates are available in the PDB database. Can also be used for theoretical models.\",            )            self.assertEqual(len(record[\"HI\"]), 1)            self.assertEqual(record[\"HI\"][0], \"Technical term: 3D-structure.\")            self.assertEqual(record[\"CA\"], \"Technical term.\")            # Testing the third record            record = next(records)            self.assertEqual(record[\"ID\"], \"3Fe-4S.\")            self.assertEqual(record[\"AC\"], \"KW-0003\")            self.assertEqual(                record[\"DE\"],                \"Protein which contains at least one 3Fe-4S iron-sulfur cluster: 3 iron atoms complexed to 4 inorganic sulfides and 3 sulfur atoms of cysteines from the protein. In a number of iron-sulfur proteins, the 4Fe-4S cluster can be reversibly converted by oxidation and loss of one iron ion to a 3Fe-4S cluster.\",            )            self.assertEqual(record[\"SY\"], \"\")            self.assertEqual(len(record[\"GO\"]), 1)            self.assertEqual(                record[\"GO\"], [\"GO:0051538; 3 iron, 4 sulfur cluster binding\"]            )            self.assertEqual(len(record[\"HI\"]), 2)            self.assertEqual(record[\"HI\"][0], \"Ligand: Iron; Iron-sulfur; 3Fe-4S.\")            self.assertEqual(record[\"HI\"][1], \"Ligand: Metal-binding; 3Fe-4S.\")            self.assertEqual(record[\"CA\"], \"Ligand.\")",
        "documentation": "parsing keywlisttxt without header and footer"
    },
    {
        "method_code": "def all_triggers(self) -> Sequence[Trigger]:                return [self]",
        "documentation": "return all trigger within this trigger"
    },
    {
        "method_code": "def __next__(self):                line = self.readline()        if not line:            raise StopIteration        return line",
        "documentation": "return the next line"
    },
    {
        "method_code": "def _translate_id(self, ent_id):                if len(ent_id) == 4:            chain_id, res_id, atom_name, icode = ent_id        else:            chain_id, res_id, atom_name = ent_id            icode = None        if isinstance(res_id, int):            ent_id = (chain_id, (\" \", res_id, \" \"), atom_name, icode)        return ent_id",
        "documentation": "return entity identifier on atom private"
    },
    {
        "method_code": "def apply_to_window(sequence, window_size, function, step=None):        seqlen = len(sequence)  # Total length of sequence to be used    if step is None:  # No step specified, so use half window-width or 1 if larger        step = max(window_size // 2, 1)    else:  # Use specified step, or 1 if greater        step = max(step, 1)    results = []  # Holds (position, value) results    # Perform the passed function on as many windows as possible, short of    # overrunning the sequence    pos = 0    while pos < seqlen - window_size + 1:        # Obtain sequence fragment        start, middle, end = pos, (pos + window_size + pos) // 2, pos + window_size        fragment = sequence[start:end]        # Apply function to the sequence fragment        value = function(fragment)        results.append((middle, value))  # Add results to list        # Advance to next fragment        pos += step    # Use the last available window on the sequence, even if it means    # re-covering old ground    if pos != seqlen - window_size:        # Obtain sequence fragment        pos = seqlen - window_size        start, middle, end = pos, (pos + window_size + pos) // 2, pos + window_size        fragment = sequence[start:end]        # Apply function to sequence fragment        value = function(fragment)        results.append((middle, value))  # Add results to list    return results",
        "documentation": "apply function to window of the given sequence return a list of position value tuples for fragment of the passed sequence of length windowsize stepped by step calculated by the passed function returned position are the midpoint of each window sequence bioseqseq object windowsize an integer describing the length of sequence to consider step an integer describing the step to take between window default windowsize function method or function that accepts a bioseqseq object a it sole argument and return a single value"
    },
    {
        "method_code": "def skip_population(self):                for line in self._handle:            if line == \"\":                return False            line = line.rstrip()            if line.upper() == \"POP\":                self.current_pop += 1                self.current_ind = 0                return True",
        "documentation": "skip the current population return true if there is another pop"
    },
    {
        "method_code": "def test_delitem_alt_ok(self):                # delitem should work with alt IDs        hit31._id_alt = [\"alt3\"]        qresult = QueryResult([hit31, hit41])        self.assertEqual(2, len(qresult))        del qresult[\"alt3\"]        self.assertEqual(1, len(qresult))        self.assertEqual(hit41, qresult[\"hit4\"])        self.assertRaises(KeyError, qresult.__getitem__, \"alt3\")        hit31._id_alt = []",
        "documentation": "test queryresultdelitem with alt id"
    },
    {
        "method_code": "def __init__(self, cmd=\"bwa\", **kwargs):                self.program_name = cmd        self.parameters = [            _StaticArgument(\"sampe\"),            _Argument(                [\"reference\"], \"Reference file name\", filename=True, is_required=True            ),            _Argument([\"sai_file1\"], \"Sai file 1\", filename=True, is_required=True),            _Argument([\"sai_file2\"], \"Sai file 2\", filename=True, is_required=True),            _Argument([\"read_file1\"], \"Read  file 1\", filename=True, is_required=True),            _Argument([\"read_file2\"], \"Read  file 2\", filename=True, is_required=True),            _Option(                [\"-a\", \"a\"],                ,                checker_function=lambda x: isinstance(x, int),                equate=False,            ),            _Option(                [\"-o\", \"o\"],                ,                checker_function=lambda x: isinstance(x, int),                equate=False,            ),            _Option(                [\"-n\", \"n\"],                ,                checker_function=lambda x: isinstance(x, int),                equate=False,            ),            _Option(                [\"-N\", \"N\"],                ,                checker_function=lambda x: isinstance(x, int),                equate=False,            ),            _Option(                [\"-r\", \"r\"],                \"Specify the read group in a format like '@RG\\tID:foo\\tSM:bar'. [null]\",                checker_function=lambda x: isinstance(x, str),                equate=False,            ),        ]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def _extract_variable(variable: str) -> Dict[str, Any]:        try:        key, value = variable.split(\"=\", 1)    except ValueError:        pass    else:        return {key: value}    try:        # Only key=value strings and JSON objexcts are valid inputs for        # variables, not arrays or strings, so we attempt to convert the parsed        # object to a dict.        return dict(json.loads(variable))    except (ValueError, TypeError) as e:        raise ValueError(            f'Could not parse variable: \"{variable}\". Please ensure variables are'            \" either in the format `key=value` or are strings containing a valid JSON\"            \" object.\"        ) from e",
        "documentation": "extract a variable from a string variable can be in the format keyvalue or a json object"
    },
    {
        "method_code": "def __deepcopy__(self, memo):                result = super().__deepcopy__(memo)        result.dynamic_params = {}        result.static_params = {}        return result",
        "documentation": "reset staticparams and dynamicparams when apiselect is deepcopied"
    },
    {
        "method_code": "def as_fastq_illumina(record: SeqRecord) -> str:        seq_str = _get_seq_string(record)    qualities_str = _get_illumina_quality_str(record)    if len(qualities_str) != len(seq_str):        raise ValueError(            \"Record %s has sequence length %i but %i quality scores\"            % (record.id, len(seq_str), len(qualities_str))        )    id_ = _clean(record.id) if record.id else \"\"    description = _clean(record.description)    if description and description.split(None, 1)[0] == id_:        title = description    elif description:        title = f\"{id_} {description}\"    else:        title = id_    return f\"@{title}\\n{seq_str}\\n+\\n{qualities_str}\\n\"",
        "documentation": "turn a seqrecord into an illumina fastq formatted string this is used internally by the seqrecords formatfastqillumina method and by the seqiowrite fastqillumina function"
    },
    {
        "method_code": "def show_only_between(self, start, end, dct=None):                d = []        if start <= end:            d = [                (k, [vv for vv in v if start <= vv <= end])                for k, v in self.between(start, end, dct).items()            ]        else:            d = [                (k, [vv for vv in v if start <= vv or vv <= end])                for k, v in self.between(start, end, dct).items()            ]        return dict(d)",
        "documentation": "return only result from within start end enzyme must cut inside startend and may also cut outside however only the cutting position within startend will be returned"
    },
    {
        "method_code": "def __bool__(self) -> bool:                return True",
        "documentation": "boolean value of an instance of this class true this behaviour is for backwards compatibility since until the len method wa added a seqrecord always evaluated a true note that in comparison a seq object will evaluate to false if it ha a zero length sequence warning the seqrecord may in future evaluate to false when it sequence is of zero length in order to better match the seq object behaviour"
    },
    {
        "method_code": "def test_allx(self):                txt_file = get_file(\"allx.hhr\")        qresults = parse(txt_file, FMT)        # test first and only qresult        qresult = next(qresults)        num_hits = 10        self.assertEqual(\"HHSUITE\", qresult.program)        self.assertEqual(\"Only X amino acids\", qresult.id)        self.assertEqual(39, qresult.seq_len)        self.assertEqual(num_hits, len(qresult))        hit = qresult[0]        self.assertEqual(\"1klr_A\", hit.id)        self.assertEqual(            \"Zinc finger Y-chromosomal protein; transcription; NMR {Synthetic} SCOP: g.37.1.1 PDB: \"            \"5znf_A 1kls_A 1xrz_A* 7znf_A\",            hit.description,        )        self.assertTrue(hit.is_included)        self.assertEqual(3.4e04, hit.evalue)        self.assertEqual(-0.01, hit.score)        self.assertEqual(1, len(hit))        hsp = hit.hsps[0]        self.assertTrue(hsp.is_included)        self.assertEqual(0, hsp.output_index)        self.assertEqual(3.4e04, hsp.evalue)        self.assertEqual(-0.01, hsp.score)        self.assertEqual(0.04, hsp.prob)        self.assertEqual(23, hsp.hit_start)        self.assertEqual(24, hsp.hit_end)        self.assertEqual(38, hsp.query_start)        self.assertEqual(39, hsp.query_end)        self.assertEqual(\"T\", hsp.hit.seq)        self.assertEqual(\"X\", hsp.query.seq)        # Check last hit        hit = qresult[num_hits - 1]        self.assertEqual(\"1zfd_A\", hit.id)        self.assertEqual(            \"SWI5; DNA binding motif, zinc finger DNA binding domain; NMR {Saccharomyces cerevisiae}\"            \" SCOP: g.37.1.1\",            hit.description,        )        self.assertTrue(hit.is_included)        self.assertEqual(3.6e04, hit.evalue)        self.assertEqual(0.03, hit.score)        self.assertEqual(1, len(hit))        # Check we can get the original last HSP from the file.        num_hsps = num_hits        self.assertEqual(num_hsps, len(qresult.hsps))        hsp = qresult.hsps[-1]        self.assertTrue(hsp.is_included)        self.assertEqual(num_hsps - 1, hsp.output_index)        self.assertEqual(3.6e04, hsp.evalue)        self.assertEqual(0.03, hsp.score)        self.assertEqual(0.03, hsp.prob)        self.assertEqual(0, hsp.hit_start)        self.assertEqual(1, hsp.hit_end)        self.assertEqual(3, hsp.query_start)        self.assertEqual(4, hsp.query_end)        self.assertEqual(\"D\", hsp.hit.seq)        self.assertEqual(\"X\", hsp.query.seq)",
        "documentation": "parsing allxhhr file"
    },
    {
        "method_code": "def test_features_spanning_origin(self):                # This first one should fail (location of the feature should be set to none), because        # the file says the sequence is linear, but there is a feature that spans the origin.        file_fails = \"GenBank/addgene-plasmid-11664-sequence-180430.gbk\"        # The right warning is raised        with warnings.catch_warnings(record=True) as caught:            warnings.simplefilter(\"always\")            record = SeqIO.read(file_fails, \"gb\")            self.assertEqual(len(caught), 1)            self.assertEqual(caught[0].category, BiopythonParserWarning)            self.assertEqual(                str(caught[0].message),                \"it appears that '8569..276' is a feature that spans the origin, but the sequence topology is undefined; setting feature location to None.\",            )        # The last feature location is None        self.assertIsNone(record.features[-1].location)        # This one is circular and should include the features that span the origin        file_succeeds = \"GenBank/addgene-plasmid-39296-sequence-49545.gbk\"        with warnings.catch_warnings(record=True) as caught:            warnings.simplefilter(\"always\")            record = SeqIO.read(file_succeeds, \"gb\")            # This gives the same error for features that span the origin            self.assertEqual(len(caught), 2)            for c in caught:                self.assertEqual(c.category, BiopythonParserWarning)                self.assertTrue(\"unintended behavior\" in str(c.message))        # The last two features should not be none        self.assertIsNotNone(record.features[-1].location)        self.assertIsNotNone(record.features[-2].location)",
        "documentation": "test that feature that span the origin on circular dna are included correctly for different way of specifying the topology"
    },
    {
        "method_code": "def _get_default_job_body_template() -> Dict[str, Any]:        return {        \"apiVersion\": \"run.googleapis.com/v1\",        \"kind\": \"Job\",        \"metadata\": {            \"name\": \"{{ name }}\",            \"annotations\": {                # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation  # noqa                \"run.googleapis.com/launch-stage\": \"BETA\",            },        },        \"spec\": {  # JobSpec            \"template\": {  # ExecutionTemplateSpec                \"spec\": {  # ExecutionSpec                    \"template\": {  # TaskTemplateSpec                        \"spec\": {  # TaskSpec                            \"containers\": [                                {                                    \"image\": \"{{ image }}\",                                    \"command\": \"{{ command }}\",                                    \"resources\": {                                        \"limits\": {                                            \"cpu\": \"{{ cpu }}\",                                            \"memory\": \"{{ memory }}\",                                        },                                        \"requests\": {                                            \"cpu\": \"{{ cpu }}\",                                            \"memory\": \"{{ memory }}\",                                        },                                    },                                }                            ],                            \"timeoutSeconds\": \"{{ timeout }}\",                            \"serviceAccountName\": \"{{ service_account_name }}\",                        }                    },                },                \"metadata\": {                    \"annotations\": {                        \"run.googleapis.com/vpc-access-connector\": \"{{ vpc_connector_name }}\"  # noqa                    }                },            },        },    }",
        "documentation": "return the default job body template used by the cloud run job"
    },
    {
        "method_code": "def alignment(self):                def is_aligned_seq(elem):            if isinstance(elem, Sequence) and elem.mol_seq.is_aligned:                return True            return False        seqs = self._filter_search(is_aligned_seq, \"preorder\", True)        records = []        lines = []        for seq in seqs:            record = seq.to_seqrecord()            lines.append(bytes(record.seq))            records.append(record)        if lines:            sequences, coordinates = Alignment.parse_printed_alignment(lines)            for sequence, record in zip(sequences, records):                record.seq = Seq(sequence)        else:            coordinates = None        return Alignment(records, coordinates)",
        "documentation": "construct an alignment object from the aligned sequence in this tree"
    },
    {
        "method_code": "def from_periods(        cls,        start: Optional[Period],        end: Optional[Period],    ) -> DateInterval:                end_date = end.open_right_endpoint() if end is not None else None        start_date = start.date if start is not None else None        return cls(start_date, end_date)",
        "documentation": "create an interval with two period a the endpoint"
    },
    {
        "method_code": "def as_fasta(record):        id = _clean(record.id)    description = _clean(record.description)    if description and description.split(None, 1)[0] == id:        # The description includes the id at the start        title = description    elif description:        title = f\"{id} {description}\"    else:        title = id    assert \"\\n\" not in title    assert \"\\r\" not in title    lines = [f\">{title}\\n\"]    data = _get_seq_string(record)  # Catches sequence being None    assert \"\\n\" not in data    assert \"\\r\" not in data    for i in range(0, len(data), 60):        lines.append(data[i : i + 60] + \"\\n\")    return \"\".join(lines)",
        "documentation": "turn a seqrecord into a fasta formatted string this is used internally by the seqrecords formatfasta method and by the seqiowrite fasta function"
    },
    {
        "method_code": "def __repr__(self):                return f\"{self.__class__.__name__}({self.parts!r}, {self.operator!r})\"",
        "documentation": "represent the compoundlocation object a string for debugging"
    },
    {
        "method_code": "def is_resuming(self, toppath):                return self._is_resuming.get(toppath, False)",
        "documentation": "return true if user want to resume import of this path you have to call askresume first to determine the return value"
    },
    {
        "method_code": "def get(self, package_ref, default=NULL):                return self._internal.get(package_ref.name, default)",
        "documentation": "beta while in beta expect both major and minor change across minor release args packageref packageref a objpackageref instance representing the key for the objprefixrecord being sought default the default value to return if the record doe not exist if not specified and no record exists exckeyerror is raised return prefixrecord"
    },
    {
        "method_code": "def sat(self, additional=None, includeIf=False, names=False, limit=0):                if self.unsat:            return None        if not self.m:            return set() if names else []        if additional:            additional = (tuple(self.names.get(c, c) for c in cc) for cc in additional)        solution = self._clauses.sat(            additional=additional, includeIf=includeIf, limit=limit        )        if solution is None:            return None        if names:            return {                nm                for nm in (self.indices.get(s) for s in solution)                if nm and nm[0] != \"!\"            }        return solution",
        "documentation": "calculate a sat solution for the current clause set returned is the list of those solution when the clause are unsatisfiable an empty list is returned"
    },
    {
        "method_code": "def prefect_db():        try:        with prefect_test_harness():            yield    except OSError as e:        if \"Directory not empty\" in str(e):            pass        else:            raise e",
        "documentation": "set up test harness for temporary db during test run"
    },
    {
        "method_code": "def test_setitem_ok_alt_existing(self):                # hit objects assignment on existing hits should also update alt IDs        hit11._id_alt = [\"alt1\"]        hit21._id_alt = [\"alt2\"]        query = QueryResult()        query[\"hit\"] = hit11        self.assertEqual(hit11, query[\"hit\"])        self.assertEqual(hit11, query[\"alt1\"])        query[\"hit\"] = hit21        self.assertEqual(hit21, query[\"hit\"])        self.assertEqual(hit21, query[\"alt2\"])        self.assertRaises(KeyError, query.__getitem__, \"alt1\")        hit11._id_alt = []        hit21._id_alt = []",
        "documentation": "test queryresultsetitem existing key"
    },
    {
        "method_code": "def ui_resource_events_url(ctx: Mapping[str, Any], obj: Any) -> Optional[str]:        from prefect.server.events.schemas.automations import Automation    from prefect.server.events.schemas.events import Resource    url = None    url_format = \"events?resource={resource_id}\"    if isinstance(obj, Automation):        url = url_format.format(resource_id=f\"prefect.automation.{obj.id}\")    elif isinstance(obj, Resource):        kind, _, id = obj.id.rpartition(\".\")        url = url_format.format(resource_id=f\"{kind}.{id}\")    elif isinstance(obj, ORMBaseModel):        kind = model_to_kind.get(type(obj))  # type: ignore        if kind:            url = url_format.format(resource_id=f\"{kind}.{obj.id}\")    if url:        return urllib.parse.urljoin(PREFECT_UI_URL.value(), url)    else:        return None",
        "documentation": "given a resource or model return a ui link to the event page filtered for that resource if an unsupported object is provided return none currently support automation resource deployment flow flowrun taskrun and workqueue object within a resource deployment flow flowrun taskrun and workqueue are supported"
    },
    {
        "method_code": "def _get(self, endpoint, **kwargs):                try:            response = self.api.get(self._make_url(endpoint), params=kwargs)        except Exception as e:            raise BeatportAPIError(                \"Error connecting to Beatport API: {}\".format(e)            )        if not response:            raise BeatportAPIError(                \"Error {0.status_code} for '{0.request.path_url}\".format(                    response                )            )        return response.json()[\"results\"]",
        "documentation": "perform a get request on a given api endpoint automatically extract result data from the response and convert http exception into pyclassbeatportapierror object"
    },
    {
        "method_code": "def get_attribute_converter(cls, beets_attr: str) -> Type[SQLiteType]:                # filesize is a special field (read from disk not db?)        if beets_attr == \"filesize\":            return int        return super().get_attribute_converter(beets_attr)",
        "documentation": "work out what data type an attribute should be for beet args beetsattr the name of the beet attribute eg title"
    },
    {
        "method_code": "def test_fasta_one_sequence(self):                record = list(SeqIO.parse(\"Registry/seqs.fasta\", \"fasta\"))[0]        input_file = \"seq.fasta\"        with open(input_file, \"w\") as handle:            SeqIO.write(record, handle, \"fasta\")        cline = XXmotifCommandline(outdir=self.out_dir, seqfile=input_file)        self.add_file_to_clean(input_file)        self.standard_test_procedure(cline)",
        "documentation": "test a fasta input file containing only one sequence"
    },
    {
        "method_code": "def characteristic(cls):                return cls.fst5, cls.fst3, cls.scd5, cls.scd3, cls.site",
        "documentation": "return a list of the enzyme characteristic a tuple the tuple contains the attribute fst first cut current strand or none fst first cut complementary strand or none scd second cut current strand or none scd second cut complementary strand or none site recognition site"
    },
    {
        "method_code": "def download_and_hash(    hasher,    url,    json_path: pathlib.Path,    session: Session,    state: RepodataState | None,    is_zst=False,    dest_path: pathlib.Path | None = None,):        if dest_path is None:        dest_path = json_path    state = state or RepodataState()    headers = build_headers(json_path, state)    timeout = context.remote_connect_timeout_secs, context.remote_read_timeout_secs    response = session.get(url, stream=True, timeout=timeout, headers=headers)    log.debug(\"%s %s\", url, response.headers)    response.raise_for_status()    length = 0    # is there a status code for which we must clear the file?    if response.status_code == 200:        if is_zst:            decompressor = zstandard.ZstdDecompressor()            writer = decompressor.stream_writer(                HashWriter(dest_path.open(\"wb\"), hasher),  # type: ignore                closefd=True,            )        else:            writer = HashWriter(dest_path.open(\"wb\"), hasher)        with writer as repodata:            for block in response.iter_content(chunk_size=1 << 14):                repodata.write(block)    if response.request:        try:            length = int(response.headers[\"Content-Length\"])        except (KeyError, ValueError, AttributeError):            pass        log.info(\"Download %d bytes %r\", length, response.request.headers)    return response",
        "documentation": "download url if it doesnt exist passing byte through hasherupdate jsonpath path of old cached data ignore etag if not exists destpath path to write new data"
    },
    {
        "method_code": "def test_wrong(self):                # To create the XML file, use        # >>> Bio.Entrez.epost(db=\"nothing\")        with open(\"Entrez/epost2.xml\", \"rb\") as stream:            self.assertRaises(RuntimeError, Entrez.read, stream)        with open(\"Entrez/epost2.xml\", \"rb\") as stream:            record = Entrez.read(stream, ignore_errors=True)        self.assertEqual(len(record), 1)        self.assertEqual(len(record.attributes), 0)        self.assertEqual(record[\"ERROR\"], \"Wrong DB name\")        self.assertEqual(record[\"ERROR\"].tag, \"ERROR\")",
        "documentation": "test parsing xml returned by epost with incorrect argument"
    },
    {
        "method_code": "def json_template(cls) -> dict:                configuration = {}        properties = cls.model_json_schema()[\"properties\"]        for k, v in properties.items():            if v.get(\"template\"):                template = v[\"template\"]            else:                template = \"{{ \" + k + \" }}\"            configuration[k] = template        return configuration",
        "documentation": "return a dict with job configuration a key and the corresponding template a value default to using the job configuration parameter name a the template variable name eg key key default variable template key template template specifically provide a template"
    },
    {
        "method_code": "def play_complicated(paths):        my_paths = copy.copy(paths)    def next_song():        my_paths.pop(0)        p.play_file(my_paths[0])    p = GstPlayer(next_song)    p.run()    p.play_file(my_paths[0])    while my_paths:        time.sleep(1)",
        "documentation": "play the file in the path one after the other by using the callback function to advance to the next song"
    },
    {
        "method_code": "def _take(iter, num):        out = []    for val in iter:        out.append(val)        num -= 1        if num <= 0:            break    return out",
        "documentation": "return a list containing the first num value in iter or fewer if the iterable end early"
    },
    {
        "method_code": "def get_next_available_vid(self):                available_vids = self.get_available_vids()        if available_vids:            return available_vids[0]        return None",
        "documentation": "return the first available vlan id in the group"
    },
    {
        "method_code": "def git_get_keywords(versionfile_abs: str) -> Dict[str, str]:        # the code embedded in _version.py can just fetch the value of these    # keywords. When used from setup.py, we don't want to import _version.py,    # so we do it with a regexp instead. This function is not used from    # _version.py.    keywords: Dict[str, str] = {}    try:        with open(versionfile_abs, \"r\") as fobj:            for line in fobj:                if line.strip().startswith(\"git_refnames =\"):                    mo = re.search(r'=\\s*\"(.*)\"', line)                    if mo:                        keywords[\"refnames\"] = mo.group(1)                if line.strip().startswith(\"git_full =\"):                    mo = re.search(r'=\\s*\"(.*)\"', line)                    if mo:                        keywords[\"full\"] = mo.group(1)                if line.strip().startswith(\"git_date =\"):                    mo = re.search(r'=\\s*\"(.*)\"', line)                    if mo:                        keywords[\"date\"] = mo.group(1)    except OSError:        pass    return keywords",
        "documentation": "extract version information from the given file"
    },
    {
        "method_code": "def add(self, key: str, dist: float):                if not 0.0 <= dist <= 1.0:            raise ValueError(f\"`dist` must be between 0.0 and 1.0, not {dist}\")        self._penalties.setdefault(key, []).append(dist)",
        "documentation": "add a distance penalty key must correspond with a configured weight setting dist must be a float between and and will be added to any existing distance penalty for the same key"
    },
    {
        "method_code": "def TaskRunStateCache(self):                return orm_models.TaskRunStateCache",
        "documentation": "a task run state cache orm model"
    },
    {
        "method_code": "def _Search(self, start=0):                # Only called from SearchAll. Is it used?        pos = self.rx.search(self.sequence, start)        return pos",
        "documentation": "search and return matchobject privat"
    },
    {
        "method_code": "def test_output004(self):                fasta_file = \"Fasta/output004.m10\"        with open(fasta_file) as handle:            alignments = list(FastaIO.FastaM10Iterator(handle))            self.assertEqual(len(alignments), 1)            self.assertEqual(len(alignments[0]), 2)            self.assertEqual(alignments[0].get_alignment_length(), 102)            self.assertEqual(                alignments[0][0].seq,                \"AAAAAAGATAAAAAATATCAAAT\"                \"AGAAGCAATAAAAAATAAAGATA\"                \"AAACTTTATTTATTGTCTATGCT\"                \"ACTGATATTTATAGCCCGAGCGA\"                \"ATTTTTCTCA\",            )            self.assertEqual(alignments[0][0].id, \"ref|NC_002127.1|:c1351-971\")            self.assertEqual(alignments[0][0].annotations[\"original_length\"], 381)            self.assertEqual(                alignments[0][1].seq,                \"AGAGAAAATAAAACAAGTAATAA\"                \"AATATTAATGGAAAAAATAAATT\"                \"CTTGTTTATTTAGACCTGATTCT\"                \"AATCACTTTTCTTGCCCGGAGTC\"                \"ATTTTTGACA\",            )            self.assertEqual(alignments[0][1].id, \"ref|NC_002695.1|:1970775-1971404\")            self.assertEqual(alignments[0][1].annotations[\"original_length\"], 630)",
        "documentation": "check outputm file"
    },
    {
        "method_code": "def test_id_desc_set(self):                for seq_type in (\"query\", \"hit\"):            for attr in (\"id\", \"description\"):                attr_name = f\"{seq_type}_{attr}\"                value = getattr(self.hsp, attr_name)                if attr == \"id\":                    # because we happen to have the same value for                    # IDs and the actual attribute name                    self.assertEqual(value, attr_name)                    for fragment in self.hsp:                        self.assertEqual(getattr(fragment, attr_name), attr_name)                else:                    self.assertEqual(value, \"<unknown description>\")                    for fragment in self.hsp:                        self.assertEqual(                            getattr(fragment, attr_name), \"<unknown description>\"                        )                new_value = \"new_\" + value                setattr(self.hsp, attr_name, new_value)                self.assertEqual(getattr(self.hsp, attr_name), new_value)                self.assertNotEqual(getattr(self.hsp, attr_name), value)                for fragment in self.hsp:                    self.assertEqual(getattr(fragment, attr_name), new_value)                    self.assertNotEqual(getattr(fragment, attr_name), value)",
        "documentation": "test hsp query and hit id and description setter"
    },
    {
        "method_code": "def _likelihood_func(t, k, w, pi, codon_cnt, codon_lst, codon_table):        from scipy.linalg import expm    Q = _get_Q(pi, k, w, codon_lst, codon_table)    P = expm(Q * t)    likelihood = 0    for i, c1 in enumerate(codon_lst):        for j, c2 in enumerate(codon_lst):            if (c1, c2) in codon_cnt:                if P[i, j] * pi[c1] <= 0:                    likelihood += codon_cnt[(c1, c2)] * 0                else:                    likelihood += codon_cnt[(c1, c2)] * log(pi[c1] * P[i, j])    return likelihood",
        "documentation": "likelihood function for ml method private"
    },
    {
        "method_code": "def solve_for_transaction(        self,        update_modifier=NULL,        deps_modifier=NULL,        prune=NULL,        ignore_pinned=NULL,        force_remove=NULL,        force_reinstall=NULL,        should_retry_solve=False,    ):                if self.prefix == context.root_prefix and context.enable_private_envs:            # This path has the ability to generate a multi-prefix transaction. The basic logic            # is in the commented out get_install_transaction() function below. Exercised at            # the integration level in the PrivateEnvIntegrationTests in test_create.py.            raise NotImplementedError()        # run pre-solve processes here before solving for a solution        context.plugin_manager.invoke_pre_solves(            self.specs_to_add,            self.specs_to_remove,        )        unlink_precs, link_precs = self.solve_for_diff(            update_modifier,            deps_modifier,            prune,            ignore_pinned,            force_remove,            force_reinstall,            should_retry_solve,        )        # TODO: Only explicitly requested remove and update specs are being included in        #   History right now. Do we need to include other categories from the solve?        # run post-solve processes here before performing the transaction        context.plugin_manager.invoke_post_solves(            self._repodata_fn,            unlink_precs,            link_precs,        )        self._notify_conda_outdated(link_precs)        return UnlinkLinkTransaction(            PrefixSetup(                self.prefix,                unlink_precs,                link_precs,                self.specs_to_remove,                self.specs_to_add,                self.neutered_specs,            )        )",
        "documentation": "give an unlinklinktransaction instance that can be used to execute the solution on an environment args depsmodifier depsmodifier see methsolvefinalstate prune bool see methsolvefinalstate ignorepinned bool see methsolvefinalstate forceremove bool see methsolvefinalstate forcereinstall bool see methsolvefordiff shouldretrysolve bool see methsolvefinalstate return unlinklinktransaction"
    },
    {
        "method_code": "def _replace_parenthesized_ambigs(seq, rev_ambig_values):        opening = seq.find(\"(\")    while opening > -1:        closing = seq.find(\")\")        if closing < 0:            raise NexusError(\"Missing closing parenthesis in: \" + seq)        elif closing < opening:            raise NexusError(\"Missing opening parenthesis in: \" + seq)        ambig = \"\".join(sorted(seq[opening + 1 : closing]))        ambig_code = rev_ambig_values[ambig.upper()]        if ambig != ambig.upper():            ambig_code = ambig_code.lower()        seq = seq[:opening] + ambig_code + seq[closing + 1 :]        opening = seq.find(\"(\")    return seq",
        "documentation": "replace ambigs in xxxacgxxx format by iupac ambiguity code private"
    },
    {
        "method_code": "def change(self, **what):                for k, v in what.items():            if k in (\"NameWidth\", \"ConsoleWidth\"):                setattr(self, k, v)                self.Cmodulo = self.ConsoleWidth % self.NameWidth                self.PrefWidth = self.ConsoleWidth - self.Cmodulo            elif k == \"sequence\":                setattr(self, \"sequence\", v)                self.search(self.sequence, self.linear)            elif k == \"rb\":                self = Analysis.__init__(self, v, self.sequence, self.linear)            elif k == \"linear\":                setattr(self, \"linear\", v)                self.search(self.sequence, v)            elif k in (\"Indent\", \"Maxsize\"):                setattr(self, k, v)            elif k in (\"Cmodulo\", \"PrefWidth\"):                raise AttributeError(                    f\"To change {k}, change NameWidth and/or ConsoleWidth\"                )            else:                raise AttributeError(f\"Analysis has no attribute {k}\")",
        "documentation": "change parameter of print output it is possible to change the width of the shell by setting selfconsolewidth to what you want selfnamewidth refer to the maximal length of the enzyme name changing one of these parameter here might not give the result you expect in which case you can settle back to a column shell or try to change selfcmodulo and selfprefwidth in printformat until you get it right"
    },
    {
        "method_code": "def add_string(self, key: str, str1: Optional[str], str2: Optional[str]):                dist = string_dist(str1, str2)        self.add(key, dist)",
        "documentation": "add a distance penalty based on the edit distance between str and str"
    },
    {
        "method_code": "def fetch_motifs_by_name(self, name):                return self.fetch_motifs(collection=None, tf_name=name)",
        "documentation": "fetch a list of jaspar motif from a jaspar db by the given tf name argument name a single name or list of name return a list of biomotifsjasparmotif object note name are not guaranteed to be unique there may be more than one motif with the same name therefore even if name specifies a single name a list of motif is returned this just call selffetchmotifscollection none tfname name this behaviour is different from the tfbs perl module getmatrixbyname method which always return a single matrix issuing a warning message and returning the first matrix retrieved in the case where multiple matrix have the same name"
    },
    {
        "method_code": "def update(self, E=None, **F):                if E is not None:            try:                alphabet = E.keys()            except AttributeError:                for key, value in E:                    self[key] = value            else:                for key in E:                    self[key] = E[key]        for key in F:            self[key] = F[key]",
        "documentation": "update the array from dictiterable e and f"
    },
    {
        "method_code": "def _str_line(self, record, length=50):                if record.seq.__class__.__name__ == \"CodonSeq\":            if len(record.seq) <= length:                return f\"{record.seq} {record.id}\"            else:                return \"%s...%s %s\" % (                    record.seq[: length - 3],                    record.seq[-3:],                    record.id,                )        else:            if len(record.seq) <= length:                return f\"{record.seq} {record.id}\"            else:                return \"%s...%s %s\" % (                    record.seq[: length - 6],                    record.seq[-3:],                    record.id,                )",
        "documentation": "return a truncated string representation of a seqrecord private this is a private function used by the str method"
    },
    {
        "method_code": "def print_more_notices_message(    total_notices: int, displayed_notices: int, viewed_notices: int) -> None:        notices_not_shown = total_notices - viewed_notices - displayed_notices    if notices_not_shown > 0:        if notices_not_shown > 1:            msg = f\"There are {notices_not_shown} more messages. To retrieve them run:\\n\\n\"        else:            msg = f\"There is {notices_not_shown} more message. To retrieve it run:\\n\\n\"        print(f\"{msg}conda notices\\n\")",
        "documentation": "conditionally show a message informing user how many more message there are"
    },
    {
        "method_code": "def get_for_device(self, device):                from .models import VLANGroup        # Find all relevant VLANGroups        q = Q()        if device.site.region:            q |= Q(                scope_type=ContentType.objects.get_by_natural_key('dcim', 'region'),                scope_id__in=device.site.region.get_ancestors(include_self=True)            )        if device.site.group:            q |= Q(                scope_type=ContentType.objects.get_by_natural_key('dcim', 'sitegroup'),                scope_id__in=device.site.group.get_ancestors(include_self=True)            )        q |= Q(            scope_type=ContentType.objects.get_by_natural_key('dcim', 'site'),            scope_id=device.site_id        )        if device.location:            q |= Q(                scope_type=ContentType.objects.get_by_natural_key('dcim', 'location'),                scope_id__in=device.location.get_ancestors(include_self=True)            )        if device.rack:            q |= Q(                scope_type=ContentType.objects.get_by_natural_key('dcim', 'rack'),                scope_id=device.rack_id            )        # Return all applicable VLANs        return self.filter(            Q(group__in=VLANGroup.objects.filter(q)) |            Q(site=device.site) |            Q(group__scope_id__isnull=True, site__isnull=True) |  # Global group VLANs            Q(group__isnull=True, site__isnull=True)  # Global VLANs        )",
        "documentation": "return all vlans available to the specified device"
    },
    {
        "method_code": "def perform_randomized_tests(n=1000):        aligner = PairwiseAligner()    aligner.internal_open_gap_score = -1    aligner.internal_extend_gap_score = -0.0    aligner.match_score = +1    aligner.mismatch_score = -1    aligner.mode = \"local\"    for i in range(n):        nBlocks1 = random.randint(1, 10)        nBlocks2 = random.randint(1, 10)        test_random(aligner, nBlocks1, nBlocks2, \"+\", \"+\")        test_random(aligner, nBlocks1, nBlocks2, \"+\", \"-\")        test_random(aligner, nBlocks1, nBlocks2, \"-\", \"+\")        test_random(aligner, nBlocks1, nBlocks2, \"-\", \"-\")        test_random_sequences(aligner, \"+\", \"+\")        test_random_sequences(aligner, \"+\", \"-\")        test_random_sequences(aligner, \"-\", \"+\")        test_random_sequences(aligner, \"-\", \"-\")",
        "documentation": "perform randomized test and compare to pslmap run this function to perform x n mapping for alignment of randomly generated sequence get the alignment in psl format and compare the result to that of pslmap"
    },
    {
        "method_code": "def _reverse_matrices(score_matrix, trace_matrix):        reverse_score_matrix = []    reverse_trace_matrix = []    # fmt: off    reverse_trace = {        1: 4, 2: 2, 3: 6, 4: 1, 5: 5, 6: 3, 7: 7, 8: 16, 9: 20, 10: 18, 11: 22, 12: 17,        13: 21, 14: 19, 15: 23, 16: 8, 17: 12, 18: 10, 19: 14, 20: 9, 21: 13, 22: 11,        23: 15, 24: 24, 25: 28, 26: 26, 27: 30, 28: 25, 29: 29, 30: 27, 31: 31,        None: None,    }    # fmt: on    for col in range(len(score_matrix[0])):        new_score_row = []        new_trace_row = []        for row in range(len(score_matrix)):            new_score_row.append(score_matrix[row][col])            new_trace_row.append(reverse_trace[trace_matrix[row][col]])        reverse_score_matrix.append(new_score_row)        reverse_trace_matrix.append(new_trace_row)    return reverse_score_matrix, reverse_trace_matrix",
        "documentation": "reverse score and trace matrix private"
    },
    {
        "method_code": "def keys(self):                return self.property_keys",
        "documentation": "return the list of residue return list of residue for which the property wa calculated rtype chainid resid chainid resid"
    },
    {
        "method_code": "def __repr__(self):                return \"%s(%i)\" % (self.__class__.__name__, int(self))",
        "documentation": "represent the location a a string for debugging"
    },
    {
        "method_code": "def test_ppbuilder_real_nonstd(self):                ppb = PPBuilder()        pp = ppb.build_peptides(self.structure, False)        self.assertEqual(len(pp), 1)        # Check the start and end positions        self.assertEqual(pp[0][0].get_id()[1], 151)        self.assertEqual(pp[0][-1].get_id()[1], 220)        # Check the sequence        s = pp[0].get_sequence()        self.assertIsInstance(s, Seq)        # Here non-standard MSE are shown as M        self.assertEqual(            \"MDIRQGPKEPFRDYVDRFYKTLRAEQASQEVKNWMTETLLVQNANPDCKTILKALGPGATLEEMMTACQG\", s        )",
        "documentation": "test ppbuilder on real pdb file allowing nonstandard amino acid"
    },
    {
        "method_code": "def make_extended(self):                for ric in self.ordered_aa_ic_list:            ric.set_angle(\"psi\", 123)            ric.set_angle(\"phi\", -104)",
        "documentation": "set all psi and phi angle to extended conformation"
    },
    {
        "method_code": "def _make_number(self, ls, title, nc, s1):                return self._make_number_only(ls, title) + self._make_nocut_only(nc, s1)",
        "documentation": "format cutting position information a a string private return a string in the form title enzyme which cut time enzyme position enzyme which cut time enzyme position position argument l is a list of cutting enzyme title is the title nc is a list of non cutting enzyme s is the sentence before the non cutting enzyme"
    },
    {
        "method_code": "def safe_open_w(path):        mkdir_p(os.path.dirname(path))    return open(path, \"w\")",
        "documentation": "open path for writing creating any parent directory a needed"
    },
    {
        "method_code": "def get_client(self, *args, **kwargs):",
        "documentation": "return a client for interacting with the external system if a service offer various client this method can accept a clienttype keyword argument to get the desired client within the service"
    },
    {
        "method_code": "def clean_up():        for filename in os.listdir(\"Emboss\"):        if filename.startswith(\"temp_\"):            try:                os.remove(filename)            except Exception:  # TODO - Which exceptions?                pass",
        "documentation": "fallback clean up method to remove temp file"
    },
    {
        "method_code": "def get_operation_id(self):                if self.is_bulk_action:            tokenized_path = self._tokenize_path()            # replace dashes as they can be problematic later in code generation            tokenized_path = [t.replace('-', '_') for t in tokenized_path]            if self.method == 'GET' and self._is_list_view():                # this shouldn't happen, but keeping it here to follow base code                action = 'list'            else:                # action = self.method_mapping[self.method.lower()]                # use bulk name so partial_update -> bulk_partial_update                action = self.view.action.lower()            if not tokenized_path:                tokenized_path.append('root')            if re.search(r'<drf_format_suffix\\w*:\\w+>', self.path_regex):                tokenized_path.append('formatted')            return '_'.join(tokenized_path + [action])        # if not bulk - just return normal id        return super().get_operation_id()",
        "documentation": "bulk serializers cause operationid conflict with nonbulk one bulk operation cause id conflict in spectacular resulting in numerous warning operationid xxx ha collision xxx resolving with numeral suffix code is modified from drfspectacularopenapiautoschemagetoperationid"
    },
    {
        "method_code": "def project(self) -> str:                return self.credentials.project",
        "documentation": "return the gcp project associated with the credential return str the gcp project associated with the credential"
    },
    {
        "method_code": "def write(self, filename=\"test1.ps\", output=\"PS\", dpi=72):                return _write(self.drawing, filename, output, dpi=dpi)",
        "documentation": "write the drawn diagram to a specified file in a specified format argument filename a string indicating the name of the output file or a handle to write to output a string indicating output format one of p pdf svg or provided the reportlab renderpm module is installed one of the bitmap format jpg bmp gif png tiff or tiff the format can be given in upper or lower case dpi an integer resolution dot per inch for bitmap format return no return value writeself filenametestps outputps dpi"
    },
    {
        "method_code": "def _artist_ids(credit: List[Dict]) -> List[str]:        artist_ids: List[str] = []    for el in credit:        if isinstance(el, dict):            artist_ids.append(el[\"artist\"][\"id\"])    return artist_ids",
        "documentation": "given a list representing an artistcredit return a list of artist id"
    },
    {
        "method_code": "def write_record(self, record):                handle = self.handle        self._write_the_first_lines(record)        # PR line (0 or 1 lines only), project identifier        #        # Assuming can't use 2 lines, we should prefer newer GenBank        # DBLINK BioProject:... entries over the older GenBank DBLINK        # Project:... lines.        #        # In either case, seems EMBL uses just \"PR    Project:...\"        # regardless of the type of ID (old numeric only, or new        # with alpha prefix), e.g. for CP002497 NCBI now uses:        #        # DBLINK      BioProject: PRJNA60715        #             BioSample: SAMN03081426        #        # While EMBL uses:        #        # XX        # PR   Project:PRJNA60715;        # XX        #        # Sorting ensures (new) BioProject:... is before old Project:...        for xref in sorted(record.dbxrefs):            if xref.startswith(\"BioProject:\"):                self._write_single_line(\"PR\", xref[3:] + \";\")                handle.write(\"XX\\n\")                break            if xref.startswith(\"Project:\"):                self._write_single_line(\"PR\", xref + \";\")                handle.write(\"XX\\n\")                break        # TODO - DT lines (date)        descr = record.description        if descr == \"<unknown description>\":            descr = \".\"        self._write_multi_line(\"DE\", descr)        handle.write(\"XX\\n\")        if \"keywords\" in record.annotations:            self._write_keywords(record)        # Should this be \"source\" or \"organism\"?        self._write_multi_line(\"OS\", self._get_annotation_str(record, \"organism\"))        try:            # List of strings            taxonomy = \"; \".join(record.annotations[\"taxonomy\"]) + \".\"        except KeyError:            taxonomy = \".\"        self._write_multi_line(\"OC\", taxonomy)        handle.write(\"XX\\n\")        if \"references\" in record.annotations:            self._write_references(record)        if \"comment\" in record.annotations:            self._write_comment(record)        handle.write(self.FEATURE_HEADER)        rec_length = len(record)        for feature in record.features:            self._write_feature(feature, rec_length)        handle.write(\"XX\\n\")        self._write_sequence(record)        handle.write(\"//\\n\")",
        "documentation": "write a single record to the output file"
    },
    {
        "method_code": "def format_alignment(self, alignment):                if not isinstance(alignment, Alignment):            raise TypeError(\"Expected an Alignment object\")        lines = []        state = alignment.column_annotations[\"state\"]        for sequence, line in zip(alignment.sequences, alignment):            try:                name = sequence.id            except AttributeError:                name = \"\"            try:                description = sequence.description            except AttributeError:                description = \"\"            if description:                lines.append(f\">{name} {description}\")            else:                lines.append(f\">{name}\")            s = \"\"            for c, m in zip(line, state):                if m == \"D\":                    s += c.upper()                elif m == \"I\":                    if c == \"-\":                        s += \".\"                    else:                        s += c.lower()            lines.append(s)        return \"\\n\".join(lines) + \"\\n\"",
        "documentation": "return a string with the alignment in the am file format"
    },
    {
        "method_code": "def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 32)",
        "documentation": "test getting the number of alignment without parsing the file"
    },
    {
        "method_code": "def _level_traverse(root, get_children):        Q = collections.deque([root])    while Q:        v = Q.popleft()        yield v        Q.extend(get_children(v))",
        "documentation": "traverse a tree in breadthfirst level order private"
    },
    {
        "method_code": "def __init__(self, sequences, table=standard_dna_table):                self._table = table        codons = {aminoacid: [] for aminoacid in table.protein_alphabet}        for codon, aminoacid in table.forward_table.items():            codons[aminoacid].append(codon)        synonymous_codons = tuple(list(codons.values()) + [table.stop_codons])        # count codon occurrences in the sequences.        counts = {c1 + c2 + c3: 0 for c1 in \"ACGT\" for c2 in \"ACGT\" for c3 in \"ACGT\"}        self.update(counts)  # just to ensure that the dictionary is sorted        # iterate over sequence and count the codons        for sequence in sequences:            try:  # SeqRecord                name = sequence.id                sequence = sequence.seq            except AttributeError:  # str, Seq, or MutableSeq                name = None            sequence = sequence.upper()            for i in range(0, len(sequence), 3):                codon = sequence[i : i + 3]                try:                    counts[codon] += 1                except KeyError:                    if name is None:                        message = f\"illegal codon '{codon}'\"                    else:                        message = f\"illegal codon '{codon}' in gene {name}\"                    raise ValueError(message) from None        # Following the description in the original paper, we use a value        # of 0.5 for codons that do not appear in the reference sequences.        for codon, count in counts.items():            if count == 0:                counts[codon] = 0.5        for codons in synonymous_codons:            denominator = max(counts[codon] for codon in codons)            for codon in codons:                self[codon] = counts[codon] / denominator",
        "documentation": "generate a codon adaptiveness table from the coding dna sequence this calculates the relative adaptiveness of each codon wij a defined by sharp li nucleic acid research from the provided codon dna sequence argument sequence an iterable over dna sequence which may be plain string seq object mutableseq object or seqrecord object table a biodatacodontablecodontable object defining the genetic code by default the standard genetic code is used"
    },
    {
        "method_code": "def get_distance(config, data_source, info):        dist = beets.autotag.Distance()    if info.data_source == data_source:        dist.add(\"source\", config[\"source_weight\"].as_number())    return dist",
        "documentation": "return the datasource weight and the maximum source weight for album or individual track"
    },
    {
        "method_code": "def to_phyloxml_container(self, **kwargs):                return Phyloxml(kwargs, phylogenies=[self])",
        "documentation": "create a new phyloxml object containing just this phylogeny"
    },
    {
        "method_code": "def persist_result_record(        self, result_record: \"ResultRecord\", holder: Optional[str] = None    ):                holder = holder or self.generate_default_holder()        return self._persist_result_record(            result_record=result_record, holder=holder, _sync=True        )",
        "documentation": "persist a result record to storage args resultrecord the result record to persist"
    },
    {
        "method_code": "def _color_from_count(self, count):                for count_start, count_end in self._color_scheme:            if count >= count_start and count <= count_end:                return self._color_scheme[(count_start, count_end)]        # if we got here we didn't find a color for the count        raise ValueError(f\"Count value {count} was not found in the color scheme.\")",
        "documentation": "translate the given count into a color using the color scheme private"
    },
    {
        "method_code": "def can_import(module_name):        try:        return __import__(module_name)    except ImportError:        return None",
        "documentation": "check we can import the requested module"
    },
    {
        "method_code": "def test_order_index_db(self):                        files = [\"GenBank/NC_000932.faa\", \"GenBank/NC_005816.faa\"]            ids = []            for f in files:                ids.extend(r.id for r in SeqIO.parse(f, \"fasta\"))            d = SeqIO.index_db(\":memory:\", files, \"fasta\")            self.assertEqual(ids, list(d))",
        "documentation": "check indexdb preserve order in multiple indexed file"
    },
    {
        "method_code": "def album_for_id(self, release_id):                self._log.debug(\"Searching for release {0}\", release_id)        release_id = self._get_id(\"album\", release_id, self.id_regex)        if release_id is None:            self._log.debug(\"Not a valid Beatport release ID.\")            return None        release = self.client.get_release(release_id)        if release:            return self._get_album_info(release)        return None",
        "documentation": "fetch a release by it beatport id and return an albuminfo object or none if the query is not a valid id or release is not found"
    },
    {
        "method_code": "def __iter__(self):                handle = self._handle        handle.seek(0)        id = None        start_offset = handle.tell()        line = handle.readline()        if not line:            # Empty file!            return        if line[0:1] != b\"@\":            raise ValueError(f\"Problem with FASTQ @ line:\\n{line!r}\")        while line:            # assert line[0]==\"@\"            # This record seems OK (so far)            id = line[1:].rstrip().split(None, 1)[0]            # Find the seq line(s)            seq_len = 0            length = len(line)            while line:                line = handle.readline()                length += len(line)                if line.startswith(b\"+\"):                    break                seq_len += len(line.strip())            if not line:                raise ValueError(\"Premature end of file in seq section\")            # assert line[0]==\"+\"            # Find the qual line(s)            qual_len = 0            while line:                if seq_len == qual_len:                    if seq_len == 0:                        # Special case, quality line should be just \"\\n\"                        line = handle.readline()                        if line.strip():                            raise ValueError(                                f\"Expected blank quality line, not {line!r}\"                            )                        length += len(line)  # Need to include the blank ling                    # Should be end of record...                    end_offset = handle.tell()                    line = handle.readline()                    if line and line[0:1] != b\"@\":                        raise ValueError(f\"Problem with line {line!r}\")                    break                else:                    line = handle.readline()                    qual_len += len(line.strip())                    length += len(line)            if seq_len != qual_len:                raise ValueError(\"Problem with quality section\")            yield id.decode(), start_offset, length            start_offset = end_offset",
        "documentation": "iterate over the sequence record in the file"
    },
    {
        "method_code": "def test_fconsense(self):                cline = FConsenseCommandline(            exes[\"fconsense\"],            intreefile=\"Phylip/horses.tree\",            outtreefile=\"test_file\",            auto=True,            filter=True,        )        stdout, stderr = cline()        # Split the next and get_taxa into two steps to help 2to3 work        tree1 = next(parse_trees(\"test_file\"))        taxa1 = tree1.get_taxa()        for tree in parse_trees(\"Phylip/horses.tree\"):            taxa2 = tree.get_taxa()            self.assertEqual(sorted(taxa1), sorted(taxa2))",
        "documentation": "calculate a consensus tree with fconsense"
    },
    {
        "method_code": "def _apply_block_structure(self, title, lines):                block = Block(\"\")        block.title = title        for line in lines:            block.commandlines.append(Commandline(line, title))        self.structured.append(block)",
        "documentation": "apply block structure to the nexus file private"
    },
    {
        "method_code": "def __new__(cls, position, left, right):                if not (position == left or position == right):            raise RuntimeError(                \"WithinPosition: %r should match left %r or \"                \"right %r\" % (position, left, right)            )        obj = int.__new__(cls, position)        obj._left = left        obj._right = right        return obj",
        "documentation": "create a withinposition object"
    },
    {
        "method_code": "def generate_lower_bounds(script_path):        globals = runpy.run_path(str(script_path))    return globals[\"generate_lower_bounds\"]",
        "documentation": "retrieves the function that generates lower bound"
    },
    {
        "method_code": "def write_file(self, alignments):                self.write_header()        count = 0        for alignment in alignments:            self.write_alignment(alignment)            count += 1        self.write_footer()        return count",
        "documentation": "use this to write an entire file containing the given alignment argument alignment a list or iterator returning multipleseqalignment object in general this method can only be called once per file"
    },
    {
        "method_code": "def is_5overhang(cls):                return True",
        "documentation": "return if the enzyme produce overhanging end true if the enzyme produce overhang sticky end related method reisoverhang reisblunt reisunknown"
    },
    {
        "method_code": "def get_client(self) -> AsyncWebClient:                return AsyncWebClient(token=self.token.get_secret_value())",
        "documentation": "return an authenticated asyncwebclient to interact with the slack api"
    },
    {
        "method_code": "def __getitem__(self, name):                try:            sequence = self.sequences[name]        except ValueError:            raise KeyError(name) from None        return SeqRecord(sequence, id=name)",
        "documentation": "return sequence associated with given name a a seqrecord object"
    },
    {
        "method_code": "def rewriter(field, rules):        def fieldfunc(item):        value = item._values_fixed[field]        for pattern, replacement in rules:            if pattern.match(value.lower()):                # Rewrite activated.                return replacement        # Not activated; return original value.        return value    return fieldfunc",
        "documentation": "create a template field function that rewrite the given field with the given rewriting rule rule must be a list of pattern replacement pair"
    },
    {
        "method_code": "def to_generic(self):                seq_parts = []        seq_names = []        parse_number = 0        n = 0        for name, start, seq, end in self.alignment:            if name == \"QUERY\":  # QUERY is the first in each alignment block                parse_number += 1                n = 0            if parse_number == 1:  # create on first_parse, append on all others                seq_parts.append(seq)                seq_names.append(name)            else:                seq_parts[n] += seq                n += 1        records = (            SeqRecord(Seq(seq), name) for (name, seq) in zip(seq_names, seq_parts)        )        return MultipleSeqAlignment(records)",
        "documentation": "retrieve generic alignment object for the given alignment instead of the tuples this return a multipleseqalignment object from bioalign through which you can manipulate and query the object thanks to james casbon for the code"
    },
    {
        "method_code": "def test_album_config(self):                self._run({\"album_path\": \".*other_album.*\"}, 0, self.all_tracks)",
        "documentation": "check that album configuration is ignored for singleton import"
    },
    {
        "method_code": "def order_clause(self) -> Optional[str]:                return None",
        "documentation": "generates a sql fragment to be used in a order by clause or none if no fragment is used ie this is a slow sort"
    },
    {
        "method_code": "def to_strings(        self,        confidence_as_branch_length=False,        branch_length_only=False,        plain=False,        plain_newick=True,        ladderize=None,        max_confidence=1.0,        format_confidence=\"%1.2f\",        format_branch_length=\"%1.5f\",    ):                # If there's a conflict in the arguments, we override plain=True        if confidence_as_branch_length or branch_length_only:            plain = False        make_info_string = self._info_factory(            plain,            confidence_as_branch_length,            branch_length_only,            max_confidence,            format_confidence,            format_branch_length,        )        def newickize(clade):                        label = clade.name or \"\"            if label:                unquoted_label = re.match(token_dict[\"unquoted node label\"], label)                if (not unquoted_label) or (unquoted_label.end() < len(label)):                    label = \"'%s'\" % label.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")            if clade.is_terminal():  # terminal                return label + make_info_string(clade, terminal=True)            else:                subtrees = (newickize(sub) for sub in clade)                return f\"({','.join(subtrees)}){label + make_info_string(clade)}\"        # Convert each tree to a string        for tree in self.trees:            if ladderize in (\"left\", \"LEFT\", \"right\", \"RIGHT\"):                # Nexus compatibility shim, kind of                tree.ladderize(reverse=(ladderize in (\"right\", \"RIGHT\")))            rawtree = newickize(tree.root) + \";\"            if plain_newick:                yield rawtree                continue            # Nexus-style (?) notation before the raw Newick tree            treeline = [\"tree\", (tree.name or \"a_tree\"), \"=\"]            if tree.weight != 1:                treeline.append(f\"[&W{round(float(tree.weight), 3)}]\")            if tree.rooted:                treeline.append(\"[&R]\")            treeline.append(rawtree)            yield \" \".join(treeline)",
        "documentation": "return an iterable of paupcompatible tree line"
    },
    {
        "method_code": "def conda_post_solves(self) -> Iterable[CondaPostSolve]:",
        "documentation": "register postsolve function in conda that are used in the general solver api after the solver ha provided the package record to add or remove from the conda environment example codeblock python from conda import plugins from condamodelsrecords import packagerecord def examplepostsolve repodatafn str unlinkprecs tuplepackagerecord linkprecs tuplepackagerecord printfuninstalling lenunlinkprecs package printfinstalling lenlinkprecs package pluginshookimpl def condapostsolves yield pluginscondapostsolve nameexamplepostsolve actionexamplepostsolve"
    },
    {
        "method_code": "def suppl_codes(cls):                supply = {k: v[0] for k, v in suppliers_dict.items()}        return supply",
        "documentation": "return a dictionary with supplier code letter code for the supplier"
    },
    {
        "method_code": "def test_sp012(self):                filename = \"sp012\"        # test the record parser        datafile = os.path.join(\"SwissProt\", filename)        seq_record = SeqIO.read(datafile, \"swiss\")        self.assertIsInstance(seq_record, SeqRecord)        self.assertEqual(seq_record.id, \"Q9Y736\")        self.assertEqual(seq_record.name, \"Q9Y736\")        self.assertEqual(seq_record.description, \"UBIQUITIN.\")        self.assertEqual(            repr(seq_record.seq),            \"Seq('MQIFVKTLTGKTITLEVESSDTIDNVKTKIQDKEGIPPDQQRLIFAGKQLEDGR...GGN')\",        )        with open(datafile) as test_handle:            record = SwissProt.read(test_handle)        # test a couple of things on the record -- this is not exhaustive        self.assertEqual(record.entry_name, \"Q9Y736\")        self.assertEqual(record.accessions, [\"Q9Y736\"])        self.assertEqual(            record.organism_classification,            [                \"Eukaryota\",                \"Fungi\",                \"Ascomycota\",                \"Pezizomycotina\",                \"Eurotiomycetes\",                \"Onygenales\",                \"Arthrodermataceae\",                \"mitosporic Arthrodermataceae\",                \"Trichophyton\",            ],        )        self.assertEqual(record.seqinfo, (153, 17238, \"01153CF30C2DEDFF\"))        self.assertEqual(len(record.features), 0)        self.assertEqual(len(record.references), 2)        self.assertEqual(            record.references[0].authors,            \"Kano R., Nakamura Y., Watanabe S., Hasegawa A.\",        )        self.assertEqual(            record.references[0].title,            \"Trichophyton mentagrophytes mRNA for ubiquitin.\",        )        self.assertEqual(len(record.references[0].references), 0)        self.assertEqual(record.references[1].authors, \"Kano R.\")        self.assertEqual(            record.references[1].title,            \"Microsporum canis mRNA for ubiquitin, complete cds.\",        )        self.assertEqual(len(record.references[1].references), 0)        # Check the two parsers agree on the essentials        self.assertEqual(seq_record.seq, record.sequence)        self.assertEqual(seq_record.description, record.description)        self.assertEqual(seq_record.name, record.entry_name)        self.assertIn(seq_record.id, record.accessions)        # Now try using the iterator - note that all these        # test cases have only one record.        # With the SequenceParser        records = list(SeqIO.parse(datafile, \"swiss\"))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SeqRecord)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].seq, seq_record.seq)        self.assertEqual(records[0].description, seq_record.description)        self.assertEqual(records[0].name, seq_record.name)        self.assertEqual(records[0].id, seq_record.id)        # With the RecordParser        with open(datafile) as test_handle:            records = list(SwissProt.parse(test_handle))        self.assertEqual(len(records), 1)        self.assertIsInstance(records[0], SwissProt.Record)        # Check matches what we got earlier without the iterator:        self.assertEqual(records[0].sequence, record.sequence)        self.assertEqual(records[0].description, record.description)        self.assertEqual(records[0].entry_name, record.entry_name)        self.assertEqual(records[0].accessions, record.accessions)",
        "documentation": "parsing swissprot file sp"
    },
    {
        "method_code": "def test_parse_bytes_stream(self):                with open(\"Entrez/pubmed1.xml\", \"rb\") as stream:            records = Entrez.parse(stream)            n = 0            for record in records:                self.assertIn(\"MedlineCitation\", record)                n += 1        self.assertEqual(n, 2)",
        "documentation": "test parsing a file opened in binary mode"
    },
    {
        "method_code": "def islower(self):                return bytes(self).islower()",
        "documentation": "return true if all ascii character in data are lowercase if there are no cased character the method return false"
    },
    {
        "method_code": "def serialize_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]:                serialized_parameters = {}        for key, value in parameters.items():            # do not serialize the bound self object            if self.ismethod and value is self.fn.__prefect_self__:                continue            if isinstance(value, (PrefectFuture, State)):                # Don't call jsonable_encoder() on a PrefectFuture or State to                # avoid triggering a __getitem__ call                serialized_parameters[key] = f\"<{type(value).__name__}>\"                continue            try:                serialized_parameters[key] = jsonable_encoder(value)            except (TypeError, ValueError):                logger.debug(                    f\"Parameter {key!r} for flow {self.name!r} is unserializable. \"                    f\"Type {type(value).__name__!r} and will not be stored \"                    \"in the backend.\"                )                serialized_parameters[key] = f\"<{type(value).__name__}>\"        return serialized_parameters",
        "documentation": "convert parameter to a serializable form us fastapis jsonableencoder to convert to json compatible object without converting everything directly to a string this maintains basic type like integer during api roundtrips"
    },
    {
        "method_code": "def startSeqXMLElement(self, name, qname, attrs):                if name != (None, \"seqXML\"):            raise ValueError(\"Failed to find the start of seqXML element\")        if qname is not None:            raise RuntimeError(\"Unexpected qname for seqXML element\")        schema = None        for key, value in attrs.items():            namespace, localname = key            if namespace is None:                if localname == \"source\":                    self.source = value                elif localname == \"sourceVersion\":                    self.sourceVersion = value                elif localname == \"seqXMLversion\":                    self.seqXMLversion = value                elif localname == \"ncbiTaxID\":                    # check if it is an integer, but store as string                    number = int(value)                    self.ncbiTaxID = value                elif localname == \"speciesName\":                    self.speciesName = value                else:                    raise ValueError(\"Unexpected attribute for XML Schema\")            elif namespace == \"http://www.w3.org/2001/XMLSchema-instance\":                if localname == \"noNamespaceSchemaLocation\":                    schema = value                else:                    raise ValueError(\"Unexpected attribute for XML Schema in namespace\")            else:                raise ValueError(                    f\"Unexpected namespace '{namespace}' for seqXML attribute\"                )        if self.seqXMLversion is None:            raise ValueError(\"Failed to find seqXMLversion\")        elif self.seqXMLversion not in (\"0.1\", \"0.2\", \"0.3\", \"0.4\"):            raise ValueError(\"Unsupported seqXMLversion\")        url = f\"http://www.seqxml.org/{self.seqXMLversion}/seqxml.xsd\"        if schema is not None and schema != url:            raise ValueError(                \"XML Schema '%s' found not consistent with reported seqXML version %s\"                % (schema, self.seqXMLversion)            )        # speciesName and ncbiTaxID attributes on the root are only supported        # in 0.4        if self.speciesName and self.seqXMLversion != \"0.4\":            raise ValueError(                \"Attribute 'speciesName' on root is only supported in version 0.4\"            )        if self.ncbiTaxID and self.seqXMLversion != \"0.4\":            raise ValueError(                \"Attribute 'ncbiTaxID' on root is only supported in version 0.4\"            )        self.endElementNS = self.endSeqXMLElement        self.startElementNS = self.startEntryElement",
        "documentation": "handle start of a seqxml element"
    },
    {
        "method_code": "def find_duplicates(self, lib):                info = self.chosen_info()        info[\"albumartist\"] = info[\"artist\"]        if info[\"artist\"] is None:            # As-is import with no artist. Skip check.            return []        # Construct a query to find duplicates with this metadata. We        # use a temporary Album object to generate any computed fields.        tmp_album = library.Album(lib, **info)        keys = config[\"import\"][\"duplicate_keys\"][\"album\"].as_str_seq()        dup_query = library.Album.all_fields_query(            {key: tmp_album.get(key) for key in keys}        )        # Don't count albums with the same files as duplicates.        task_paths = {i.path for i in self.items if i}        duplicates = []        for album in lib.albums(dup_query):            # Check whether the album paths are all present in the task            # i.e. album is being completely re-imported by the task,            # in which case it is not a duplicate (will be replaced).            album_paths = {i.path for i in album.items()}            if not (album_paths <= task_paths):                duplicates.append(album)        return duplicates",
        "documentation": "return a list of album from lib with the same artist and album name a the task"
    },
    {
        "method_code": "def sort(self, key=None, reverse=False):                if key is None:            self._records.sort(key=lambda r: r.id, reverse=reverse)        else:            self._records.sort(key=key, reverse=reverse)",
        "documentation": "sort the row seqrecord object of the alignment in place this sort the row alphabetically using the seqrecord object id by default the sorting can be controlled by supplying a key function which must map each seqrecord to a sort value this is useful if you want to add two alignment which use the same record identifier but in a different order for example from bioseq import seq from bioseqrecord import seqrecord from bioalign import multipleseqalignment align multipleseqalignment seqrecordseqacgt idhuman seqrecordseqacgg idmouse seqrecordseqacgc idchicken align multipleseqalignment seqrecordseqcggt idmouse seqrecordseqcgtt idhuman seqrecordseqcgct idchicken if you simple try and add these without sorting you get this printalign align alignment with row and column acgtcggt unknown id acggcgtt unknown id acgccgct chicken consult the seqrecord documentation which explains why you get a default value when annotation like the identifier doesnt match up however if we sort the alignment first then add them we get the desired result alignsort alignsort printalign align alignment with row and column acgccgct chicken acgtcgtt human acggcggt mouse a an example using a different sort order you could sort on the gc content of each sequence from biosequtils import gcfraction printalign alignment with row and column acgc chicken acgt human acgg mouse alignsortkey lambda record gcfractionrecordseq printalign alignment with row and column acgt human acgc chicken acgg mouse there is also a reverse argument so if you wanted to sort by id but backwards alignsortreversetrue printalign alignment with row and column acgg mouse acgt human acgc chicken"
    },
    {
        "method_code": "def get_data(self):                data = []        for xval in self.data:            yval = self.data[xval]            data.append((xval, yval))        data.sort()        return data",
        "documentation": "return data a a list of sorted position value tuples"
    },
    {
        "method_code": "def get_eta(self):                try:            return int(self.get_bytes_left() // self.get_speed())        except ZeroDivisionError:            return 0",
        "documentation": "get established time of arrival"
    },
    {
        "method_code": "def simpleElement(self, name, content=None):                self.startElement(name, attrs={})        if content:            self.characters(content)        self.endElement(name)",
        "documentation": "create an xml element without child with the given content"
    },
    {
        "method_code": "def __new__(cls, alphabet=None, dims=None, data=None, dtype=float):                if isinstance(data, dict):            if alphabet is not None:                raise ValueError(\"alphabet should be None if data is a dict\")            if dims is not None:                raise ValueError(\"dims should be None if data is a dict\")            alphabet = []            single_letters = True            for key in data:                if isinstance(key, str):                    if dims is None:                        dims = 1                    elif dims != 1:                        raise ValueError(\"inconsistent dimensions in data\")                    alphabet.append(key)                elif isinstance(key, tuple):                    if dims is None:                        dims = len(key)                    elif dims != len(key):                        raise ValueError(\"inconsistent dimensions in data\")                    if dims == 1:                        if not isinstance(key, str):                            raise ValueError(\"expected string\")                        if len(key) > 1:                            single_letters = False                        alphabet.append(key)                    elif dims == 2:                        for letter in key:                            if not isinstance(letter, str):                                raise ValueError(\"expected string\")                            if len(letter) > 1:                                single_letters = False                            alphabet.append(letter)                    else:                        raise ValueError(                            \"data array should be 1- or 2- dimensional \"                            \"(found %d dimensions) in key\" % dims                        )            alphabet = sorted(set(alphabet))            if single_letters:                alphabet = \"\".join(alphabet)            else:                alphabet = tuple(alphabet)            n = len(alphabet)            if dims == 1:                shape = (n,)            elif dims == 2:                shape = (n, n)            else:  # dims is None                raise ValueError(\"data is an empty dictionary\")            obj = super().__new__(cls, shape, dtype)            if dims == 1:                for i, key in enumerate(alphabet):                    obj[i] = data.get(letter, 0.0)            elif dims == 2:                for i1, letter1 in enumerate(alphabet):                    for i2, letter2 in enumerate(alphabet):                        key = (letter1, letter2)                        value = data.get(key, 0.0)                        obj[i1, i2] = value            obj._alphabet = alphabet            return obj        if alphabet is None:            alphabet = string.ascii_uppercase        elif not (isinstance(alphabet, (str, tuple))):            raise ValueError(\"alphabet should be a string or a tuple\")        n = len(alphabet)        if data is None:            if dims is None:                dims = 1            elif dims not in (1, 2):                raise ValueError(\"dims should be 1 or 2 (found %s)\" % dims)            shape = (n,) * dims        else:            if dims is None:                shape = data.shape                dims = len(shape)                if dims == 1:                    pass                elif dims == 2:                    if shape[0] != shape[1]:                        raise ValueError(\"data array is not square\")                else:                    raise ValueError(                        \"data array should be 1- or 2- dimensional \"                        \"(found %d dimensions) \" % dims                    )            else:                shape = (n,) * dims                if data.shape != shape:                    raise ValueError(                        \"data shape has inconsistent shape (expected (%s), found (%s))\"                        % (shape, data.shape)                    )        obj = super().__new__(cls, shape, dtype)        if data is None:            obj[:] = 0.0        else:            obj[:] = data        obj._alphabet = alphabet        return obj",
        "documentation": "create a new array instance"
    },
    {
        "method_code": "def test_good_lyrics(self):                url = \"https://genius.com/Ttng-chinchilla-lyrics\"        mock = MockFetchUrl()        lyrics = genius._scrape_lyrics_from_html(mock(url))        assert lyrics is not None        assert lyrics.count(\"\\n\") == 28",
        "documentation": "ensure we are able to scrape a page with lyric"
    },
    {
        "method_code": "def parse(self, **kwargs):                self.parse_handle_to_graph(**kwargs)        return self.parse_graph()",
        "documentation": "parse the text stream this object wa initialized with"
    },
    {
        "method_code": "def temp_offline(self, msg=\"\"):                self.fail(\"temp. offline\")",
        "documentation": "fail and indicates file ist temporary offline the core may take consequence"
    },
    {
        "method_code": "def test_truncated_xml(self):                from io import BytesIO        from Bio.Entrez.Parser import CorruptedXMLError        truncated_xml = b        stream = BytesIO()        stream.write(truncated_xml)        stream.seek(0)        records = Entrez.parse(stream)        self.assertRaises(CorruptedXMLError, next, records)",
        "documentation": "test error handling for a truncated xml declaration"
    },
    {
        "method_code": "def accept_atom(self, atom):                return 1",
        "documentation": "overload this to reject atom for output"
    },
    {
        "method_code": "def test_pathobj(self):                        d = SeqIO.index_db(Path(\"Roche/triple_sff.idx\"))            self.assertEqual(54, len(d))",
        "documentation": "load existing index from a pathlibpath object"
    },
    {
        "method_code": "def write_ctl_file(self):                # Make sure all paths are relative to the working directory        self._set_rel_paths()        with open(self.ctl_file, \"w\") as ctl_handle:            ctl_handle.write(f\"seqfile = {self._rel_alignment}\\n\")            ctl_handle.write(f\"outfile = {self._rel_out_file}\\n\")            ctl_handle.write(f\"treefile = {self._rel_tree}\\n\")            for option in self._options.items():                if option[1] is None:                    # If an option has a value of None, there's no need                    # to write it in the control file; it's normally just                    # commented out.                    continue                if option[0] == \"model_options\":                    continue                # If \"model\" is 9 or 10, it may be followed in the cotnrol                # file by further options such as                # [1 (TC CT AG GA)]                # or                # [5 (AC CA) (AG GA) (AT TA) (CG GC) (CT TC)]                # which are to be stored in \"model_options\" as a string.                if option[0] == \"model\" and option[1] in [9, 10]:                    if self._options[\"model_options\"] is not None:                        ctl_handle.write(                            f\"model = {option[1]}  {self._options['model_options']}\"                        )                        continue                ctl_handle.write(f\"{option[0]} = {option[1]}\\n\")",
        "documentation": "dynamically build a baseml control file from the option the control file is written to the location specified by the ctlfile property of the baseml class"
    },
    {
        "method_code": "def api_healthcheck(self) -> Optional[Exception]:                try:            self._client.get(\"/health\")            return None        except Exception as exc:            return exc",
        "documentation": "attempt to connect to the api and return the encountered exception if not successful if successful return none"
    },
    {
        "method_code": "def items(self):                for key in self:            yield key, self[key]",
        "documentation": "iterate over namespace bioseqdatabase in the database"
    },
    {
        "method_code": "def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"NAME_DESC\": orm_models.BlockDocument.name.desc(),            \"NAME_ASC\": orm_models.BlockDocument.name.asc(),            \"BLOCK_TYPE_AND_NAME_ASC\": sa.text(\"block_type_name asc, name asc\"),        }        return sort_mapping[self.value]",
        "documentation": "return an expression used to sort block document"
    },
    {
        "method_code": "def test_water_file4(self):                # Setup,        query = \"DVCTGKALCDPVTQNIKTYPVKIENLRVMI\"        out_file = \"Emboss/temp_test4.water\"        in_file = \"SwissProt/P0A186.txt\"        self.assertTrue(os.path.isfile(in_file))        if os.path.isfile(out_file):            os.remove(out_file)        cline = WaterCommandline(cmd=exes[\"water\"])        cline.set_parameter(\"-asequence\", f\"asis:{query}\")        cline.set_parameter(\"-bsequence\", in_file)        # EMBOSS should work this out, but let's be explicit:        cline.set_parameter(\"-sprotein\", True)        # TODO - Tell water this is a SwissProt file!        cline.set_parameter(\"-gapopen\", \"20\")        cline.set_parameter(\"-gapextend\", \"5\")        cline.set_parameter(\"-outfile\", out_file)        self.assertEqual(str(eval(repr(cline))), str(cline))        # Run the tool,        self.run_water(cline)        # Check we can parse the output and it is sensible...        self.pairwise_alignment_check(            query,            SeqIO.parse(in_file, \"swiss\"),            AlignIO.parse(out_file, \"emboss\"),            local=True,        )        # Clean up,        os.remove(out_file)",
        "documentation": "run water with the asis trick and swissprot file output to a file"
    },
    {
        "method_code": "def calc_rho_all(self, fname):                raise NotImplementedError",
        "documentation": "provision for estimating spatial structure from allele size for all population"
    },
    {
        "method_code": "def test_exn_22_o_vulgar_fshifts(self):                exn_file = get_file(\"exn_22_o_vulgar_fshifts.exn\")        qresult = read(exn_file, self.fmt)        # check common attributes        for hit in qresult:            self.assertEqual(qresult.id, hit.query_id)            for hsp in hit:                self.assertEqual(hit.id, hsp.hit_id)                self.assertEqual(qresult.id, hsp.query_id)        self.assertEqual(\"gi|296143771|ref|NM_001180731.1|\", qresult.id)        self.assertEqual(\"<unknown description>\", qresult.description)        self.assertEqual(\"exonerate\", qresult.program)        self.assertEqual(1, len(qresult))        # first hit        hit = qresult[0]        self.assertEqual(\"gi|296143771|ref|NM_001180731.1|\", hit.id)        self.assertEqual(\"<unknown description>\", hit.description)        self.assertEqual(2, len(hit))        # first hit, first hsp        hsp = qresult[0][0]        self.assertEqual(213, hsp.score)        self.assertEqual(1, hsp[0].query_strand)        self.assertEqual(1, hsp[0].hit_strand)        self.assertEqual(0, hsp.query_start)        self.assertEqual(465, hsp.hit_start)        self.assertEqual(160, hsp.query_end)        self.assertEqual(630, hsp.hit_end)        self.assertEqual(            [(0, 93), (94, 127), (127, 139), (139, 160)], hsp.query_range_all[:5]        )        self.assertEqual(            [(465, 558), (558, 591), (593, 605), (609, 630)], hsp.hit_range_all[:5]        )        self.assertEqual([], hsp.query_split_codons)        self.assertEqual([], hsp.hit_split_codons)        self.assertEqual(            \" C 93 93 F 1 0 C 33 33 F 0 2 C 12 12 F 0 4 C 21 21\", hsp.vulgar_comp        )        # first hit, second hsp        hsp = qresult[0][1]        self.assertEqual(201, hsp.score)        self.assertEqual(-1, hsp[0].query_strand)        self.assertEqual(-1, hsp[0].hit_strand)        self.assertEqual(1, hsp.query_start)        self.assertEqual(466, hsp.hit_start)        self.assertEqual(158, hsp.query_end)        self.assertEqual(628, hsp.hit_end)        self.assertEqual([(95, 158), (1, 94)], hsp.query_range_all[:5])        self.assertEqual([(559, 628), (466, 559)], hsp.hit_range_all[:5])        self.assertEqual([], hsp.query_split_codons)        self.assertEqual([], hsp.hit_split_codons)        self.assertEqual(\" C 24 24 G 0 6 C 39 39 F 1 0 C 93 93\", hsp.vulgar_comp)",
        "documentation": "test parsing exonerate output exnovulgarfshiftsexn"
    },
    {
        "method_code": "def inverse_indices(self):                a = [-np.ones(len(sequence), int) for sequence in self.sequences]        n, m = self.coordinates.shape        steps = np.diff(self.coordinates, 1)        aligned = sum(steps != 0, 0) > 1        # True for steps in which at least two sequences align, False if a gap        steps = steps[:, aligned]        rcs = np.zeros(n, bool)        for i, row in enumerate(steps):            if (row >= 0).all():                rcs[i] = False            elif (row <= 0).all():                rcs[i] = True            else:                raise ValueError(f\"Inconsistent steps in row {i}\")        i = 0        j = 0        for k in range(m - 1):            starts = self.coordinates[:, k]            ends = self.coordinates[:, k + 1]            for row, start, end, rc in zip(a, starts, ends, rcs):                if rc == False and start < end:  # noqa: E712                    j = i + end - start                    row[start:end] = range(i, j)                elif rc == True and start > end:  # noqa: E712                    j = i + start - end                    if end > 0:                        row[start - 1 : end - 1 : -1] = range(i, j)                    elif start > 0:                        row[start - 1 :: -1] = range(i, j)            i = j        return a",
        "documentation": "return the alignment column index for each letter in each sequence this property return a list of d numpy array the number of array is equal to the number of aligned sequence and the length of each array is equal to the length of the corresponding sequence for each letter in each sequence the array contains the corresponding column index in the alignment letter not included in the alignment are indicated by for example from bio import align aligner alignpairwisealigner alignermode local alignment aligneraligngaactgg aatg alignment alignment printalignment target aactg query aatg blankline alignmentinverseindices array array alignment alignment printalignment target aactgg query aatg blankline alignmentinverseindices array array alignment aligneraligngaactgg catt strand alignment alignment printalignment target aactg query aatg blankline alignmentinverseindices array array alignment alignment printalignment target aactgg query aatg blankline alignmentinverseindices array array"
    },
    {
        "method_code": "def destroy_transition(self, from_state, to_state):                try:            del self.transition_prob[(from_state, to_state)]            del self.transition_pseudo[(from_state, to_state)]        except KeyError:            raise KeyError(                f\"Transition from {from_state} to {to_state} is already disallowed.\"            )",
        "documentation": "restrict transition between the two state raise keyerror if the transition is not currently allowed"
    },
    {
        "method_code": "def pop(self, hit_key=-1, default=__marker):                # if key is an integer (index)        # get the ID for the Hit object at that index        if isinstance(hit_key, int):            # raise the appropriate error if there is no hit            if not self:                raise IndexError(\"pop from empty list\")            hit_key = list(self.hit_keys)[hit_key]        try:            hit = self._items.pop(hit_key)            # remove all alternative IDs of the popped hit            for alt_id in hit.id_all[1:]:                self.__alt_hit_ids.pop(alt_id, None)        except KeyError:            try:                hit = self.pop(self.__alt_hit_ids[hit_key])            except KeyError:                # hit_key is not a valid id                # use the default if it has been set                if default is not self.__marker:                    hit = default                else:                    raise KeyError(hit_key) from None        return hit",
        "documentation": "remove the specified hit key and return the hit object param hitkey key of the hit object to return type hitkey int or string param default return value if no hit exists with the given key type default object by default pop will remove and return the last hit object in the queryresult object to remove specific hit object you can use it integer index or hit key from bio import searchio qresult nextsearchioparseblastmirnaxml blastxml lenqresult for hit in qresult printhitid girefnr girefnr girefnr girefnr girefnr remove the last hit qresultpop hitidgirefxm queryid hsps remove the first hit qresultpop hitidgirefnr queryid hsps remove hit with the given id qresultpopgirefnr hitidgirefnr queryid hsps"
    },
    {
        "method_code": "def child_edges(self, parent):                if parent not in self._adjacency_list:            raise ValueError(\"Unknown <parent> node: \" + str(parent))        return [            (x, self._edge_map[(parent, x)])            for x in sorted(self._adjacency_list[parent])        ]",
        "documentation": "return a list of child label pair for parent"
    },
    {
        "method_code": "def from_environment(    name, prefix, no_builds=False, ignore_channels=False, from_history=False):        pd = PrefixData(prefix, pip_interop_enabled=True)    variables = pd.get_environment_env_vars()    if from_history:        history = History(prefix).get_requested_specs_map()        deps = [str(package) for package in history.values()]        return Environment(            name=name,            dependencies=deps,            channels=list(context.channels),            prefix=prefix,            variables=variables,        )    precs = tuple(PrefixGraph(pd.iter_records()).graph)    grouped_precs = groupby(lambda x: x.package_type, precs)    conda_precs = sorted(        (            *grouped_precs.get(None, ()),            *grouped_precs.get(PackageType.NOARCH_GENERIC, ()),            *grouped_precs.get(PackageType.NOARCH_PYTHON, ()),        ),        key=lambda x: x.name,    )    pip_precs = sorted(        (            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_WHEEL, ()),            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_EGG_MANAGEABLE, ()),            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_EGG_UNMANAGEABLE, ()),        ),        key=lambda x: x.name,    )    if no_builds:        dependencies = [\"=\".join((a.name, a.version)) for a in conda_precs]    else:        dependencies = [\"=\".join((a.name, a.version, a.build)) for a in conda_precs]    if pip_precs:        dependencies.append({\"pip\": [f\"{a.name}=={a.version}\" for a in pip_precs]})    channels = list(context.channels)    if not ignore_channels:        for prec in conda_precs:            canonical_name = prec.channel.canonical_name            if canonical_name not in channels:                channels.insert(0, canonical_name)    return Environment(        name=name,        dependencies=dependencies,        channels=channels,        prefix=prefix,        variables=variables,    )",
        "documentation": "get environment object from prefix args name the name of environment prefix the path of prefix nobuilds whether ha build requirement ignorechannels whether ignorechannels fromhistory whether environment file should be based on explicit spec in history return environment object"
    },
    {
        "method_code": "def schemaHandler(self, name, attrs):                key = \"%s noNamespaceSchemaLocation\" % self.schema_namespace        schema = attrs[key]        handle = self.open_xsd_file(os.path.basename(schema))        # if there is no local xsd file grab the url and parse the file        if not handle:            handle = urlopen(schema)            text = handle.read()            self.save_xsd_file(os.path.basename(schema), text)            handle.close()            self.parse_xsd(ET.fromstring(text))        else:            self.parse_xsd(ET.fromstring(handle.read()))            handle.close()        # continue handling the element        self.startElementHandler(name, attrs)        # reset the element handler        self.parser.StartElementHandler = self.startElementHandler",
        "documentation": "process the xml schema before processing the element"
    },
    {
        "method_code": "def _draw_title(self, cur_drawing, title, start_x, start_y, end_x, end_y):                x_center = start_x + (end_x - start_x) / 2        y_pos = end_y + (self.padding_percent * (start_y - end_y)) / 2        title_string = String(x_center, y_pos, title)        title_string.fontName = \"Helvetica-Bold\"        title_string.fontSize = self.chart_title_size        title_string.textAnchor = \"middle\"        cur_drawing.add(title_string)",
        "documentation": "add the title of the figure to the drawing private"
    },
    {
        "method_code": "def startDocument(self):                self._write(            '<?xml version=\"1.0\"?>\\n'            '<!DOCTYPE BlastOutput PUBLIC \"-//NCBI//NCBI BlastOutput/EN\" '            '\"http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd\">\\n'        )",
        "documentation": "start the xml document"
    },
    {
        "method_code": "def _load_from_block_document(        cls, block_document: BlockDocument, validate: bool = True    ) -> \"Self\":                try:            return cls._from_block_document(block_document)        except ValidationError as e:            if not validate:                missing_fields = tuple(err[\"loc\"][0] for err in e.errors())                missing_block_data = {field: None for field in missing_fields}                warnings.warn(                    f\"Could not fully load {block_document.name!r} of block type\"                    f\" {cls.get_block_type_slug()!r} - this is likely because one or more\"                    \" required fields were added to the schema for\"                    f\" {cls.__name__!r} that did not exist on the class when this block\"                    \" was last saved. Please specify values for new field(s):\"                    f\" {listrepr(missing_fields)}, then run\"                    f' `{cls.__name__}.save(\"{block_document.name}\", overwrite=True)`,'                    \" and load this block again before attempting to use it.\"                )                return cls.model_construct(**block_document.data, **missing_block_data)            raise RuntimeError(                f\"Unable to load {block_document.name!r} of block type\"                f\" {cls.get_block_type_slug()!r} due to failed validation. To load without\"                \" validation, try loading again with `validate=False`.\"            ) from e",
        "documentation": "load a block from a given block document if a block document for a given block type is saved with a different schema than the current class calling load a warning will be raised if the current class schema is a subset of the block document schema the block can be loaded a normal using the default validate true if the current class schema is a superset of the block document schema load must be called with validate set to false to prevent a validation error in this case the block attribute will default to none and must be set manually and saved to a new block document before the block can be used a expected args blockdocument the block document used to instantiate a block validate if false the block document will be loaded without pydantic validating the block schema this is useful if the block schema ha changed clientside since the block document referred to by name wa saved raise valueerror if the requested block document is not found return an instance of the current class hydrated with the data stored in the block document with the specified name"
    },
    {
        "method_code": "def __init__(self, server, sock):                super().__init__(server, sock)",
        "documentation": "create a new connection for the accepted socket client"
    },
    {
        "method_code": "def __ne__(self, other: object) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "documentation": "define the notequalto operand not implemented"
    },
    {
        "method_code": "def parse(handle, format):        # Try and give helpful error messages:    if not isinstance(format, str):        raise TypeError(\"Need a string for the file format (lower case)\")    if not format:        raise ValueError(\"Format required (lower case string)\")    if format != format.lower():        raise ValueError(f\"Format string '{format}' should be lower case\")    if format not in _FormatToIterator:        raise ValueError(f\"Unknown format '{format}'\")    with as_handle(handle) as fp:        yield from _FormatToIterator[format](fp)",
        "documentation": "turn a phenotype file into an iterator returning platerecords handle handle to the file or the filename a a string note older version of biopython only took a handle format lower case string describing the file format typical usage opening a file to read in and looping over the record from bio import phenotype filename phenotypeplatescsv for record in phenotypeparsefilename pmcsv printid s recordid printnumber of well i lenrecord id pm number of well id pm number of well use the biophenotyperead function when you expect a single record only"
    },
    {
        "method_code": "def test_hmmerdomtab_30_single(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_004.out\")        raw =         self.check_raw(filename, \"gi|126362951:116-221\", raw)",
        "documentation": "test hmmscandomtab raw string retrieval hmmer single query domtabhmmscanout"
    },
    {
        "method_code": "def get_link_data(self, id):                self.c.execute(            \"SELECT id,url,name,size,status,error,plugin,package,linkorder FROM links WHERE id=?\",            (str(id),),        )        r = self.c.fetchone()        if not r:            return None        data = {            r[0]: {                \"id\": r[0],                \"url\": r[1],                \"name\": r[2],                \"size\": r[3],                \"format_size\": format.size(r[3]),                \"status\": r[4],                \"statusmsg\": self.pyload.files.status_msg[r[4]],                \"error\": r[5],                \"plugin\": r[6],                \"package\": r[7],                \"order\": r[8],            }        }        return data",
        "documentation": "get link information a dict"
    },
    {
        "method_code": "def fragments(self):                return list(chain(*self._items))",
        "documentation": "access the hspfragment object contained in the hit"
    },
    {
        "method_code": "def SetPattern(self, pattern):                self.pattern = pattern        self.rx_pattern = self.IUPAC2regex(pattern)        self.rx = re.compile(self.rx_pattern)",
        "documentation": "convert search pattern to regular expression"
    },
    {
        "method_code": "def __str__(self):                output = self._reference_line()        output += self._authors_line()        output += self._consrtm_line()        output += self._title_line()        output += self._journal_line()        output += self._medline_line()        output += self._pubmed_line()        output += self._remark_line()        return output",
        "documentation": "convert the reference to a genbank format string"
    },
    {
        "method_code": "def startEntryElement(self, name, qname, attrs):                if name != (None, \"entry\"):            raise ValueError(\"Expected to find the start of an entry element\")        if qname is not None:            raise RuntimeError(\"Unexpected qname for entry element\")        record = SeqRecord(None, id=None)        if self.speciesName is not None:            record.annotations[\"organism\"] = self.speciesName        if self.ncbiTaxID is not None:            record.annotations[\"ncbi_taxid\"] = self.ncbiTaxID        record.annotations[\"source\"] = self.source        for key, value in attrs.items():            namespace, localname = key            if namespace is None:                if localname == \"id\":                    record.id = value                elif localname == \"source\" and (                    self.seqXMLversion == \"0.3\" or self.seqXMLversion == \"0.4\"                ):                    record.annotations[\"source\"] = value                else:                    raise ValueError(                        f\"Unexpected attribute {localname} in entry element\"                    )            else:                raise ValueError(                    f\"Unexpected namespace '{namespace}' for entry attribute\"                )        if record.id is None:            raise ValueError(\"Failed to find entry ID\")        self.records.append(record)        if self.seqXMLversion == \"0.1\":            self.startElementNS = self.startEntryFieldElementVersion01        else:            self.startElementNS = self.startEntryFieldElement        self.endElementNS = self.endEntryElement",
        "documentation": "set new entry with id and the optional entry source private"
    },
    {
        "method_code": "def get_account_plugins(self):                plugins = []        for plugin in self.accounts.keys():            plugins.append(self.get_account_plugin(plugin))        return plugins",
        "documentation": "get all account instance"
    },
    {
        "method_code": "def null(self) -> N:                # Note that this default implementation only makes sense for T = N.        # It would be better to implement `null()` only in subclasses, or        # have a field null_type similar to `model_type` and use that here.        return cast(N, self.model_type())",
        "documentation": "the value to be exposed when the underlying value is none"
    },
    {
        "method_code": "def element(self):                # The root is this Relation element        relation = ET.Element(\"relation\")        relation.attrib = {            \"entry1\": str(self._entry1),            \"entry2\": str(self._entry2),            \"type\": self.type,        }        for name, value in self.subtypes:            subtype = ET.Element(\"subtype\")            subtype.attrib = {\"name\": name, \"value\": str(value)}            relation.append(subtype)        return relation",
        "documentation": "return kgml element describing the relation"
    },
    {
        "method_code": "def _num_difference(obj_a, obj_b):        attrs_a = set(obj_a.__dict__)    attrs_b = set(obj_b.__dict__)    diff = attrs_a.symmetric_difference(attrs_b)    privates = len([x for x in diff if x.startswith(\"_\")])    return len(diff) - privates",
        "documentation": "return the number of instance attribute present only in one object"
    },
    {
        "method_code": "def test_blank_seq(self):                self.write_read(os.path.join(\"GenBank\", \"blank_seq.gb\"), \"gb\", [\"gb\"])",
        "documentation": "write and read back blankseqgb"
    },
    {
        "method_code": "def threshold_patser(self):                return self.threshold_fpr(fpr=2**-self.ic)",
        "documentation": "threshold selection mimicking the behaviour of patser hertz stormo software it selects such a threshold that the logfpricm note the actual patser software us natural logarithm instead of log so the number are not directly comparable"
    },
    {
        "method_code": "def confirm(message, **kwargs):        return Confirm.ask(f\"[bold][green]?[/] {message}[/]\", **kwargs)",
        "documentation": "utility to prompt the user for confirmation with consistent styling"
    },
    {
        "method_code": "def _fetch_info(self, items, write, force):                tags = self.config[\"tags\"].as_str_seq()        for item in items:            # If we're not forcing re-downloading for all tracks, check            # whether the data is already present. We use one            # representative field name to check for previously fetched            # data.            if not force:                mood_str = item.get(\"mood_acoustic\", \"\")                if mood_str:                    self._log.info(\"data already present for: {}\", item)                    continue            # We can only fetch data for tracks with MBIDs.            if not item.mb_trackid:                continue            self._log.info(\"getting data for: {}\", item)            data = self._get_data(item.mb_trackid)            if data:                for attr, val in self._map_data_to_scheme(data, ABSCHEME):                    if not tags or attr in tags:                        self._log.debug(                            \"attribute {} of {} set to {}\", attr, item, val                        )                        setattr(item, attr, val)                    else:                        self._log.debug(                            \"skipping attribute {} of {}\"                            \" (value {}) due to config\",                            attr,                            item,                            val,                        )                item.store()                if write:                    item.try_write()",
        "documentation": "fetch additional information from acousticbrainz for the item"
    },
    {
        "method_code": "def write_file(self, qresults):                handle = self.handle        qresult_counter, hit_counter, hsp_counter, frag_counter = 0, 0, 0, 0        try:            first_qresult = next(qresults)        except StopIteration:            handle.write(self._build_header())        else:            # write header            handle.write(self._build_header(first_qresult))            # and then the qresults            for qresult in chain([first_qresult], qresults):                if qresult:                    handle.write(self._build_row(qresult))                    qresult_counter += 1                    hit_counter += len(qresult)                    hsp_counter += sum(len(hit) for hit in qresult)                    frag_counter += sum(len(hit.fragments) for hit in qresult)        return qresult_counter, hit_counter, hsp_counter, frag_counter",
        "documentation": "write to the handle return a tuple of how many queryresult hit and hsp object were written"
    },
    {
        "method_code": "def _parse_tag_data(elem_code, elem_num, raw_data):        if elem_code in _BYTEFMT:        # because '>1s' unpack differently from '>s'        if elem_num == 1:            num = \"\"        else:            num = str(elem_num)        fmt = \">\" + num + _BYTEFMT[elem_code]        assert len(raw_data) == struct.calcsize(fmt)        data = struct.unpack(fmt, raw_data)        # no need to use tuple if len(data) == 1        # also if data is date / time        if elem_code not in [10, 11] and len(data) == 1:            data = data[0]        # account for different data types        if elem_code == 2:            return data        elif elem_code == 10:            return str(datetime.date(*data))        elif elem_code == 11:            return str(datetime.time(*data[:3]))        elif elem_code == 13:            return bool(data)        elif elem_code == 18:            return data[1:]        elif elem_code == 19:            return data[:-1]        else:            return data    else:        return None",
        "documentation": "return single data value private argument elemcode what kind of data elemnum how many data point rawdata abi file object from which the tag would be unpacked"
    },
    {
        "method_code": "def __getitem__(self, i):                return self._ar[i]",
        "documentation": "return value of array index i"
    },
    {
        "method_code": "def resolve_permission(name):        try:        app_label, codename = name.split('.')        action, model_name = codename.rsplit('_', 1)    except ValueError:        raise ValueError(            _(\"Invalid permission name: {name}. Must be in the format <app_label>.<action>_<model>\").format(name=name)        )    return app_label, action, model_name",
        "documentation": "given a permission name return the applabel action and modelname component for example dcimviewsite return dcim view site param name permission name in the format applabelactionmodel"
    },
    {
        "method_code": "def to_cache(cls, instance, custom_fields=None):                values = []        # Capture built-in fields        for name, weight in cls.fields:            try:                type_ = cls.get_field_type(instance, name)            except FieldDoesNotExist:                # Not a concrete field; handle as an object attribute                type_ = cls.get_attr_type(instance, name)            value = cls.get_field_value(instance, name)            if type_ and value:                values.append(                    ObjectFieldValue(name, type_, weight, value)                )        # Capture custom fields        if getattr(instance, 'custom_field_data', None):            if custom_fields is None:                custom_fields = instance.custom_fields            for cf in custom_fields:                type_ = cf.search_type                value = instance.custom_field_data.get(cf.name)                weight = cf.search_weight                if type_ and value and weight:                    values.append(                        ObjectFieldValue(f'cf_{cf.name}', type_, weight, value)                    )        return values",
        "documentation": "return a list of objectfieldvalue representing the instance field to be cached args instance the instance being cached customfields an iterable of customfields to include when caching the instance if none all custom field defined for the model will be included this can also be provided during bulk caching to avoid looking up the available custom field for each instance"
    },
    {
        "method_code": "def clear_oob_ip(instance, **kwargs):        if device := Device.objects.filter(oob_ip=instance).first():        device.snapshot()        device.oob_ip = None        device.save()",
        "documentation": "when an ipaddress is deleted trigger save on any device for which it wa a oob ip"
    },
    {
        "method_code": "def versions_from_parentdir(    parentdir_prefix: str,    root: str,    verbose: bool,) -> Dict[str, Any]:        rootdirs = []    for _ in range(3):        dirname = os.path.basename(root)        if dirname.startswith(parentdir_prefix):            return {                \"version\": dirname[len(parentdir_prefix) :],                \"full-revisionid\": None,                \"dirty\": False,                \"error\": None,                \"date\": None,            }        rootdirs.append(root)        root = os.path.dirname(root)  # up a level    if verbose:        print(            \"Tried directories %s but none started with prefix %s\"            % (str(rootdirs), parentdir_prefix)        )    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")",
        "documentation": "try to determine the version from the parent directory name source tarballs conventionally unpack into a directory that includes both the project name and a version string we will also support searching up two directory level for an appropriately named parent directory"
    },
    {
        "method_code": "def test_ppbuilder_real(self):                ppb = PPBuilder()        pp = ppb.build_peptides(self.structure)        self.assertEqual(len(pp), 3)        # Check termini        self.assertEqual(pp[0][0].get_id()[1], 152)        self.assertEqual(pp[0][-1].get_id()[1], 184)        self.assertEqual(pp[1][0].get_id()[1], 186)        self.assertEqual(pp[1][-1].get_id()[1], 213)        self.assertEqual(pp[2][0].get_id()[1], 216)        self.assertEqual(pp[2][-1].get_id()[1], 220)        # Now check sequences        pp0_seq = pp[0].get_sequence()        pp1_seq = pp[1].get_sequence()        pp2_seq = pp[2].get_sequence()        self.assertIsInstance(pp0_seq, Seq)        self.assertEqual(pp0_seq, \"DIRQGPKEPFRDYVDRFYKTLRAEQASQEVKNW\")        self.assertEqual(pp1_seq, \"TETLLVQNANPDCKTILKALGPGATLEE\")        self.assertEqual(pp2_seq, \"TACQG\")",
        "documentation": "test ppbuilder on real pdb file"
    },
    {
        "method_code": "def read_title_and_seq(filename):        with open(filename) as handle:        title = handle.readline().rstrip()        assert title.startswith(\">\")        seq = \"\"        for line in handle:            if line.startswith(\">\"):                break            seq += line.strip()    return title[1:], seq",
        "documentation": "crude parser that get the first record from a fasta file"
    },
    {
        "method_code": "def __str__(self):                rep = [self.comment_line + \"\\n\"]        rep.append(\"\\n\".join(self.loci_list) + \"\\n\")        for pop in self.populations:            rep.append(\"Pop\\n\")            for indiv in pop:                name, markers = indiv                rep.append(name)                rep.append(\",\")                for marker in markers:                    rep.append(\" \")                    for al in marker:                        if al is None:                            al = \"0\"                        aStr = str(al)                        while len(aStr) < self.marker_len:                            aStr = \"\".join([\"0\", aStr])                        rep.append(aStr)                rep.append(\"\\n\")        return \"\".join(rep)",
        "documentation": "return reconstruct a genepop textual representation"
    },
    {
        "method_code": "def test_password_validation_enforced(self):                self.add_permissions('users.add_user')        data = {            'username': 'new_user',            'password': 'F1a',            'confirm_password': 'F1a',        }        # Password too short        request = {            'path': self._get_url('add'),            'data': data,        }        response = self.client.post(**request)        self.assertHttpStatus(response, 200)        # Password long enough        data['password'] = 'fooBarFoo123'        data['confirm_password'] = 'fooBarFoo123'        self.assertHttpStatus(self.client.post(**request), 302)        # Password no number        data['password'] = 'FooBarFooBar'        data['confirm_password'] = 'FooBarFooBar'        self.assertHttpStatus(self.client.post(**request), 200)        # Password no letter        data['password'] = '123456789123'        data['confirm_password'] = '123456789123'        self.assertHttpStatus(self.client.post(**request), 200)        # Password no uppercase        data['password'] = 'foobar123abc'        data['confirm_password'] = 'foobar123abc'        self.assertHttpStatus(self.client.post(**request), 200)        # Password no lowercase        data['password'] = 'FOOBAR123ABC'        data['confirm_password'] = 'FOOBAR123ABC'        self.assertHttpStatus(self.client.post(**request), 200)",
        "documentation": "test that any configured password validation rule authpasswordvalidators are enforced"
    },
    {
        "method_code": "def get_bfactor(self):                return self.bfactor",
        "documentation": "return b factor"
    },
    {
        "method_code": "def score(self, seqA, seqB, strand=\"+\"):                if isinstance(seqA, (Seq, MutableSeq, SeqRecord)):            seqA = bytes(seqA)        if strand == \"-\":            seqB = reverse_complement(seqB)        if isinstance(seqB, (Seq, MutableSeq, SeqRecord)):            seqB = bytes(seqB)        return super().score(seqA, seqB, strand)",
        "documentation": "return the alignment score of two sequence using pairwisealigner"
    },
    {
        "method_code": "def import_stages():        stages = []    for plugin in find_plugins():        stages += plugin.get_import_stages()    return stages",
        "documentation": "get a list of import stage function defined by plugins"
    },
    {
        "method_code": "def frame1(self, seq, translation_table=1):                return translate(seq, table=translation_table)",
        "documentation": "translate first reading frame"
    },
    {
        "method_code": "def _parse_iso_timestamp(iso_timestamp: str | None) -> datetime | None:                if iso_timestamp is None:            return None        try:            return datetime.fromisoformat(iso_timestamp)        except ValueError:            return None",
        "documentation": "parse iso timestamp and fail over to a default value of none"
    },
    {
        "method_code": "def utilization_graph(utilization, warning_threshold=75, danger_threshold=90):        if utilization == 100:        bar_class = 'bg-secondary'    elif danger_threshold and utilization >= danger_threshold:        bar_class = 'bg-danger'    elif warning_threshold and utilization >= warning_threshold:        bar_class = 'bg-warning'    elif warning_threshold or danger_threshold:        bar_class = 'bg-success'    else:        bar_class = 'bg-gray'    return {        'utilization': utilization,        'bar_class': bar_class,    }",
        "documentation": "display a horizontal bar graph indicating a percentage of utilization"
    },
    {
        "method_code": "def test_E3MFGYR02_index_in_middle(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_index_in_middle.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "documentation": "write and read back emfgyrindexinmiddlesff"
    },
    {
        "method_code": "def test_year_single_year(self):                self._setup_config(bucket_year=[\"1950s\", \"1970s\"])        assert self.plugin._tmpl_bucket(\"1959\") == \"1950s\"        assert self.plugin._tmpl_bucket(\"1969\") == \"1950s\"",
        "documentation": "if a single year is given range start from this year and stop at the year preceding the one of next bucket"
    },
    {
        "method_code": "def __contains__(self, char: str) -> bool:                return char in self.seq",
        "documentation": "implement the in keyword search the sequence eg from bio import seqio record seqioreadfastasweetpeanu fasta gaattc in record false aaa in record true this essentially act a a proxy for using in on the sequence gaattc in recordseq false aaa in recordseq true note that you can also use seq object a the query from bioseq import seq seqaaa in record true see also the seq object contains method"
    },
    {
        "method_code": "def _recommendation(    results: Sequence[AlbumMatch | TrackMatch],) -> Recommendation:        if not results:        # No candidates: no recommendation.        return Recommendation.none    # Basic distance thresholding.    min_dist = results[0].distance    if min_dist < config[\"match\"][\"strong_rec_thresh\"].as_number():        # Strong recommendation level.        rec = Recommendation.strong    elif min_dist <= config[\"match\"][\"medium_rec_thresh\"].as_number():        # Medium recommendation level.        rec = Recommendation.medium    elif len(results) == 1:        # Only a single candidate.        rec = Recommendation.low    elif (        results[1].distance - min_dist        >= config[\"match\"][\"rec_gap_thresh\"].as_number()    ):        # Gap between first two candidates is large.        rec = Recommendation.low    else:        # No conclusion. Return immediately. Can't be downgraded any further.        return Recommendation.none    # Downgrade to the max rec if it is lower than the current rec for an    # applied penalty.    keys = set(min_dist.keys())    if isinstance(results[0], hooks.AlbumMatch):        for track_dist in min_dist.tracks.values():            keys.update(list(track_dist.keys()))    max_rec_view = config[\"match\"][\"max_rec\"]    for key in keys:        if key in list(max_rec_view.keys()):            max_rec = max_rec_view[key].as_choice(                {                    \"strong\": Recommendation.strong,                    \"medium\": Recommendation.medium,                    \"low\": Recommendation.low,                    \"none\": Recommendation.none,                }            )            rec = min(rec, max_rec)    return rec",
        "documentation": "given a sorted list of albummatch or trackmatch object return a recommendation based on the result distance if the recommendation is higher than the configured maximum for an applied penalty the recommendation will be downgraded to the configured maximum for that penalty"
    },
    {
        "method_code": "def test_000_genbank_bad_loc_wrap_warning(self):                path = \"GenBank/bad_loc_wrap.gb\"        with warnings.catch_warnings():            warnings.simplefilter(\"error\", BiopythonParserWarning)            with open(path) as handle:                with self.assertRaises(BiopythonParserWarning) as cm:                    GenBank.read(handle)                self.assertEqual(                    \"Non-standard feature line wrapping (didn't break on comma)?\",                    str(cm.exception),                )",
        "documentation": "feature line wrapping warning"
    },
    {
        "method_code": "def test_1_flawedpdb_permissive(self):                with warnings.catch_warnings(record=True) as w:            warnings.simplefilter(\"always\", PDBConstructionWarning)            # Trigger warnings            self.permissive.get_structure(\"example\", \"PDB/a_structure.pdb\")            self.assertEqual(len(w), 15)            for wrn, msg in zip(                w,                [                    # Expected warning messages:                    \"Used element 'N' for Atom (name=N) with given element ''\",                    \"Used element 'C' for Atom (name=CA) with given element ''\",                    \"Atom names ' CA ' and 'CA  ' differ only in spaces at line 18.\",                    \"Used element 'CA' for Atom (name=CA  ) with given element ''\",                    \"Atom N defined twice in residue <Residue ARG het=  resseq=2 icode= > at line 22.\",                    \"disordered atom found with blank altloc before line 34.\",                    \"Residue (' ', 4, ' ') redefined at line 44.\",                    \"Blank altlocs in duplicate residue SER (' ', 4, ' ') of chain 'A' at line 44.\",                    \"Residue (' ', 10, ' ') redefined at line 76.\",                    \"Residue (' ', 14, ' ') redefined at line 107.\",                    \"Residue (' ', 16, ' ') redefined at line 136.\",                    \"Residue (' ', 80, ' ') redefined at line 634.\",                    \"Residue (' ', 81, ' ') redefined at line 647.\",                    \"Ignoring unrecognized record 'ATOM 1' at line 777\",                    \"Atom O defined twice in residue <Residue HOH het=W resseq=67 icode= > at line 904.\",                ],            ):                self.assertIn(msg, str(wrn))",
        "documentation": "parse a flawed pdb file in permissive mode check warning"
    },
    {
        "method_code": "def add_product(self, product_id):                if self._pathway is not None:            if int(product_id) not in self._pathway.entries:                raise ValueError(                    \"Couldn't add product, no node ID %d in Pathway\" % product_id                )        self._products.add(int(product_id))",
        "documentation": "add a product identified by it node id to the reaction"
    },
    {
        "method_code": "def length(self):                n = len(self.coordinates)        if n == 0:  # no sequences            return 0        steps = np.diff(self.coordinates, 1)        aligned = sum(steps != 0, 0) > 1        # True for steps in which at least two sequences align, False if a gap        for i in range(n):            row = steps[i, aligned]            if (row >= 0).all():                pass            elif (row <= 0).all():                steps[i, :] = -steps[i, :]            else:                raise ValueError(f\"Inconsistent steps in row {i}\")        gaps = steps.max(0)        if not ((steps == gaps) | (steps <= 0)).all():            raise ValueError(\"Unequal step sizes in alignment\")        return int(sum(gaps))",
        "documentation": "return the alignment length ie the number of column when printed the alignment length is the number of column in the alignment when it is printed and is equal to the sum of the number of match number of mismatch and the total length of gap in the target and query sequence section beyond the aligned segment are not included in the number of column for example from bio import align aligner alignpairwisealigner alignermode global alignment aligneraligngacctg cgatcg alignment alignment printalignment target gacctg query cgatcg blankline alignmentlength alignermode local alignment aligneraligngacctg cgatcg alignment alignment printalignment target gacctg query gatcg blankline lenalignment alignmentlength"
    },
    {
        "method_code": "def imported(self, session: ImportSession, task: ImportTask):                if self.config[\"auto\"]:            if task.is_album:                self.handle_album(task.album, False, self.force_on_import)            else:                # Should be a SingletonImportTask                assert hasattr(task, \"item\")                self.handle_track(task.item, False, self.force_on_import)",
        "documentation": "add replay gain info to item or album of task"
    },
    {
        "method_code": "def return_v_or_none(v: Optional[str]) -> Optional[str]:        if not v:        return None    return v",
        "documentation": "make sure that empty string are treated a none"
    },
    {
        "method_code": "def test_retrieve_pdb_file_small_pdb(self):                structure = \"127d\"        self.check(            structure, os.path.join(structure[1:3], f\"pdb{structure}.ent\"), \"pdb\"        )",
        "documentation": "test retrieving the small molecule in pdb format"
    },
    {
        "method_code": "def database_label_expression(self, db: PrefectDBInterface, time_interval: float):                time_delta = self.as_timedelta(time_interval)        if db.dialect.name == \"postgresql\":            # The date_bin function can do the bucketing for us:            # https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-BIN            return sa.func.to_char(                sa.func.date_bin(time_delta, db.Event.occurred, PIVOT_DATETIME),                'YYYY-MM-DD\"T\"HH24:MI:SSTZH:TZM',            )        elif db.dialect.name == \"sqlite\":            # We can't use date_bin in SQLite, so we have to do the bucketing manually            seconds_since_epoch = sa.func.strftime(\"%s\", db.Event.occurred)            # Convert the total seconds of the timedelta to a constant in SQL            bucket_size = time_delta.total_seconds()            # Perform integer division and multiplication to find the bucket start epoch using SQL functions            bucket_start_epoch = sa.func.cast(                (sa.cast(seconds_since_epoch, sa.Integer) / bucket_size) * bucket_size,                sa.Integer,            )            bucket_datetime = sa.func.strftime(                \"%Y-%m-%dT%H:%M:%SZ\", sa.func.datetime(bucket_start_epoch, \"unixepoch\")            )            return bucket_datetime        else:            raise NotImplementedError(f\"Dialect {db.dialect.name} is not supported.\")",
        "documentation": "return the sql expression to label a time bucket"
    },
    {
        "method_code": "def __init__(self, source):                super().__init__(source, mode=\"b\", fmt=\"GCK\")        # Skip file header        # GCK files start with a 24-bytes header. Bytes 4 and 8 seem to        # always be 12, maybe this could act as a magic cookie. Bytes        # 17-20 and 21-24 contain variable values of unknown meaning.        # check if file is empty        data = self.stream.read(24)        if not data:            raise ValueError(\"Empty file.\")        if len(data) < 24:            raise ValueError(\"Improper header, cannot read 24 bytes from stream\")",
        "documentation": "break up a gck file into seqrecord object note that a gck file can only contain one sequence so this iterator will always return a single record"
    },
    {
        "method_code": "def test_E3MFGYR02_no_manifest(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_no_manifest.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "documentation": "write and read back emfgyrnomanifestsff"
    },
    {
        "method_code": "def get_s3_client(self) -> S3Client:                return self.get_client(client_type=ClientType.S3)",
        "documentation": "get an authenticated s client return an authenticated s client"
    },
    {
        "method_code": "def _find_root_block_schema(    block_schemas_with_references: List[        Tuple[BlockSchema, Optional[str], Optional[UUID]]    ],):        return next(        (            copy(block_schema)            for (                block_schema,                _,                parent_block_schema_id,            ) in block_schemas_with_references            if parent_block_schema_id is None        ),        None,    )",
        "documentation": "attempt to find the root block schema from a list of block schema with reference return none if a root block schema is not found return only the first potential root block schema if multiple are found"
    },
    {
        "method_code": "def write_record(self, record: SeqRecord) -> None:                self._record_written = True        handle = self.handle        wrap = self.wrap        if self.record2title:            title = self.clean(self.record2title(record))        else:            id_ = self.clean(record.id) if record.id else \"\"            description = self.clean(record.description)            if description and description.split(None, 1)[0] == id_:                # The description includes the id at the start                title = description            elif description:                title = f\"{id} {description}\"            else:                title = id_        handle.write(f\">{title}\\n\")        qualities = _get_phred_quality(record)        try:            # This rounds to the nearest integer.            # TODO - can we record a float in a qual file?            qualities_strs = [(\"%i\" % round(q, 0)) for q in qualities]        except TypeError:            if None in qualities:                raise TypeError(\"A quality value of None was found\") from None            else:                raise        if wrap is not None and wrap > 5:            # Fast wrapping            data = \" \".join(qualities_strs)            while True:                if len(data) <= wrap:                    self.handle.write(data + \"\\n\")                    break                else:                    # By construction there must be spaces in the first X chars                    # (unless we have X digit or higher quality scores!)                    i = data.rfind(\" \", 0, wrap)                    handle.write(data[:i] + \"\\n\")                    data = data[i + 1 :]        elif wrap:            # Safe wrapping            while qualities_strs:                line = qualities_strs.pop(0)                while qualities_strs and len(line) + 1 + len(qualities_strs[0]) < wrap:                    line += \" \" + qualities_strs.pop(0)                handle.write(line + \"\\n\")        else:            # No wrapping            data = \" \".join(qualities_strs)            handle.write(data + \"\\n\")",
        "documentation": "write a single qual record to the file"
    },
    {
        "method_code": "def prep_object_data(self, requested_objects, available_objects, parent):                return requested_objects",
        "documentation": "prepare data by setting any programmatically determined object attribute eg next available vlan id on the request data"
    },
    {
        "method_code": "def __init__(self, cmd=\"fdnapars\", **kwargs):                self.parameters = [            _Option(                [\"-sequence\", \"sequence\"],                \"seq file to use (phylip)\",                filename=True,                is_required=True,            ),            _Option([\"-intreefile\", \"intreefile\"], \"Phylip tree file\"),            _Option([\"-weights\", \"weights\"], \"weights file\"),            _Option([\"-maxtrees\", \"maxtrees\"], \"max trees to save during run\"),            _Option([\"-thorough\", \"thorough\"], \"more thorough search (Y/n)\"),            _Option([\"-rearrange\", \"rearrange\"], \"Rearrange on just 1 best tree (Y/n)\"),            _Option(                [\"-transversion\", \"transversion\"], \"Use tranversion parsimony (y/N)\"            ),            _Option(                [\"-njumble\", \"njumble\"],                \"number of times to randomise input order (default is 0)\",            ),            _Option([\"-seed\", \"seed\"], \"provide random seed\"),            _Option([\"-outgrno\", \"outgrno\"], \"Specify outgroup\"),            _Option([\"-thresh\", \"thresh\"], \"Use threshold parsimony (y/N)\"),            _Option([\"-threshold\", \"threshold\"], \"Threshold value\"),            _Option([\"-trout\", \"trout\"], \"Write trees to file (Y/n)\"),            _Option([\"-outtreefile\", \"outtreefile\"], \"filename for output tree\"),            _Option([\"-dotdiff\", \"dotdiff\"], \"Use dot-differencing? [Y/n]\"),        ]        _EmbossCommandLine.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def test_fasta(self):                records1 = list(SeqIO.parse(\"Quality/example.fasta\", \"fasta\"))        records2 = list(SeqIO.parse(\"Quality/example.fastq\", \"fastq\"))        self.compare_records(records1, records2)",
        "documentation": "check fastq parsing match fasta parsing"
    },
    {
        "method_code": "def _run_point_mutation_tests(self, parser):                structure = parser.get_structure(\"example\", \"PDB/3JQH.cif\")        # Residue 1 and 15 should be disordered.        res_1 = structure[0][\"A\"][1]        res_15 = structure[0][\"A\"][15]        # Cursory check -- this would be true even if the residue just        # contained some disordered atoms.        self.assertTrue(res_1.is_disordered(), \"Residue 1 is disordered\")        self.assertTrue(res_15.is_disordered(), \"Residue 15 is disordered\")        # Check a non-mutated residue just to be sure we didn't break the        # parser and cause everything to be disordered.        self.assertFalse(            structure[0][\"A\"][13].is_disordered(), \"Residue 13 is not disordered\"        )        # Check that the residue types were parsed correctly.        self.assertEqual(            set(res_1.disordered_get_id_list()),            {\"PRO\", \"SER\"},            \"Residue 1 is proline/serine\",        )        self.assertEqual(            set(res_15.disordered_get_id_list()),            {\"ARG\", \"GLN\", \"GLU\"},            \"Residue 15 is arginine/glutamine/glutamic acid\",        )        # Quickly check that we can switch between residues and that the        # correct set of residues was parsed.        res_1.disordered_select(\"PRO\")        self.assertAlmostEqual(            res_1[\"CA\"].get_occupancy(), 0.83, 2, \"Residue 1 proline occupancy correcy\"        )        res_1.disordered_select(\"SER\")        self.assertAlmostEqual(            res_1[\"CA\"].get_occupancy(), 0.17, 2, \"Residue 1 serine occupancy correcy\"        )",
        "documentation": "shared test code for testing point mutation"
    },
    {
        "method_code": "def touch_nonadmin(prefix):        if on_win and exists(join(context.root_prefix, \".nonadmin\")):        if not isdir(prefix):            os.makedirs(prefix)        with open_utf8(join(prefix, \".nonadmin\"), \"w\") as fo:            fo.write(\"\")",
        "documentation": "creates prefixnonadmin if sysprefixnonadmin exists on window"
    },
    {
        "method_code": "def _sub_set(self, wanted):                # It seems that this method is not used in the whole class!        return {k: v for k, v in self.mapping.items() if k in wanted}",
        "documentation": "filter result for key which are in wanted private internal use only return a dict screen the result through wanted set keep only the result for which the enzyme is in wanted set"
    },
    {
        "method_code": "def test_render_KGML_modify(self):                # We test rendering of the original KGML for KO01100,        # modifying line width for the lipid pathway        p = self.data        with open(p[0].infilename) as f:            pathway = read(f)            mod_rs = [                e                for e in pathway.orthologs                if len(set(e.name.split()).intersection(self.ko_ids))            ]            for r in mod_rs:                for g in r.graphics:                    g.width = 10            kgml_map = KGMLCanvas(pathway)            kgml_map.draw(p[0].output_stem + \"_widths.pdf\")        # We test rendering of the original KGML for KO3070,        # modifying the reaction colours for each ortholog entry        with open(p[1].infilename) as f:            pathway = read(f)            orthologs = list(pathway.orthologs)            # Use Biopython's ColorSpiral to generate colours            cs = ColorSpiral(a=2, b=0.2, v_init=0.85, v_final=0.5, jitter=0.03)            colors = cs.get_colors(len(orthologs))            for o, c in zip(orthologs, colors):                for g in o.graphics:                    g.bgcolor = c            kgml_map = KGMLCanvas(pathway)            pathway.image = p[1].pathway_image            kgml_map.import_imagemap = p[1].show_pathway_image            kgml_map.draw(p[1].output_stem + \"_colors.pdf\")",
        "documentation": "rendering of kgml to pdf with modification"
    },
    {
        "method_code": "def test_sma_cutting(self):                self.assertTrue(SmaI.is_blunt())        self.assertFalse(SmaI.is_3overhang())        self.assertFalse(SmaI.is_5overhang())        self.assertEqual(SmaI.overhang(), \"blunt\")        parts = SmaI.catalyse(self.smasite_seq)        self.assertEqual(len(parts), 2)        self.assertEqual(str(parts[1]), \"GGGAAAA\")        parts = SmaI.catalyze(self.smasite_seq)        self.assertEqual(len(parts), 2)",
        "documentation": "test basic cutting with smai blunt cutter"
    },
    {
        "method_code": "def play_simple(paths):        p = GstPlayer()    p.run()    for path in paths:        p.play_file(path)        p.block()",
        "documentation": "play the file in path in a straightforward way without using the player callback function"
    },
    {
        "method_code": "def call_soon_in_loop_thread(        __call: Union[Callable[[], Awaitable[T]], Call[Awaitable[T]]],        timeout: Optional[float] = None,    ) -> Call[T]:                call = _cast_to_call(__call)        runner = get_global_loop()        call.set_timeout(timeout)        runner.submit(call)        return call",
        "documentation": "schedule a call for execution in the global event loop thread return the submitted call"
    },
    {
        "method_code": "def __le__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data <= other._data        elif isinstance(other, str):            return self._data <= other.encode(\"ASCII\")        else:            return self._data <= other",
        "documentation": "implement the lessthan or equal operand"
    },
    {
        "method_code": "def is_parent_of(self, parent, grandchild):                if grandchild == parent or grandchild in self.chain[parent].get_succ():            return True        else:            for sn in self.chain[parent].get_succ():                if self.is_parent_of(sn, grandchild):                    return True            else:                return False",
        "documentation": "check if grandchild is a subnode of parent"
    },
    {
        "method_code": "def is_mapping(obj):        return isinstance(obj, Mapping)",
        "documentation": "check if object is mapping"
    },
    {
        "method_code": "def _load_seqfeature_locations(self, feature, seqfeature_id):                # TODO - Record an ontology for the locations (using location.term_id)        # which for now as in BioPerl we leave defaulting to NULL.        try:            if feature.location.operator != \"join\":                # e.g. order locations... we don't record \"order\" so it                # will become a \"join\" on reloading. What does BioPerl do?                import warnings                from Bio import BiopythonWarning                warnings.warn(                    \"%s location operators are not fully supported\"                    % feature.location_operator,                    BiopythonWarning,                )        except AttributeError:            pass        # This will be a list of length one for a SimpleLocation:        parts = feature.location.parts        if parts and {loc.strand for loc in parts} == {-1}:            # To mimic prior behaviour of Biopython+BioSQL, reverse order            parts = parts[::-1]            # TODO - Check what BioPerl does; see also BioSeq.py code        for rank, loc in enumerate(parts):            self._insert_location(loc, rank + 1, seqfeature_id)",
        "documentation": "load all of the location for a seqfeature into table private this add the location related to the seqfeature into the seqfeaturelocation table fuzzies are not handled right now for a simple location ie we have a single table row with seqstart seqend locationrank for split location ie we would have three row table with start end rank start end rank start end rank"
    },
    {
        "method_code": "def randomize(        self,        ntax=None,        taxon_list=None,        branchlength=1.0,        branchlength_sd=None,        bifurcate=True,    ):                if not ntax and taxon_list:            ntax = len(taxon_list)        elif not taxon_list and ntax:            taxon_list = [\"taxon\" + str(i + 1) for i in range(ntax)]        elif not ntax and not taxon_list:            raise TreeError(\"Either number of taxa or list of taxa must be specified.\")        elif ntax != len(taxon_list):            raise TreeError(\"Length of taxon list must correspond to ntax.\")        # initiate self with empty root        self.__init__()        terminals = self.get_terminals()        # bifurcate randomly at terminal nodes until ntax is reached        while len(terminals) < ntax:            newsplit = random.choice(terminals)            new_terminals = self.split(parent_id=newsplit, branchlength=branchlength)            # if desired, give some variation to the branch length            if branchlength_sd:                for nt in new_terminals:                    bl = random.gauss(branchlength, branchlength_sd)                    if bl < 0:                        bl = 0                    self.node(nt).data.branchlength = bl            terminals.extend(new_terminals)            terminals.remove(newsplit)        # distribute taxon labels randomly        random.shuffle(taxon_list)        for node, name in zip(terminals, taxon_list):            self.node(node).data.taxon = name",
        "documentation": "generate a random tree with ntax taxon andor taxon from taxlabels newtree randomizeselfntaxnonetaxonlistnonebranchlengthbranchlengthsdnonebifurcatetrue tree are bifurcating by default polytomies not yet supported"
    },
    {
        "method_code": "def test_qual_out(self):                records = SeqIO.parse(\"Quality/example.fastq\", \"fastq\")        h = StringIO()        SeqIO.write(records, h, \"qual\")        with open(\"Quality/example.qual\") as expected:            self.assertEqual(h.getvalue(), expected.read())",
        "documentation": "check fastq to qual output"
    },
    {
        "method_code": "def test_reporters_json_is_true(testdata):        args = SimpleNamespace(json=True)    reset_context((), args)    assert context.reporters == (        {            \"backend\": \"json\",            \"output\": \"stdout\",            \"quiet\": False,            \"verbosity\": context.verbosity,        },    )    reset_context()",
        "documentation": "ensure that the reporter property return the correct value when contextjson is true"
    },
    {
        "method_code": "def get_python_requirements(self):                return self._get_multiple_data([\"requires_python\"])",
        "documentation": "new in version this field specifies the python version that the distribution is guaranteed to be compatible with installation tool may look at this when picking which version of a project to install the value must be in the format specified in version specifier this field may be followed by an environment marker after a semicolon example frozenset sysplatform win"
    },
    {
        "method_code": "def test_sampe(self):                        self.create_fasta_index()            # Generate sai files from paired end data            self.do_aln(self.infile1, self.saifile1)            self.do_aln(self.infile2, self.saifile2)            cmdline = BwaSampeCommandline(bwa_exe)            cmdline.set_parameter(\"reference\", self.reference_file)            cmdline.set_parameter(\"sai_file1\", self.saifile1)            cmdline.set_parameter(\"sai_file2\", self.saifile2)            cmdline.set_parameter(\"read_file1\", self.infile1)            cmdline.set_parameter(\"read_file2\", self.infile2)            stdout, stderr = cmdline(stdout=self.samfile)            with open(self.samfile) as handle:                headline = handle.readline()            self.assertTrue(                headline.startswith(\"@SQ\"),                f\"Error generating sam files:\\n{cmdline}\\nOutput starts:{headline}\",            )",
        "documentation": "test for generating samfile by paired end sequencing"
    },
    {
        "method_code": "def _populate_or_format_command(self):                try:            command = self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][                \"containers\"            ][0].get(\"command\")            if command is None:                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][                    \"containers\"                ][0][\"command\"] = shlex.split(self._base_flow_run_command())            elif isinstance(command, str):                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][                    \"containers\"                ][0][\"command\"] = shlex.split(command)        except KeyError:            raise ValueError(                \"Unable to verify command due to invalid job body template.\"            )",
        "documentation": "ensures that the command is present in the job manifest populates the command with the prefect m prefectengine if a command is not present"
    },
    {
        "method_code": "def calc_allele_genotype_freqs(self, fname):                self._run_genepop([\"INF\"], [5, 1], fname)        # First pass, general information        # num_loci = None        # num_pops = None        # with open(fname + \".INF\") as f:        #     line = f.readline()        #     while (num_loci is None or num_pops is None) and line != '':        #         m = re.search(\"Number of populations detected : ([0-9+])\", l)        #         if m is not None:        #             num_pops = _gp_int(m.group(1))        #          m = re.search(\"Number of loci detected        : ([0-9+])\", l)        #          if m is not None:        #              num_loci = _gp_int(m.group(1))        #          line = f.readline()        def pop_parser(self):            if hasattr(self, \"old_line\"):                line = self.old_line                del self.old_line            else:                line = self.stream.readline()            loci_content = {}            while line != \"\":                line = line.rstrip()                if \"Tables of allelic frequencies for each locus\" in line:                    return self.curr_pop, loci_content                match = re.match(\".*Pop: (.+) Locus: (.+)\", line)                if match is not None:                    pop = match.group(1).rstrip()                    locus = match.group(2)                    if not hasattr(self, \"first_locus\"):                        self.first_locus = locus                    if hasattr(self, \"curr_pop\"):                        if self.first_locus == locus:                            old_pop = self.curr_pop                            # self.curr_pop = pop                            self.old_line = line                            del self.first_locus                            del self.curr_pop                            return old_pop, loci_content                    self.curr_pop = pop                else:                    line = self.stream.readline()                    continue                geno_list = []                line = self.stream.readline()                if \"No data\" in line:                    continue                while \"Genotypes  Obs.\" not in line:                    line = self.stream.readline()                while line != \"\\n\":                    m2 = re.match(\" +([0-9]+) , ([0-9]+) *([0-9]+) *(.+)\", line)                    if m2 is not None:                        geno_list.append(                            (                                _gp_int(m2.group(1)),                                _gp_int(m2.group(2)),                                _gp_int(m2.group(3)),                                _gp_float(m2.group(4)),                            )                        )                    else:                        line = self.stream.readline()                        continue                    line = self.stream.readline()                while \"Expected number of ho\" not in line:                    line = self.stream.readline()                expHo = _gp_float(line[38:])                line = self.stream.readline()                obsHo = _gp_int(line[38:])                line = self.stream.readline()                expHe = _gp_float(line[38:])                line = self.stream.readline()                obsHe = _gp_int(line[38:])                line = self.stream.readline()                while \"Sample count\" not in line:                    line = self.stream.readline()                line = self.stream.readline()                freq_fis = {}                overall_fis = None                while \"----\" not in line:                    vals = [x for x in line.rstrip().split(\" \") if x != \"\"]                    if vals[0] == \"Tot\":                        overall_fis = (                            _gp_int(vals[1]),                            _gp_float(vals[2]),                            _gp_float(vals[3]),                        )                    else:                        freq_fis[_gp_int(vals[0])] = (                            _gp_int(vals[1]),                            _gp_float(vals[2]),                            _gp_float(vals[3]),                        )                    line = self.stream.readline()                loci_content[locus] = (                    geno_list,                    (expHo, obsHo, expHe, obsHe),                    freq_fis,                    overall_fis,                )            self.done = True            raise StopIteration        def locus_parser(self):            line = self.stream.readline()            while line != \"\":                line = line.rstrip()                match = re.match(\" Locus: (.+)\", line)                if match is not None:                    locus = match.group(1)                    alleles, table = _read_allele_freq_table(self.stream)                    return locus, alleles, table                line = self.stream.readline()            self.done = True            raise StopIteration        shutil.copyfile(fname + \".INF\", fname + \".IN2\")        pop_iter = _FileIterator(pop_parser, fname + \".INF\")        locus_iter = _FileIterator(locus_parser, fname + \".IN2\")        return (pop_iter, locus_iter)",
        "documentation": "calculate allele and genotype frequency per locus and per sample parameter fname file name return tuple with element population iterator with population name locus dictionary with key locus name and content tuple a genotype list with allele allele observed expected expected homozygote observed hm expected heterozygote observed ht allele frequencyfis dictionary with allele a key and count frequency fis weir cockerham total a a pair count fis weir cockerham fis robertson hill locus iterator with locus name allele list population list with a triple population name list of allele frequency in the same order a allele list above number of gene will create a file called fnameinf"
    },
    {
        "method_code": "def test_blastp(self):                self.check(\"blastp\", Applications.NcbiblastpCommandline)",
        "documentation": "check all blastp argument are supported"
    },
    {
        "method_code": "def test_that_password_is_changed(self):                obj_perm = ObjectPermission(            name='Test permission',            actions=['change']        )        obj_perm.save()        obj_perm.users.add(self.user)        obj_perm.object_types.add(ObjectType.objects.get_for_model(self.model))        user_credentials = {            'username': 'newuser',            'password': 'abc123FOO',        }        user = User.objects.create_user(**user_credentials)        data = {            'password': 'FooBarFooBar1'        }        url = reverse('users-api:user-detail', kwargs={'pk': user.id})        response = self.client.patch(url, data, format='json', **self.header)        self.assertEqual(response.status_code, 200)        user.refresh_from_db()        self.assertTrue(user.check_password(data['password']))",
        "documentation": "test that password is changed"
    },
    {
        "method_code": "def information_mixer(self, file1, file2, file3):                #        #   Mix all the information from the 3 files and produce a coherent        #   restriction record.        #        methfile = self.removestart(file1)        sitefile = self.removestart(file2)        supplier = self.removestart(file3)        i1, i2 = 0, 0        oldblock = None        try:            while True:                block, i1 = self.getblock(methfile, i1)                bl = self.get(block)                line = (sitefile[i2].strip()).split()                name = line[0]                if name == bl[0]:                    line.append(bl[1])  # -> methylation                    line.append(bl[2])  # -> suppliers                else:                    bl = self.get(oldblock)                    if line[0] == bl[0]:                        line.append(bl[1])                        line.append(bl[2])                        i2 += 1                    else:                        raise TypeError                oldblock = block                i2 += 1                try:                    line = self.parseline(line)                except OverhangError:  # overhang error                    n = name  # do not include the enzyme                    if not bl[2]:                        print(f\"Anyway, {n} is not commercially available.\\n\")                    else:                        print(f\"Unfortunately, {n} is commercially available.\\n\")                    continue                # Hyphens and dots can't be used as a Python name, nor as a                # group name in a regular expression. e.g. 'CviKI-1',                # 'R2.BceSIV'                name = name.replace(\"-\", \"_\").replace(\".\", \"_\")                if name in enzymedict:                    #                    #   deal with TaqII and its two sites.                    #                    print(f\"\\nWARNING : {name} has two different sites.\\n\")                    other = line[0].replace(\"-\", \"_\").replace(\".\", \"_\")                    dna = Seq(line[1])                    sense1 = regex(dna)                    antisense1 = regex(dna.reverse_complement())                    dna = Seq(enzymedict[other][0])                    sense2 = regex(dna)                    antisense2 = regex(dna.reverse_complement())                    sense = f\"(?=(?P<{other}>{sense1})|{sense2})\"                    antisense = f\"(?=(?P<{other}_as>{antisense1}|{antisense2}))\"                    reg = sense + \"|\" + antisense                    line[1] = line[1] + \"|\" + enzymedict[other][0]                    line[-1] = reg                #                #   the data to produce the enzyme class are then stored in                #   enzymedict.                #                enzymedict[name] = line[1:]  # element zero was the name        except IndexError:            pass        for i in supplier:            #            #   construction of the list of suppliers.            #            t = i.strip().split(\" \", 1)            suppliersdict[t[0]] = (t[1], [])",
        "documentation": "combine extracted data from the three embossxxxx file"
    },
    {
        "method_code": "def test_separate_penalize_end_gaps(self):                align = pairwise2.align.globalms(            \"AT\", \"AGG\", 1.0, -0.5, -1.75, -0.25, penalize_end_gaps=(True, False)        )        self.assertEqual(align[0], (\"A--T\", \"AGG-\", -1.0, 0, 4))",
        "documentation": "test alignment where endgaps are differently penalized"
    },
    {
        "method_code": "def __init__(self, master=None):                tk.Toplevel.__init__(self, master)        self.menubar = tk.Menu(self)        self.filemenu = tk.Menu(self.menubar)        self.filemenu.add_command(label=\"Save\", command=self.save)        self.filemenu.add_separator()        self.filemenu.add_command(label=\"Dismiss\", command=self.destroy)        self.menubar.add_cascade(label=\"File\", menu=self.filemenu)        self.configure(menu=self.menubar)        self.yscroll = ttk.Scrollbar(self, orient=\"vertical\")        self.tid = tk.Text(self, width=88, yscrollcommand=self.yscroll.set)        self.yscroll.configure(command=self.tid.yview)        self.tid.pack(side=\"left\", fill=\"both\", expand=1)        self.yscroll.pack(side=\"right\", fill=\"y\")",
        "documentation": "set up notepad window"
    },
    {
        "method_code": "def run_plan_elevated(plan):        if any(step[\"result\"] == Result.NEEDS_SUDO for step in plan):        if on_win:            from ..common._os.windows import run_as_admin            temp_path = None            try:                with Utf8NamedTemporaryFile(\"w+\", suffix=\".json\", delete=False) as tf:                    # the default mode is 'w+b', and universal new lines don't work in that mode                    tf.write(                        json.dumps(                            plan, ensure_ascii=False, default=lambda x: x.__dict__                        )                    )                    temp_path = tf.name                python_exe = f'\"{abspath(sys.executable)}\"'                hinstance, error_code = run_as_admin(                    (python_exe, \"-m\", \"conda.core.initialize\", f'\"{temp_path}\"')                )                if error_code is not None:                    print(                        f\"ERROR during elevated execution.\\n  rc: {error_code}\",                        file=sys.stderr,                    )                with open_utf8(temp_path) as fh:                    _plan = json.loads(ensure_text_type(fh.read()))            finally:                if temp_path and lexists(temp_path):                    rm_rf(temp_path)        else:            stdin = json.dumps(plan, ensure_ascii=False, default=lambda x: x.__dict__)            result = subprocess_call(                f\"sudo {sys.executable} -m conda.core.initialize\",                env={},                path=os.getcwd(),                stdin=stdin,            )            stderr = result.stderr.strip()            if stderr:                print(stderr, file=sys.stderr)            _plan = json.loads(result.stdout.strip())        del plan[:]        plan.extend(_plan)",
        "documentation": "the strategy of this function differs between unix and window both strategy use a subprocess call where the subprocess is run with elevated privilege the executable invoked with the subprocess is python m condacoreinitialize so see the if name main at the bottom of this module for unix platform we convert the plan list to json and then call this module with sudo python m condacoreinitialize while piping the plan json to stdin we collect json from stdout for the result of the plan execution with elevated privilege for window we create a temporary file that hold the json content of the plan the subprocess read the content of the file modifies the content of the file with updated execution status and then close the file this process then read the content of that file for the individual operation execution result and then deletes the file"
    },
    {
        "method_code": "def _build_filtered_query(self, name, **filters):                        # TODO: This should be extended to support AND, OR multi-lookups            if filters:                for field_name, params in filters.items():                    lookup = params['lookup']                    value = params['value']                    if lookup:                        query = f'{{{lookup}: \"{value}\"}}'                        filter_string = f'{field_name}: {query}'                    else:                        filter_string = f'{field_name}: \"{value}\"'                filter_string = f'(filters: {{{filter_string}}})'            else:                filter_string = ''            return self._build_query_with_filter(name, filter_string)",
        "documentation": "create a filtered query ie devicelistfilters name icontains akron"
    },
    {
        "method_code": "def test_basics_vector(self):                counts = substitution_matrices.Array(\"XYZ\")        self.assertEqual(            str(counts),            ,        )        self.assertEqual(counts.alphabet, \"XYZ\")        counts[\"X\"] = -9        counts[2] = 5.5        self.assertEqual(            str(counts),            ,        )        self.assertAlmostEqual(counts[0], -9)        with self.assertRaises(IndexError):            counts[\"U\"]        with self.assertRaises(IndexError):            counts[\"V\"] = 3.0        with self.assertRaises(IndexError):            counts[8]",
        "documentation": "test basic vector operation"
    },
    {
        "method_code": "def iter_records(self):                return self._internal.iter_records()",
        "documentation": "beta while in beta expect both major and minor change across minor release return iterablepackagecacherecord a generator over all record contained in the package cache instance warning this is a generator that is exhausted on first use"
    },
    {
        "method_code": "def __init__(        self,        parent=None,        pagesize=\"A3\",        orientation=\"landscape\",        x=0.05,        y=0.05,        xl=None,        xr=None,        yt=None,        yb=None,        start=None,        end=None,        tracklines=0,        fragments=10,        fragment_size=None,        track_size=0.75,        cross_track_links=None,    ):                # Use the superclass' instantiation method        AbstractDrawer.__init__(            self,            parent,            pagesize,            orientation,            x,            y,            xl,            xr,            yt,            yb,            start,            end,            tracklines,            cross_track_links,        )        # Useful measurements on the page        self.fragments = fragments        if fragment_size is not None:            self.fragment_size = fragment_size        else:            if self.fragments == 1:                # For single fragments, default to full height                self.fragment_size = 1            else:                # Otherwise keep a 10% gap between fragments                self.fragment_size = 0.9        self.track_size = track_size",
        "documentation": "initialize argument parent diagram object containing the data that the drawer draw pagesize string describing the iso size of the image or a tuple of pixel orientation string describing the required orientation of the final drawing landscape or portrait x float describing the relative size of the x margin to the page y float describing the relative size of the y margin to the page xl float describing the relative size of the left x margin to the page override x xl float describing the relative size of the left x margin to the page override x xr float describing the relative size of the right x margin to the page override x yt float describing the relative size of the top y margin to the page override y yb float describing the relative size of the lower y margin to the page override y start int the position to begin drawing the diagram at end int the position to stop drawing the diagram at tracklines boolean flag to show or not line delineating track on the diagram fragment int the number of equal fragment into which the sequence should be divided for drawing fragmentsize float the proportion of the available height for the fragment that should be taken up in drawing tracksize the proportion of the available track height that should be taken up in drawing crosstracklinks list of tuples each with four entry track a feature a track b feature b to be linked"
    },
    {
        "method_code": "def _build_m3u_filename(basename):        basename = re.sub(r\"[\\s,/\\\\'\\\"]\", \"_\", basename)    date = datetime.datetime.now().strftime(\"%Y%m%d_%Hh%M\")    path = normpath(        os.path.join(            config[\"importfeeds\"][\"dir\"].as_filename(),            date + \"_\" + basename + \".m3u\",        )    )    return path",
        "documentation": "build unique mu filename by appending given basename to current date"
    },
    {
        "method_code": "def hardlink(path: bytes, dest: bytes, replace: bool = False):        if samefile(path, dest):        return    if os.path.exists(syspath(dest)) and not replace:        raise FilesystemError(\"file exists\", \"rename\", (path, dest))    try:        os.link(syspath(path), syspath(dest))    except NotImplementedError:        raise FilesystemError(            \"OS does not support hard links.\" \"link\",            (path, dest),            traceback.format_exc(),        )    except OSError as exc:        if exc.errno == errno.EXDEV:            raise FilesystemError(                \"Cannot hard link across devices.\" \"link\",                (path, dest),                traceback.format_exc(),            )        else:            raise FilesystemError(                exc, \"link\", (path, dest), traceback.format_exc()            )",
        "documentation": "create a hard link from path to dest raise an oserror if dest already exists unless replace is true doe nothing if path dest"
    },
    {
        "method_code": "def test_pubmed4(self):                # List all available links in PubMed, except for libraries, for        # PMIDs 12085856 and 12085853        # To create the XML file, use        # >>> Bio.Entrez.elink(dbfrom=\"pubmed\", id=\"12085856,12085853\", cmd=\"llinks\")        with open(\"Entrez/elink6.xml\", \"rb\") as stream:            record = Entrez.read(stream)        self.assertEqual(record[0][\"DbFrom\"], \"pubmed\")        self.assertEqual(len(record[0][\"IdUrlList\"]), 2)        self.assertEqual(record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"Id\"], \"12085856\")        self.assertEqual(len(record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"]), 2)        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Category\"], [\"Medical\"]        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Url\"],            \"http://www.nlm.nih.gov/medlineplus/coronaryarterybypasssurgery.html\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Attribute\"],            [\"free resource\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"SubjectType\"],            [\"consumer health\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"IconUrl\"],            \"//www.ncbi.nlm.nih.gov/corehtml/query/egifs/http:--www.nlm.nih.gov-medlineplus-images-linkout_sm.gif\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Provider\"][\"Name\"],            \"MedlinePlus Health Information\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Provider\"][\"NameAbbr\"],            \"MEDPLUS\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Provider\"][\"Id\"], \"3162\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Provider\"][\"Url\"],            \"http://medlineplus.gov/\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"Provider\"][\"IconUrl\"],            \"http://www.nlm.nih.gov/medlineplus/images/linkout_sm.gif\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][0][\"LinkName\"],            \"Coronary Artery Bypass Surgery\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Category\"],            [\"Education\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Attribute\"],            [\"free resource\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"SubjectType\"],            [\"online tutorials/courses\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Url\"],            \"http://symptomresearch.nih.gov/chapter_1/index.htm\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Provider\"][\"Name\"],            \"New England Research Institutes Inc.\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Provider\"][\"NameAbbr\"],            \"NERI\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Provider\"][\"Id\"], \"3291\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][0][\"ObjUrl\"][1][\"Provider\"][\"Url\"],            \"http://www.symptomresearch.com\",        )        self.assertEqual(len(record[0][\"IdUrlList\"][\"IdUrlSet\"][1]), 2)        self.assertEqual(record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"Id\"], \"12085853\")        self.assertEqual(len(record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"]), 3)        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Category\"], [\"Medical\"]        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Url\"],            \"http://www.nlm.nih.gov/medlineplus/arrhythmia.html\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"IconUrl\"],            \"//www.ncbi.nlm.nih.gov/corehtml/query/egifs/http:--www.nlm.nih.gov-medlineplus-images-linkout_sm.gif\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Attribute\"],            [\"free resource\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"SubjectType\"],            [\"consumer health\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"LinkName\"], \"Arrhythmia\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Provider\"][\"Name\"],            \"MedlinePlus Health Information\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Provider\"][\"NameAbbr\"],            \"MEDPLUS\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Provider\"][\"Id\"], \"3162\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][0][\"Provider\"][\"Url\"],            \"http://medlineplus.gov/\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Category\"], [\"Medical\"]        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Attribute\"],            [\"free resource\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Url\"],            \"http://www.nlm.nih.gov/medlineplus/exerciseandphysicalfitness.html\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"IconUrl\"],            \"//www.ncbi.nlm.nih.gov/corehtml/query/egifs/http:--www.nlm.nih.gov-medlineplus-images-linkout_sm.gif\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"LinkName\"],            \"Exercise and Physical Fitness\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"SubjectType\"],            [\"consumer health\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Provider\"][\"Name\"],            \"MedlinePlus Health Information\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Provider\"][\"NameAbbr\"],            \"MEDPLUS\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Provider\"][\"Id\"], \"3162\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][1][\"Provider\"][\"Url\"],            \"http://medlineplus.gov/\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Category\"], [\"Medical\"]        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Attribute\"],            [\"free resource\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Url\"],            \"http://www.nlm.nih.gov/medlineplus/pacemakersandimplantabledefibrillators.html\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"IconUrl\"],            \"//www.ncbi.nlm.nih.gov/corehtml/query/egifs/http:--www.nlm.nih.gov-medlineplus-images-linkout_sm.gif\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"LinkName\"],            \"Pacemakers and Implantable Defibrillators\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"SubjectType\"],            [\"consumer health\"],        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Provider\"][\"Name\"],            \"MedlinePlus Health Information\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Provider\"][\"NameAbbr\"],            \"MEDPLUS\",        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Provider\"][\"Id\"], \"3162\"        )        self.assertEqual(            record[0][\"IdUrlList\"][\"IdUrlSet\"][1][\"ObjUrl\"][2][\"Provider\"][\"Url\"],            \"http://medlineplus.gov/\",        )",
        "documentation": "test parsing pubmed link returned by elink fourth test"
    },
    {
        "method_code": "def format_outlier_version_name(version: str):        return version.replace(\"-ce\", \"\").replace(\"-ee\", \"\")",
        "documentation": "format outlier docker version name to pas packagingversionparse validation current case are simple but creates stub for more complicated formatting if eventually needed example outlier version that throw a parsing exception ce variant of community edition label ee variant of enterprise edition label args version str raw docker version value return str value that can pas packagingversionparse validation"
    },
    {
        "method_code": "def initialize_options(self):                self.offline = None",
        "documentation": "noop initialise option"
    },
    {
        "method_code": "def stop_downloads(self, file_ids):                pyfiles = list(self.pyload.files.cache.values())        for pyfile in pyfiles:            if pyfile.id in file_ids:                pyfile.abort_download()",
        "documentation": "abort specific downloads param fileids list of file id return"
    },
    {
        "method_code": "def __make_new_index(self):                # make the tables        self._con.execute(\"CREATE TABLE meta_data (key TEXT, value TEXT);\")        self._con.execute(            \"INSERT INTO meta_data (key, value) VALUES (?, ?);\",            (\"version\", MAFINDEX_VERSION),        )        self._con.execute(            \"INSERT INTO meta_data (key, value) VALUES ('record_count', -1);\"        )        self._con.execute(            \"INSERT INTO meta_data (key, value) VALUES (?, ?);\",            (\"target_seqname\", self._target_seqname),        )        # Determine whether to store maf file as relative to the index or absolute        # See https://github.com/biopython/biopython/pull/381        if not os.path.isabs(self._maf_file) and not os.path.isabs(            self._index_filename        ):            # Since the user gave both maf file and index as relative paths,            # we will store the maf file relative to the index.            # Note for cross platform use (e.g. shared drive over SAMBA),            # convert any Windows slash into Unix style for rel paths.            # example: ucsc_mm9_chr10.maf            mafpath = os.path.relpath(self._maf_file, self._relative_path).replace(                os.path.sep, \"/\"            )        elif (            os.path.dirname(os.path.abspath(self._maf_file)) + os.path.sep        ).startswith(self._relative_path + os.path.sep):            # Since maf file is in same directory or sub directory,            # might as well make this into a relative path:            mafpath = os.path.relpath(self._maf_file, self._relative_path).replace(                os.path.sep, \"/\"            )        else:            # Default to storing as an absolute path            # example: /home/bli/src/biopython/Tests/MAF/ucsc_mm9_chr10.maf            mafpath = os.path.abspath(self._maf_file)        self._con.execute(            \"INSERT INTO meta_data (key, value) VALUES (?, ?);\",            (\"filename\", mafpath),        )        self._con.execute(            \"CREATE TABLE offset_data (bin INTEGER, start INTEGER, end INTEGER, offset INTEGER);\"        )        insert_count = 0        # iterate over the entire file and insert in batches        mafindex_func = self.__maf_indexer()        while True:            batch = list(islice(mafindex_func, 100))            if not batch:                break            # batch is made from self.__maf_indexer(),            # which yields zero-based \"inclusive\" start and end coordinates            self._con.executemany(                \"INSERT INTO offset_data (bin, start, end, offset) VALUES (?,?,?,?);\",                batch,            )            self._con.commit()            insert_count += len(batch)        # then make indexes on the relevant fields        self._con.execute(\"CREATE INDEX IF NOT EXISTS bin_index ON offset_data(bin);\")        self._con.execute(            \"CREATE INDEX IF NOT EXISTS start_index ON offset_data(start);\"        )        self._con.execute(\"CREATE INDEX IF NOT EXISTS end_index ON offset_data(end);\")        self._con.execute(            f\"UPDATE meta_data SET value = '{insert_count}' WHERE key = 'record_count'\"        )        self._con.commit()        return insert_count",
        "documentation": "read maf file and generate sqlite index private"
    },
    {
        "method_code": "def gap_consensus(self, threshold=0.7, ambiguous=\"X\", require_multiple=False):                warnings.warn(            \"The `gap_consensus` method is deprecated and will be removed \"            \"in a future release of Biopython. As an alternative, you can \"            \"convert the multiple sequence alignment object to a new-style \"            \"Alignment object by via its `.alignment` property, and then \"            \"create a Motif object. You can then use the `.consensus` or \"            \"`.degenerate_consensus` property of the Motif object to get a \"            \"consensus sequence. For more control over how the consensus \"            \"sequence is calculated, you can call the `calculate_consensus` \"            \"method on the `.counts` property of the Motif object. This is an \"            \"example for a multiple sequence alignment `msa` of DNA \"            \"nucleotides:\"            \"\\n\"            \">>> from Bio.Seq import Seq\\n\"            \">>> from Bio.SeqRecord import SeqRecord\\n\"            \">>> from Bio.Align import MultipleSeqAlignment\\n\"            \">>> from Bio.Align.AlignInfo import SummaryInfo\\n\"            \">>> msa = MultipleSeqAlignment([SeqRecord(Seq('ACGT')),\\n\"            \"...                             SeqRecord(Seq('AT-T')),\\n\"            \"...                             SeqRecord(Seq('CT-T')),\\n\"            \"...                             SeqRecord(Seq('GT-T'))])\\n\"            \">>> summary = SummaryInfo(msa)\\n\"            \">>> gap_consensus = summary.gap_consensus(ambiguous='N')\\n\"            \">>> print(gap_consensus)\\n\"            \"NT-T\\n\"            \">>> alignment = msa.alignment\\n\"            \">>> from Bio.motifs import Motif\\n\"            \">>> motif = Motif('ACGT-', alignment)  # include '-' in alphabet\\n\"            \">>> print(motif.consensus)\\n\"            \"AT-T\\n\"            \">>> print(motif.degenerate_consensus)\\n\"            \"VT-T\\n\"            \">>> counts = motif.counts\\n\"            \">>> consensus = counts.calculate_consensus(identity=0.7)\\n\"            \">>> print(consensus)\\n\"            \"NT-T\\n\"            \"\\n\"            \"If your multiple sequence alignment object was obtained using \"            \"Bio.AlignIO, then you can obtain a new-style Alignment object \"            \"directly by using Bio.Align.read instead of Bio.AlignIO.read, \"            \"or Bio.Align.parse instead of Bio.AlignIO.parse.\",            BiopythonDeprecationWarning,        )        consensus = \"\"        # find the length of the consensus we are creating        con_len = self.alignment.get_alignment_length()        # go through each seq item        for n in range(con_len):            # keep track of the counts of the different atoms we get            atom_dict = Counter()            num_atoms = 0            for record in self.alignment:                # make sure we haven't run past the end of any sequences                # if they are of different lengths                try:                    c = record[n]                except IndexError:                    continue                atom_dict[c] += 1                num_atoms += 1            max_atoms = []            max_size = 0            for atom in atom_dict:                if atom_dict[atom] > max_size:                    max_atoms = [atom]                    max_size = atom_dict[atom]                elif atom_dict[atom] == max_size:                    max_atoms.append(atom)            if require_multiple and num_atoms == 1:                consensus += ambiguous            elif len(max_atoms) == 1 and max_size / num_atoms >= threshold:                consensus += max_atoms[0]            else:                consensus += ambiguous        return Seq(consensus)",
        "documentation": "output a fast consensus sequence of the alignment allowing gap same a dumbconsensus but allows gap on the output thing to do let the user define that with only one gap the result character in consensus is gap let the user select gap character now it take the same a input"
    },
    {
        "method_code": "def album_for_id(releaseid: str) -> Optional[beets.autotag.hooks.AlbumInfo]:        log.debug(\"Requesting MusicBrainz release {}\", releaseid)    albumid = _parse_id(releaseid)    if not albumid:        log.debug(\"Invalid MBID ({0}).\", releaseid)        return None    try:        res = musicbrainzngs.get_release_by_id(albumid, RELEASE_INCLUDES)        # resolve linked release relations        actual_res = None        if res[\"release\"].get(\"status\") == \"Pseudo-Release\":            actual_res = _find_actual_release_from_pseudo_release(res)    except musicbrainzngs.ResponseError:        log.debug(\"Album ID match failed.\")        return None    except musicbrainzngs.MusicBrainzError as exc:        raise MusicBrainzAPIError(            exc, \"get release by ID\", albumid, traceback.format_exc()        )    # release is potentially a pseudo release    release = album_info(res[\"release\"])    # should be None unless we're dealing with a pseudo release    if actual_res is not None:        actual_release = album_info(actual_res[\"release\"])        return _merge_pseudo_and_actual_album(release, actual_release)    else:        return release",
        "documentation": "fetch an album by it musicbrainz id and return an albuminfo object or none if the album is not found may raise a musicbrainzapierror"
    },
    {
        "method_code": "def _is_well(self, obj):                # Value should be of WellRecord type        if not isinstance(obj, WellRecord):            raise ValueError(                f\"A WellRecord type object is needed as value (got {type(obj)})\"            )",
        "documentation": "check if the given object is a wellrecord object private used both for the class constructor and the setitem method"
    },
    {
        "method_code": "def __init__(self, plateid, wells=None):                self.id = plateid        if wells is None:            wells = []        # Similar behaviour as GenBank        # Contains all the attributes        self.qualifiers = {}        # Well_id --> WellRecord objects        self._wells = {}        try:            for w in wells:                self._is_well(w)                self[w.id] = w        except TypeError:            raise TypeError(                \"You must provide an iterator-like object containing the single wells\"            )        self._update()",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def frequencies(self):                coordinates = self.coordinates.copy()        sequences = list(self.sequences)        steps = np.diff(self.coordinates, 1)        aligned = sum(steps != 0, 0) > 1        # True for steps in which at least two sequences align, False if a gap        for i, sequence in enumerate(sequences):            row = steps[i, aligned]            if (row >= 0).all():                pass            elif (row <= 0).all():                sequences[i] = reverse_complement(sequence)                coordinates[i, :] = len(sequence) - coordinates[i, :]                steps[i, :] = -steps[i, :]            else:                raise ValueError(f\"Inconsistent steps in row {i}\")        gaps = steps.max(0)        if not ((steps == gaps) | (steps <= 0)).all():            raise ValueError(\"Unequal step sizes in alignment\")        n = len(steps)        length = sum(gaps)        counts = {}        for i in range(n):            sequence = sequences[i]            try:                weight = sequence.annotations.get(\"weight\", 1.0)            except AttributeError:                weight = 1.0            k = coordinates[i, 0]            m = 0            for step, gap in zip(steps[i], gaps):                if step > 0:                    j = k + step                    n = m + step                    try:                        subsequence = bytes(sequence[k:j])                    except TypeError:  # str                        subsequence = bytes(sequence[k:j], \"UTF8\")                    for index, letter in zip(range(m, n), subsequence):                        character = chr(letter)                        row = counts.get(character)                        if row is None:                            row = np.zeros(length)                            counts[character] = row                        row[index] += weight                    k = j                    m = n                elif step < 0:                    k += step                else:  # step == 0                    n = m + gap                    character = \"-\"                    row = counts.get(character)                    if row is None:                        row = np.zeros(length)                        counts[character] = row                    row[m:n] += weight                    m = n        return counts",
        "documentation": "return the frequency of each letter in each column of the alignment gap are represented by a dash character for example from bio import align aligner alignpairwisealigner alignermode global alignment aligneraligngacctg cgatcg alignment alignment printalignment target gacctg query cgatcg blankline alignmentfrequencies array g array a array c array t array alignermode local alignment aligneraligngacctg cgatcg alignment alignment printalignment target gacctg query gatcg blankline alignmentfrequencies g array a array c array t array array"
    },
    {
        "method_code": "def names(self) -> Set[str]:                return set(self.profiles_by_name.keys())",
        "documentation": "return a set of profile name in this collection"
    },
    {
        "method_code": "def __getitem__(self, time):                if isinstance(time, slice):            # Fix the missing values in the slice            if time.start is None:                start = 0            else:                start = time.start            if time.stop is None:                stop = max(self.get_times())            else:                stop = time.stop            time = np.arange(start, stop, time.step)            return [float(value) for value in self._interpolate(time)]        elif isinstance(time, (float, int)):            return float(self._interpolate(time))        raise ValueError(\"Invalid index\")",
        "documentation": "return a subset of signal or a single signal"
    },
    {
        "method_code": "def _get_graphql_base_name(self):                        base_name = self.model._meta.verbose_name.lower().replace(' ', '_')            return getattr(self, 'graphql_base_name', base_name)",
        "documentation": "return graphqlbasename if set otherwise construct the base name for the query field from the model verbose name"
    },
    {
        "method_code": "def test_listed_on_envs_txt_file(    tmp_path: Path, mocker: MockerFixture, env_ok: tuple[Path, str, str, str, str]):        prefix, _, _, _, _ = env_ok    tmp_envs_txt_file = tmp_path / \"envs.txt\"    tmp_envs_txt_file.write_text(f\"{prefix}\")    mocker.patch(        \"conda.plugins.subcommands.doctor.health_checks.get_user_environments_txt_file\",        return_value=tmp_envs_txt_file,    )    assert check_envs_txt_file(prefix)",
        "documentation": "test that run for the case when the env is listed on the environmentstxt file"
    },
    {
        "method_code": "def test_gc_fraction(self):                self.assertAlmostEqual(gc_fraction(\"\", \"ignore\"), 0, places=3)        self.assertAlmostEqual(gc_fraction(\"\", \"weighted\"), 0, places=3)        self.assertAlmostEqual(gc_fraction(\"\", \"remove\"), 0, places=3)        seq = \"ACGGGCTACCGTATAGGCAAGAGATGATGCCC\"        self.assertAlmostEqual(gc_fraction(seq, \"ignore\"), 0.5625, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"weighted\"), 0.5625, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"remove\"), 0.5625, places=3)        seq = \"ACTGSSSS\"        self.assertAlmostEqual(gc_fraction(seq, \"ignore\"), 0.75, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"weighted\"), 0.75, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"remove\"), 0.75, places=3)        # Test RNA sequence        seq = \"GGAUCUUCGGAUCU\"        self.assertAlmostEqual(gc_fraction(seq, \"ignore\"), 0.5, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"weighted\"), 0.5, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"remove\"), 0.5, places=3)        # Test ambiguous nucleotide behaviour        seq = \"CCTGNN\"        self.assertAlmostEqual(gc_fraction(seq, \"ignore\"), 0.5, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"weighted\"), 0.667, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"remove\"), 0.75, places=3)        seq = \"GDVV\"        self.assertAlmostEqual(gc_fraction(seq, \"ignore\"), 0.25, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"weighted\"), 0.6667, places=3)        self.assertAlmostEqual(gc_fraction(seq, \"remove\"), 1.00, places=3)        with self.assertRaises(ValueError):            gc_fraction(seq, \"other string\")",
        "documentation": "test gcfraction function"
    },
    {
        "method_code": "def handle_created(self, session):                tasks = plugins.send(\"import_task_created\", session=session, task=self)        if not tasks:            tasks = [self]        else:            # The plugins gave us a list of lists of tasks. Flatten it.            tasks = [t for inner in tasks for t in inner]        return tasks",
        "documentation": "send the importtaskcreated event for this task return a list of task that should continue through the pipeline by default this is a list containing only the task itself but plugins can replace the task with new one"
    },
    {
        "method_code": "def removesuffix(self, suffix):                # Want to do just this, but need Python 3.9+        # return bytes(self).removesuffix(suffix)        data = bytes(self)        try:            return data.removesuffix(suffix)        except AttributeError:            if data.startswith(suffix):                return data[: -len(suffix)]            else:                return data",
        "documentation": "remove the suffix if present"
    },
    {
        "method_code": "def av_data_dir(self):                # TODO (AV): Find ways to make this user configurable?        return join(self.conda_prefix, \"etc\", \"conda\")",
        "documentation": "where critical artifact verification data eg various public key can be found"
    },
    {
        "method_code": "def write(obj, handle, **kwargs):        trees = list(obj)    writer = NewickIO.Writer(trees)    nexus_trees = [        TREE_TEMPLATE % {\"index\": idx + 1, \"tree\": nwk}        for idx, nwk in enumerate(            writer.to_strings(plain=False, plain_newick=True, **kwargs)        )    ]    tax_labels = [str(x.name) for x in chain(*(t.get_terminals() for t in trees))]    text = NEX_TEMPLATE % {        \"count\": len(tax_labels),        \"labels\": \" \".join(tax_labels),        \"trees\": \"\\n\".join(nexus_trees),    }    handle.write(text)    return len(nexus_trees)",
        "documentation": "write a new nexus file containing the given tree us a simple nexus template and the newickio writer to serialize just the tree and minimal supporting info needed for a valid nexus file"
    },
    {
        "method_code": "def __init__(self):                self._clear()",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def get_drawn_levels(self):                return sorted(key for key in self.tracks if not self.tracks[key].hide)",
        "documentation": "return a sorted list of level occupied by track these track are not explicitly hidden"
    },
    {
        "method_code": "def _flow_run_work_queue_join_clause(self, flow_run, work_queue):                return sa.and_(flow_run.work_queue_name == work_queue.name)",
        "documentation": "on clause for for joining flow run to work queue used by selfgetscheduledflowrunsfromworkqueue allowing just this function to be changed on a perdialect basis"
    },
    {
        "method_code": "def fetch_seqid_by_accession(self, dbid, name):                sql = \"select bioentry_id from bioentry where accession = %s\"        fields = [name]        if dbid:            sql += \" and biodatabase_id = %s\"            fields.append(dbid)        self.execute(sql, fields)        rv = self.cursor.fetchall()        if not rv:            raise IndexError(f\"Cannot find accession {name!r}\")        if len(rv) > 1:            raise IndexError(f\"More than one entry with accession {name!r}\")        return rv[0][0]",
        "documentation": "return the internal id for a sequence using it accession argument dbid the internal id for the subdatabase name the accession of the sequence corresponds to the accession column of the bioentry table of the sql schema"
    },
    {
        "method_code": "def repodata_parsed(self, state: dict | RepodataState) -> dict | None:                session = get_session(self._url)        if not context.ssl_verify:            disable_ssl_verify_warning()        repodata_url = f\"{self._url}/{self._repodata_fn}\"        # XXX won't modify caller's state dict        state_ = self._repodata_state_copy(state)        # at this point, self._cache.state == state == state_        temp_path = (            self._cache.cache_dir / f\"{self._cache.name}.{os.urandom(2).hex()}.tmp\"        )        try:            with conda_http_errors(self._url, self._repodata_fn):                repodata_json_or_none = fetch.request_url_jlap_state(                    repodata_url,                    state_,                    session=session,                    cache=self._cache,                    temp_path=temp_path,                )                # update caller's state dict-or-RepodataState. Do this before                # the self._cache.replace() call which also writes state, then                # signal not to write state to caller.                state.update(state_)                state[URL_KEY] = self._url                headers = state.get(\"jlap\", {}).get(                    \"headers\"                )  # XXX overwrite headers in jlapper.request_url_jlap_state                if headers:                    state[ETAG_KEY] = headers.get(\"etag\")                    state[LAST_MODIFIED_KEY] = headers.get(\"last-modified\")                    state[CACHE_CONTROL_KEY] = headers.get(\"cache-control\")                self._cache.state.update(state)            if temp_path.exists():                self._cache.replace(temp_path)        except fetch.Jlap304NotModified:            raise Response304ContentUnchanged()        finally:            # Clean up the temporary file. In the successful case it raises            # OSError as self._cache_replace() removed temp_file.            try:                temp_path.unlink()            except OSError:                pass        if repodata_json_or_none is None:  # common            # Indicate that subdir_data mustn't rewrite cache_path_json            raise RepodataOnDisk()        else:            return repodata_json_or_none",
        "documentation": "jlap ha to parse the json anyway use this to avoid a redundant parse when repodata is updated when repodata is not updated it doesnt matter whether this function or the caller read from a file"
    },
    {
        "method_code": "def get_object_with_snapshot(self):                obj = super().get_object()        if hasattr(obj, 'snapshot'):            obj.snapshot()        return obj",
        "documentation": "save a prechange snapshot of the object immediately after retrieving it this snapshot will be used to record the before data in the changelog"
    },
    {
        "method_code": "def get_widget(self, id):                id = str(id)        config = dict(self.config[id])  # Copy to avoid mutating instance data        widget_class = get_widget_class(config.pop('class'))        return widget_class(id=id, **config)",
        "documentation": "instantiate and return a widget by it id"
    },
    {
        "method_code": "def _parse_dna_packet(length, data, record):        if record.seq:        raise ValueError(\"The file contains more than one DNA packet\")    flags, sequence = unpack(\">B%ds\" % (length - 1), data)    record.seq = Seq(sequence.decode(\"ASCII\"))    record.annotations[\"molecule_type\"] = \"DNA\"    if flags & 0x01:        record.annotations[\"topology\"] = \"circular\"    else:        record.annotations[\"topology\"] = \"linear\"",
        "documentation": "parse a dna sequence packet a dna sequence packet contains a single byte flag followed by the sequence itself"
    },
    {
        "method_code": "def test_fail_empty_entity(self):                sasa = ShrakeRupley()        r = copy.deepcopy(self.model[\"A\"].child_list[0])        for a in list(r):            r.detach_child(a.name)  # empty residue        self.assertEqual(len(r.child_list), 0)        with self.assertRaisesRegex(ValueError, \"Entity has no child atoms\"):            sasa.compute(r)",
        "documentation": "raise exception on invalid level parameter s c"
    },
    {
        "method_code": "def get_length(self, ak_spec: Union[str, BKT]) -> Optional[float]:                hed_lst, ak_spec2 = self.pick_length(ak_spec)        if hed_lst is None or ak_spec2 is None:            return None        for hed in hed_lst:            val = hed.get_length(ak_spec2)            if val is not None:                return val        return None",
        "documentation": "get bond length for specified atom pair see methpicklength for akspec and detail"
    },
    {
        "method_code": "def test_to_networkx(self):                tree = Phylo.read(EX_DOLLO, \"phyloxml\")        G = Phylo.to_networkx(tree)        self.assertEqual(len(G.nodes()), 659)",
        "documentation": "tree to graph conversion if networkx is available"
    },
    {
        "method_code": "def test_embl13(self):                sequences = [\"CGACTTTCCACTGCCCTCTACGCCCGCGCAATGGGTCGTA...CTACGTT\"]        ids = [\"Test\"]        names = [\"Tester\"]        lengths = [120]        alignment = None        messages = {            \"embl\": \"failed to understand molecule_type 'unspecified'\",            \"fastq\": \"No suitable quality scores found in letter_annotations of SeqRecord (id=Test).\",            \"fastq-illumina\": \"No suitable quality scores found in letter_annotations of SeqRecord (id=Test).\",            \"fastq-solexa\": \"No suitable quality scores found in letter_annotations of SeqRecord (id=Test).\",            \"imgt\": \"failed to understand molecule_type 'unspecified'\",            \"phd\": \"No suitable quality scores found in letter_annotations of SeqRecord (id=Test).\",            \"qual\": \"No suitable quality scores found in letter_annotations of SeqRecord (id=Test).\",            \"seqxml\": \"unknown molecule_type 'unspecified'\",            \"sff\": \"Missing SFF flow information\",            \"nexus\": \"Need the molecule type to be defined\",        }        self.perform_test(            \"embl\",            False,            \"EMBL/location_wrap.embl\",            1,            ids,            names,            sequences,            lengths,            alignment,            messages,        )",
        "documentation": "test parsing embl file with wrapped location and unspecified type"
    },
    {
        "method_code": "def test_add_track_error(self):                self.assertRaises(ValueError, self.gdd.add_track, None, 1)",
        "documentation": "test adding unspecified track"
    },
    {
        "method_code": "def has_propriety(obj, name):        attr = getattr(obj, name, None)    return attr and not callable(attr)",
        "documentation": "check if propriety name wa defined in obj"
    },
    {
        "method_code": "def collapse(self, id):                if id not in self.chain:            raise ChainException(\"Unknown ID: \" + str(id))        prev_id = self.chain[id].get_prev()        self.chain[prev_id].remove_succ(id)        succ_ids = self.chain[id].get_succ()        for i in succ_ids:            self.chain[i].set_prev(prev_id)        self.chain[prev_id].add_succ(succ_ids)        node = self.chain[id]        self.kill(id)        return node",
        "documentation": "delete node from chain and relinks successor to predecessor"
    },
    {
        "method_code": "def test_a_albumartist_edit_apply(self, mock_write):                self.run_mocked_command(            {\"replacements\": {\"album artist\": \"modified album artist\"}},            # Apply changes.            [\"a\"],            args=[\"-a\"],        )        self.album.load()        self.assertCounts(mock_write, write_call_count=self.TRACK_COUNT)        assert self.album.albumartist == \"the modified album artist\"        self.assertItemFieldsModified(            self.album.items(), self.items_orig, [\"albumartist\", \"mtime\"]        )",
        "documentation": "album query a edit albumartist field apply change"
    },
    {
        "method_code": "def rollback(self):                return self.conn.rollback()",
        "documentation": "rollback the current transaction"
    },
    {
        "method_code": "def status_from_tag(tag: str = \"info\") -> str:        status_map = {        'warning': 'warning',        'success': 'success',        'error': 'danger',        'danger': 'danger',        'debug': 'info',        'info': 'info',    }    return status_map.get(tag.lower(), 'info')",
        "documentation": "determine bootstrap theme statuslevel from djangos messageleveltag"
    },
    {
        "method_code": "def get_configs(self) -> Dict[str, Any]:                # since GcpCredentials will always define a project        self_copy = self.copy()        if self_copy.project is not None:            self_copy.credentials.project = None        all_configs_json = self._populate_configs_json(            {}, self_copy.model_fields, model=self_copy        )        # decouple prefect-gcp from prefect-dbt        # by mapping all the keys dbt gcp accepts        # https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup        rename_keys = {            # dbt            \"type\": \"type\",            \"schema\": \"schema\",            \"threads\": \"threads\",            # general            \"dataset\": \"schema\",            \"method\": \"method\",            \"project\": \"project\",            # service-account            \"service_account_file\": \"keyfile\",            # service-account json            \"service_account_info\": \"keyfile_json\",            # oauth secrets            \"refresh_token\": \"refresh_token\",            \"client_id\": \"client_id\",            \"client_secret\": \"client_secret\",            \"token_uri\": \"token_uri\",            # optional            \"priority\": \"priority\",            \"timeout_seconds\": \"timeout_seconds\",            \"location\": \"location\",            \"maximum_bytes_billed\": \"maximum_bytes_billed\",            \"scopes\": \"scopes\",            \"impersonate_service_account\": \"impersonate_service_account\",            \"execution_project\": \"execution_project\",        }        configs_json = {}        extras = self.extras or {}        for key in all_configs_json.keys():            if key not in rename_keys and key not in extras:                # skip invalid keys                continue            # rename key to something dbt profile expects            dbt_key = rename_keys.get(key) or key            configs_json[dbt_key] = all_configs_json[key]        if \"keyfile_json\" in configs_json:            configs_json[\"method\"] = \"service-account-json\"        elif \"keyfile\" in configs_json:            configs_json[\"method\"] = \"service-account\"            configs_json[\"keyfile\"] = str(configs_json[\"keyfile\"])        else:            configs_json[\"method\"] = \"oauth-secrets\"            # through gcloud application-default login            google_credentials = (                self_copy.credentials.get_credentials_from_service_account()            )            if hasattr(google_credentials, \"token\"):                request = Request()                google_credentials.refresh(request)                configs_json[\"token\"] = google_credentials.token            else:                for key in (\"refresh_token\", \"client_id\", \"client_secret\", \"token_uri\"):                    configs_json[key] = getattr(google_credentials, key)        if \"project\" not in configs_json:            raise ValueError(                \"The keyword, project, must be provided in either \"                \"GcpCredentials or BigQueryTargetConfigs\"            )        return configs_json",
        "documentation": "return the dbt configs specific to bigquery profile return a configs json"
    },
    {
        "method_code": "def test_distplot_rebuild(self):                # load input chain        for _chn1 in self.cif_4ZHL.get_chains():            break        # create atomArray and compute distplot and dihedral signs array        _chn1.atom_to_internal_coordinates()        _c1ic = _chn1.internal_coord        atmNameNdx = AtomKey.fields.atm        CaSelect = [            _c1ic.atomArrayIndex.get(k)            for k in _c1ic.atomArrayIndex.keys()            if k.akl[atmNameNdx] == \"CA\"        ]        dplot0 = _chn1.internal_coord.distance_plot(filter=CaSelect)        self.assertAlmostEqual(            dplot0[3, 9],            16.296,            places=3,            msg=\"fail generate distance plot with filter\",        )        dplot1 = _chn1.internal_coord.distance_plot()        dsigns = _chn1.internal_coord.dihedral_signs()        # load second copy (same again) input chain        for _chn2 in self.cif_4ZHL2.get_chains():            break        # create internal coord structures but do not compute di/hedra        cic2 = _chn2.internal_coord = IC_Chain(_chn2)        cic2.init_edra()        # load relevant interatomic distances from chn1 distance plot        cic2.distplot_to_dh_arrays(dplot1, dsigns)        # compute di/hedra angles from dh_arrays        cic2.distance_to_internal_coordinates()        # clear chn2 atom coordinates        # cic2.atomArrayValid[:] = False  # done in distance_to_internal_coordinates        # initialize values but this is redundant to Valid=False above        cic2.atomArray = np.zeros((cic2.AAsiz, 4), dtype=np.float64)        cic2.atomArray[:, 3] = 1.0        # 4zhl has chain breaks so copy initial coords of each segment        cic2.copy_initNCaCs(_chn1.internal_coord)        # compute chn2 atom coords from di/hedra data        cic2.internal_to_atom_coordinates()        # generate distance plot from second chain, confirm minimal distance        # from original        dp2 = cic2.distance_plot()        dpdiff = np.abs(dplot1 - dp2)        # print(np.amax(dpdiff))        self.assertTrue(np.amax(dpdiff) < 0.000001)",
        "documentation": "build identical structure from distplot and chirality data"
    },
    {
        "method_code": "def test_hmmpfam_23_break_in_end_of_seq(self):                results = parse(path.join(\"Hmmer\", \"text_23_hmmpfam_004.out\"), self.fmt)        res = next(results)        self.assertEqual(\"PKSI-KS\", res[0].id)        self.assertEqual(\"PKSI-FK\", res[1].id)",
        "documentation": "test parsing hmmpfam file with a line break in the end of seq marker file texthmmpfamout"
    },
    {
        "method_code": "def _set_strand(self, value):                # Should this be allowed/encouraged?        for loc in self.parts:            loc.strand = value",
        "documentation": "set function for the strand property private"
    },
    {
        "method_code": "def __enter__(self) -> \"SyncPrefectClient\":                if self._closed:            # httpx.Client does not allow reuse so we will not either.            raise RuntimeError(                \"The client cannot be started again after closing. \"                \"Retrieve a new client with `get_client()` instead.\"            )        self._context_stack += 1        if self._started:            # allow reentrancy            return self        self._client.__enter__()        self._started = True        return self",
        "documentation": "start the client if the client is already started this will raise an exception if the client is already closed this will raise an exception use a new client instance instead"
    },
    {
        "method_code": "def conda_solvers():        yield plugins.CondaSolver(        name=\"test\",        backend=Solver,    )",
        "documentation": "the conda plugin hook implementation to load the solver into conda"
    },
    {
        "method_code": "def get_first_available_prefix(self):                available_prefixes = self.get_available_prefixes()        if not available_prefixes:            return None        return available_prefixes.iter_cidrs()[0]",
        "documentation": "return the first available child prefix within the prefix or none"
    },
    {
        "method_code": "def set_track_heights(self):                bot_track = min(min(self.drawn_tracks), 1)        top_track = max(self.drawn_tracks)  # The 'highest' track to draw        trackunit_sum = 0  # Total number of 'units' taken up by all tracks        trackunits = {}  # Start and & units for each track keyed by track number        heightholder = 0  # placeholder variable        for track in range(bot_track, top_track + 1):  # track numbers to 'draw'            try:                trackheight = self._parent[track].height  # Get track height            except Exception:  # TODO: ValueError? IndexError?                trackheight = 1            trackunit_sum += trackheight  # increment total track unit height            trackunits[track] = (heightholder, heightholder + trackheight)            heightholder += trackheight  # move to next height        max_radius = 0.5 * min(self.pagewidth, self.pageheight)        trackunit_height = max_radius * (1 - self.circle_core) / trackunit_sum        track_core = max_radius * self.circle_core        # Calculate top and bottom radii for each track        self.track_radii = {}  # The inner, outer and center radii for each track        track_crop = (            trackunit_height * (1 - self.track_size) / 2.0        )  # 'step back' in pixels        for track in trackunits:            top = trackunits[track][1] * trackunit_height - track_crop + track_core            btm = trackunits[track][0] * trackunit_height + track_crop + track_core            ctr = btm + (top - btm) / 2.0            self.track_radii[track] = (btm, ctr, top)",
        "documentation": "initialize track height since track may not be of identical height the bottom and top radius for each track is stored in a dictionary selftrackradii keyed by track number"
    },
    {
        "method_code": "def test_mmcifio_select(self):                # This method has an internal class defined        # Selection class to filter all alpha carbons        class CAonly(Select):                        def accept_atom(self, atom):                if atom.name == \"CA\" and atom.element == \"C\":                    return 1        struct1 = self.structure        # Write to temp file        self.io.set_structure(struct1)        filenumber, filename = tempfile.mkstemp()        os.close(filenumber)        try:            self.io.save(filename, CAonly())            struct2 = self.mmcif_parser.get_structure(\"1a8o\", filename)            nresidues = len(list(struct2.get_residues()))            self.assertEqual(nresidues, 70)        finally:            os.remove(filename)",
        "documentation": "write a selection of the structure using a select subclass"
    },
    {
        "method_code": "def test_list_available_asns(self):                rir = RIR.objects.first()        asnrange = ASNRange.objects.create(name='Range 1', slug='range-1', rir=rir, start=101, end=110)        url = reverse('ipam-api:asnrange-available-asns', kwargs={'pk': asnrange.pk})        self.add_permissions('ipam.view_asnrange', 'ipam.view_asn')        response = self.client.get(url, **self.header)        self.assertHttpStatus(response, status.HTTP_200_OK)        self.assertEqual(len(response.data), 10)",
        "documentation": "test retrieval of all available asns within a parent range"
    },
    {
        "method_code": "def test_edit_apply_asis(self):                # Edit track titles.        self.run_mocked_interpreter(            {\"replacements\": {\"Tag Track\": \"Edited Track\"}},            # eDit, Apply changes.            [\"d\", \"a\"],        )        # Check that only the 'title' field is modified.        self.assertItemFieldsModified(            self.lib.items(),            self.items_orig,            [\"title\"],            self.IGNORED            + [                \"albumartist\",                \"mb_albumartistid\",                \"mb_albumartistids\",            ],        )        assert all(\"Edited Track\" in i.title for i in self.lib.items())        # Ensure album is *not* fetched from a candidate.        assert self.lib.albums()[0].mb_albumid == \"\"",
        "documentation": "edit the album field for all item in the library apply change using the original item tag"
    },
    {
        "method_code": "def test_local_identifier():        # a valid versionstr should match itself    versions = (        \"1.7.0\",        \"1.7.0.post123\",        \"1.7.0.post123.gabcdef9\",        \"1.7.0.post123+gabcdef9\",    )    for version in versions:        m = VersionSpec(version)        assert m.match(version)",
        "documentation": "the separator for the local identifier should be either or"
    },
    {
        "method_code": "def versions_dir(self) -> Path:                return (            Path(prefect.server.database.__file__).parent            / \"migrations\"            / \"versions\"            / \"sqlite\"        )",
        "documentation": "directory containing migration"
    },
    {
        "method_code": "def __add__(self, offset):                return self.__class__(            int(self) + offset, [p + offset for p in self.position_choices]        )",
        "documentation": "return a copy of the position object with it location shifted private"
    },
    {
        "method_code": "def test_reading(self):                path = \"Blat/bigbed_extended.littleendian.bb\"        alignments = Align.parse(path, \"bigbed\")        self.assertEqual(alignments.byteorder, \"<\")        self.check_alignments(alignments)        path = \"Blat/bigbed_extended.bigendian.bb\"        alignments = Align.parse(path, \"bigbed\")        self.assertEqual(alignments.byteorder, \">\")        self.check_alignments(alignments)",
        "documentation": "test parsing bigbedextendedbb"
    },
    {
        "method_code": "def _get_pi(seq1, seq2, cmethod, codon_table):        # TODO:    # Stop codon should not be allowed according to Yang.    # Try to modify this!    pi = {}    if cmethod == \"F1x4\":        fcodon = {\"A\": 0, \"G\": 0, \"C\": 0, \"T\": 0}        for i in seq1 + seq2:            if i != \"---\":                for c in i:                    fcodon[c] += 1        tot = sum(fcodon.values())        fcodon = {j: k / tot for j, k in fcodon.items()}        for i in codon_table.forward_table.keys() + codon_table.stop_codons:            if \"U\" not in i:                pi[i] = fcodon[i[0]] * fcodon[i[1]] * fcodon[i[2]]    elif cmethod == \"F3x4\":        # three codon position        fcodon = [            {\"A\": 0, \"G\": 0, \"C\": 0, \"T\": 0},            {\"A\": 0, \"G\": 0, \"C\": 0, \"T\": 0},            {\"A\": 0, \"G\": 0, \"C\": 0, \"T\": 0},        ]        for i in seq1 + seq2:            if i != \"---\":                fcodon[0][i[0]] += 1                fcodon[1][i[1]] += 1                fcodon[2][i[2]] += 1        for i in range(3):            tot = sum(fcodon[i].values())            fcodon[i] = {j: k / tot for j, k in fcodon[i].items()}        for i in list(codon_table.forward_table.keys()) + codon_table.stop_codons:            if \"U\" not in i:                pi[i] = fcodon[0][i[0]] * fcodon[1][i[1]] * fcodon[2][i[2]]    elif cmethod == \"F61\":        for i in codon_table.forward_table.keys() + codon_table.stop_codons:            if \"U\" not in i:                pi[i] = 0.1        for i in seq1 + seq2:            if i != \"---\":                pi[i] += 1        tot = sum(pi.values())        pi = {j: k / tot for j, k in pi.items()}    return pi",
        "documentation": "obtain codon frequency dict pi from two codon list private this function is designed for ml method available counting method cfreq are fx fx and f"
    },
    {
        "method_code": "def test_Sequence(self):                trees = list(PhyloXMLIO.parse(EX_PHYLO))        # Simple element with id_source        seq0 = trees[4].clade[1].sequences[0]        self.assertIsInstance(seq0, PX.Sequence)        self.assertEqual(seq0.id_source, \"z\")        self.assertEqual(seq0.symbol, \"ADHX\")        self.assertEqual(seq0.accession.source, \"ncbi\")        self.assertEqual(seq0.accession.value, \"Q17335\")        self.assertEqual(seq0.name, \"alcohol dehydrogenase\")        self.assertEqual(seq0.annotations[0].ref, \"InterPro:IPR002085\")        # More complete elements        seq1 = trees[5].clade[0, 0].sequences[0]        seq2 = trees[5].clade[0, 1].sequences[0]        seq3 = trees[5].clade[1].sequences[0]        for seq, sym, acc, name, mol_seq, ann_refs in zip(            (seq1, seq2, seq3),            (\"ADHX\", \"RT4I1\", \"ADHB\"),            (\"P81431\", \"Q54II4\", \"Q04945\"),            (                \"Alcohol dehydrogenase class-3\",                \"Reticulon-4-interacting protein 1 homolog, mitochondrial precursor\",                \"NADH-dependent butanol dehydrogenase B\",            ),            (                \"TDATGKPIKCMAAIAWEAKKPLSIEEVEVAPPKSGEVRIKILHSGVCHTD\",                \"MKGILLNGYGESLDLLEYKTDLPVPKPIKSQVLIKIHSTSINPLDNVMRK\",                \"MVDFEYSIPTRIFFGKDKINVLGRELKKYGSKVLIVYGGGSIKRNGIYDK\",            ),            (                (\"EC:1.1.1.1\", \"GO:0004022\"),                (\"GO:0008270\", \"GO:0016491\"),                (\"GO:0046872\", \"KEGG:Tetrachloroethene degradation\"),            ),        ):            self.assertIsInstance(seq, PX.Sequence)            self.assertEqual(seq.symbol, sym)            self.assertEqual(seq.accession.source, \"UniProtKB\")            self.assertEqual(seq.accession.value, acc)            self.assertEqual(seq.name, name)            self.assertEqual(seq.mol_seq.value, mol_seq)            self.assertEqual(seq.annotations[0].ref, ann_refs[0])            self.assertEqual(seq.annotations[1].ref, ann_refs[1])",
        "documentation": "instantiation of sequence object also check accession and annotation type"
    },
    {
        "method_code": "def local(self):                return self.local_method is not None",
        "documentation": "a boolean indicating whether the resizing method is performed locally ie pil or imagemagick"
    },
    {
        "method_code": "def tag_log(self, status, paths):                self.logger.info(\"{0} {1}\", status, displayable_path(paths))",
        "documentation": "log a message about a given album to the importer log the status should reflect the reason the album couldnt be tagged"
    },
    {
        "method_code": "def __sub__(self, other):                sup = SVDSuperimposer()        sup.set(self.coords_ca, other.coords_ca)        sup.run()        return sup.get_rms()",
        "documentation": "return rmsd between two fragment return rmsd between fragment rtype float example this is an incomplete but illustrative example rmsd fragment fragment"
    },
    {
        "method_code": "def _send_exception_to_thread(thread: threading.Thread, exc_type: Type[BaseException]):        if not thread.ident:        raise ValueError(\"Thread is not started.\")    ret = ctypes.pythonapi.PyThreadState_SetAsyncExc(        ctypes.c_long(thread.ident), ctypes.py_object(exc_type)    )    if ret == 0:        raise ValueError(\"Thread not found.\")",
        "documentation": "raise an exception in a thread this will not interrupt longrunning system call like sleep or wait"
    },
    {
        "method_code": "def contains(self, other):                xorbit = self ^ other        return xorbit.count(\"1\") == self.count(\"1\") - other.count(\"1\")",
        "documentation": "check if current bitstr contains another one bitstr that is to say the bitstrindexone is a subset of bitstrindexone example contains be careful also contains actually all bitstring object contain allzero bitstring of the same length"
    },
    {
        "method_code": "def _finish_backtrace(sequenceA, sequenceB, ali_seqA, ali_seqB, row, col, gap_char):        if row:        ali_seqA += sequenceA[row - 1 :: -1]    if col:        ali_seqB += sequenceB[col - 1 :: -1]    if row > col:        ali_seqB += gap_char * (len(ali_seqA) - len(ali_seqB))    elif col > row:        ali_seqA += gap_char * (len(ali_seqB) - len(ali_seqA))    return ali_seqA, ali_seqB",
        "documentation": "add remaining sequence and fill with gap if necessary private"
    },
    {
        "method_code": "def __init__(self, host, port):                self._closed = False        self.host = host        self.port = port        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)        self.sock.bind((host, port))        self.sock.listen(5)",
        "documentation": "create a listening socket on the given hostname and port"
    },
    {
        "method_code": "def versions_dir(self):                ...",
        "documentation": "directory containing migration"
    },
    {
        "method_code": "def save_changes(self, objs):                # Save to the database and possibly write tags.        for ob in objs:            if ob._dirty:                self._log.debug(\"saving changes to {}\", ob)                ob.try_sync(ui.should_write(), ui.should_move())",
        "documentation": "save a list of updated model object to the database"
    },
    {
        "method_code": "def find(cls):                pass",
        "documentation": "check if system statisfy dependency"
    },
    {
        "method_code": "def _getters(cls: Type[\"Model\"]):                # We could cache this if it becomes a performance problem to        # gather the getter mapping every time.        raise NotImplementedError()",
        "documentation": "return a mapping from field name to getter function"
    },
    {
        "method_code": "def name(text, sep=\"_\", allow_whitespaces=True):        bc = uniquify(_WINBADCHARS + _MACBADCHARS + _UNIXBADCHARS)    repl = r\"\".join(bc)    if not allow_whitespaces:        repl += \" \"    res = chars(text, repl, sep).strip()    if res.lower() in _WINBADWORDS:        res = (sep or \"_\") + res    return res",
        "documentation": "remove invalid character"
    },
    {
        "method_code": "def _update(self):                self._rows = sorted({x[0] for x in self._wells})        self._columns = sorted({x[1:] for x in self._wells})",
        "documentation": "update the row and column string identifier private"
    },
    {
        "method_code": "def get_settings(self) -> dict[str, ParameterLoader]:                return {            config_param.name.lower(): (config_param.parameter, config_param.aliases)            for config_param in self.get_hook_results(\"settings\")        }",
        "documentation": "return a mapping of plugin setting name to parameterloader class this method intentionally overwrites any duplicate that may be present"
    },
    {
        "method_code": "def test_nucleotide(self):                # In Nucleotide display records for GIs 28864546 and 28800981        # in xml retrieval mode        # To create the XML file, use        # >>> Bio.Entrez.esummary(db=\"nucleotide\", id=\"28864546,28800981\",        #                         retmode=\"xml\")        with open(\"Entrez/esummary4.xml\", \"rb\") as stream:            record = Entrez.read(stream)        self.assertEqual(record[0][\"Id\"], \"28864546\")        self.assertEqual(record[0][\"Caption\"], \"AY207443\")        self.assertEqual(            record[0][\"Title\"],            \"Homo sapiens alpha hemoglobin (HBZP) pseudogene 3' UTR/AluJo repeat breakpoint junction\",        )        self.assertEqual(record[0][\"Extra\"], \"gi|28864546|gb|AY207443.1|[28864546]\")        self.assertEqual(record[0][\"Gi\"], 28864546)        self.assertEqual(record[0][\"CreateDate\"], \"2003/03/05\")        self.assertEqual(record[0][\"UpdateDate\"], \"2003/03/05\")        self.assertEqual(record[0][\"Flags\"], 0)        self.assertEqual(record[0][\"TaxId\"], 9606)        self.assertEqual(record[0][\"Length\"], 491)        self.assertEqual(record[0][\"Status\"], \"live\")        self.assertEqual(record[0][\"ReplacedBy\"], \"\")        self.assertEqual(record[0][\"Comment\"], \"  \")        self.assertEqual(record[1][\"Id\"], \"28800981\")        self.assertEqual(record[1][\"Caption\"], \"AY205604\")        self.assertEqual(            record[1][\"Title\"], \"Homo sapiens hemochromatosis (HFE) mRNA, partial cds\"        )        self.assertEqual(record[1][\"Extra\"], \"gi|28800981|gb|AY205604.1|[28800981]\")        self.assertEqual(record[1][\"Gi\"], 28800981)        self.assertEqual(record[1][\"CreateDate\"], \"2003/03/03\")        self.assertEqual(record[1][\"UpdateDate\"], \"2003/03/03\")        self.assertEqual(record[1][\"Flags\"], 0)        self.assertEqual(record[1][\"TaxId\"], 9606)        self.assertEqual(record[1][\"Length\"], 860)        self.assertEqual(record[1][\"Status\"], \"live\")        self.assertEqual(record[1][\"ReplacedBy\"], \"\")        self.assertEqual(record[1][\"Comment\"], \"  \")",
        "documentation": "test parsing xml returned by esummary from the nucleotide database"
    },
    {
        "method_code": "def test_mul_method_exceptions(self):                for seq in test_seqs + protein_seqs:            with self.assertRaises(TypeError):                seq * 3.0            with self.assertRaises(TypeError):                seq * \"\"",
        "documentation": "test mul method exception"
    },
    {
        "method_code": "def build_year_spans(year_spans_str):        spans = []    for elem in year_spans_str:        spans.append(span_from_str(elem))    complete_year_spans(spans)    return spans",
        "documentation": "build a chronologically ordered list of span dict from unordered span stringlist"
    },
    {
        "method_code": "def assertEqualTimes(self, first, second, msg=None):  # noqa                assert first == pytest.approx(second, rel=1e-4), msg",
        "documentation": "for comparing file modification time at a sufficient precision"
    },
    {
        "method_code": "def move_files(self, file_ids, package_id):                # TODO: implement        pass",
        "documentation": "move multiple file to another package param fileids list of file id param packageid destination package return"
    },
    {
        "method_code": "def calc_rho_pair(self, fname):                raise NotImplementedError",
        "documentation": "provision for estimating spatial structure from allele size for all population pair"
    },
    {
        "method_code": "def __repr__(self):                return \"<Select all>\"",
        "documentation": "represent the output a a string for debugging"
    },
    {
        "method_code": "def __init__(self, eavesdrop_on: str, level: int = logging.NOTSET):                super().__init__(level=level)        self.eavesdrop_on = eavesdrop_on        self._target_logger = None        # It's important that we use a very minimalistic formatter for use cases where        # we may present these logs back to the user.  We shouldn't leak filenames,        # versions, or other environmental information.        self.formatter = logging.Formatter(\"[%(levelname)s]: %(message)s\")",
        "documentation": "args eavesdropon str the name of the logger to eavesdrop on level int the minimum log level to eavesdrop on if omitted all level are captured"
    },
    {
        "method_code": "def test_gap_here_only_1(self):                seq1 = \"AAAABBBAAAACCCCCCCCCCCCCCAAAABBBAAAA\"        seq2 = \"AABBBAAAACCCCAAAABBBAA\"        def no_gaps(x, y):                        return -2000 - y        def specific_gaps(x, y):                        breaks = [0, 11, len(seq2)]            return (-2 - y) if x in breaks else (-2000 - y)        alignments = pairwise2.align.globalmc(seq1, seq2, 1, -1, no_gaps, specific_gaps)        self.assertEqual(len(alignments), 1)        formatted = pairwise2.format_alignment(*alignments[0])        self.assertEqual(            formatted,            ,  # noqa: W291        )",
        "documentation": "open a gap in second sequence only"
    },
    {
        "method_code": "def _cast_smtp_server(cls, value: str):                return _cast_to_enum(value, SMTPServer)",
        "documentation": "cast the smtpserver to an smtpserver enum member if valid"
    },
    {
        "method_code": "def __str__(self):                nodenum = len(self._adjacency_list)        edgenum = reduce(            lambda x, y: x + y, [len(v) for v in self._adjacency_list.values()]        )        labelnum = len(self._label_map)        return (            \"<Graph: \"            + str(nodenum)            + \" node(s), \"            + str(edgenum)            + \" edge(s), \"            + str(labelnum)            + \" unique label(s)>\"        )",
        "documentation": "return a concise string description of this graph"
    },
    {
        "method_code": "def _load_bgzf_block(handle, text_mode=False):        magic = handle.read(4)    if not magic:        # End of file - should we signal this differently now?        # See https://www.python.org/dev/peps/pep-0479/        raise StopIteration    if magic != _bgzf_magic:        raise ValueError(            r\"A BGZF (e.g. a BAM file) block should start with \"            r\"%r, not %r; handle.tell() now says %r\"            % (_bgzf_magic, magic, handle.tell())        )    gzip_mod_time, gzip_extra_flags, gzip_os, extra_len = struct.unpack(        \"<LBBH\", handle.read(8)    )    block_size = None    x_len = 0    while x_len < extra_len:        subfield_id = handle.read(2)        subfield_len = struct.unpack(\"<H\", handle.read(2))[0]  # uint16_t        subfield_data = handle.read(subfield_len)        x_len += subfield_len + 4        if subfield_id == _bytes_BC:            if subfield_len != 2:                raise ValueError(\"Wrong BC payload length\")            if block_size is not None:                raise ValueError(\"Two BC subfields?\")            block_size = struct.unpack(\"<H\", subfield_data)[0] + 1  # uint16_t    if x_len != extra_len:        raise ValueError(f\"x_len and extra_len differ {x_len}, {extra_len}\")    if block_size is None:        raise ValueError(\"Missing BC, this isn't a BGZF file!\")    # Now comes the compressed data, CRC, and length of uncompressed data.    deflate_size = block_size - 1 - extra_len - 19    d = zlib.decompressobj(-15)  # Negative window size means no headers    data = d.decompress(handle.read(deflate_size)) + d.flush()    expected_crc = handle.read(4)    expected_size = struct.unpack(\"<I\", handle.read(4))[0]    if expected_size != len(data):        raise RuntimeError(\"Decompressed to %i, not %i\" % (len(data), expected_size))    # Should cope with a mix of Python platforms...    crc = zlib.crc32(data)    if crc < 0:        crc = struct.pack(\"<i\", crc)    else:        crc = struct.pack(\"<I\", crc)    if expected_crc != crc:        raise RuntimeError(f\"CRC is {crc}, not {expected_crc}\")    if text_mode:        # Note ISO-8859-1 aka Latin-1 preserves first 256 chars        # (i.e. ASCII), but critically is a single byte encoding        return block_size, data.decode(\"latin-1\")    else:        return block_size, data",
        "documentation": "load the next bgzf block of compressed data private return a tuple block size and data or at end of file will raise stopiteration"
    },
    {
        "method_code": "def test_blastxml_2222_blastx_001(self):                filename = \"Blast/xml_2222_blastx_001.xml\"        self.check_index(filename, self.fmt)",
        "documentation": "test blastxml indexing blast"
    },
    {
        "method_code": "def _test_pop_hz_both(        self,        fname,        type,        ext,        enum_test=True,        dememorization=10000,        batches=20,        iterations=5000,    ):                opts = self._get_opts(dememorization, batches, iterations, enum_test)        self._run_genepop([ext], [1, type], fname, opts)        def hw_func(self):            return _hw_func(self.stream, False)        return _FileIterator(hw_func, fname + ext)",
        "documentation": "use hardyweinberg test for heterozygote deficiencyexcess private return a population iterator containing a dictionary where dictionarylocuspval se fiswc fisrh step some locus have a none if the info is not available se might be none for enumeration"
    },
    {
        "method_code": "def batch_fetch_art(self, lib, albums, force, quiet):                for album in albums:            if (                album.artpath                and not force                and os.path.isfile(syspath(album.artpath))            ):                if not quiet:                    message = ui.colorize(                        \"text_highlight_minor\", \"has album art\"                    )                    self._log.info(\"{0}: {1}\", album, message)            else:                # In ordinary invocations, look for images on the                # filesystem. When forcing, however, always go to the Web                # sources.                local_paths = None if force else [album.path]                candidate = self.art_for_album(album, local_paths)                if candidate:                    self._set_art(album, candidate)                    message = ui.colorize(\"text_success\", \"found album art\")                else:                    message = ui.colorize(\"text_error\", \"no art found\")                self._log.info(\"{0}: {1}\", album, message)",
        "documentation": "fetch album art for each of the album this implement the manual fetchart cli command"
    },
    {
        "method_code": "def commands(self):                deezer_update_cmd = ui.Subcommand(            \"deezerupdate\", help=f\"Update {self.data_source} rank\"        )        def func(lib, opts, args):            items = lib.items(ui.decargs(args))            self.deezerupdate(items, ui.should_write())        deezer_update_cmd.func = func        return [deezer_update_cmd]",
        "documentation": "add beet ui command to interact with deezer"
    },
    {
        "method_code": "def draw_greytrack(self, track):                greytrack_bgs = []  # Holds track backgrounds        greytrack_labels = []  # Holds track foreground labels        if not track.greytrack:  # No greytrack required, return early            return [], []        # Get track location        btm, ctr, top = self.track_radii[self.current_track_level]        start, end = self._current_track_start_end()        startangle, startcos, startsin = self.canvas_angle(start)        endangle, endcos, endsin = self.canvas_angle(end)        # Make background        if track.start is not None or track.end is not None:            # Draw an arc, leaving out the wedge            p = ArcPath(strokeColor=track.scale_color, fillColor=None)            greytrack_bgs.append(                self._draw_arc(                    btm, top, startangle, endangle, colors.Color(0.96, 0.96, 0.96)                )            )        elif self.sweep < 1:            # Make a partial circle, a large arc box            # This method assumes the correct center for us.            greytrack_bgs.append(                self._draw_arc(                    btm, top, 0, 2 * pi * self.sweep, colors.Color(0.96, 0.96, 0.96)                )            )        else:            # Make a full circle (using a VERY thick linewidth)            greytrack_bgs.append(                Circle(                    self.xcenter,                    self.ycenter,                    ctr,                    strokeColor=colors.Color(0.96, 0.96, 0.96),                    fillColor=None,                    strokeWidth=top - btm,                )            )        if track.greytrack_labels:            # Labels are required for this track            labelstep = self.length // track.greytrack_labels  # label interval            for pos in range(self.start, self.end, labelstep):                label = String(                    0,                    0,                    track.name,  # Add a new label at                    fontName=track.greytrack_font,  # each interval                    fontSize=track.greytrack_fontsize,                    fillColor=track.greytrack_fontcolor,                )                theta, costheta, sintheta = self.canvas_angle(pos)                if theta < startangle or endangle < theta:                    continue                x, y = (                    self.xcenter + btm * sintheta,                    self.ycenter + btm * costheta,                )  # start text halfway up marker                labelgroup = Group(label)                labelangle = (                    self.sweep * 2 * pi * (pos - self.start) / self.length - pi / 2                )                if theta > pi:                    label.textAnchor = \"end\"  # Anchor end of text to inner radius                    labelangle += pi  # and reorient it                cosA, sinA = cos(labelangle), sin(labelangle)                labelgroup.transform = (cosA, -sinA, sinA, cosA, x, y)                if not self.length - x <= labelstep:  # Don't overrun the circle                    greytrack_labels.append(labelgroup)        return greytrack_bgs, greytrack_labels",
        "documentation": "drawing element for grey background to passed track object"
    },
    {
        "method_code": "def sanitize_name(name, width=None):        name = name.strip()    for char in \"[](),\":        name = name.replace(char, \"\")    for char in \":;\":        name = name.replace(char, \"|\")    if width is not None:        name = name[:width]    return name",
        "documentation": "sanitise sequence identifier for output remove the banned character and replaces the character with the name is truncated to width character if specified"
    },
    {
        "method_code": "def _expire_lock(self, key: str):                with self._locks_dict_lock:            if key in self._locks:                lock_info = self._locks[key]                if lock_info[\"lock\"].locked():                    lock_info[\"lock\"].release()                if lock_info[\"expiration_timer\"]:                    lock_info[\"expiration_timer\"].cancel()                del self._locks[key]",
        "documentation": "expire the lock for the given key used a a callback for the expiration timer of a lock args key the key of the lock to expire"
    },
    {
        "method_code": "def test_get_single_object_without_custom_field_data(self):                site1 = Site.objects.get(name='Site 1')        url = reverse('dcim-api:site-detail', kwargs={'pk': site1.pk})        self.add_permissions('dcim.view_site')        response = self.client.get(url, **self.header)        self.assertEqual(response.data['name'], site1.name)        self.assertEqual(response.data['custom_fields'], {            'text_field': None,            'longtext_field': None,            'integer_field': None,            'decimal_field': None,            'boolean_field': None,            'date_field': None,            'datetime_field': None,            'url_field': None,            'json_field': None,            'select_field': None,            'multiselect_field': None,            'object_field': None,            'multiobject_field': None,        })",
        "documentation": "validate that custom field are present on an object even if it ha no value defined"
    },
    {
        "method_code": "def get(cls, client: Resource, namespace: str, job_name: str):                request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")        response = request.execute()        return cls(            metadata=response[\"metadata\"],            spec=response[\"spec\"],            status=response[\"status\"],            name=response[\"metadata\"][\"name\"],            ready_condition=cls._get_ready_condition(response),            execution_status=cls._get_execution_status(response),        )",
        "documentation": "make a get request to the gcp job api and return a job instance"
    },
    {
        "method_code": "def item_candidates(item, artist, title):        for plugin in find_plugins():        yield from plugin.item_candidates(item, artist, title)",
        "documentation": "get musicbrainz candidate for an item from the plugins"
    },
    {
        "method_code": "def setUp(self):                # Create a directory to work in        self.temp_dir = tempfile.mkdtemp(prefix=\"biopython-test\")",
        "documentation": "initialise temporary directory"
    },
    {
        "method_code": "def test_old_contents(self):                        filenames, flag = raw_filenames(\"Roche/triple_sff.idx\")            self.assertIsNone(flag)            self.assertEqual(                filenames, [\"E3MFGYR02_no_manifest.sff\", \"greek.sff\", \"paired.sff\"]            )            filenames, flag = raw_filenames(\"Roche/triple_sff_rel_paths.idx\")            self.assertTrue(flag)            self.assertEqual(                filenames, [\"E3MFGYR02_no_manifest.sff\", \"greek.sff\", \"paired.sff\"]            )",
        "documentation": "check actual filename in existing index"
    },
    {
        "method_code": "def close_pool(self):                if self.pool is not None:            self.pool.close()            self.pool.join()            self.exc_watcher.join()            self.pool = None",
        "documentation": "regularly close the threadpool instance in selfpool"
    },
    {
        "method_code": "def test_create_multiple_available_vlans(self):                self.add_permissions('ipam.view_vlangroup', 'ipam.view_vlan', 'ipam.add_vlan')        vlangroup = VLANGroup.objects.first()        vlans = (            VLAN(vid=1, name='VLAN 1', group=vlangroup),            VLAN(vid=3, name='VLAN 3', group=vlangroup),            VLAN(vid=5, name='VLAN 5', group=vlangroup),        )        VLAN.objects.bulk_create(vlans)        data = (            {\"name\": \"First VLAN\"},            {\"name\": \"Second VLAN\"},            {\"name\": \"Third VLAN\"},        )        url = reverse('ipam-api:vlangroup-available-vlans', kwargs={'pk': vlangroup.pk})        response = self.client.post(url, data, format='json', **self.header)        self.assertHttpStatus(response, status.HTTP_201_CREATED)        self.assertEqual(len(response.data), 3)        self.assertEqual(response.data[0]['name'], data[0]['name'])        self.assertEqual(response.data[0]['group']['id'], vlangroup.pk)        self.assertEqual(response.data[0]['vid'], 2)        self.assertEqual(response.data[1]['name'], data[1]['name'])        self.assertEqual(response.data[1]['group']['id'], vlangroup.pk)        self.assertEqual(response.data[1]['vid'], 4)        self.assertEqual(response.data[2]['name'], data[2]['name'])        self.assertEqual(response.data[2]['group']['id'], vlangroup.pk)        self.assertEqual(response.data[2]['vid'], 6)",
        "documentation": "test the creation of multiple available vlans"
    },
    {
        "method_code": "def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"Pir\")        # Skip any text before the first record (e.g. blank lines, comments)        for line in self.stream:            if line[0] == \">\":                self._line = line                break        else:            self._line = None",
        "documentation": "iterate over a pir file and yield seqrecord object source filelike object or a path to a file example with opennbrfdmbprotpir a handle for record in piriteratorhandle print length i recordid lenrecord hlahla length hlahla length hlahla length hlahla length hlahla length hlahla length"
    },
    {
        "method_code": "def clear_config():        if hasattr(_thread_locals, 'config'):        del _thread_locals.config        logger.debug(\"Cleared configuration\")",
        "documentation": "delete the currently loaded configuration if any"
    },
    {
        "method_code": "def trace(self, request, pk):                obj = get_object_or_404(self.queryset, pk=pk)        # Initialize the path array        path = []        # Render SVG image if requested        if request.GET.get('render', None) == 'svg':            try:                width = int(request.GET.get('width', CABLE_TRACE_SVG_DEFAULT_WIDTH))            except (ValueError, TypeError):                width = CABLE_TRACE_SVG_DEFAULT_WIDTH            drawing = CableTraceSVG(obj, base_url=request.build_absolute_uri('/'), width=width)            return HttpResponse(drawing.render().tostring(), content_type='image/svg+xml')        # Serialize path objects, iterating over each three-tuple in the path        for near_ends, cable, far_ends in obj.trace():            if near_ends:                serializer_a = get_serializer_for_model(near_ends[0])                near_ends = serializer_a(near_ends, nested=True, many=True, context={'request': request}).data            else:                # Path is split; stop here                break            if cable:                cable = serializers.TracedCableSerializer(cable[0], context={'request': request}).data            if far_ends:                serializer_b = get_serializer_for_model(far_ends[0])                far_ends = serializer_b(far_ends, nested=True, many=True, context={'request': request}).data            path.append((near_ends, cable, far_ends))        return Response(path)",
        "documentation": "trace a complete cable path and return each segment a a threetuple of termination cable termination"
    },
    {
        "method_code": "def draw(self):                # Instantiate the drawing canvas        self.drawing = Drawing(self.pagesize[0], self.pagesize[1])        feature_elements = []  # holds feature elements        feature_labels = []  # holds feature labels        greytrack_bgs = []  # holds track background        greytrack_labels = []  # holds track foreground labels        scale_axes = []  # holds scale axes        scale_labels = []  # holds scale axis labels        # Get tracks to be drawn and set track sizes        self.drawn_tracks = self._parent.get_drawn_levels()        self.set_track_heights()        # Go through each track in the parent (if it is to be drawn) one by        # one and collate the data as drawing elements        for track_level in self._parent.get_drawn_levels():            self.current_track_level = track_level            track = self._parent[track_level]            gbgs, glabels = self.draw_greytrack(track)  # Greytracks            greytrack_bgs.append(gbgs)            greytrack_labels.append(glabels)            features, flabels = self.draw_track(track)  # Features and graphs            feature_elements.append(features)            feature_labels.append(flabels)            if track.scale:                axes, slabels = self.draw_scale(track)  # Scale axes                scale_axes.append(axes)                scale_labels.append(slabels)        feature_cross_links = []        for cross_link_obj in self.cross_track_links:            cross_link_elements = self.draw_cross_link(cross_link_obj)            if cross_link_elements:                feature_cross_links.append(cross_link_elements)        # Groups listed in order of addition to page (from back to front)        # Draw track backgrounds        # Draw feature cross track links        # Draw features and graphs        # Draw scale axes        # Draw scale labels        # Draw feature labels        # Draw track labels        element_groups = [            greytrack_bgs,            feature_cross_links,            feature_elements,            scale_axes,            scale_labels,            feature_labels,            greytrack_labels,        ]        for element_group in element_groups:            for element_list in element_group:                [self.drawing.add(element) for element in element_list]        if self.tracklines:            # Draw test tracks over top of diagram            self.draw_test_tracks()",
        "documentation": "draw a circular diagram of the stored data"
    },
    {
        "method_code": "def write(trees, handle, plain=False, **kwargs):        return Writer(trees).write(handle, plain=plain, **kwargs)",
        "documentation": "write a tree in cdao format to the given file handle return number of tree written"
    },
    {
        "method_code": "def _class_matcher(target_cls):        def match(node):        return isinstance(node, target_cls)    return match",
        "documentation": "match a node if it an instance of the given class private"
    },
    {
        "method_code": "def update(self, values):                super().update(values)        if self.mtime == 0 and \"mtime\" in values:            self.mtime = values[\"mtime\"]",
        "documentation": "set all keyvalue pair in the mapping if mtime is specified it is not reset a it might otherwise be"
    },
    {
        "method_code": "def __contains__(self, key) -> bool:                return key in self.keys(computed=True)",
        "documentation": "determine whether key is an attribute on this object"
    },
    {
        "method_code": "def invalid(self):                [x.captcha_invalid(self) for x in self.handler]",
        "documentation": "indicates the captcha wa not correct"
    },
    {
        "method_code": "def writeToSQL(self, db_handle):                cur = db_handle.cursor()        cur.execute(\"DROP TABLE IF EXISTS astral\")        cur.execute(\"CREATE TABLE astral (sid CHAR(8), seq TEXT, PRIMARY KEY (sid))\")        for dom in self.fasta_dict:            cur.execute(                \"INSERT INTO astral (sid,seq) values (%s,%s)\",                (dom, self.fasta_dict[dom].seq),            )        for i in astralBibIds:            cur.execute(\"ALTER TABLE astral ADD (id\" + str(i) + \" TINYINT)\")            for d in self.domainsClusteredById(i):                cur.execute(\"UPDATE astral SET id\" + str(i) + \"=1  WHERE sid=%s\", d.sid)        for ev in astralEvs:            cur.execute(\"ALTER TABLE astral ADD (\" + astralEv_to_sql[ev] + \" TINYINT)\")            for d in self.domainsClusteredByEv(ev):                cur.execute(                    \"UPDATE astral SET \" + astralEv_to_sql[ev] + \"=1  WHERE sid=%s\",                    d.sid,                )",
        "documentation": "write the astral database to a mysql database"
    },
    {
        "method_code": "def __init__(self):                pass",
        "documentation": "init a dummy backend class for mocked pil test"
    },
    {
        "method_code": "def get_container_client(self, container: str, database: str) -> \"ContainerProxy\":                database_client = self.get_database_client(database)        container_client = database_client.get_container_client(container=container)        return container_client",
        "documentation": "return an authenticated container client used for querying args container name of the cosmos db container to retrieve from database name of the cosmos db database example create an authorized container session python import o from prefect import flow from prefectazure import azureblobstoragecredentials flow def examplegetcontainerclientflow connectionstring osgetenvazurecosmosconnectionstring azurecredentials azurecosmosdbcredentials connectionstringconnectionstring containerclient azurecredentialsgetcontainerclientcontainer return containerclient examplegetcontainerclientflow"
    },
    {
        "method_code": "def check_sufficient_available(self, requested_objects, available_objects):                return len(requested_objects) <= len(available_objects)",
        "documentation": "check if there exist a sufficient number of available object to satisfy the request"
    },
    {
        "method_code": "def annotation(self, elem):                return PX.Annotation(            desc=_collapse_wspace(_get_child_text(elem, \"desc\")),            confidence=_get_child_as(elem, \"confidence\", self.confidence),            properties=_get_children_as(elem, \"property\", self.property),            uri=_get_child_as(elem, \"uri\", self.uri),            **elem.attrib,        )",
        "documentation": "create annotation object"
    },
    {
        "method_code": "def __repr__(self) -> str:                return self.id",
        "documentation": "repr string from id"
    },
    {
        "method_code": "def describe_for_cli(self, indent: int = 0) -> str:                return textwrap.indent(            \"\\n\".join(                [                    \"In this order:\",                    \"\\n\".join(                        [                            trigger.describe_for_cli(indent=indent + 1)                            for trigger in self.triggers                        ]                    ),                ]            ),            prefix=\"  \" * indent,        )",
        "documentation": "return a humanreadable description of this trigger for the cli"
    },
    {
        "method_code": "def __repr__(self):                return self._repr",
        "documentation": "return a string representation of the file object"
    },
    {
        "method_code": "def _solve(prefix, specs, args, env, *_, **kwargs):        # TODO: support all various ways this happens    # Including 'nodefaults' in the channels list disables the defaults    channel_urls = [chan for chan in env.channels if chan != \"nodefaults\"]    if \"nodefaults\" not in env.channels:        channel_urls.extend(context.channels)    _channel_priority_map = prioritize_channels(channel_urls)    channels = IndexedSet(Channel(url) for url in _channel_priority_map)    subdirs = IndexedSet(basename(url) for url in _channel_priority_map)    solver_backend = context.plugin_manager.get_cached_solver_backend()    solver = solver_backend(prefix, channels, subdirs, specs_to_add=specs)    return solver",
        "documentation": "solve the environment"
    },
    {
        "method_code": "def get_permitted_actions(self, user, model=None):                model = model or self.queryset.model        # Resolve required permissions for each action        permitted_actions = []        for action in self.actions:            required_permissions = [                get_permission_for_model(model, name) for name in self.actions.get(action, set())            ]            if not required_permissions or user.has_perms(required_permissions):                permitted_actions.append(action)        return permitted_actions",
        "documentation": "return a tuple of action for which the given user is permitted to do"
    },
    {
        "method_code": "def process_exception(self, request, exception):                # Don't catch exceptions when in debug mode        if settings.DEBUG:            return        # Cleanly handle exceptions that occur from REST API requests        if is_api_request(request):            return handle_rest_api_exception(request)        # Ignore Http404s (defer to Django's built-in 404 handling)        if isinstance(exception, Http404):            return        # Determine the type of exception. If it's a common issue, return a custom error page with instructions.        custom_template = None        if isinstance(exception, ProgrammingError):            custom_template = 'exceptions/programming_error.html'        elif isinstance(exception, ImportError):            custom_template = 'exceptions/import_error.html'        elif isinstance(exception, PermissionError):            custom_template = 'exceptions/permission_error.html'        # Return a custom error message, or fall back to Django's default 500 error handling        if custom_template:            return handler_500(request, template_name=custom_template)",
        "documentation": "implement custom error handling logic for production deployment"
    },
    {
        "method_code": "def Cancelled(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.CANCELLED, **kwargs)",
        "documentation": "convenience function for creating cancelled state return state a cancelled state"
    },
    {
        "method_code": "def load(s):        try:        out = []        for d in yaml.safe_load_all(s):            if not isinstance(d, dict):                raise ParseError(                    \"each entry must be a dictionary; found {}\".format(                        type(d).__name__                    )                )            # Convert all keys to strings. They started out as strings,            # but the user may have inadvertently messed this up.            out.append({str(k): v for k, v in d.items()})    except yaml.YAMLError as e:        raise ParseError(f\"invalid YAML: {e}\")    return out",
        "documentation": "read a sequence of yaml document back to a list of dictionary with string key can raise a parseerror"
    },
    {
        "method_code": "def set_global_level(self, level):                self.default_level = level        self.setLevel(level)",
        "documentation": "set the level on the current thread the default value for all thread"
    },
    {
        "method_code": "def _populate_command_if_not_present(self):                try:            command = self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][                0            ].get(\"args\")            if command is None:                self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][                    \"args\"                ] = shlex.split(self._base_flow_run_command())            elif isinstance(command, str):                self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][                    \"args\"                ] = shlex.split(command)            elif not isinstance(command, list):                raise ValueError(                    \"Invalid job manifest template: 'command' must be a string or list.\"                )        except KeyError:            raise ValueError(                \"Unable to verify command due to invalid job manifest template.\"            )",
        "documentation": "ensures that the command is present in the job manifest populates the command with the prefect m prefectengine if a command is not present"
    },
    {
        "method_code": "def test_is_page_candidate_fuzzy_match(self):                s = self.source        url = s[\"url\"] + s[\"path\"]        url_title = \"example.com | Beats song by John doe\"        # very small diffs (typo) are ok eg 'beats' vs 'beets' with same artist        assert google.is_page_candidate(            url, url_title, s[\"title\"], s[\"artist\"]        ), url        # reject different title        url_title = \"example.com | seets bong lyrics by John doe\"        assert not google.is_page_candidate(            url, url_title, s[\"title\"], s[\"artist\"]        ), url",
        "documentation": "test matching html page title with song info when song info are not present in the title"
    },
    {
        "method_code": "def convert(in_file, in_format, out_file, out_format, molecule_type=None):        if molecule_type:        if not isinstance(molecule_type, str):            raise TypeError(f\"Molecule type should be a string, not {molecule_type!r}\")        elif (            \"DNA\" in molecule_type            or \"RNA\" in molecule_type            or \"protein\" in molecule_type        ):            pass        else:            raise ValueError(f\"Unexpected molecule type, {molecule_type!r}\")    # TODO - Add optimised versions of important conversions    # For now just off load the work to SeqIO parse/write    # Don't open the output file until we've checked the input is OK:    alignments = parse(in_file, in_format, None)    if molecule_type:        # Edit the records on the fly to set molecule type        def over_ride(alignment):                        for record in alignment:                record.annotations[\"molecule_type\"] = molecule_type            return alignment        alignments = (over_ride(_) for _ in alignments)    return write(alignments, out_file, out_format)",
        "documentation": "convert between two alignment file return number of alignment argument infile an input handle or filename informat input file format lower case string output an output handle or filename outfile output file format lower case string moleculetype optional molecule type to apply string containing dna rna or protein note if you provide an output filename it will be opened which will overwrite any existing file without warning this may happen if even the conversion is aborted eg an invalid outformat name is given some output format require the molecule type be specified where this cannot be determined by the parser for example converting to fasta clustal or phylip format to nexus from io import stringio from bio import alignio handle stringio alignioconvertphyliphorsesphy phylip handle nexus dna printhandlegetvalue nexus begin data dimension ntax nchar format datatypedna missing gap matrix mesohippus aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa hypohippus aaacccccccaaaaaaaaacaaaaaaaaaaaaaaaaaaaa archaeohip caaaaaaaaaaaaaaaacacaaaaaaaaaaaaaaaaaaaa parahippus caaacaacaacaaaaaaaacaaaaaaaaaaaaaaaaaaaa merychippu ccaaccaccaccccacacccaaaaaaaaaaaaaaaaaaaa m secundu ccaaccaccacccacaccccaaaaaaaaaaaaaaaaaaaa nannipus ccaaccacaaccccacacccaaaaaaaaaaaaaaaaaaaa neohippari ccaaccccccccccacacccaaaaaaaaaaaaaaaaaaaa calippus ccaaccacaacccacaccccaaaaaaaaaaaaaaaaaaaa pliohippus cccacccccccccacaccccaaaaaaaaaaaaaaaaaaaa end blankline"
    },
    {
        "method_code": "def startDBRefElement(self, attrs):                TYPE = None        source = None        ID = None        for key, value in attrs.items():            namespace, localname = key            if namespace is None:                if localname == \"type\":                    TYPE = value                elif localname == \"source\":                    source = value                elif localname == \"id\":                    ID = value                else:                    raise ValueError(                        f\"Unexpected attribute '{key}' found for DBRef element\"                    )            else:                raise ValueError(                    f\"Unexpected namespace '{namespace}' for DBRef attribute\"                )        # The attributes \"source\" and \"id\" are required, and \"type\" in versions        # 0.2-0.3:        if source is None:            raise ValueError(\"Failed to find source for DBRef element\")        if ID is None:            raise ValueError(\"Failed to find id for DBRef element\")        if TYPE is None and (            self.seqXMLversion == \"0.2\" or self.seqXMLversion == \"0.3\"        ):            raise ValueError(\"Failed to find type for DBRef element\")        if self.data is not None:            raise RuntimeError(f\"Unexpected data found: '{self.data}'\")        self.data = \"\"        record = self.records[-1]        dbxref = f\"{source}:{ID}\"        if dbxref not in record.dbxrefs:            record.dbxrefs.append(dbxref)        self.endElementNS = self.endDBRefElement",
        "documentation": "parse a database cross reference"
    },
    {
        "method_code": "def gc_content(self):                raise Exception(\"Cannot compute the %GC composition of a PSSM\")",
        "documentation": "compute the gcratio"
    },
    {
        "method_code": "def test_Other(self):                phx = PhyloXMLIO.read(EX_PHYLO)        otr = phx.other[0]        self.assertIsInstance(otr, PX.Other)        self.assertEqual(otr.tag, \"alignment\")        self.assertEqual(otr.namespace, \"http://example.org/align\")        self.assertEqual(len(otr.children), 3)        for child, name, value in zip(            otr,            (\"A\", \"B\", \"C\"),            (                \"acgtcgcggcccgtggaagtcctctcct\",                \"aggtcgcggcctgtggaagtcctctcct\",                \"taaatcgc--cccgtgg-agtccc-cct\",            ),        ):            self.assertEqual(child.tag, \"seq\")            self.assertEqual(child.attributes[\"name\"], name)            self.assertEqual(child.value, value)",
        "documentation": "instantiation of other object"
    },
    {
        "method_code": "def verify(self, password=None):                pass",
        "documentation": "testing with extractor builtin method raise error if password is needed integrity is questionable or else"
    },
    {
        "method_code": "def _split_multi_line(text, max_len):                # TODO - Do the line splitting while preserving white space?        text = text.strip()        if len(text) <= max_len:            return [text]        words = text.split()        text = \"\"        while words and len(text) + 1 + len(words[0]) <= max_len:            text += \" \" + words.pop(0)            text = text.strip()        # assert len(text) <= max_len        answer = [text]        while words:            text = words.pop(0)            while words and len(text) + 1 + len(words[0]) <= max_len:                text += \" \" + words.pop(0)                text = text.strip()            # assert len(text) <= max_len            answer.append(text)        assert not words        return answer",
        "documentation": "return a list of string private any single word which are too long get returned a a whole line eg url without an exception or warning"
    },
    {
        "method_code": "def add_track(self, track, track_level):                if track is None:            raise ValueError(\"Must specify track\")        if track_level not in self.tracks:  # No track at that level            self.tracks[track_level] = track  # so just add it        else:  # Already a track there, so shunt all higher tracks up one            occupied_levels = sorted(                self.get_levels()            )  # Get list of occupied levels...            occupied_levels.reverse()  # ...reverse it (highest first)            for val in occupied_levels:                # If track value >= that to be added                if val >= track.track_level:                    self.tracks[val + 1] = self.tracks[val]  # ...increment by 1            self.tracks[track_level] = track  # And put the new track in        self.tracks[track_level].track_level = track_level",
        "documentation": "add a track object to the diagram it also accepts instruction to place it at a particular level on the diagram argument track track object to draw tracklevel an integer the level at which the track will be drawn above an arbitrary baseline addtrackself track tracklevel"
    },
    {
        "method_code": "def test_pdbio_write_pqr_structure(self):                # Create a PDBIO object in pqr mode with example_structure as an argument        io = PDBIO(is_pqr=True)        io.set_structure(self.example_structure)        # Write to a temporary file        filenumber, filename = tempfile.mkstemp()        os.close(filenumber)        try:            # Export example_structure to a temp file            io.save(filename)            # Parse exported structure            output_struct = self.pqr_parser.get_structure(\"1a8o\", filename)            # Comparisons            self.assertEqual(                len(output_struct), len(self.example_structure)            )  # Structure Length            original_residues = len(list(self.example_structure.get_residues()))            parsed_residues = len(list(output_struct.get_residues()))            self.assertEqual(parsed_residues, original_residues)  # Number of Residues            # Atom-wise comparison            original_atoms = self.example_structure.get_atoms()            for atom in output_struct.get_atoms():                self.assertEqual(atom, next(original_atoms))        finally:            os.remove(filename)",
        "documentation": "write a full structure using pdbio"
    },
    {
        "method_code": "def _atypes(self, item: Album):                types = self.config[\"types\"].as_pairs()        ignore_va = self.config[\"ignore_va\"].as_str_seq()        bracket = self.config[\"bracket\"].as_str()        # Assign a left and right bracket or leave blank if argument is empty.        if len(bracket) == 2:            bracket_l = bracket[0]            bracket_r = bracket[1]        else:            bracket_l = \"\"            bracket_r = \"\"        res = \"\"        albumtypes = item.albumtypes        is_va = item.mb_albumartistid == VARIOUS_ARTISTS_ID        for type in types:            if type[0] in albumtypes and type[1]:                if not is_va or (type[0] not in ignore_va and is_va):                    res += f\"{bracket_l}{type[1]}{bracket_r}\"        return res",
        "documentation": "return a formatted string based on album type"
    },
    {
        "method_code": "def _patch_for_local_exports(name, subdir_data):        _alias_canonical_channel_name_cache_to_file_prefixed(name, subdir_data)    # we need to override the modification time here so the    # cache hits this subdir_data object from the local copy too    # - without this, the legacy solver will use the local dump too    # and there's no need for that extra work    # (check conda.core.subdir_data.SubdirDataType.__call__ for    # details)    _sync_channel_to_disk(subdir_data)    subdir_data._mtime = float(\"inf\")",
        "documentation": "this function is only temporary and meant to patch wrong undesirable testing behaviour it should end up being replaced with the new classbased backendagnostic solver test"
    },
    {
        "method_code": "def tasks(self):                # Check whether this is an archive.        if self.is_archive:            archive_task = self.unarchive()            if not archive_task:                return        # Search for music in the directory.        for dirs, paths in self.paths():            if self.session.config[\"singletons\"]:                for path in paths:                    tasks = self._create(self.singleton(path))                    yield from tasks                yield self.sentinel(dirs)            else:                tasks = self._create(self.album(paths, dirs))                yield from tasks        # Produce the final sentinel for this toppath to indicate that        # it is finished. This is usually just a SentinelImportTask, but        # for archive imports, send the archive task instead (to remove        # the extracted directory).        if self.is_archive:            yield archive_task        else:            yield self.sentinel()",
        "documentation": "yield all import task for music found in the userspecified path selftoppath any necessary sentinel task are also produced during generation update selfskipped and selfimported with the number of task that were not produced due to incremental mode or resumed import and the number of concrete task actually produced respectively if selftoppath is an archive it is adjusted to point to the extracted data"
    },
    {
        "method_code": "def _convert(self, s: str) -> Optional[float]:                if not s:            return None        try:            return util.raw_seconds_short(s)        except ValueError:            try:                return float(s)            except ValueError:                raise InvalidQueryArgumentValueError(                    s, \"a M:SS string or a float\"                )",
        "documentation": "convert a ms or numeric string to a float return none if s is empty raise an invalidqueryerror if the string cannot be converted"
    },
    {
        "method_code": "def read(handle):        xms_doc = minidom.parse(handle)    record = XMSScanner(xms_doc).record    return record",
        "documentation": "read motif in xms matrix format from a file handle xms is an xml format for describing regulatory motif and pssms this format wa defined by thomas down and used in the nestedmica and motifexplorer program"
    },
    {
        "method_code": "def event_tracking(request):        current_request.set(request)    events_queue.set({})    yield    # Flush queued webhooks to RQ    if events := list(events_queue.get().values()):        flush_events(events)    # Clear context vars    current_request.set(None)    events_queue.set({})",
        "documentation": "queue interesting event in memory while processing a request then flush that queue for processing by the event pipline before returning the response param request wsgirequest object with a unique id set"
    },
    {
        "method_code": "def test_known_solver(plugin_manager: CondaPluginManager):        assert plugin_manager.load_plugins(VerboseSolverPlugin) == 1    assert plugin_manager.get_solver_backend(\"verbose-classic\") == VerboseSolver",
        "documentation": "cover getting a solver that exists"
    },
    {
        "method_code": "def substitute(self, values={}, functions={}):                try:            res = self.compiled(values, functions)        except Exception:  # Handle any exceptions thrown by compiled version.            res = self.interpret(values, functions)        return res",
        "documentation": "evaluate the template given the value and function"
    },
    {
        "method_code": "def rating(self, play_count, skip_count, rating, skipped):                if skipped:            rolling = rating - rating / 2.0        else:            rolling = rating + (1.0 - rating) / 2.0        stable = (play_count + 1.0) / (play_count + skip_count + 2.0)        return self.rating_mix * stable + (1.0 - self.rating_mix) * rolling",
        "documentation": "calculate a new rating for a song based on play count skip count old rating and the fact if it wa skipped or not"
    },
    {
        "method_code": "def _set_header_reference(self):                self._header.reference = self._value",
        "documentation": "record any article reference describing the algorithm private save this to put on each blast record object"
    },
    {
        "method_code": "def __iter__(self):                for row in self._con.execute(            \"SELECT key FROM offset_data ORDER BY file_number, offset;\"        ):            yield str(row[0])",
        "documentation": "iterate over the key"
    },
    {
        "method_code": "def difference(self, other: \"Dihedron\") -> float:                return Dihedron.angle_dif(self.angle, other.angle)",
        "documentation": "get angle difference between this and other angle"
    },
    {
        "method_code": "def test_read_write(self):                path = os.path.join(\"Align\", \"hg38.chrom.sizes\")        sizes = substitution_matrices.read(path, np.int64)        # Note that sum(sizes) below is larger than 2147483647, and won't        # fit in an int on a 32-bits machine.        self.assertEqual(len(sizes), 455)        self.assertEqual(sizes[\"chr1\"], 248956422)        self.assertEqual(sizes[\"chr2\"], 242193529)        self.assertEqual(sizes[\"chr3\"], 198295559)        self.assertEqual(sizes[\"chr4\"], 190214555)        self.assertEqual(sizes[\"chr5\"], 181538259)        self.assertEqual(sum(sizes), 3209286105)        text = str(sizes)        lines = text.split(\"\\n\")        self.assertEqual(lines[0], \"chr1 248956422\")        self.assertEqual(lines[1], \"chr2 242193529\")        self.assertEqual(lines[2], \"chr3 198295559\")        self.assertEqual(lines[3], \"chr4 190214555\")        self.assertEqual(lines[4], \"chr5 181538259\")",
        "documentation": "test reading and writing substitution matrix"
    },
    {
        "method_code": "def getAscendent(self, node_type):                if node_type in _nodetype_to_code:            node_type = _nodetype_to_code[node_type]        if self.scop:            return self.scop.getAscendentFromSQL(self, node_type)        else:            n = self            if n.type == node_type:                return None            while n.type != node_type:                if n.type == \"ro\":                    return None  # Fell of the top of the hierarchy                n = n.getParent()            return n",
        "documentation": "return the ancenstor node of the given type or none node type can be a two letter code or longer description eg fa or family"
    },
    {
        "method_code": "def __len__(self):                return self.length",
        "documentation": "return length of the fragment"
    },
    {
        "method_code": "def setUp(self):                with open(os.path.join(\"GenBank\", \"NC_005816.gb\")) as handle:            self.record = SeqIO.read(handle, \"genbank\")        self.gdd = Diagram(\"Test Diagram\")        # Add a track of features,        self.gdd.new_track(            1, greytrack=True, name=\"CDS Features\", greytrack_labels=0, height=0.5        )",
        "documentation": "test setup just load a genbank file a a seqrecord"
    },
    {
        "method_code": "def clade_relation(self, elem):                return PX.CladeRelation(            elem.get(\"type\"),            elem.get(\"id_ref_0\"),            elem.get(\"id_ref_1\"),            distance=elem.get(\"distance\"),            confidence=_get_child_as(elem, \"confidence\", self.confidence),        )",
        "documentation": "create clade relationship object"
    },
    {
        "method_code": "def add_available_ipaddresses(prefix, ipaddress_list, is_pool=False):        output = []    prev_ip = None    # Ignore the network and broadcast addresses for non-pool IPv4 prefixes larger than /31.    if prefix.version == 4 and prefix.prefixlen < 31 and not is_pool:        first_ip_in_prefix = netaddr.IPAddress(prefix.first + 1)        last_ip_in_prefix = netaddr.IPAddress(prefix.last - 1)    else:        first_ip_in_prefix = netaddr.IPAddress(prefix.first)        last_ip_in_prefix = netaddr.IPAddress(prefix.last)    if not ipaddress_list:        return [(            int(last_ip_in_prefix - first_ip_in_prefix + 1),            '{}/{}'.format(first_ip_in_prefix, prefix.prefixlen)        )]    # Account for any available IPs before the first real IP    if ipaddress_list[0].address.ip > first_ip_in_prefix:        skipped_count = int(ipaddress_list[0].address.ip - first_ip_in_prefix)        first_skipped = '{}/{}'.format(first_ip_in_prefix, prefix.prefixlen)        output.append((skipped_count, first_skipped))    # Iterate through existing IPs and annotate free ranges    for ip in ipaddress_list:        if prev_ip:            diff = int(ip.address.ip - prev_ip.address.ip)            if diff > 1:                first_skipped = '{}/{}'.format(prev_ip.address.ip + 1, prefix.prefixlen)                output.append((diff - 1, first_skipped))        output.append(ip)        prev_ip = ip    # Include any remaining available IPs    if prev_ip.address.ip < last_ip_in_prefix:        skipped_count = int(last_ip_in_prefix - prev_ip.address.ip)        first_skipped = '{}/{}'.format(prev_ip.address.ip + 1, prefix.prefixlen)        output.append((skipped_count, first_skipped))    return output",
        "documentation": "annotate range of available ip address within a given prefix if ispool is true the first and last ip will be considered usable regardless of mask length"
    },
    {
        "method_code": "def initialize_run(        self,        task_run_id: Optional[UUID] = None,        dependencies: Optional[Dict[str, Set[TaskRunInput]]] = None,    ) -> Generator[\"SyncTaskRunEngine\", Any, Any]:                with hydrated_context(self.context):            with SyncClientContext.get_or_create() as client_ctx:                self._client = client_ctx.client                self._is_started = True                try:                    if not self.task_run:                        self.task_run = run_coro_as_sync(                            self.task.create_local_run(                                id=task_run_id,                                parameters=self.parameters,                                flow_run_context=FlowRunContext.get(),                                parent_task_run_context=TaskRunContext.get(),                                wait_for=self.wait_for,                                extra_task_inputs=dependencies,                            )                        )                        # Emit an event to capture that the task run was in the `PENDING` state.                        self._last_event = emit_task_run_state_change_event(                            task_run=self.task_run,                            initial_state=None,                            validated_state=self.task_run.state,                        )                    with self.setup_run_context():                        # setup_run_context might update the task run name, so log creation here                        self.logger.info(                            f\"Created task run {self.task_run.name!r} for task {self.task.name!r}\"                        )                        yield self                except TerminationSignal as exc:                    # TerminationSignals are caught and handled as crashes                    self.handle_crash(exc)                    raise exc                except Exception:                    # regular exceptions are caught and re-raised to the user                    raise                except (Pause, Abort) as exc:                    # Do not capture internal signals as crashes                    if isinstance(exc, Abort):                        self.logger.error(\"Task run was aborted: %s\", exc)                    raise                except GeneratorExit:                    # Do not capture generator exits as crashes                    raise                except BaseException as exc:                    # BaseExceptions are caught and handled as crashes                    self.handle_crash(exc)                    raise                finally:                    self.log_finished_message()                    self._is_started = False                    self._client = None",
        "documentation": "enters a client context and creates a task run if needed"
    },
    {
        "method_code": "def _read_pstring(handle):        length = unpack(\">B\", _read(handle, 1))[0]    return unpack(\"%ds\" % length, _read(handle, length))[0].decode(\"ASCII\")",
        "documentation": "read a pascal string a pascal string comprises a single byte giving the length of the string followed by a many byte"
    },
    {
        "method_code": "def only_return_value_in_test_mode(settings, value):        if PREFECT_TEST_MODE.value_from(settings):        return value    else:        return None",
        "documentation": "valuecallback for prefecttestsetting that only allows access during test mode"
    },
    {
        "method_code": "def _stream_encoding(stream, default=\"utf-8\"):        # Configured override?    encoding = config[\"terminal_encoding\"].get()    if encoding:        return encoding    # For testing: When sys.stdout or sys.stdin is a StringIO under the    # test harness, it doesn't have an `encoding` attribute. Just use    # UTF-8.    if not hasattr(stream, \"encoding\"):        return default    # Python's guessed output stream encoding, or UTF-8 as a fallback    # (e.g., when piped to a file).    return stream.encoding or default",
        "documentation": "a helper for inencoding and outencoding get the stream preferred encoding using a configured override or a default fallback if neither is not specified"
    },
    {
        "method_code": "def resolve_serializer(serializer: ResultSerializer) -> Serializer:        if isinstance(serializer, Serializer):        return serializer    elif isinstance(serializer, str):        return Serializer(type=serializer)    else:        raise TypeError(            \"Result serializer must be one of the following types: 'Serializer', \"            f\"'str'. Got unsupported type {type(serializer).__name__!r}.\"        )",
        "documentation": "resolve one of the valid resultserializer input type into a serializer instance"
    },
    {
        "method_code": "def extract(structure, chain_id, start, end, filename):        sel = ChainSelector(chain_id, start, end)    io = PDBIO()    io.set_structure(structure)    io.save(filename, sel)",
        "documentation": "write out selected portion to filename"
    },
    {
        "method_code": "def get_utilization(self):                # Determine unoccupied units        total_units = len(list(self.units))        available_units = self.get_available_units(u_height=0.5, ignore_excluded_devices=True)        # Remove reserved units        for ru in self.get_reserved_units():            for u in drange(ru, ru + 1, 0.5):                if u in available_units:                    available_units.remove(u)        occupied_unit_count = total_units - len(available_units)        percentage = float(occupied_unit_count) / total_units * 100        return percentage",
        "documentation": "determine the utilization rate of the rack and return it a a percentage occupied and reserved unit both count a utilized"
    },
    {
        "method_code": "def extract(self, password=None):                raise NotImplementedError",
        "documentation": "extract the archive raise specific error in case of failure"
    },
    {
        "method_code": "def finalize(self, session):                # Update progress.        if session.want_resume:            self.save_progress()        if session.config[\"incremental\"] and not (            # Should we skip recording to incremental list?            self.skip            and session.config[\"incremental_skip_later\"]        ):            self.save_history()        self.cleanup(            copy=session.config[\"copy\"],            delete=session.config[\"delete\"],            move=session.config[\"move\"],        )        if not self.skip:            self._emit_imported(session.lib)",
        "documentation": "save progress clean up file and emit plugin event"
    },
    {
        "method_code": "def full(self, linear=True):                return self.mapping",
        "documentation": "perform analysis with all enzyme of batch and return all result full restriction map of the sequence a a dictionary"
    },
    {
        "method_code": "def strictly_equals(        self: _Self, other: _Self, compare_coordinates: bool = False    ) -> bool:                if not isinstance(other, type(self)):            return False        if self.id != other.id:            return False        if len(self.child_list) != len(other.child_list):            return False        for left_child, right_child in zip(self.child_list, other.child_list):            assert hasattr(left_child, \"strictly_equals\")            if not left_child.strictly_equals(right_child, compare_coordinates):                return False        return True",
        "documentation": "compare this entity to the other entity for equality recursively compare the child of this entity to the other entity child compare most property including name and id param other the entity to compare this entity with type other entity param comparecoordinates whether to compare atomic coordinate type comparecoordinates bool return whether the two entity are strictly equal rtype bool"
    },
    {
        "method_code": "def test_permissive_transfac_parser(self):                # The test file MA0056.1.transfac was obtained from the JASPAR database        # in a TRANSFAC-like format.        # Khan, A. et al. JASPAR 2018: update of the open-access database of        # transcription factor binding profiles and its web framework.        # Nucleic Acids Res. 2018; 46:D260-D266,        path = \"motifs/MA0056.1.transfac\"        with open(path) as stream:            self.assertRaises(ValueError, motifs.parse, stream, \"TRANSFAC\")        with open(path) as stream:            records = motifs.parse(stream, \"TRANSFAC\", strict=False)        motif = records[0]        self.assertEqual(sorted(motif.keys()), [\"AC\", \"DE\", \"ID\"])        self.assertEqual(motif[\"AC\"], \"MA0056.1\")        self.assertEqual(motif[\"DE\"], \"MA0056.1 MZF1 ; From JASPAR 2018\")        self.assertEqual(motif[\"ID\"], \"MZF1\")        self.assertEqual(motif.counts.length, 6)        self.assertEqual(len(motif.counts), 4)        self.assertEqual(motif.counts[\"A\", 0], 3.0)        self.assertEqual(motif.counts[\"A\", 1], 0.0)        self.assertEqual(motif.counts[\"A\", 2], 2.0)        self.assertEqual(motif.counts[\"A\", 3], 0.0)        self.assertEqual(motif.counts[\"A\", 4], 0.0)        self.assertEqual(motif.counts[\"A\", 5], 18.0)        self.assertEqual(motif.counts[\"C\", 0], 5.0)        self.assertEqual(motif.counts[\"C\", 1], 0.0)        self.assertEqual(motif.counts[\"C\", 2], 0.0)        self.assertEqual(motif.counts[\"C\", 3], 0.0)        self.assertEqual(motif.counts[\"C\", 4], 0.0)        self.assertEqual(motif.counts[\"C\", 5], 0.0)        self.assertEqual(motif.counts[\"G\", 0], 4.0)        self.assertEqual(motif.counts[\"G\", 1], 19.0)        self.assertEqual(motif.counts[\"G\", 2], 18.0)        self.assertEqual(motif.counts[\"G\", 3], 19.0)        self.assertEqual(motif.counts[\"G\", 4], 20.0)        self.assertEqual(motif.counts[\"G\", 5], 2.0)        self.assertEqual(motif.counts[\"T\", 0], 8.0)        self.assertEqual(motif.counts[\"T\", 1], 1.0)        self.assertEqual(motif.counts[\"T\", 2], 0.0)        self.assertEqual(motif.counts[\"T\", 3], 1.0)        self.assertEqual(motif.counts[\"T\", 4], 0.0)        self.assertEqual(motif.counts[\"T\", 5], 0.0)        self.assertEqual(motif.consensus, \"TGGGGA\")        self.assertEqual(motif.degenerate_consensus, \"NGGGGA\")        self.assertTrue(            np.allclose(                motif.relative_entropy,                np.array(                    [                        0.09629830394265171,                        1.7136030428840439,                        1.5310044064107189,                        1.7136030428840439,                        2.0,                        1.5310044064107189,                    ]                ),            )        )        self.assertEqual(motif[1:-3].degenerate_consensus, \"GG\")        self.assertTrue(            np.allclose(                motif[1:-3].relative_entropy,                np.array([1.7136030428840439, 1.5310044064107189]),            )        )",
        "documentation": "parse the transfaclike file motifsmatransfac"
    },
    {
        "method_code": "def item_candidates(self, item, artist, title):                if not self.discogs_client:            return []        if not artist and not title:            self._log.debug(                \"Skipping Discogs query. File missing artist and \" \"title tags.\"            )            return []        query = f\"{artist} {title}\"        try:            albums = self.get_albums(query)        except DiscogsAPIError as e:            self._log.debug(\"API Error: {0} (query: {1})\", e, query)            if e.status_code == 401:                self.reset_auth()                return self.item_candidates(item, artist, title)            else:                return []        except CONNECTION_ERRORS:            self._log.debug(\"Connection error in track search\", exc_info=True)        candidates = []        for album_cur in albums:            self._log.debug(\"searching within album {0}\", album_cur.album)            track_result = self.get_track_from_album_by_title(                album_cur, item[\"title\"]            )            if track_result:                candidates.append(track_result)        # first 10 results, don't overwhelm with options        return candidates[:10]",
        "documentation": "return a list of trackinfo object for search api result matching title and artist param item singleton item to be matched type item beetslibraryitem param artist the artist of the track to be matched type artist str param title the title of the track to be matched type title str return candidate trackinfo object rtype listbeetsautotaghookstrackinfo"
    },
    {
        "method_code": "def download_http_errors(url: str):        # This complex exception translation strategy is reminiscent of def    # conda_http_errors(url, repodata_fn): in gateways/repodata    try:        yield    except ConnectionResetError as e:        log.debug(f\"{e}, trying again\")        # where does retry happen?        raise    except RequestsProxyError:        raise ProxyError()  # see #3962    except InvalidSchema as e:        if \"SOCKS\" in str(e):            message = dals(                            )            raise CondaDependencyError(message)        else:            raise    except SSLError as e:        # SSLError: either an invalid certificate or OpenSSL is unavailable        try:            import ssl  # noqa: F401        except ImportError:            raise CondaSSLError(                dals(                    f                )            )        else:            raise CondaSSLError(                dals(                    f                )            )    except (ConnectionError, HTTPError) as e:        help_message = dals(                    )        raise CondaHTTPError(            help_message,            url,            getattr(e.response, \"status_code\", None),            getattr(e.response, \"reason\", None),            getattr(e.response, \"elapsed\", None),            e.response,            caused_by=e,        )",
        "documentation": "exception translator used inside download"
    },
    {
        "method_code": "def scan_download(self, rules, read_size=1_048_576):                if not self.last_download:            self.log_warning(self._(\"No file to scan\"))            return        dl_file = os.fsdecode(self.last_download)  # TODO: Recheck in 0.6.x        with open(dl_file, mode=\"rb\") as fp:            content = fp.read(read_size)        for name, rule in rules.items():            if isinstance(rule, bytes):                if rule in content:                    return name            elif isinstance(rule, str):                raise TypeError(f\"Cannot check binary data with string rule '{name}'\")            elif hasattr(rule, \"search\"):                m = rule.search(content)                if m is not None:                    self.last_check = m                    return name            elif callable(rule):                return rule(content)",
        "documentation": "check the content of the last downloaded file re match is saved to lastcheck param rule dict with name and rule to match compiled regexp or string param readsize size to read and scan return dictionary key of the first rule that matched"
    },
    {
        "method_code": "def restart_file(self, file_id):                self.pyload.files.restart_file(int(file_id))",
        "documentation": "reset file status so it will be downloaded again param fileid file id"
    },
    {
        "method_code": "def run(self):                self.startup_time = time.time()        def start():            yield bluelet.spawn(                bluelet.server(                    self.ctrl_host,                    self.ctrl_port,                    ControlConnection.handler(self),                )            )            yield bluelet.server(                self.host, self.port, MPDConnection.handler(self)            )        bluelet.run(start())",
        "documentation": "block and start listening for connection from client an interrupt c close the server"
    },
    {
        "method_code": "def __repr__(self):                return \"%s(hit_id=%r, query_id=%r, %r fragments)\" % (            self.__class__.__name__,            self.hit_id,            self.query_id,            len(self),        )",
        "documentation": "return string representation of hsp object"
    },
    {
        "method_code": "def test_record_loading(self):                test_record = self.db.lookup(accession=\"X55053\")        self.assertEqual(test_record.name, \"ATCOR66M\")        self.assertEqual(test_record.id, \"X55053.1\")        self.assertEqual(test_record.description, \"A.thaliana cor6.6 mRNA\")        self.assertEqual(test_record.annotations[\"molecule_type\"], \"DNA\")        self.assertEqual(test_record.seq[:20], \"AACAAAACACACATCAAAAA\")        test_record = self.db.lookup(accession=\"X62281\")        self.assertEqual(test_record.name, \"ATKIN2\")        self.assertEqual(test_record.id, \"X62281.1\")        self.assertEqual(test_record.description, \"A.thaliana kin2 gene\")        self.assertEqual(test_record.annotations[\"molecule_type\"], \"DNA\")        self.assertEqual(test_record.seq[:10], \"ATTTGGCCTA\")",
        "documentation": "make sure all record are correctly loaded"
    },
    {
        "method_code": "def __getattr__(self, name):                return getattr(self._codon_table, name)",
        "documentation": "forward attribute lookup to the original table"
    },
    {
        "method_code": "def _count_emissions(self, training_seq, emission_counts):                for index in range(len(training_seq.emissions)):            cur_state = training_seq.states[index]            cur_emission = training_seq.emissions[index]            try:                emission_counts[(cur_state, cur_emission)] += 1            except KeyError:                raise KeyError(f\"Unexpected emission ({cur_state}, {cur_emission})\")        return emission_counts",
        "documentation": "add emission from the training sequence to the current count private argument trainingseq a trainingsequence with state and emission to get the count from emissioncounts the current emission count to add to"
    },
    {
        "method_code": "def __init__(        self,        base_image: str,        base_directory: Path = None,        platform: Optional[str] = None,        context: Path = None,    ):                self.base_directory = base_directory or context or Path().absolute()        self.temporary_directory = None        self.context = context        self.platform = platform        self.dockerfile_lines = []        if self.context:            dockerfile_path: Path = self.context / \"Dockerfile\"            if dockerfile_path.exists():                raise ValueError(f\"There is already a Dockerfile at {context}\")        self.add_line(f\"FROM {base_image}\")",
        "documentation": "create an imagebuilder args baseimage the base image to use basedirectory the starting point on your host for relative file location defaulting to the current directory context use this path a the build context if not provided will create a temporary directory for the context return the image id"
    },
    {
        "method_code": "def _is_missing_container(ready_condition: Dict) -> bool:                if (            ready_condition.get(\"state\") == \"CONTAINER_FAILED\"            and ready_condition.get(\"reason\") == \"ContainerMissing\"        ):            return True        return False",
        "documentation": "check if the job is missing a container args readycondition the ready condition for the job return whether the job is missing a container"
    },
    {
        "method_code": "def __init__(self, cmd=\"fprotpars\", **kwargs):                self.parameters = [            _Option(                [\"-sequence\", \"sequence\"],                \"seq file to use (phylip)\",                filename=True,                is_required=True,            ),            _Option([\"-intreefile\", \"intreefile\"], \"Phylip tree file to score\"),            _Option(                [\"-outtreefile\", \"outtreefile\"],                \"phylip tree output file\",                filename=True,                is_required=True,            ),            _Option([\"-weights\", \"weights\"], \"weights file\"),            _Option([\"-whichcode\", \"whichcode\"], \"which genetic code, [U,M,V,F,Y]]\"),            _Option(                [\"-njumble\", \"njumble\"],                \"number of times to randomise input order (default is 0)\",            ),            _Option([\"-seed\", \"seed\"], \"provide random seed\"),            _Option([\"-outgrno\", \"outgrno\"], \"Specify outgroup\"),            _Option([\"-thresh\", \"thresh\"], \"Use threshold parsimony (y/N)\"),            _Option([\"-threshold\", \"threshold\"], \"Threshold value\"),            _Option([\"-trout\", \"trout\"], \"Write trees to file (Y/n)\"),            _Option([\"-dotdiff\", \"dotdiff\"], \"Use dot-differencing? [Y/n]\"),        ]        _EmbossCommandLine.__init__(self, cmd, **kwargs)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def autocommit(self, conn, y=1):",
        "documentation": "set autocommit on the database connection"
    },
    {
        "method_code": "def test_301_create_path_via_existing_cable(self):                interface1 = Interface.objects.create(device=self.device, name='Interface 1')        interface2 = Interface.objects.create(device=self.device, name='Interface 2')        rearport1 = RearPort.objects.create(device=self.device, name='Rear Port 1', positions=1)        rearport2 = RearPort.objects.create(device=self.device, name='Rear Port 2', positions=1)        frontport1 = FrontPort.objects.create(            device=self.device, name='Front Port 1', rear_port=rearport1, rear_port_position=1        )        frontport2 = FrontPort.objects.create(            device=self.device, name='Front Port 2', rear_port=rearport2, rear_port_position=1        )        # Create cable 2        cable2 = Cable(            a_terminations=[rearport1],            b_terminations=[rearport2]        )        cable2.save()        self.assertEqual(CablePath.objects.count(), 0)        # Create cable1        cable1 = Cable(            a_terminations=[interface1],            b_terminations=[frontport1]        )        cable1.save()        self.assertPathExists(            (interface1, cable1, frontport1, rearport1, cable2, rearport2, frontport2),            is_complete=False        )        self.assertEqual(CablePath.objects.count(), 1)        # Create cable 3        cable3 = Cable(            a_terminations=[frontport2],            b_terminations=[interface2]        )        cable3.save()        self.assertPathExists(            (interface1, cable1, frontport1, rearport1, cable2, rearport2, frontport2, cable3, interface2),            is_complete=True,            is_active=True        )        self.assertPathExists(            (interface2, cable3, frontport2, rearport2, cable2, rearport1, frontport1, cable1, interface1),            is_complete=True,            is_active=True        )        self.assertEqual(CablePath.objects.count(), 2)",
        "documentation": "if c fp rp c rp fp c if"
    },
    {
        "method_code": "def _tags_to_annotations(tags):        annotations = {}    for tag in tags:        parts = tag.split(\":\")        if len(parts) < 3:            raise ValueError(f\"Segment line has invalid tag: {tag}.\")        if re.fullmatch(r\"[A-Za-z][A-Za-z0-9]\", parts[0]) is None:            warnings.warn(                f\"Tag has invalid name: {parts[0]}. Are they tab delimited?\",                BiopythonWarning,            )        parts[2] = \":\".join(parts[2:])  # tag value may contain : characters        annotations[parts[0]] = (parts[1], parts[2])        # Check type of the tag and raise warning on a mismatch. These RegExs        # are part of the 1.0 standard.        if parts[1] not in \"AifZJHB\":            warnings.warn(f\"Tag has invalid type: {parts[1]}\", BiopythonWarning)        elif parts[1] == \"A\" and re.fullmatch(r\"[!-~]\", parts[2]) is None:            warnings.warn(                f\"Tag has incorrect type. Expected printable character, got {parts[2]}.\",                BiopythonWarning,            )        elif parts[1] == \"i\" and re.fullmatch(r\"[-+]?[0-9]+\", parts[2]) is None:            warnings.warn(                f\"Tag has incorrect type. Expected signed integer, got {parts[2]}.\",                BiopythonWarning,            )        elif (            parts[1] == \"f\"            and re.fullmatch(r\"[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\", parts[2])            is None        ):            warnings.warn(                f\"Tag has incorrect type. Expected float, got {parts[2]}.\",                BiopythonWarning,            )        elif parts[1] == \"Z\" and re.fullmatch(r\"[ !-~]+\", parts[2]) is None:            warnings.warn(                f\"Tag has incorrect type. Expected printable string, got {parts[2]}.\",                BiopythonWarning,            )        elif parts[1] == \"J\" and re.fullmatch(r\"[ !-~]+\", parts[2]) is None:            warnings.warn(                f\"Tag has incorrect type. Expected JSON excluding new-line and tab characters, got {parts[2]}.\",                BiopythonWarning,            )        elif parts[1] == \"H\" and re.fullmatch(r\"[0-9A-F]+\", parts[2]) is None:            warnings.warn(                f\"Tag has incorrect type. Expected byte array in hex format, got {parts[2]}.\",                BiopythonWarning,            )        elif (            parts[1] == \"B\"            and re.fullmatch(                r\"[cCsSiIf](,[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?)+\", parts[2]            )            is None        ):            warnings.warn(                f\"Tag has incorrect type. Expected array of integers or floats, got {parts[2]}.\",                BiopythonWarning,            )    return annotations",
        "documentation": "build an annotation dictionary from a list of tag private"
    },
    {
        "method_code": "def debug_mode_log_level(settings, value):        if PREFECT_DEBUG_MODE.value_from(settings):        return \"DEBUG\"    else:        return value",
        "documentation": "valuecallback for prefectlogginglevel that override the log level to debug when debug mode is enabled"
    },
    {
        "method_code": "def test_pubmed1(self):                # To create the XML file, use        # >>> Bio.Entrez.esearch(db=\"pubmed\", term=\"biopython\")        with open(\"Entrez/esearch1.xml\", \"rb\") as stream:            record = Entrez.read(stream)        self.assertEqual(record[\"Count\"], \"5\")        self.assertEqual(record[\"RetMax\"], \"5\")        self.assertEqual(record[\"RetStart\"], \"0\")        self.assertEqual(len(record[\"IdList\"]), 5)        self.assertEqual(record[\"IdList\"][0], \"16403221\")        self.assertEqual(record[\"IdList\"][1], \"16377612\")        self.assertEqual(record[\"IdList\"][2], \"14871861\")        self.assertEqual(record[\"IdList\"][3], \"14630660\")        self.assertEqual(record[\"IdList\"][4], \"12230038\")        self.assertEqual(len(record[\"TranslationSet\"]), 0)        self.assertEqual(len(record[\"TranslationStack\"]), 2)        self.assertEqual(record[\"TranslationStack\"][0][\"Term\"], \"biopython[All Fields]\")        self.assertEqual(record[\"TranslationStack\"][0][\"Field\"], \"All Fields\")        self.assertEqual(record[\"TranslationStack\"][0][\"Count\"], \"5\")        self.assertEqual(record[\"TranslationStack\"][0][\"Explode\"], \"Y\")        self.assertEqual(record[\"TranslationStack\"][0].tag, \"TermSet\")        self.assertEqual(record[\"TranslationStack\"][1], \"GROUP\")        self.assertEqual(record[\"TranslationStack\"][1].tag, \"OP\")        self.assertEqual(record[\"QueryTranslation\"], \"biopython[All Fields]\")",
        "documentation": "test parsing xml returned by esearch from pubmed first test"
    },
    {
        "method_code": "def test_unicode_exception(self):                with open(EX_NEWICK_BOM, encoding=\"utf-8\") as handle:            tree = Phylo.read(handle, \"newick\")        self.assertEqual(len(tree.get_terminals()), 3)",
        "documentation": "read a newick file with a unicode byte order mark bom"
    },
    {
        "method_code": "def _write_hsps(self, hsps):                xml = self.xml        for num, hsp in enumerate(hsps):            xml.startParent(\"Hsp\")            xml.simpleElement(\"Hsp_num\", str(num + 1))            for elem, attr in _WRITE_MAPS[\"hsp\"]:                elem = \"Hsp_\" + elem                try:                    content = self._adjust_output(hsp, elem, attr)                # make sure any elements that is not present is optional                # in the DTD                except AttributeError:                    if elem not in _DTD_OPT:                        raise ValueError(f\"Element {elem} (attribute {attr}) not found\")                else:                    xml.simpleElement(elem, str(content))            self.hsp_counter += 1            self.frag_counter += len(hsp.fragments)            xml.endParent()",
        "documentation": "write hsp object private"
    },
    {
        "method_code": "def parse_sorted_query(    model_cls: Type[Model],    parts: List[str],    prefixes: Dict = {},    case_insensitive: bool = True,) -> Tuple[query.Query, Sort]:        # Separate query token and sort token.    query_parts = []    sort_parts = []    # Split up query in to comma-separated subqueries, each representing    # an AndQuery, which need to be joined together in one OrQuery    subquery_parts = []    for part in parts + [\",\"]:        if part.endswith(\",\"):            # Ensure we can catch \"foo, bar\" as well as \"foo , bar\"            last_subquery_part = part[:-1]            if last_subquery_part:                subquery_parts.append(last_subquery_part)            # Parse the subquery in to a single AndQuery            # TODO: Avoid needlessly wrapping AndQueries containing 1 subquery?            query_parts.append(                query_from_strings(                    query.AndQuery, model_cls, prefixes, subquery_parts                )            )            del subquery_parts[:]        else:            # Sort parts (1) end in + or -, (2) don't have a field, and            # (3) consist of more than just the + or -.            if part.endswith((\"+\", \"-\")) and \":\" not in part and len(part) > 1:                sort_parts.append(part)            else:                subquery_parts.append(part)    # Avoid needlessly wrapping single statements in an OR    q = query.OrQuery(query_parts) if len(query_parts) > 1 else query_parts[0]    s = sort_from_strings(model_cls, sort_parts, case_insensitive)    return q, s",
        "documentation": "given a list of string create the query and sort that they represent"
    },
    {
        "method_code": "def testStr(self):                with open(self.filename) as f:            for line in f:                record = Cla.Record(line)                # The SCOP Classification file format which can be found at                # http://scop.mrc-lmb.cam.ac.uk/scop/release-notes.html states                # that the list of classification hierarchy key-value pairs is                # unordered, therefore we need only check that they are all                # there, NOT that they are in the same order.                # End of line is platform dependent. Strip it off                expected_hierarchy = line.rstrip().split(\"\\t\")[5].split(\",\")                expected_hierarchy = dict(                    pair.split(\"=\") for pair in expected_hierarchy                )                actual_hierarchy = str(record).rstrip().split(\"\\t\")[5].split(\",\")                actual_hierarchy = dict(pair.split(\"=\") for pair in actual_hierarchy)                self.assertEqual(len(actual_hierarchy), len(expected_hierarchy))                for key, actual_value in actual_hierarchy.items():                    self.assertEqual(actual_value, expected_hierarchy[key])",
        "documentation": "test if we can convert each record to a string correctly"
    },
    {
        "method_code": "def query_tasks(session):        if session.config[\"singletons\"]:        # Search for items.        for item in session.lib.items(session.query):            task = SingletonImportTask(None, item)            for task in task.handle_created(session):                yield task    else:        # Search for albums.        for album in session.lib.albums(session.query):            log.debug(                \"yielding album {0}: {1} - {2}\",                album.id,                album.albumartist,                album.album,            )            items = list(album.items())            _freshen_items(items)            task = ImportTask(None, [album.item_dir()], items)            for task in task.handle_created(session):                yield task",
        "documentation": "a generator that work a a dropinreplacement for readtasks instead of finding file from the filesystem a query is used to match item from the library"
    },
    {
        "method_code": "def __init__(self, lib, loghandler, paths, query):                self.lib = lib        self.logger = self._setup_logging(loghandler)        self.paths = paths        self.query = query        self._is_resuming = {}        self._merged_items = set()        self._merged_dirs = set()        # Normalize the paths.        if self.paths:            self.paths = list(map(normpath, self.paths))",
        "documentation": "create a session lib is a library object loghandler is a logginghandler either path or query is nonnull and indicates the source of file to be imported"
    },
    {
        "method_code": "def _tokenfile(self):                return self.config[\"tokenfile\"].get(confuse.Filename(in_app_dir=True))",
        "documentation": "get the path to the json file for storing the oauth token"
    },
    {
        "method_code": "def none_accept_ranges():        response = flask.Response(\"test content test content test content\")    response.headers[\"Accept-Ranges\"] = \"none\"    return response",
        "documentation": "return an empty request with acceptranges set to none"
    },
    {
        "method_code": "def __format__(self, format_spec):                if format_spec:            from io import StringIO            from Bio.Phylo import _io            handle = StringIO()            _io.write([self], handle, format_spec)            return handle.getvalue()        else:            # Follow python convention and default to using __str__            return str(self)",
        "documentation": "serialize the tree a a string in the specified file format this method support python format builtin function param formatspec a lowercase string supported by biophylowrite a an output file format"
    },
    {
        "method_code": "def testdata() -> None:        reset_context()    context._set_raw_data(        {            \"testdata\": YamlRawParameter.make_raw_parameters(                \"testdata\",                yaml_round_trip_load(                    dals(                                            )                ),            )        }    )    Channel._reset_state()",
        "documentation": "some note about the test in this class the pkgsanaconda channel is migrated while the pkgspro channel is not thus testpkgsfree and testpkgspro have substantially different behavior"
    },
    {
        "method_code": "def compile_func(arg_names, statements, name=\"_the_func\", debug=False):        args_fields = {        \"args\": [ast.arg(arg=n, annotation=None) for n in arg_names],        \"kwonlyargs\": [],        \"kw_defaults\": [],        \"defaults\": [ex_literal(None) for _ in arg_names],    }    args_fields[\"posonlyargs\"] = []    args = ast.arguments(**args_fields)    func_def = ast.FunctionDef(        name=name,        args=args,        body=statements,        decorator_list=[],    )    # The ast.Module signature changed in 3.8 to accept a list of types to    # ignore.    mod = ast.Module([func_def], [])    ast.fix_missing_locations(mod)    prog = compile(mod, \"<generated>\", \"exec\")    # Debug: show bytecode.    if debug:        dis.dis(prog)        for const in prog.co_consts:            if isinstance(const, types.CodeType):                dis.dis(const)    the_locals = {}    exec(prog, {}, the_locals)    return the_locals[name]",
        "documentation": "compile a list of statement a the body of a function and return the resulting python function if debug then print out the bytecode of the compiled function"
    },
    {
        "method_code": "def set_parameter(self, name, value=None):                set_option = False        for parameter in self.parameters:            if name in parameter.names:                if isinstance(parameter, _Switch):                    if value is None:                        import warnings                        warnings.warn(                            \"For a switch type argument like %s, \"                            \"we expect a boolean.  None is treated \"                            \"as FALSE!\" % parameter.names[-1]                        )                    parameter.is_set = bool(value)                    set_option = True                else:                    if value is not None:                        self._check_value(value, name, parameter.checker_function)                        parameter.value = value                    parameter.is_set = True                    set_option = True        if not set_option:            raise ValueError(f\"Option name {name} was not found.\")",
        "documentation": "set a commandline option for a program obsolete every parameter is available via a property and a a named keyword when creating the instance using either of these is preferred to this legacy setparameter method which is now obsolete and likely to be deprecated and later removed in future release"
    },
    {
        "method_code": "def validate_k8s_job_required_components(cls, value: Dict[str, Any]):        from prefect.utilities.pydantic import JsonPatch    patch = JsonPatch.from_diff(value, cls.base_job_manifest())    missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])    if missing_paths:        raise ValueError(            \"Job is missing required attributes at the following paths: \"            f\"{', '.join(missing_paths)}\"        )    return value",
        "documentation": "validate that a kubernetes job manifest ha all required component"
    },
    {
        "method_code": "def apply_data(self, objs, old_data, new_data):                if len(old_data) != len(new_data):            self._log.warning(                \"number of objects changed from {} to {}\",                len(old_data),                len(new_data),            )        obj_by_id = {o.id: o for o in objs}        ignore_fields = self.config[\"ignore_fields\"].as_str_seq()        for old_dict, new_dict in zip(old_data, new_data):            # Prohibit any changes to forbidden fields to avoid            # clobbering `id` and such by mistake.            forbidden = False            for key in ignore_fields:                if old_dict.get(key) != new_dict.get(key):                    self._log.warning(\"ignoring object whose {} changed\", key)                    forbidden = True                    break            if forbidden:                continue            id_ = int(old_dict[\"id\"])            apply_(obj_by_id[id_], new_dict)",
        "documentation": "take potentiallyupdated data and apply it to a set of model object the object are not written back to the database so the change are temporary"
    },
    {
        "method_code": "def set_default_image(values: dict) -> dict:        job = values.get(\"job\")    image = values.get(\"image\")    job_image = (        job[\"spec\"][\"template\"][\"spec\"][\"containers\"][0].get(\"image\") if job else None    )    if not image and not job_image:        values[\"image\"] = get_prefect_image_name()    return values",
        "documentation": "set the default image for a kubernetes job if not provided"
    },
    {
        "method_code": "def count(self, sub, start=None, end=None):                return self.seq.count(sub, start, end)",
        "documentation": "return the number of nonoverlapping occurrence of sub in seqstartend optional argument start and end are interpreted a in slice notation this method behaves a the count method of python string"
    },
    {
        "method_code": "def commands(self):                # Import locally to reduce impact on initialization time.        from .cli.conda_argparse import find_builtin_commands, generate_parser        from .cli.find_commands import find_commands        # return value meant to be written to stdout        # Hidden commands to provide metadata to shells.        return \"\\n\".join(            sorted(                find_builtin_commands(generate_parser()) + tuple(find_commands(True))            )        )",
        "documentation": "return a list of possible subcommands that are valid immediately following conda at the command line this method is generally only used by tabcompletion"
    },
    {
        "method_code": "def process_clade(self, clade):                if (            (clade.name)            and not (self.values_are_confidence or self.comments_are_confidence)            and (clade.confidence is None)            and (clade.clades)        ):            clade.confidence = _parse_confidence(clade.name)            if clade.confidence is not None:                clade.name = None        try:            parent = clade.parent        except AttributeError:            pass        else:            parent.clades.append(clade)            del clade.parent            return parent",
        "documentation": "remove node parent and return it final processing of parsed clade"
    },
    {
        "method_code": "def _check_if_identical_deployment_in_prefect_file(    untemplated_deploy_config: Dict, prefect_file: Path = Path(\"prefect.yaml\")) -> bool:        user_specified_deploy_config = _format_deployment_for_saving_to_prefect_file(        untemplated_deploy_config    )    if prefect_file.exists():        with prefect_file.open(mode=\"r\") as f:            parsed_prefect_file_contents = yaml.safe_load(f)            existing_deployments = parsed_prefect_file_contents.get(\"deployments\")            if existing_deployments is not None:                for deploy_config in existing_deployments:                    if deploy_config == user_specified_deploy_config:                        return True    return False",
        "documentation": "check if the given deploy config is identical to an existing deploy config in the prefectyaml file meaning that there have been no update and prompting to save is unnecessary args untemplateddeployconfig a deploy config that ha not been templated"
    },
    {
        "method_code": "def render_field(field, bulk_nullable=False, label=None):        return {        'field': field,        'label': label or field.label,        'bulk_nullable': bulk_nullable or getattr(field, '_nullable', False),    }",
        "documentation": "render a single form field from template"
    },
    {
        "method_code": "def get_id(self):                return self.id",
        "documentation": "return the id of the atom which is it atom name"
    },
    {
        "method_code": "def write(self, handle, cdao_to_obo=True, **kwargs):                self.cdao_to_obo = cdao_to_obo        # set XML namespaces        root_node = ElementTree.Element(\"nex:nexml\")        root_node.set(\"version\", VERSION)        root_node.set(\"xmlns\", DEFAULT_NAMESPACE)        root_node.set(\"xsi:schemaLocation\", SCHEMA)        for prefix, uri in NAMESPACES.items():            root_node.set(f\"xmlns:{prefix}\", uri)        otus = ElementTree.SubElement(            root_node, \"otus\", **{\"id\": \"tax\", \"label\": \"RootTaxaBlock\"}        )        # create trees        trees = ElementTree.SubElement(            root_node,            \"trees\",            **{\"id\": \"Trees\", \"label\": \"TreesBlockFromXML\", \"otus\": \"tax\"},        )        count = 0        tus = set()        for tree in self.trees:            this_tree = ElementTree.SubElement(                trees, \"tree\", **{\"id\": self.new_label(\"tree\")}            )            first_clade = tree.clade            tus.update(self._write_tree(first_clade, this_tree, rooted=tree.rooted))            count += 1        # create OTUs        for tu in tus:            otu = ElementTree.SubElement(otus, \"otu\", **{\"id\": tu})        # write XML document to file handle        # xml_doc = ElementTree.ElementTree(root_node)        # xml_doc.write(handle,        #              xml_declaration=True, encoding='utf-8',        #              method='xml')        # use xml.dom.minodom for pretty printing        rough_string = ElementTree.tostring(root_node, \"utf-8\")        reparsed = minidom.parseString(rough_string)        try:            # XML handles ought to be in binary mode            handle.write(reparsed.toprettyxml(indent=\"  \").encode(\"utf8\"))        except TypeError:            # Fall back for text mode            handle.write(reparsed.toprettyxml(indent=\"  \"))        return count",
        "documentation": "write this instance tree to a file handle"
    },
    {
        "method_code": "def cache_path_state(self):                return self.repo_cache.cache_path_state",
        "documentation": "outofband etag and other state needed by the repointerface"
    },
    {
        "method_code": "def _get_container_group(        client: ContainerInstanceManagementClient,        resource_group_name: str,        container_group_name: str,    ) -> ContainerGroup:                return client.container_groups.get(            resource_group_name=resource_group_name,            container_group_name=container_group_name,        )",
        "documentation": "get the container group from azure"
    },
    {
        "method_code": "def flush(self):                while len(self._buffer) >= 65536:            self._write_block(self._buffer[:65535])            self._buffer = self._buffer[65535:]        self._write_block(self._buffer)        self._buffer = b\"\"        self._handle.flush()",
        "documentation": "flush data explicitally"
    },
    {
        "method_code": "def multi_rot_Z(angle_rads: np.ndarray) -> np.ndarray:        rz = np.empty((angle_rads.shape[0], 4, 4))    rz[...] = np.identity(4)    rz[:, 0, 0] = rz[:, 1, 1] = np.cos(angle_rads)    rz[:, 1, 0] = np.sin(angle_rads)    rz[:, 0, 1] = -rz[:, 1, 0]    return rz",
        "documentation": "create entry numpy z rotation matrix for entry angle param entry int number of matrix generated param anglerads numpy array of angle return entry x x homogeneous rotation matrix"
    },
    {
        "method_code": "def create_notice_cache_files(    cache_dir: Path,    cache_files: Sequence[str],    messages_json_seq: Sequence[dict],) -> None:        for message_json, file in zip(messages_json_seq, cache_files):        with cache_dir.joinpath(file).open(\"w\") as fp:            json.dump(message_json, fp)",
        "documentation": "creates the cache file that we use in test"
    },
    {
        "method_code": "def _set_session_type(allow_write):                with connection.cursor() as cursor:            mode = 'READ WRITE' if allow_write else 'READ ONLY'            cursor.execute(f'SET SESSION CHARACTERISTICS AS TRANSACTION {mode};')",
        "documentation": "prevent any writerelated database operation args allowwrite bool if true write operation will be permitted"
    },
    {
        "method_code": "def __str__(self) -> str:                if self.name:            return self.name[:37] + \"...\" if len(self.name) > 40 else self.name        return self.__class__.__name__",
        "documentation": "return name of the class instance"
    },
    {
        "method_code": "def __init__(self, target, bedN=12):                if bedN < 3 or bedN > 12:            raise ValueError(\"bedN must be between 3 and 12\")        super().__init__(target)        self.bedN = bedN",
        "documentation": "create an alignmentwriter object argument target output stream or file name bedn number of column in the bed file this must be between and default value is"
    },
    {
        "method_code": "def test_nucleotide_genbank_acc_version(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"acc_version\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"X52960.1\")",
        "documentation": "biotogowsentrynucleotide x fieldaccversion"
    },
    {
        "method_code": "def size(self):                expected_exceptions = [OSError]        try:            from botocore.exceptions import ClientError            expected_exceptions.append(ClientError)        except ImportError:            pass        try:            return self.image.size        except tuple(expected_exceptions):            return None",
        "documentation": "wrapper around imagesize to suppress an oserror in case the file is inaccessible also opportunistically catch other exception that we know other storage backends to throw"
    },
    {
        "method_code": "def _genbank_convert_fasta(in_file, out_file):        # We don't need to parse the features...    records = GenBankScanner().parse_records(in_file, do_features=False)    return SeqIO.write(records, out_file, \"fasta\")",
        "documentation": "fast genbank to fasta private"
    },
    {
        "method_code": "def get_taxa(self, node_id=None):                if node_id is None:            node_id = self.root        if node_id not in self.chain:            raise TreeError(\"Unknown node_id: %d.\" % node_id)        if self.chain[node_id].succ == []:            if self.chain[node_id].data:                return [self.chain[node_id].data.taxon]            else:                return None        else:            list = []            for succ in self.chain[node_id].succ:                list.extend(self.get_taxa(succ))            return list",
        "documentation": "return a list of all otus downwards from a node node gettaxaselfnodeidnone"
    },
    {
        "method_code": "def sync_data(self):                raise NotImplementedError(_(\"{class_name} must implement a sync_data() method.\").format(            class_name=self.__class__        ))",
        "documentation": "inheriting model must override this method with specific logic to copy data from the assigned datafile to the local instance this method should not call save on the instance"
    },
    {
        "method_code": "def test_create_valid_env_with_variables(env1: str, conda_cli: CondaCLIFixture):        create_env(ENVIRONMENT_CA_CERTIFICATES_WITH_VARIABLES)    conda_cli(\"env\", \"create\")    assert env_is_created(env1)    stdout, _, _ = conda_cli(        *(\"env\", \"config\", \"vars\", \"list\"),        f\"--name={env1}\",        \"--json\",    )    output_env_vars = json.loads(stdout)    assert output_env_vars == {        \"DUDE\": \"woah\",        \"SWEET\": \"yaaa\",        \"API_KEY\": \"AaBbCcDd===EeFf\",    }    stdout, _, _ = conda_cli(\"info\", \"--json\")    parsed = json.loads(stdout)    assert [env for env in parsed[\"envs\"] if env.endswith(env1)]",
        "documentation": "creates an environmentyml file and creates and environment with it"
    },
    {
        "method_code": "def calc_ibd_haplo(self, fname, stat=\"a\", scale=\"Log\", min_dist=0.00001):                return self._calc_ibd(fname, 6, stat, scale, min_dist)",
        "documentation": "calculate isolation by distance statistic for haploid data see calcibd for parameter detail note that each pop can only have a single individual and the individual name ha to be the sample coordinate"
    },
    {
        "method_code": "def load_meta_sources():        meta_sources = {}    for module_path, class_name in SOURCES.items():        module = import_module(METASYNC_MODULE + \".\" + module_path)        meta_sources[class_name.lower()] = getattr(module, class_name)    return meta_sources",
        "documentation": "return a dictionary of all the metasources eg itunes itunes with isinstanceitunes metasource true"
    },
    {
        "method_code": "def __exit__(self, exc_type, exc_val, exc_tb):                self.close()",
        "documentation": "exit the context manager and close the dockerclient"
    },
    {
        "method_code": "def feed(self, handle, consumer, do_features=True):                # Should work with both EMBL and GenBank files provided the        # equivalent Bio.GenBank._FeatureConsumer methods are called...        self.set_handle(handle)        if not self.find_start():            # Could not find (another) record            consumer.data = None            return False        # We use the above class methods to parse the file into a simplified format.        # The first line, header lines and any misc lines after the features will be        # dealt with by GenBank / EMBL specific derived classes.        # First line and header:        self._feed_first_line(consumer, self.line)        self._feed_header_lines(consumer, self.parse_header())        # Features (common to both EMBL and GenBank):        if do_features:            self._feed_feature_table(consumer, self.parse_features(skip=False))        else:            self.parse_features(skip=True)  # ignore the data        # Footer and sequence        misc_lines, sequence_string = self.parse_footer()        self._feed_misc_lines(consumer, misc_lines)        consumer.sequence(sequence_string)        # Calls to consumer.base_number() do nothing anyway        consumer.record_end(\"//\")        assert self.line == \"//\"        # And we are done        return True",
        "documentation": "feed a set of data into the consumer this method is intended for use with the old code in biogenbank argument handle a handle with the information to parse consumer the consumer that should be informed of event dofeatures boolean should the feature be parsed skipping the feature can be much faster return value true passed a record false did not find a record"
    },
    {
        "method_code": "def __next__(self) -> SeqRecord:                q_mapping = FastqPhredIterator.q_mapping        try:            title_line, seq_string, quality_string = next(self._data)        except StopIteration:            raise StopIteration from None        descr = title_line        id = descr.split()[0]        name = id        record = SeqRecord(Seq(seq_string), id=id, name=name, description=descr)        try:            qualities = [q_mapping[letter2] for letter2 in quality_string]        except KeyError:            raise ValueError(\"Invalid character in quality string\") from None        # For speed, will now use a dirty trick to speed up assigning the        # qualities. We do this to bypass the length check imposed by the        # per-letter-annotations restricted dict (as this has already been        # checked by FastqGeneralIterator). This is equivalent to:        # record.letter_annotations[\"phred_quality\"] = qualities        dict.__setitem__(record._per_letter_annotations, \"phred_quality\", qualities)        return record",
        "documentation": "parse the file and generate seqrecord object"
    },
    {
        "method_code": "def __str__(self):                lines = []        # set query id line        qid_line = \"Query: %s\" % self.query_id        lines.append(qid_line)        if self.query_description:            line = \"       %s\" % self.query_description            line = line[:77] + \"...\" if len(line) > 80 else line            lines.append(line)        # set hit id line        hid_line = \"  Hit: %s\" % self.id        try:            seq_len = self.seq_len        except AttributeError:            pass        else:            hid_line += \" (%i)\" % seq_len        lines.append(hid_line)        if self.description:            line = \"       %s\" % self.description            line = line[:77] + \"...\" if len(line) > 80 else line            lines.append(line)        # set attributes lines        for key, value in sorted(self.attributes.items()):            lines.append(f\" {key}: {value}\")        # set dbxrefs line        if self.dbxrefs:            lines.append(\"Database cross-references: \" + \", \".join(self.dbxrefs))        # set hsp line and table        if not self.hsps:            lines.append(\" HSPs: ?\")        else:            lines.append(                \" HSPs: %s  %s  %s  %s  %s  %s\"                % (\"-\" * 4, \"-\" * 8, \"-\" * 9, \"-\" * 6, \"-\" * 15, \"-\" * 21)            )            pattern = \"%11s  %8s  %9s  %6s  %15s  %21s\"            lines.append(                pattern                % (\"#\", \"E-value\", \"Bit score\", \"Span\", \"Query range\", \"Hit range\")            )            lines.append(                pattern % (\"-\" * 4, \"-\" * 8, \"-\" * 9, \"-\" * 6, \"-\" * 15, \"-\" * 21)            )            for idx, hsp in enumerate(self.hsps):                # evalue                evalue = getattr_str(hsp, \"evalue\", fmt=\"%.2g\")                # bitscore                bitscore = getattr_str(hsp, \"bitscore\", fmt=\"%.2f\")                # alignment length                aln_span = getattr_str(hsp, \"aln_span\")                # query region                query_start = getattr_str(hsp, \"query_start\")                query_end = getattr_str(hsp, \"query_end\")                query_range = f\"[{query_start}:{query_end}]\"                # max column length is 18                query_range = (                    query_range[:13] + \"~]\" if len(query_range) > 15 else query_range                )                # hit region                hit_start = getattr_str(hsp, \"hit_start\")                hit_end = getattr_str(hsp, \"hit_end\")                hit_range = f\"[{hit_start}:{hit_end}]\"                hit_range = hit_range[:19] + \"~]\" if len(hit_range) > 21 else hit_range                # append the hsp row                lines.append(                    pattern % (idx, evalue, bitscore, aln_span, query_range, hit_range)                )        return \"\\n\".join(lines)",
        "documentation": "return a human readable summary of the hit object"
    },
    {
        "method_code": "def sanitize(self, name):                keepcharacters = (\" \", \"\\t\", \".\", \"_\")        replacecharacters = (\" \", \"\\t\")        return \"\".join(            c if c not in replacecharacters else \"_\"            for c in name.strip()            if c.isalnum() or c in keepcharacters        ).strip(\"_\")",
        "documentation": "turn imgur gallery title into a safe package and folder name"
    },
    {
        "method_code": "def _update_pKs_tables(self):                pos_pKs = positive_pKs.copy()        neg_pKs = negative_pKs.copy()        nterm, cterm = self.sequence[0], self.sequence[-1]        if nterm in pKnterminal:            pos_pKs[\"Nterm\"] = pKnterminal[nterm]        if cterm in pKcterminal:            neg_pKs[\"Cterm\"] = pKcterminal[cterm]        return pos_pKs, neg_pKs",
        "documentation": "update pks table with seq specific value for n and ctermini"
    },
    {
        "method_code": "def keyset_from_paused_state(state: \"State\") -> Keyset:        if not state.is_paused():        raise RuntimeError(f\"{state.type.value!r} is unsupported.\")    state_name = state.name or \"\"    base_key = f\"{state_name.lower()}-{str(state.state_details.pause_key)}\"    return keyset_from_base_key(base_key)",
        "documentation": "get the keyset for the given paused state args state state the state to get the keyset for"
    },
    {
        "method_code": "def test_single_sequence(self):                input_file = \"Fasta/f001\"        self.assertTrue(os.path.isfile(input_file))        self.assertEqual(len(list(SeqIO.parse(input_file, \"fasta\"))), 1)        cline = MSAProbsCommandline(msaprobs_exe, infile=input_file)        try:            stdout, stderr = cline()        except ApplicationError as err:            if sys.platform == \"win32\":                expected = 0xC0000005            elif sys.platform == \"darwin\":                expected = -11            else:                expected = 139  # TODO: Check return codes on various other platforms            self.assertEqual(expected, err.returncode)        else:            self.fail(f\"Should have failed, returned:\\n{stdout}\\n{stderr}\")",
        "documentation": "test an input file containing a single sequence"
    },
    {
        "method_code": "def peek_word(self, word):                return \"\".join(self.buffer[: len(word)]) == word",
        "documentation": "return a word stored in the buffer"
    },
    {
        "method_code": "def set_bounds(self, start, end):                low, high = self._parent.range()  # Extent of tracks        if start is not None and end is not None and start > end:            start, end = end, start        if start is None or start < 0:  # Check validity of passed args and            start = 0  # default to 0        if end is None or end < 0:            end = high + 1  # default to track range top limit        self.start, self.end = int(start), int(end)        self.length = self.end - self.start + 1",
        "documentation": "set start and end point for the drawing a a whole argument start the first base or feature mark to draw from end the last base or feature mark to draw to"
    },
    {
        "method_code": "def test_cealigner_nucleic(self):                ref = \"PDB/1LCD.cif\"        mob = \"PDB/1LCD.cif\"        parser = MMCIFParser(QUIET=1)        s1 = parser.get_structure(\"1lcd_ref\", ref)        s2 = parser.get_structure(\"1lcd_mob\", mob)        aligner = CEAligner()        aligner.set_reference(s1)        aligner.align(s2)        self.assertAlmostEqual(aligner.rms, 0.0, places=3)",
        "documentation": "test aligning lcd on lcd"
    },
    {
        "method_code": "def test_init_id_only(self):                hit = Hit(id=\"hit1\")        self.assertEqual(\"hit1\", hit.id)        self.assertIsNone(hit.description)        self.assertIsNone(hit.query_id)        self.assertIsNone(hit.query_description)",
        "documentation": "test hitinit with id only"
    },
    {
        "method_code": "def get_filters(cls):                filters = super().get_filters()        additional_filters = {}        for existing_filter_name, existing_filter in filters.items():            additional_filters.update(cls.get_additional_lookups(existing_filter_name, existing_filter))        filters.update(additional_filters)        return filters",
        "documentation": "override filter generation to support dynamic lookup expression for certain filter type for specific filter type new filter are created based on defined lookup expression in the form fieldnamelookupexpr"
    },
    {
        "method_code": "def _get_term_id(self, name, ontology_id=None, definition=None, identifier=None):                # try to get the term id        sql = \"SELECT term_id FROM term WHERE name = %s\"        fields = [name]        if ontology_id:            sql += \" AND ontology_id = %s\"            fields.append(ontology_id)        id_results = self.adaptor.execute_and_fetchall(sql, fields)        # something is wrong        if len(id_results) > 1:            raise ValueError(f\"Multiple term ids for {name}: {id_results!r}\")        elif len(id_results) == 1:            return id_results[0][0]        else:            sql = (                \"INSERT INTO term (name, definition,\"                \" identifier, ontology_id)\"                \" VALUES (%s, %s, %s, %s)\"            )            self.adaptor.execute(sql, (name, definition, identifier, ontology_id))            return self.adaptor.last_id(\"term\")",
        "documentation": "get the id that corresponds to a term private this look through the term table for a the given term if it is not found a new id corresponding to this term is created in either case the id corresponding to that term is returned so that you can reference it in another table the ontologyid should be used to disambiguate the term"
    },
    {
        "method_code": "def _tmpl_unique(        self,        name,        keys,        disam,        bracket,        item_id,        db_item,        item_keys,        skip_item,        initial_subqueries=None,    ):                memokey = self._tmpl_unique_memokey(name, keys, disam, item_id)        memoval = self.lib._memotable.get(memokey)        if memoval is not None:            return memoval        if skip_item(db_item):            self.lib._memotable[memokey] = \"\"            return \"\"        keys = keys or beets.config[name][\"keys\"].as_str()        disam = disam or beets.config[name][\"disambiguators\"].as_str()        if bracket is None:            bracket = beets.config[name][\"bracket\"].as_str()        keys = keys.split()        disam = disam.split()        # Assign a left and right bracket or leave blank if argument is empty.        if len(bracket) == 2:            bracket_l = bracket[0]            bracket_r = bracket[1]        else:            bracket_l = \"\"            bracket_r = \"\"        # Find matching items to disambiguate with.        subqueries = []        if initial_subqueries is not None:            subqueries.extend(initial_subqueries)        for key in keys:            value = db_item.get(key, \"\")            # Use slow queries for flexible attributes.            fast = key in item_keys            subqueries.append(dbcore.MatchQuery(key, value, fast))        query = dbcore.AndQuery(subqueries)        ambigous_items = (            self.lib.items(query)            if isinstance(db_item, Item)            else self.lib.albums(query)        )        # If there's only one item to matching these details, then do        # nothing.        if len(ambigous_items) == 1:            self.lib._memotable[memokey] = \"\"            return \"\"        # Find the first disambiguator that distinguishes the items.        for disambiguator in disam:            # Get the value for each item for the current field.            disam_values = {s.get(disambiguator, \"\") for s in ambigous_items}            # If the set of unique values is equal to the number of            # items in the disambiguation set, we're done -- this is            # sufficient disambiguation.            if len(disam_values) == len(ambigous_items):                break        else:            # No disambiguator distinguished all fields.            res = f\" {bracket_l}{item_id}{bracket_r}\"            self.lib._memotable[memokey] = res            return res        # Flatten disambiguation value into a string.        disam_value = db_item.formatted(for_path=True).get(disambiguator)        # Return empty string if disambiguator is empty.        if disam_value:            res = f\" {bracket_l}{disam_value}{bracket_r}\"        else:            res = \"\"        self.lib._memotable[memokey] = res        return res",
        "documentation": "generate a string that is guaranteed to be unique among all item of the same type a dbitem who share the same set of key a field from disam is used in the string if one is sufficient to disambiguate the item otherwise a fallback opaque value is used both key and disam should be given a whitespaceseparated list of field name while bracket is a pair of character to be used a bracket surrounding the disambiguator or empty to have no bracket name is the name of the template it is also the name of the configuration section where the default value of the parameter are stored skipitem is a function that must return true when the template should return an empty string initialsubqueries is a list of subqueries that should be included in the query to find the ambiguous item"
    },
    {
        "method_code": "def test_AE017046(self):                self.write_read(os.path.join(\"EMBL\", \"AE017046.embl\"), \"embl\")",
        "documentation": "write and read back aeembl"
    },
    {
        "method_code": "def test_output_filename_with_spaces(self):                input_file = \"Registry/seqs.fasta\"        output_file = \"temp with spaces.aln\"        cline = ClustalOmegaCommandline(            clustalo_exe, infile=input_file, outfile=output_file, outfmt=\"clustal\"        )        self.standard_test_procedure(cline)        alignment = Align.read(cline.outfile, \"clustal\")        self.assertEqual(            str(alignment),            ,        )        self.assertEqual(            alignment.column_annotations[\"clustal_consensus\"],            \"                    *      *    *      *              *           * **       *    *                         *       **               * *      *              *   *              *     * *   *    *       *        *                       **       *          *          * *      *     * *    \",        )",
        "documentation": "test an output filename containing space"
    },
    {
        "method_code": "def _albumtotal(self):                if self.disctotal == 1 or not beets.config[\"per_disc_numbering\"]:            return self.items()[0].tracktotal        counted = []        total = 0        for item in self.items():            if item.disc in counted:                continue            total += item.tracktotal            counted.append(item.disc)            if len(counted) == self.disctotal:                break        return total",
        "documentation": "return the total number of track on all disc on the album"
    },
    {
        "method_code": "def test_random_example_fastq(self):                self.check_random(\"Quality/example.fastq.bgz\")",
        "documentation": "check random access to qualityexamplefastqbgz unix newlines"
    },
    {
        "method_code": "def test_uniprot(self):                fields = set(TogoWS._get_entry_fields(\"uniprot\"))        self.assertTrue(fields.issuperset([\"definition\", \"entry_id\", \"seq\"]), fields)",
        "documentation": "check supported field for uniprot database"
    },
    {
        "method_code": "def testModels(self):                parser = MMCIFParser(QUIET=1)        f_parser = FastMMCIFParser(QUIET=1)        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            structure = parser.get_structure(\"example\", \"PDB/1LCD.cif\")            f_structure = f_parser.get_structure(\"example\", \"PDB/1LCD.cif\")        self.assertEqual(len(structure), 3)        self.assertEqual(len(f_structure), 3)        for ppbuild in [PPBuilder(), CaPPBuilder()]:            # ==========================================================            # Check that serial_num (model column) is stored properly            self.assertEqual(structure[0].serial_num, 1)            self.assertEqual(structure[1].serial_num, 2)            self.assertEqual(structure[2].serial_num, 3)            # First try allowing non-standard amino acids,            polypeptides = ppbuild.build_peptides(structure[0], False)            self.assertEqual(len(polypeptides), 1)            pp = polypeptides[0]            # Check the start and end positions            self.assertEqual(pp[0].get_id()[1], 1)            self.assertEqual(pp[-1].get_id()[1], 51)            # Check the sequence            s = pp.get_sequence()            self.assertIsInstance(s, Seq)            # Here non-standard MSE are shown as M            self.assertEqual(\"MKPVTLYDVAEYAGVSYQTVSRVVNQASHVSAKTREKVEAAMAELNYIPNR\", s)            # ==========================================================            # Now try strict version with only standard amino acids            polypeptides = ppbuild.build_peptides(structure[0], True)            self.assertEqual(len(polypeptides), 1)            pp = polypeptides[0]            # Check the start and end positions            self.assertEqual(pp[0].get_id()[1], 1)            self.assertEqual(pp[-1].get_id()[1], 51)            # Check the sequence            s = pp.get_sequence()            self.assertIsInstance(s, Seq)            self.assertEqual(\"MKPVTLYDVAEYAGVSYQTVSRVVNQASHVSAKTREKVEAAMAELNYIPNR\", s)        # This structure contains several models with multiple lengths.        # The tests were failing.        structure = parser.get_structure(\"example\", \"PDB/2OFG.cif\")        self.assertEqual(len(structure), 3)",
        "documentation": "test file with multiple model"
    },
    {
        "method_code": "def test_example_fastq(self):                temp_file = self.temp_file        self.rewrite(\"Quality/example.fastq.gz\", temp_file)        self.check_blocks(\"Quality/example.fastq.bgz\", temp_file)",
        "documentation": "reproduce bgzf compression for a fastq file"
    },
    {
        "method_code": "def __exit__(self, *exc_info):                self.logger.debug(\"Shutting down Ray cluster...\")        ray.shutdown()        super().__exit__(*exc_info)",
        "documentation": "shuts down the cluster"
    },
    {
        "method_code": "def test_is_page_candidate_exact_match(self):                from bs4 import BeautifulSoup, SoupStrainer        s = self.source        url = str(s[\"url\"] + s[\"path\"])        html = raw_backend.fetch_url(url)        soup = BeautifulSoup(            html, \"html.parser\", parse_only=SoupStrainer(\"title\")        )        assert google.is_page_candidate(            url, soup.title.string, s[\"title\"], s[\"artist\"]        ), url",
        "documentation": "test matching html page title with song info when song info are present in the title"
    },
    {
        "method_code": "def cancel(self):                with self._condition:            # Unlike the stdlib, we allow attempted cancellation of RUNNING futures            if self._state in [RUNNING]:                if self._cancel_scope is None:                    return False                elif not self._cancel_scope.cancelled():                    # Perform cancellation                    if not self._cancel_scope.cancel():                        return False            if self._state in [FINISHED]:                return False            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:                return True            # Normally cancel callbacks are handled by the cancel scope but if there            # is not one let's respect them still            if not self._cancel_scope:                for callback in self._cancel_callbacks:                    callback()            self._state = CANCELLED            self._condition.notify_all()        self._invoke_callbacks()        return True",
        "documentation": "cancel the future if possible return true if the future wa cancelled false otherwise a future cannot be cancelled if it ha already completed"
    },
    {
        "method_code": "def timedout(self) -> bool:                return self.future.timedout()",
        "documentation": "check if the call timed out"
    },
    {
        "method_code": "def test_long_jaggy(self):                self.long_sigils(\"JAGGY\")",
        "documentation": "feature jaggy sigil head within bounding box"
    },
    {
        "method_code": "def cast_to_json(self, json_obj):",
        "documentation": "cast to json object if necessary"
    },
    {
        "method_code": "def test_parse_release_without_required_fields(self):                release = Bag(data={}, refresh=lambda *args: None)        with capture_log() as logs:            d = DiscogsPlugin().get_album_info(release)        assert d is None        assert \"Release does not contain the required fields\" in logs[0]",
        "documentation": "test parsing of a release that doe not have the required field"
    },
    {
        "method_code": "def is_iterable(obj, strict=False):        return isinstance(obj, Iterable) and (        strict or not isinstance(obj, str) or not isinstance(obj, bytes)    )",
        "documentation": "check if object is iterable type str excluded if strictfalse"
    },
    {
        "method_code": "def get_tracks_from_listens(self, listens):                tracks = []        for track in listens:            if track[\"track_metadata\"].get(\"release_name\") is None:                continue            mbid_mapping = track[\"track_metadata\"].get(\"mbid_mapping\", {})            # print(json.dumps(track, indent=4, sort_keys=True))            if mbid_mapping.get(\"recording_mbid\") is None:                # search for the track using title and release                mbid = self.get_mb_recording_id(track)            tracks.append(                {                    \"album\": {                        \"name\": track[\"track_metadata\"].get(\"release_name\")                    },                    \"name\": track[\"track_metadata\"].get(\"track_name\"),                    \"artist\": {                        \"name\": track[\"track_metadata\"].get(\"artist_name\")                    },                    \"mbid\": mbid,                    \"release_mbid\": mbid_mapping.get(\"release_mbid\"),                    \"listened_at\": track.get(\"listened_at\"),                }            )        return tracks",
        "documentation": "return a list of track from a list of listens"
    },
    {
        "method_code": "def Pending(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.PENDING, **kwargs)",
        "documentation": "convenience function for creating pending state return state a pending state"
    },
    {
        "method_code": "def test_example_wnts_xml(self):                temp_file = self.temp_file        self.rewrite(\"Blast/wnts.xml.bgz\", temp_file)        self.check_blocks(\"Blast/wnts.xml.bgz\", temp_file)",
        "documentation": "reproduce bgzf compression for wntsxml blast file"
    },
    {
        "method_code": "def cached_cuda_version():        return cuda_version()",
        "documentation": "a cached version of the cuda detection system"
    },
    {
        "method_code": "def data_callback_factory(variable):        def callback(data):        variable.write(data)    return callback",
        "documentation": "return a callback suitable for use by the ftp library this callback will repeatedly save data into the variable provided to this function this variable should be a filelike structure"
    },
    {
        "method_code": "def _gather_deployment_trigger_definitions(    trigger_flags: List[str], existing_triggers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:        if trigger_flags:        trigger_specs = []        for t in trigger_flags:            try:                if t.endswith(\".yaml\"):                    with open(t, \"r\") as f:                        trigger_specs.extend(yaml.safe_load(f).get(\"triggers\", []))                elif t.endswith(\".json\"):                    with open(t, \"r\") as f:                        trigger_specs.extend(json.load(f).get(\"triggers\", []))                else:                    trigger_specs.append(json.loads(t))            except Exception as e:                raise ValueError(f\"Failed to parse trigger: {t}. Error: {str(e)}\")        return trigger_specs    return existing_triggers",
        "documentation": "parses trigger flag from cli and existing deployment config in prefectyaml args triggerflags trigger passed via cli either a json string or file path existingtriggers trigger from existing deployment configuration return list of trigger specification raise valueerror if trigger flag is not a valid json string or file path"
    },
    {
        "method_code": "def save(self, filepath, select=_select, preserve_atom_numbering=False):                # Similar to the PDBIO save method, we check if the filepath is a        # string for a filepath or an open file handle        if isinstance(filepath, str):            fp = open(filepath, \"w\")            close_file = True        else:            fp = filepath            close_file = False        # Decide whether to save a Structure object or an mmCIF dictionary        if hasattr(self, \"structure\"):            self._save_structure(fp, select, preserve_atom_numbering)        elif hasattr(self, \"dic\"):            self._save_dict(fp)        else:            raise ValueError(                \"Use set_structure or set_dict to set a structure or dictionary to write out\"            )        if close_file:            fp.close()",
        "documentation": "save the structure to a file param filepath output file type filepath string or filehandle param select selects which entity will be written type select object typically select is a subclass of lselect it should have the following method acceptmodelmodel acceptchainchain acceptresidueresidue acceptatomatom these method should return if the entity is to be written out otherwise"
    },
    {
        "method_code": "def _supplement_index_with_cache(index: dict[Any, Any]) -> None:        # supplement index with packages from the cache    for pcrec in PackageCacheData.get_all_extracted_entries():        if pcrec in index:            # The downloaded repodata takes priority            current_record = index[pcrec]            index[pcrec] = PackageCacheRecord.from_objects(current_record, pcrec)        else:            index[pcrec] = pcrec",
        "documentation": "supplement the given index with package from the cache param index the package index to supplement"
    },
    {
        "method_code": "def make_temp_prefix(name=None, use_restricted_unicode=False, _temp_prefix=None):        if not _temp_prefix:        _temp_prefix = _get_temp_prefix(            name=name, use_restricted_unicode=use_restricted_unicode        )    try:        os.makedirs(_temp_prefix)    except:        pass    assert isdir(_temp_prefix)    return _temp_prefix",
        "documentation": "when the env you are creating will be used to install python on window only a restricted amount of unicode will work and probably only those char in your current codepage so the character in unicodecharactersrestricted should probably be randomly generated from that instead the problem here is that the current codepage need to be able to handle sysprefix otherwise ntpath will fall over"
    },
    {
        "method_code": "def __repr__(self):                return f\"{self.__class__.__name__}()\"",
        "documentation": "represent the unknownposition object a a string for debugging"
    },
    {
        "method_code": "def get_levels(self):                return sorted(self.tracks)",
        "documentation": "return a sorted list of level occupied by track in the diagram"
    },
    {
        "method_code": "def distance(    items: Sequence[Item],    album_info: AlbumInfo,    mapping: Dict[Item, TrackInfo],) -> Distance:        likelies, _ = current_metadata(items)    dist = hooks.Distance()    # Artist, if not various.    if not album_info.va:        dist.add_string(\"artist\", likelies[\"artist\"], album_info.artist)    # Album.    dist.add_string(\"album\", likelies[\"album\"], album_info.album)    # Current or preferred media.    if album_info.media:        # Preferred media options.        patterns = config[\"match\"][\"preferred\"][\"media\"].as_str_seq()        patterns = cast(Sequence[str], patterns)        options = [re.compile(r\"(\\d+x)?(%s)\" % pat, re.I) for pat in patterns]        if options:            dist.add_priority(\"media\", album_info.media, options)        # Current media.        elif likelies[\"media\"]:            dist.add_equality(\"media\", album_info.media, likelies[\"media\"])    # Mediums.    if likelies[\"disctotal\"] and album_info.mediums:        dist.add_number(\"mediums\", likelies[\"disctotal\"], album_info.mediums)    # Prefer earliest release.    if album_info.year and config[\"match\"][\"preferred\"][\"original_year\"]:        # Assume 1889 (earliest first gramophone discs) if we don't know the        # original year.        original = album_info.original_year or 1889        diff = abs(album_info.year - original)        diff_max = abs(datetime.date.today().year - original)        dist.add_ratio(\"year\", diff, diff_max)    # Year.    elif likelies[\"year\"] and album_info.year:        if likelies[\"year\"] in (album_info.year, album_info.original_year):            # No penalty for matching release or original year.            dist.add(\"year\", 0.0)        elif album_info.original_year:            # Prefer matchest closest to the release year.            diff = abs(likelies[\"year\"] - album_info.year)            diff_max = abs(                datetime.date.today().year - album_info.original_year            )            dist.add_ratio(\"year\", diff, diff_max)        else:            # Full penalty when there is no original year.            dist.add(\"year\", 1.0)    # Preferred countries.    patterns = config[\"match\"][\"preferred\"][\"countries\"].as_str_seq()    patterns = cast(Sequence[str], patterns)    options = [re.compile(pat, re.I) for pat in patterns]    if album_info.country and options:        dist.add_priority(\"country\", album_info.country, options)    # Country.    elif likelies[\"country\"] and album_info.country:        dist.add_string(\"country\", likelies[\"country\"], album_info.country)    # Label.    if likelies[\"label\"] and album_info.label:        dist.add_string(\"label\", likelies[\"label\"], album_info.label)    # Catalog number.    if likelies[\"catalognum\"] and album_info.catalognum:        dist.add_string(            \"catalognum\", likelies[\"catalognum\"], album_info.catalognum        )    # Disambiguation.    if likelies[\"albumdisambig\"] and album_info.albumdisambig:        dist.add_string(            \"albumdisambig\", likelies[\"albumdisambig\"], album_info.albumdisambig        )    # Album ID.    if likelies[\"mb_albumid\"]:        dist.add_equality(            \"album_id\", likelies[\"mb_albumid\"], album_info.album_id        )    # Tracks.    dist.tracks = {}    for item, track in mapping.items():        dist.tracks[track] = track_distance(item, track, album_info.va)        dist.add(\"tracks\", dist.tracks[track].distance)    # Missing tracks.    for _ in range(len(album_info.tracks) - len(mapping)):        dist.add(\"missing_tracks\", 1.0)    # Unmatched tracks.    for _ in range(len(items) - len(mapping)):        dist.add(\"unmatched_tracks\", 1.0)    # Plugins.    dist.update(plugins.album_distance(items, album_info, mapping))    return dist",
        "documentation": "determines how significant an album metadata change would be return a distance object albuminfo is an albuminfo object reflecting the album to be compared item is a sequence of all item object that will be matched order is not important mapping is a dictionary mapping item to trackinfo object the key are a subset of item and the value are a subset of albuminfotracks"
    },
    {
        "method_code": "def fetch_latest_parsed(self) -> tuple[dict, RepodataState]:                parsed, state = self.fetch_latest()        if isinstance(parsed, str):            try:                return json.loads(parsed), state            except json.JSONDecodeError as e:                e.args = (                    f'{e.args[0]}; got \"{parsed[:ERROR_SNIPPET_LENGTH]}\"',                    *e.args[1:],                )                raise        else:            return parsed, state",
        "documentation": "retrieve parsed latest or latestcached repodata a a dict update cache return repodata content state including cache header"
    },
    {
        "method_code": "def test_4p79(self):                txt_file = get_file(\"4p79_hhsearch_server_NOssm.hhr\")        qresults = parse(txt_file, FMT)        # test first and only qresult        qresult = next(qresults)        num_hits = 8        self.assertEqual(\"HHSUITE\", qresult.program)        self.assertEqual(\"4P79:A|PDBID|CHAIN|SEQUENCE\", qresult.id)        self.assertEqual(198, qresult.seq_len)        self.assertEqual(num_hits, len(qresult))        hit = qresult[0]        self.assertEqual(\"4P79_A\", hit.id)        self.assertEqual(            \"cell adhesion protein; cell adhesion, tight junction, membrane; HET: OLC\"            \", MSE; 2.4A {Mus musculus}\",            hit.description,        )        self.assertTrue(hit.is_included)        self.assertEqual(6.8e-32, hit.evalue)        self.assertEqual(194.63, hit.score)        self.assertEqual(1, len(hit))        hsp = hit.hsps[0]        self.assertTrue(hsp.is_included)        self.assertEqual(0, hsp.output_index)        self.assertEqual(6.8e-32, hsp.evalue)        self.assertEqual(194.63, hsp.score)        self.assertEqual(99.94, hsp.prob)        self.assertEqual(0, hsp.hit_start)        self.assertEqual(198, hsp.hit_end)        self.assertEqual(0, hsp.query_start)        self.assertEqual(198, hsp.query_end)        self.assertEqual(            \"GSEFMSVAVETFGFFMSALGLLMLGLTLSNSYWRVSTVHGNVITTNTIFENLWYSCATDSLGVSNCWDFPSMLALSGYVQ\"            \"GCRALMITAILLGFLGLFLGMVGLRATNVGNMDLSKKAKLLAIAGTLHILAGACGMVAISWYAVNITTDFFNPLYAGTKY\"            \"ELGPALYLGWSASLLSILGGICVFSTAAASSKEEPATR\",            hsp.query.seq,        )        self.assertEqual(            \"GSEFMSVAVETFGFFMSALGLLMLGLTLSNSYWRVSTVHGNVITTNTIFENLWYSCATDSLGVSNCWDFPSMLALSGYVQ\"            \"GCRALMITAILLGFLGLFLGMVGLRATNVGNMDLSKKAKLLAIAGTLHILAGACGMVAISWYAVNITTDFFNPLYAGTKY\"            \"ELGPALYLGWSASLLSILGGICVFSTAAASSKEEPATR\",            hsp.hit.seq,        )        # Check last hit        hit = qresult[num_hits - 1]        self.assertEqual(\"5YQ7_F\", hit.id)        self.assertEqual(            \"Beta subunit of light-harvesting 1; Photosynthetic core complex, PHOTOSYNTHESIS; \"            \"HET: MQE, BCL, HEM, KGD, BPH;{Roseiflexus castenholzii}; Related PDB entries: 5YQ7_V\"            \" 5YQ7_3 5YQ7_T 5YQ7_J 5YQ7_9 5YQ7_N 5YQ7_A 5YQ7_P 5YQ7_H 5YQ7_D 5YQ7_5 5YQ7_7 5YQ7_1 \"            \"5YQ7_R\",            hit.description,        )        self.assertTrue(hit.is_included)        self.assertEqual(6.7, hit.evalue)        self.assertEqual(20.51, hit.score)        self.assertEqual(1, len(hit))        # Check we can get the original last HSP from the file.        num_hsps = num_hits        self.assertEqual(num_hsps, len(qresult.hsps))        hsp = qresult.hsps[-1]        self.assertTrue(hsp.is_included)        self.assertEqual(num_hsps - 1, hsp.output_index)        self.assertEqual(6.7, hsp.evalue)        self.assertEqual(20.51, hsp.score)        self.assertEqual(52.07, hsp.prob)        self.assertEqual(8, hsp.hit_start)        self.assertEqual(42, hsp.hit_end)        self.assertEqual(5, hsp.query_start)        self.assertEqual(37, hsp.query_end)        self.assertEqual(\"RTSVVVSTLLGLVMALLIHFVVLSSGAFNWLRAP\", hsp.hit.seq)        self.assertEqual(\"SVAVETFGFFMSALGLLMLGLTLSNS--YWRVST\", hsp.query.seq)",
        "documentation": "parsing phhsearchservernossmhhr file"
    },
    {
        "method_code": "def plugin_navbar(context):        return _get_registered_content(None, 'navbar', context)",
        "documentation": "render any navbar content embedded by plugins"
    },
    {
        "method_code": "def lock(        self,        key: str,        holder: Optional[str] = None,        acquire_timeout: Optional[float] = None,        hold_timeout: Optional[float] = None,    ):                self.acquire_lock(            key=key,            holder=holder,            acquire_timeout=acquire_timeout,            hold_timeout=hold_timeout,        )        try:            yield        finally:            self.release_lock(key=key, holder=holder)",
        "documentation": "context manager to lock the transaction record during the execution of the nested code block args key unique identifier for the transaction record holder unique identifier for the holder of the lock if not provided a default holder based on the current host process and thread will be used acquiretimeout max number of second to wait for the record to become available if it is locked while attempting to acquire a lock pas to attempt to acquire a lock without waiting block indefinitely by default holdtimeout max number of second to hold the lock for hold the lock indefinitely by default example hold a lock while during an operation python with transactionrecordkeymytransactionrecordkeylock dostuff"
    },
    {
        "method_code": "def test_update_with_pinned_packages(    test_recipes_channel: Path,    tmp_env: TmpEnvFixture,    conda_cli: CondaCLIFixture,):        with tmp_env(\"dependent=1.0\") as prefix:        assert package_is_installed(prefix, \"dependent=1.0\")        assert package_is_installed(prefix, \"dependency=1.0\")        # removing the history allows dependent to be updated too        (prefix / \"conda-meta\" / \"history\").write_text(\"\")        conda_cli(\"update\", f\"--prefix={prefix}\", \"dependency\", \"--yes\")        PrefixData._cache_.clear()        assert not package_is_installed(prefix, \"dependent=1.0\")        assert not package_is_installed(prefix, \"dependency=1.0\")        assert package_is_installed(prefix, \"dependent=2.0\")        assert package_is_installed(prefix, \"dependency=2.0\")",
        "documentation": "when a dependency is updated we update the dependent package too regression test for"
    },
    {
        "method_code": "def get_header(self):                return self.header",
        "documentation": "return the header"
    },
    {
        "method_code": "def reverse(self):                reactants = {}        for r in self.reactants:            reactants[r] = -self.reactants[r]        return Reaction(reactants, self.catalysts, self.reversible, self.data)",
        "documentation": "return a new reaction that is the reverse of self"
    },
    {
        "method_code": "def test_append_alt_id_exists_alt(self):                # append should raise an error if alt ID already exists as primary ID        hit21._id_alt = [\"hit1\"]        qresult = QueryResult([hit11])        self.assertRaises(ValueError, qresult.append, hit21)        hit21._id_alt = []",
        "documentation": "test queryresultappend when alt id exists a primary"
    },
    {
        "method_code": "def write_alignment(self, alignment):                # Check inputs        for name in (s.id.strip() for s in alignment):            if any(c in name for c in string.whitespace):                raise ValueError(f\"Whitespace not allowed in identifier: {name}\")        # Calculate a truncation length - maximum length of sequence ID plus a        # single character for padding        # If no sequences, set id_width to 1. super(...) call will raise a        # ValueError        if len(alignment) == 0:            id_width = 1        else:            id_width = max(len(s.id.strip()) for s in alignment) + 1        super().write_alignment(alignment, id_width)",
        "documentation": "write a relaxed phylip alignment"
    },
    {
        "method_code": "def __getnewargs__(self):                return (int(self), self._left, self._right)",
        "documentation": "return the argument accepted by new necessary to allow pickling and unpickling of class instance"
    },
    {
        "method_code": "def test_no_altered_files(env_ok: tuple[Path, str, str, str, str]):        prefix, _, _, _, _ = env_ok    assert find_altered_packages(prefix) == {}",
        "documentation": "test that run for the case with no altered file"
    },
    {
        "method_code": "def task_run_request_requires_arn_if_no_task_definition_given(self) -> Self:                if (            not (self.task_run_request or {}).get(\"taskDefinition\")            and not self.task_definition        ):            raise ValueError(                \"A task definition must be provided if a task definition ARN is not \"                \"present on the task run request.\"            )        return self",
        "documentation": "if no task definition is provided a task definition arn must be present on the task run request"
    },
    {
        "method_code": "def _translate_id(self, ent_id):                chain_id, res_id = ent_id        if isinstance(res_id, int):            ent_id = (chain_id, (\" \", res_id, \" \"))        return ent_id",
        "documentation": "return entity identifier on residue private"
    },
    {
        "method_code": "def rstrip(self, chars=None, inplace=False):                if isinstance(chars, _SeqAbstractBaseClass):            chars = bytes(chars)        elif isinstance(chars, str):            chars = chars.encode(\"ASCII\")        try:            data = self._data.rstrip(chars)        except TypeError:            raise TypeError(                \"argument must be None or a string, Seq, MutableSeq, or bytes-like object\"            ) from None        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        else:            return self.__class__(data)",
        "documentation": "return a sequence object with trailing end stripped with default argument trailing whitespace is removed seq seq acgt seqrstrip seq acgt seq seq acgt if char is given and not none remove character in char from the trailing end instead the order of the character to be removed is not important seqacgacgttacgrstripgca seqacgacgtt a copy of the sequence is returned if inplace is false the default value if inplace is true the sequence is stripped inplace and returned seq mutableseq acgt seqrstrip mutableseq acgt seq mutableseq acgt seqrstripinplacetrue mutableseq acgt seq mutableseq acgt a seq object are immutable a typeerror is raised if rstrip is called on a seq object with inplacetrue see also the strip and lstrip method"
    },
    {
        "method_code": "def __radd__(self, other):                if isinstance(other, SimpleLocation):            return CompoundLocation([other] + self.parts, self.operator)        elif isinstance(other, int):            return self._shift(other)        else:            raise NotImplementedError",
        "documentation": "add a feature to the left"
    },
    {
        "method_code": "def test_Distribution(self):                tree = list(PhyloXMLIO.parse(EX_PHYLO))[10]        hirschweg = tree.clade[0, 0].distributions[0]        nagoya = tree.clade[0, 1].distributions[0]        eth_zurich = tree.clade[0, 2].distributions[0]        san_diego = tree.clade[1].distributions[0]        for dist, desc, lati, longi, alti in zip(            (hirschweg, nagoya, eth_zurich, san_diego),            (                \"Hirschweg, Winterthur, Switzerland\",                \"Nagoya, Aichi, Japan\",                \"ETH Z\u00fcrich\",                \"San Diego\",            ),            (47.481277, 35.155904, 47.376334, 32.880933),            (8.769303, 136.915863, 8.548108, -117.217543),            (472, 10, 452, 104),        ):            self.assertIsInstance(dist, PX.Distribution)            self.assertEqual(dist.desc, desc)            point = dist.points[0]            self.assertIsInstance(point, PX.Point)            self.assertEqual(point.geodetic_datum, \"WGS84\")            self.assertEqual(point.lat, lati)            self.assertEqual(point.long, longi)            self.assertEqual(point.alt, alti)",
        "documentation": "instantiation of distribution object also check point type and safe unicode handling"
    },
    {
        "method_code": "def remove_relation(self, relation):                self._relations.remove(relation)",
        "documentation": "remove a relation element from the pathway"
    },
    {
        "method_code": "def test_deprecated_binstar(conda_cli: CondaCLIFixture):        with pytest.warns(FutureWarning), pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"create\", \"conda-test/env-remote\")    with pytest.warns(FutureWarning), pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"update\", \"conda-test/env-remote\")",
        "documentation": "test retrieving an environment using the binstarspec ie it retrieves it from anacondaorg this test the remoteorigin command line argument"
    },
    {
        "method_code": "def calc_ibd_diplo(self, fname, stat=\"a\", scale=\"Log\", min_dist=0.00001):                return self._calc_ibd(fname, 5, stat, scale, min_dist)",
        "documentation": "calculate isolation by distance statistic for diploid data see calcibd for parameter detail note that each pop can only have a single individual and the individual name ha to be the sample coordinate"
    },
    {
        "method_code": "def __str__(self):                out = \"one-of(\"        for position in self.position_choices:            out += f\"{position},\"        # replace the last comma with the closing parenthesis        return out[:-1] + \")\"",
        "documentation": "return a representation of the oneofposition object with python counting"
    },
    {
        "method_code": "def _inject_formatted_env_vars(self):                worker_pool_specs = self.job_spec[\"worker_pool_specs\"]        formatted_env_vars = [            {\"name\": key, \"value\": value} for key, value in self.env.items()        ]        worker_pool_specs[0][\"container_spec\"][\"env\"] = formatted_env_vars",
        "documentation": "inject environment variable in the vertex jobspec configuration in the correct format which is sourced from the basejobconfiguration this method is invoked by prepareforflowrun"
    },
    {
        "method_code": "def __init__(self, config: ConfigView, log: Logger):                self._log = log",
        "documentation": "initialize the backend with the configuration view for the plugin"
    },
    {
        "method_code": "def excludes(self, event: Event) -> bool:                return not self.includes(event)",
        "documentation": "would the given filter exclude this event"
    },
    {
        "method_code": "def __str__(self):                answer = f\"[{self._start}:{self._end}]\"        if self.ref and self.ref_db:            answer = f\"{self.ref_db}:{self.ref}{answer}\"        elif self.ref:            answer = self.ref + answer        # Is ref_db without ref meaningful?        if self.strand is None:            return answer        elif self.strand == +1:            return answer + \"(+)\"        elif self.strand == -1:            return answer + \"(-)\"        else:            # strand = 0, stranded but strand unknown, ? in GFF3            return answer + \"(?)\"",
        "documentation": "return a representation of the simplelocation object with python counting for the simple case this us the python splicing syntax zero based counting which genbank would call one based counting"
    },
    {
        "method_code": "def __contains__(self, value):                if not isinstance(value, int):            raise ValueError(                \"Currently we only support checking for integer \"                \"positions being within a SimpleLocation.\"            )        if value < self._start or value >= self._end:            return False        else:            return True",
        "documentation": "check if an integer position is within the simplelocation object note that extra care may be needed for fuzzy location eg from bioseqfeature import simplelocation from bioseqfeature import beforeposition afterposition loc simplelocationbeforeposition afterposition lenloc i for i in range if i in loc"
    },
    {
        "method_code": "def test_singleton_config(self):                self._run({\"singleton_path\": \".*other_album.*\"}, 3, self.all_tracks)",
        "documentation": "check that singleton configuration is ignored for album import"
    },
    {
        "method_code": "def dump_record(prec: PackageRecord) -> dict[str, Any]:        return {k: v for k, v in prec.dump().items() if k not in IGNORE_FIELDS}",
        "documentation": "return a dictionary of keyvalue pair from prec key included in ignorefields are not returned param prec a packagerecord object return a dictionary of element dumped from prec"
    },
    {
        "method_code": "def close(self):                proxies = self._proxies        while proxies:            proxies.popitem()[1]._handle.close()",
        "documentation": "close any open file handle"
    },
    {
        "method_code": "def __sub__(self, other):                if isinstance(other, Vector):            a = self._ar - other._ar        else:            a = self._ar - np.array(other)        return Vector(a)",
        "documentation": "return vectorother vector or scalar"
    },
    {
        "method_code": "def molecular_weight(    seq, seq_type=\"DNA\", double_stranded=False, circular=False, monoisotopic=False):        try:        seq = seq.seq    except AttributeError:  # not a  SeqRecord object        pass    seq = \"\".join(str(seq).split()).upper()  # Do the minimum formatting    if seq_type == \"DNA\":        if monoisotopic:            weight_table = IUPACData.monoisotopic_unambiguous_dna_weights        else:            weight_table = IUPACData.unambiguous_dna_weights    elif seq_type == \"RNA\":        if monoisotopic:            weight_table = IUPACData.monoisotopic_unambiguous_rna_weights        else:            weight_table = IUPACData.unambiguous_rna_weights    elif seq_type == \"protein\":        if monoisotopic:            weight_table = IUPACData.monoisotopic_protein_weights        else:            weight_table = IUPACData.protein_weights    else:        raise ValueError(f\"Allowed seq_types are DNA, RNA or protein, not {seq_type!r}\")    if monoisotopic:        water = 18.010565    else:        water = 18.0153    try:        weight = sum(weight_table[x] for x in seq) - (len(seq) - 1) * water        if circular:            weight -= water    except KeyError as e:        raise ValueError(            f\"'{e}' is not a valid unambiguous letter for {seq_type}\"        ) from None    if double_stranded:        if seq_type == \"protein\":            raise ValueError(\"protein sequences cannot be double-stranded\")        elif seq_type == \"DNA\":            seq = complement(seq)        elif seq_type == \"RNA\":            seq = complement_rna(seq)        weight += sum(weight_table[x] for x in seq) - (len(seq) - 1) * water        if circular:            weight -= water    return weight",
        "documentation": "calculate the molecular mass of dna rna or protein sequence a float only unambiguous letter are allowed nucleotide sequence are assumed to have a phosphate argument seq string seq or seqrecord object seqtype the default is to assume dna override this with a string dna rna or protein doublestranded calculate the mass for the double stranded molecule circular is the molecule circular ha no end monoisotopic use the monoisotopic mass table printf molecularweightagc printf molecularweightseqagc however it is better to be explicit for example with string printf molecularweightagc dna printf molecularweightagc rna printf molecularweightagc protein"
    },
    {
        "method_code": "def test_alpha_regex(self):                self._setup_config(            bucket_alpha=[\"foo\", \"bar\"],            bucket_alpha_regex={\"foo\": \"^[a-d]\", \"bar\": \"^[e-z]\"},        )        assert self.plugin._tmpl_bucket(\"alpha\") == \"foo\"        assert self.plugin._tmpl_bucket(\"delta\") == \"foo\"        assert self.plugin._tmpl_bucket(\"zeta\") == \"bar\"        assert self.plugin._tmpl_bucket(\"Alpha\") == \"A\"",
        "documentation": "check regex is used"
    },
    {
        "method_code": "def max_distance(self) -> float:                dist_max = 0.0        for key, penalty in self._penalties.items():            dist_max += len(penalty) * self._weights[key]        return dist_max",
        "documentation": "return the maximum distance penalty normalization factor"
    },
    {
        "method_code": "def _is_main_block(node: ast.AST):        if isinstance(node, ast.If):        try:            # Check if the condition is `if __name__ == \"__main__\":`            if (                isinstance(node.test, ast.Compare)                and isinstance(node.test.left, ast.Name)                and node.test.left.id == \"__name__\"                and isinstance(node.test.comparators[0], ast.Constant)                and node.test.comparators[0].value == \"__main__\"            ):                return True        except AttributeError:            pass    return False",
        "documentation": "check if the node is an if name main block"
    },
    {
        "method_code": "def fsbsize(path):        path = os.fsdecode(path)    if os.name == \"nt\":        import ctypes        drive = \"{}\\\\\".format(os.path.splitdrive(path)[0])        cluster_sectors, sector_size = ctypes.c_longlong(0)        ctypes.windll.kernel32.GetDiskFreeSpaceW(            ctypes.c_wchar_p(drive),            ctypes.pointer(cluster_sectors),            ctypes.pointer(sector_size),            None,            None,        )        return cluster_sectors * sector_size    else:        return os.statvfs(path).f_frsize",
        "documentation": "get optimal file system buffer size in byte for io call"
    },
    {
        "method_code": "def test_a_album_edit_apply(self, mock_write):                self.run_mocked_command(            {\"replacements\": {\"\\u00e4lbum\": \"modified \\u00e4lbum\"}},            # Apply changes.            [\"a\"],            args=[\"-a\"],        )        self.album.load()        self.assertCounts(mock_write, write_call_count=self.TRACK_COUNT)        assert self.album.album == \"modified \\u00e4lbum\"        self.assertItemFieldsModified(            self.album.items(), self.items_orig, [\"album\", \"mtime\"]        )",
        "documentation": "album query a edit album field apply change"
    },
    {
        "method_code": "def create_default_ignore_file(path: str) -> bool:        path = pathlib.Path(path)    ignore_file = path / \".prefectignore\"    if ignore_file.exists():        return False    default_file = pathlib.Path(prefect.__module_path__) / \".prefectignore\"    with ignore_file.open(mode=\"w\") as f:        f.write(default_file.read_text())    return True",
        "documentation": "creates default ignore file in the provided path if one doe not already exist return boolean specifying whether a file wa created"
    },
    {
        "method_code": "def test_invalid(self):                # To create the XML file, use        # >>> Bio.Entrez.epost(db=\"pubmed\", id=99999999999999999999999999999999)        with open(\"Entrez/epost3.xml\", \"rb\") as stream:            record = Entrez.read(stream)        self.assertEqual(record[\"InvalidIdList\"], [\"-1\"])        self.assertEqual(record[\"QueryKey\"], \"1\")        self.assertEqual(            record[\"WebEnv\"],            \"08AIUeBsfIk6BfdzKnd3GM2RtCudczC9jm5aeb4US0o7azCTQCeCsr-xg0@1EDE54E680D03C40_0011SID\",        )",
        "documentation": "test parsing xml returned by epost with invalid id overflow tag"
    },
    {
        "method_code": "def get_module_scripts(scriptmodule):        def get_name(cls):        # For child objects in submodules use the full import path w/o the root module as the name        return cls.full_name.split(\".\", maxsplit=1)[1]    loader = SourceFileLoader(get_python_name(scriptmodule), get_full_path(scriptmodule))    try:        module = loader.load_module()    except FileNotFoundError:        return {}    scripts = {}    ordered = getattr(module, 'script_order', [])    for cls in ordered:        scripts[get_name(cls)] = cls    for name, cls in inspect.getmembers(module, is_script):        if cls not in ordered:            scripts[get_name(cls)] = cls    return scripts",
        "documentation": "return a dictionary mapping of name and script class inside the passed scriptmodule"
    },
    {
        "method_code": "def find_files(spec):        r  # NOQA        fd = WIN32_FIND_DATA()        handle = FindFirstFile(spec, byref(fd))        while True:            if handle == INVALID_HANDLE_VALUE:                raise OSError()            yield fd            fd = WIN32_FIND_DATA()            res = FindNextFile(handle, byref(fd))            if res == 0:  # error                error = WindowsError()                if error.code == ERROR_NO_MORE_FILES:                    break                else:                    raise error        # todo: how to close handle when generator is destroyed?        # hint: catch GeneratorExit        windll.kernel32.FindClose(handle)",
        "documentation": "a pythonic wrapper around the findfirstfilefindnextfile win api rootfiles tuplefindfilesrc lenrootfiles true rootfilesfilename rootfilesfilename false this test might fail on a nonstandard installation window in fdfilename for fd in rootfiles true"
    },
    {
        "method_code": "def __radd__(self, other):                if isinstance(other, int):            return self._shift(other)        else:            return NotImplemented",
        "documentation": "return a simplelocation object by shifting the location by an integer amount"
    },
    {
        "method_code": "def _get_default_arm_template():        return {        \"$schema\": \"https://schema.management.azure.com/schemas/2019-08-01/deploymentTemplate.json#\",  # noqa        \"contentVersion\": \"1.0.0.0\",        \"parameters\": {            \"location\": {                \"type\": \"string\",                \"defaultValue\": \"[resourceGroup().location]\",                \"metadata\": {\"description\": \"Location for all resources.\"},            },            \"container_group_name\": {                \"type\": \"string\",                \"defaultValue\": \"[uniqueString(resourceGroup().id)]\",                \"metadata\": {                    \"description\": \"The name of the container group to create.\"                },            },            \"container_name\": {                \"type\": \"string\",                \"defaultValue\": \"[uniqueString(resourceGroup().id)]\",                \"metadata\": {\"description\": \"The name of the container to create.\"},            },        },        \"resources\": [            {                \"type\": \"Microsoft.ContainerInstance/containerGroups\",                \"apiVersion\": \"2022-09-01\",                \"name\": \"[parameters('container_group_name')]\",                \"location\": \"[parameters('location')]\",                \"properties\": {                    \"containers\": [                        {                            \"name\": \"[parameters('container_name')]\",                            \"properties\": {                                \"image\": \"{{ image }}\",                                \"command\": \"{{ command }}\",                                \"resources\": {                                    \"requests\": {                                        \"cpu\": \"{{ cpu }}\",                                        \"memoryInGB\": \"{{ memory }}\",                                    }                                },                                \"environmentVariables\": [],                            },                        }                    ],                    \"osType\": \"Linux\",                    \"restartPolicy\": \"Never\",                },            }        ],    }",
        "documentation": "get the default arm template for creating a container group"
    },
    {
        "method_code": "def get_handle(self):                self.set_request_context(            self.p.url, self.p.get, self.p.post, self.p.referer, self.p.cj        )        self.c.setopt(pycurl.WRITEFUNCTION, self.write_body)        self.c.setopt(pycurl.HEADERFUNCTION, self.write_header)        # request all bytes, since some servers in russia seems to have a defect        # arithmetic unit        fs_name = self.p.info.get_chunk_filename(self.id)        if self.resume:            self.fp = open(fs_name, mode=\"ab\")            self.arrived = self.fp.tell()            if not self.arrived:                self.arrived = os.stat(fs_name).st_size            if self.range:                #: do nothing if chunk already finished                if self.arrived + self.range[0] >= self.range[1]:                    return None                range = self.format_range()                self.log.debug(f\"Chunk {self.id + 1} chunked with range {range}\")                self.c.setopt(pycurl.RANGE, range)            else:                self.log.debug(f\"Resume File from {self.arrived}\")                self.c.setopt(pycurl.RESUME_FROM, self.arrived)        else:            if self.range:                range = self.format_range()                self.log.debug(f\"Chunk {self.id + 1} chunked with range {range}\")                self.c.setopt(pycurl.RANGE, range)            self.fp = open(fs_name, mode=\"wb\")        return self.c",
        "documentation": "return a curl handle ready to use for performmultiperform"
    },
    {
        "method_code": "def __add_labels(self, graphics):                if graphics.type == \"line\":            # We use the midpoint of the line - sort of - we take the median            # line segment (list-wise, not in terms of length), and use the            # midpoint of that line.  We could have other options here,            # maybe even parameterising it to a proportion of the total line            # length.            mid_idx = len(graphics.coords) * 0.5            if int(mid_idx) != mid_idx:                idx1, idx2 = int(mid_idx - 0.5), int(mid_idx + 0.5)            else:                idx1, idx2 = int(mid_idx - 1), int(mid_idx)            x1, y1 = graphics.coords[idx1]            x2, y2 = graphics.coords[idx2]            x, y = 0.5 * (x1 + x2), 0.5 * (y1 + y2)        elif graphics.type == \"circle\":            x, y = graphics.x, graphics.y        elif graphics.type in (\"rectangle\", \"roundrectangle\"):            x, y = graphics.x, graphics.y        # How big so we want the text, and how many characters?        if graphics._parent.type == \"map\":            text = graphics.name            self.drawing.setFont(self.fontname, self.fontsize + 2)        elif len(graphics.name) < 15:            text = graphics.name        else:            text = graphics.name[:12] + \"...\"        self.drawing.drawCentredString(x, y, text)        self.drawing.setFont(self.fontname, self.fontsize)",
        "documentation": "add label for the passed graphic object to the map private we dont check that the label fit inside object such a circle rectanglesroundrectangles"
    },
    {
        "method_code": "def __repr__(self):                return f\"<Atom {self.get_id()}>\"",
        "documentation": "print atom object a atom atomname"
    },
    {
        "method_code": "def test_203_multiple_paths_via_pass_through(self):                interface1 = Interface.objects.create(device=self.device, name='Interface 1')        interface2 = Interface.objects.create(device=self.device, name='Interface 2')        interface3 = Interface.objects.create(device=self.device, name='Interface 3')        interface4 = Interface.objects.create(device=self.device, name='Interface 4')        rearport1 = RearPort.objects.create(device=self.device, name='Rear Port 1', positions=4)        rearport2 = RearPort.objects.create(device=self.device, name='Rear Port 2', positions=4)        frontport1_1 = FrontPort.objects.create(            device=self.device, name='Front Port 1:1', rear_port=rearport1, rear_port_position=1        )        frontport1_2 = FrontPort.objects.create(            device=self.device, name='Front Port 1:2', rear_port=rearport1, rear_port_position=2        )        frontport2_1 = FrontPort.objects.create(            device=self.device, name='Front Port 2:1', rear_port=rearport2, rear_port_position=1        )        frontport2_2 = FrontPort.objects.create(            device=self.device, name='Front Port 2:2', rear_port=rearport2, rear_port_position=2        )        # Create cables 1-2        cable1 = Cable(            a_terminations=[interface1],            b_terminations=[frontport1_1]        )        cable1.save()        cable2 = Cable(            a_terminations=[interface2],            b_terminations=[frontport1_2]        )        cable2.save()        self.assertPathExists(            (interface1, cable1, frontport1_1, rearport1),            is_complete=False        )        self.assertPathExists(            (interface2, cable2, frontport1_2, rearport1),            is_complete=False        )        self.assertEqual(CablePath.objects.count(), 2)        # Create cable 3        cable3 = Cable(            a_terminations=[rearport1],            b_terminations=[rearport2]        )        cable3.save()        self.assertPathExists(            (interface1, cable1, frontport1_1, rearport1, cable3, rearport2, frontport2_1),            is_complete=False        )        self.assertPathExists(            (interface2, cable2, frontport1_2, rearport1, cable3, rearport2, frontport2_2),            is_complete=False        )        self.assertEqual(CablePath.objects.count(), 2)        # Create cables 4-5        cable4 = Cable(            a_terminations=[frontport2_1],            b_terminations=[interface3]        )        cable4.save()        cable5 = Cable(            a_terminations=[frontport2_2],            b_terminations=[interface4]        )        cable5.save()        path1 = self.assertPathExists(            (interface1, cable1, frontport1_1, rearport1, cable3, rearport2, frontport2_1, cable4, interface3),            is_complete=True,            is_active=True        )        path2 = self.assertPathExists(            (interface2, cable2, frontport1_2, rearport1, cable3, rearport2, frontport2_2, cable5, interface4),            is_complete=True,            is_active=True        )        path3 = self.assertPathExists(            (interface3, cable4, frontport2_1, rearport2, cable3, rearport1, frontport1_1, cable1, interface1),            is_complete=True,            is_active=True        )        path4 = self.assertPathExists(            (interface4, cable5, frontport2_2, rearport2, cable3, rearport1, frontport1_2, cable2, interface2),            is_complete=True,            is_active=True        )        self.assertEqual(CablePath.objects.count(), 4)        # Test SVG generation        CableTraceSVG(interface1).render()        # Delete cable 3        cable3.delete()        # Check for four partial paths; one from each interface        self.assertEqual(CablePath.objects.filter(is_complete=False).count(), 4)        self.assertEqual(CablePath.objects.filter(is_complete=True).count(), 0)        interface1.refresh_from_db()        interface2.refresh_from_db()        interface3.refresh_from_db()        interface4.refresh_from_db()        self.assertPathIsSet(interface1, path1)        self.assertPathIsSet(interface2, path2)        self.assertPathIsSet(interface3, path3)        self.assertPathIsSet(interface4, path4)",
        "documentation": "if c fp rp c rp fp c if if c fp fp c if"
    },
    {
        "method_code": "def get_id(self):                return self.id",
        "documentation": "return the id"
    },
    {
        "method_code": "def add_residue(self, resname, ca_coord):                if self.counter >= self.length:            raise PDBException(\"Fragment boundary exceeded.\")        self.resname_list.append(resname)        self.coords_ca[self.counter] = ca_coord        self.counter = self.counter + 1",
        "documentation": "add a residue param resname residue name eg gly type resname string param cacoord the calpha coordinate of the residue type cacoord numpy array with length"
    },
    {
        "method_code": "def __next__(self):                if self._parser is None:            lines = []            while True:                line = self.handle.readline()                if not line:                    return None  # Premature end of file?                lines.append(line)                if line.rstrip() == \"//\":                    break            return \"\".join(lines)        try:            return self._parser.parse(self.handle)        except StopIteration:            return None",
        "documentation": "return the next genbank record from the handle will return none if we ran out of record"
    },
    {
        "method_code": "def cleanup(self, **kwargs):                if self.extracted:            log.debug(                \"Removing extracted directory: {0}\",                displayable_path(self.toppath),            )            shutil.rmtree(syspath(self.toppath))",
        "documentation": "remove the temporary directory the archive wa extracted to"
    },
    {
        "method_code": "def database_value_expression(self, time_interval: float):                # The date_bin function can do the bucketing for us:        # https://www.postgresql.org/docs/14/functions-datetime.html#FUNCTIONS-DATETIME-BIN        db = provide_database_interface()        delta = self.as_timedelta(time_interval)        if db.dialect.name == \"postgresql\":            return sa.cast(                sa.func.floor(                    sa.extract(                        \"epoch\",                        (                            sa.func.date_bin(delta, db.Event.occurred, PIVOT_DATETIME)                            - PIVOT_DATETIME                        ),                    )                    / delta.total_seconds(),                ),                sa.Text,            )        elif db.dialect.name == \"sqlite\":            # Convert pivot date and event date to strings formatted as seconds since the epoch            pivot_timestamp = sa.func.strftime(                \"%s\", PIVOT_DATETIME.strftime(\"%Y-%m-%d %H:%M:%S\")            )            event_timestamp = sa.func.strftime(\"%s\", db.Event.occurred)            seconds_since_pivot = event_timestamp - pivot_timestamp            # Calculate the bucket index by dividing by the interval in seconds and flooring the result            bucket_index = sa.func.floor(                sa.cast(seconds_since_pivot, sa.Integer) / delta.total_seconds()            )            return sa.cast(bucket_index, sa.Text)        else:            raise NotImplementedError(f\"Dialect {db.dialect.name} is not supported.\")",
        "documentation": "return the sql expression to place an event in a time bucket"
    },
    {
        "method_code": "def submit(self, call: \"Call\") -> \"Call\":",
        "documentation": "submit a call to execute elsewhere the call result can be retrieved with callresult return the call for convenience"
    },
    {
        "method_code": "def test_repodata_info_jsondecodeerror(    package_server: socket,    use_jlap: bool,    monkeypatch,):        host, port = package_server.getsockname()    base = f\"http://{host}:{port}/test\"    channel_url = f\"{base}/osx-64\"    with env_vars(        {\"CONDA_PLATFORM\": \"osx-64\", \"CONDA_EXPERIMENTAL\": \"jlap\" if use_jlap else \"\"},        stack_callback=conda_tests_ctxt_mgmt_def_pol,    ):        SubdirData.clear_cached_local_channel_data(            exclude_file=False        )  # definitely clears them, including normally-excluded file:// urls        test_channel = Channel(channel_url)        sd = SubdirData(channel=test_channel)        print(sd.repodata_fn)        assert sd._loaded is False        # shoud automatically fetch and load        assert len(list(sd.iter_records()))        assert sd._loaded is True        # Corrupt the cache state. Double json could happen when (unadvisably)        # running conda in parallel, before we added locks-by-default.        sd.cache_path_state.write_text(sd.cache_path_state.read_text() * 2)        # now try to re-download        SubdirData.clear_cached_local_channel_data(exclude_file=False)        sd2 = SubdirData(channel=test_channel)        # caplog fixture was able to capture urllib3 logs but not conda's. Could        # be due to setting propagate=False on conda's root loggers. Instead,        # mock warning() to save messages.        records = []        def warning(*args, **kwargs):            records.append(args)        monkeypatch.setattr(conda.gateways.repodata.log, \"warning\", warning)        sd2.load()        assert any(record[0].startswith(\"JSONDecodeError\") for record in records)",
        "documentation": "test that cache metadata file work correctly"
    },
    {
        "method_code": "def _do_query(lib, query, album, also_items=True):        if album:        albums = list(lib.albums(query))        items = []        if also_items:            for al in albums:                items += al.items()    else:        albums = []        items = list(lib.items(query))    if album and not albums:        raise ui.UserError(\"No matching albums found.\")    elif not album and not items:        raise ui.UserError(\"No matching items found.\")    return items, albums",
        "documentation": "for command that operate on matched item performs a query and return a list of matching item and a list of matching album the latter is only nonempty when album is true raise a usererror if no item match alsoitems control whether when fetching album the associated item should be fetched also"
    },
    {
        "method_code": "def retrieve(self, key=None, default=None):                entry = self.plugin.pyload.db.get_storage(self.plugin.classname, key)        if key:            if entry is None:                value = default            else:                value = json.loads(b85decode(entry).decode())        else:            if not entry:                value = default            else:                value = {k: json.loads(b85decode(v).decode()) for k, v in entry.items()}        return value",
        "documentation": "retrieves saved value or dict of all saved entry if key is none"
    },
    {
        "method_code": "def append(self, record):                if self._records:            self._append(record, self.get_alignment_length())        else:            self._append(record)",
        "documentation": "add one more seqrecord object to the alignment a a new row this must have the same length a the original alignment unless this is the first record from bio import alignio align alignioreadclustalwopuntiaaln clustal printalign alignment with row and column tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacataaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf lenalign well now construct a dummy record to append a an example from bioseq import seq from bioseqrecord import seqrecord dummy seqrecordseqn iddummy now append this to the alignment alignappenddummy printalign alignment with row and column tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacataaaagaagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf tatacattaaaggagggggatgcggataaatggaaaggcgaaagaga gigbafaf nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn dummy lenalign"
    },
    {
        "method_code": "def __init__(self):",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def test_output006(self):                fasta_file = \"Fasta/output006.m10\"        with open(fasta_file) as handle:            alignments = list(FastaIO.FastaM10Iterator(handle))            self.assertEqual(len(alignments), 1)            self.assertEqual(len(alignments[0]), 2)            self.assertEqual(alignments[0].get_alignment_length(), 131)            self.assertEqual(                alignments[0][0].seq,                \"GCAACGCTTCAAGAACTGGAATT\"                \"AGGAACCGTGACAACGATTAATG\"                \"AGGAGATTTATGAAGAGGGTTCT\"                \"TCGATTTTAGGCCAATCGGAAGG\"                \"AATTATGTAGCAAGTCCATCAGA\"                \"AAATGGAAGAAGTCAT\",            )            self.assertEqual(alignments[0][0].id, \"query\")            self.assertEqual(alignments[0][0].annotations[\"original_length\"], 131)            self.assertEqual(                alignments[0][1].seq,                \"GCAACGCTTCAAGAACTGGAATT\"                \"AGGAACCGTGACAACGATTAATG\"                \"AGGAGATTTATGAAGAGGGTTCT\"                \"TCGATTTTAGGCCAATCGGAAGG\"                \"AATTATGTAGCAAGTCCATCAGA\"                \"AAATGGAAGTAGTCAT\",            )            self.assertEqual(alignments[0][1].id, \"gi|116660610|gb|EG558221.1|EG558221\")            self.assertEqual(alignments[0][1].annotations[\"original_length\"], 573)",
        "documentation": "check outputm file"
    },
    {
        "method_code": "def create_result_thread(self, data, add=False):                self.timestamp = time.time() + timedelta(minutes=5).total_seconds()        rid = self.result_ids        self.result_ids += 1        InfoThread(self, data, rid=rid, add=add)        return rid",
        "documentation": "creates a thread to fetch online status return result id"
    },
    {
        "method_code": "def dumps(self, obj: D) -> bytes:",
        "documentation": "encode the object into a blob of byte"
    },
    {
        "method_code": "def find_bucket_alpha(self, s):                for i, span in enumerate(self.alpha_spans):            if span.match(s):                return self.config[\"bucket_alpha\"].get()[i]        return s[0].upper()",
        "documentation": "return alpharange bucket that match given string or return the string initial if no matching bucket"
    },
    {
        "method_code": "def test_30_hmmscan_008(self):                txt_file = get_file(\"text_30_hmmscan_008.out\")        qresults = parse(txt_file, FMT)        counter = 0        # test first result        qresult = next(qresults)        counter += 1        self.assertEqual(\"hmmscan\", qresult.program)        self.assertEqual(\"/home/bow/db/hmmer/Pfam-A.hmm\", qresult.target)        self.assertEqual(\"3.0\", qresult.version)        self.assertEqual(\"gi|22748937|ref|NP_065801.1|\", qresult.id)        self.assertEqual(\"exportin-5 [Homo sapiens]\", qresult.description)        self.assertEqual(1204, qresult.seq_len)        self.assertEqual(2, len(qresult))        hit = qresult[0]        self.assertEqual(\"Xpo1\", hit.id)        self.assertEqual(\"Exportin 1-like protein\", hit.description)        self.assertTrue(hit.is_included)        self.assertEqual(7.8e-34, hit.evalue)        self.assertEqual(116.6, hit.bitscore)        self.assertEqual(7.8, hit.bias)        self.assertEqual(2.8, hit.domain_exp_num)        self.assertEqual(2, hit.domain_obs_num)        self.assertEqual(2, len(hit))        hsp = hit.hsps[0]        self.assertEqual(1, hsp.domain_index)        self.assertTrue(hsp.is_included)        self.assertEqual(116.1, hsp.bitscore)        self.assertEqual(3.4, hsp.bias)        self.assertEqual(1.6e-37, hsp.evalue_cond)        self.assertEqual(1.1e-33, hsp.evalue)        self.assertEqual(1, hsp.hit_start)        self.assertEqual(148, hsp.hit_end)        self.assertEqual(\".]\", hsp.hit_endtype)        self.assertEqual(109, hsp.query_start)        self.assertEqual(271, hsp.query_end)        self.assertEqual(\"..\", hsp.query_endtype)        self.assertEqual(108, hsp.env_start)        self.assertEqual(271, hsp.env_end)        self.assertEqual(\"..\", hsp.env_endtype)        self.assertEqual(0.98, hsp.acc_avg)        self.assertEqual(            \"HHHHHHHHHHHHHHHHHHTTTTSTTHHHHHHHHHHG-HHHHHHHHHHHHHHHHHHCCS-TTTS-CCCHHHHHHHCHHHHHHHHHHHHHHHC-TT-..................HHHHHHHHHHHHHHCTTS-CHHCHCS...HHHHHCHHCCSCCCHHHHHHHH\",            hsp.aln_annotation[\"CS\"],        )        self.assertEqual(            \"kflrnklaealaelflqeypnqWpsffddllsllssspsglelllriLkvlpeEiadfsrskleqerrnelkdllrsqvqkilelllqileqsvskk...............sselveatLkclsswvswidiglivnsp..llsllfqlLndpelreaAvecL\",            hsp.hit.seq,        )        self.assertEqual(            \"NHIKDALSRIVVEMIKREWPQHWPDMLIELDTLSKQGETQTELVMFILLRLAEDVVTF--QTLPPQRRRDIQQTLTQNMERIFSFLLNTLQENVNKYqqvktdtsqeskaqaNCRVGVAALNTLAGYIDWVSMSHITAENckLLEILCLLLNEQELQLGAAECL\",            hsp.query.seq,        )        self.assertEqual(            \"89******************************************************99..79*********************************99*****************************************8889*********************8\",            hsp.aln_annotation[\"PP\"],        )        self.assertEqual(            \"+++++ l+++++e++++e+p++Wp+++ +l  l++++++++el++ iL++l+e++++f  ++l  +rr++++++l++++++i+++ll+ l+++v+k+               ++++  a+L++l+ +++w+++++i +++  ll++l+ lLn++el+  A+ecL\",            hsp.aln_annotation[\"similarity\"],        )        hsp = hit.hsps[-1]        self.assertEqual(2, hsp.domain_index)        self.assertFalse(hsp.is_included)        self.assertEqual(-1.8, hsp.bitscore)        self.assertEqual(0.0, hsp.bias)        self.assertEqual(0.35, hsp.evalue_cond)        self.assertEqual(2.4e03, hsp.evalue)        self.assertEqual(111, hsp.hit_start)        self.assertEqual(139, hsp.hit_end)        self.assertEqual(\"..\", hsp.hit_endtype)        self.assertEqual(498, hsp.query_start)        self.assertEqual(525, hsp.query_end)        self.assertEqual(\"..\", hsp.query_endtype)        self.assertEqual(495, hsp.env_start)        self.assertEqual(529, hsp.env_end)        self.assertEqual(\"..\", hsp.env_endtype)        self.assertEqual(0.86, hsp.acc_avg)        self.assertEqual(\"HHCTTS-CHHCHCS.HHHHHCHHCCSCC\", hsp.aln_annotation[\"CS\"])        self.assertEqual(\"swvswidiglivnspllsllfqlLndpe\", hsp.hit.seq)        self.assertEqual(\"SFVQWEAMTLFLES-VITQMFRTLNREE\", hsp.query.seq)        self.assertEqual(\"899*********98.8888899998776\", hsp.aln_annotation[\"PP\"])        self.assertEqual(            \"s+v+w  ++l+++s +++ +f+ Ln++e\",            hsp.aln_annotation[\"similarity\"],        )        hit = qresult[-1]        self.assertEqual(\"IBN_N\", hit.id)        self.assertEqual(\"Importin-beta N-terminal domain\", hit.description)        self.assertTrue(hit.is_included)        self.assertEqual(0.0039, hit.evalue)        self.assertEqual(16.9, hit.bitscore)        self.assertEqual(0.0, hit.bias)        self.assertEqual(2.7, hit.domain_exp_num)        self.assertEqual(2, hit.domain_obs_num)        self.assertEqual(2, len(hit))        hsp = hit.hsps[0]        self.assertEqual(1, hsp.domain_index)        self.assertTrue(hsp.is_included)        self.assertEqual(14.0, hsp.bitscore)        self.assertEqual(0.0, hsp.bias)        self.assertEqual(4.8e-06, hsp.evalue_cond)        self.assertEqual(0.033, hsp.evalue)        self.assertEqual(3, hsp.hit_start)        self.assertEqual(75, hsp.hit_end)        self.assertEqual(\"..\", hsp.hit_endtype)        self.assertEqual(35, hsp.query_start)        self.assertEqual(98, hsp.query_end)        self.assertEqual(\"..\", hsp.query_endtype)        self.assertEqual(32, hsp.env_start)        self.assertEqual(100, hsp.env_end)        self.assertEqual(\"..\", hsp.env_endtype)        self.assertEqual(0.87, hsp.acc_avg)        self.assertEqual(            \"HHHHHHHSCTHHHHHHHHHHHTTTSTHHHHHHHHHHHHHHHHHSCCHHHHHHHHCS-HHHHHHHHHHHHHHH\",            hsp.aln_annotation[\"CS\"],        )        self.assertEqual(            \"qLnqlekqkPgflsallqilanksldlevRqlAalyLknlItkhWkseeaqrqqqlpeeekelIrnnllnll\",            hsp.hit.seq,        )        self.assertEqual(            \"FCEEFKEKCPICVPCGLRLA-EKTQVAIVRHFGLQILEHVVKFRWN--------GMSRLEKVYLKNSVMELI\",            hsp.query.seq,        )        self.assertEqual(            \"56788886699*********.6555899******************........999999****99999887\",            hsp.aln_annotation[\"PP\"],        )        self.assertEqual(            \" ++++++  P ++++ l+++ +k+    vR++++++L++ ++ +W+         ++  ek +++n++ +l+\",            hsp.aln_annotation[\"similarity\"],        )        hsp = hit.hsps[-1]        self.assertEqual(2, hsp.domain_index)        self.assertFalse(hsp.is_included)        self.assertEqual(-3.3, hsp.bitscore)        self.assertEqual(0.0, hsp.bias)        self.assertEqual(1.2, hsp.evalue_cond)        self.assertEqual(8e03, hsp.evalue)        self.assertEqual(56, hsp.hit_start)        self.assertEqual(75, hsp.hit_end)        self.assertEqual(\"..\", hsp.hit_endtype)        self.assertEqual(167, hsp.query_start)        self.assertEqual(186, hsp.query_end)        self.assertEqual(\"..\", hsp.query_endtype)        self.assertEqual(164, hsp.env_start)        self.assertEqual(187, hsp.env_end)        self.assertEqual(\"..\", hsp.env_endtype)        self.assertEqual(0.85, hsp.acc_avg)        self.assertEqual(\"HCS-HHHHHHHHHHHHHHH\", hsp.aln_annotation[\"CS\"])        self.assertEqual(\"qqlpeeekelIrnnllnll\", hsp.hit.seq)        self.assertEqual(\"QTLPPQRRRDIQQTLTQNM\", hsp.query.seq)        self.assertEqual(\"6899*******99998865\", hsp.aln_annotation[\"PP\"])        self.assertEqual(\"q+lp++ + +I ++l + +\", hsp.aln_annotation[\"similarity\"])        # test if we've properly finished iteration        self.assertRaises(StopIteration, next, qresults)        self.assertEqual(1, counter)",
        "documentation": "parsing hmmscan texthmmscan"
    },
    {
        "method_code": "def test_getitem_only_query(self):                # getitem should work if only query is present        self.fragment.query = \"AATCG\"        self.assertEqual(\"ATCG\", self.fragment[1:].query.seq)",
        "documentation": "test hspfragmentgetitem only query"
    },
    {
        "method_code": "def __init__(self, parser):                self.parser = parser        self.start_methods = {}        self.end_methods = {}        XMLHandler._schema_methods = self.start_methods, self.end_methods",
        "documentation": "initialize the xml schema parser"
    },
    {
        "method_code": "def _mod1(cls, other):                if issubclass(other, Ov5):            return cls._mod2(other)        else:            return False",
        "documentation": "test if other enzyme produce compatible end for enzyme private for internal use only test for the compatibility of restriction ending of re and other"
    },
    {
        "method_code": "def _upload(self, error_report) -> None:                from .base.context import context        post_upload = False        if context.report_errors is False:            # no prompt and no submission            do_upload = False        elif context.report_errors is True or context.always_yes:            # no prompt and submit            do_upload = True        elif context.json or context.quiet or not self._isatty:            # never prompt under these conditions, submit iff always_yes            do_upload = bool(not context.offline and context.always_yes)        else:            # prompt whether to submit            do_upload = self._ask_upload()            post_upload = True        # the upload state is one of the following:        #   - True: upload error report        #   - False: do not upload error report        #   - None: while prompting a timeout occurred        if do_upload:            # user wants report to be submitted            self._execute_upload(error_report)        if post_upload:            # post submission text            self._post_upload(do_upload)",
        "documentation": "determine whether or not to upload the error report"
    },
    {
        "method_code": "def __ne__(self, other):                for left, right in zip_longest(self.sequences, other.sequences):            try:                left = left.seq            except AttributeError:                pass            try:                right = right.seq            except AttributeError:                pass            if left != right:                return True        return not np.array_equal(self.coordinates, other.coordinates)",
        "documentation": "check if two alignment object have different alignment"
    },
    {
        "method_code": "def __setstate__(self, data: dict):                self.__dict__.update(data)        self._start_connection()",
        "documentation": "reset connection and cursor upon loading"
    },
    {
        "method_code": "def _read_pfm(handle):        alphabet = \"ACGT\"    counts = {}    for letter, line in zip(alphabet, handle):        words = line.split()        # if there is a letter in the beginning, ignore it        if words[0] == letter:            words = words[1:]        counts[letter] = [float(x) for x in words]    motif = Motif(matrix_id=None, name=None, alphabet=alphabet, counts=counts)    motif.mask = \"*\" * motif.length    record = Record()    record.append(motif)    return record",
        "documentation": "read the motif from a jaspar pfm file private"
    },
    {
        "method_code": "def init_graphics(self):                tk.Toplevel.__init__(self, self.master)        self.frame = ttk.Frame(self)        self.frame.pack(fill=tk.BOTH, expand=1)        self.search_entry = ttk.Entry(self.frame)        self.search_entry.pack(fill=tk.BOTH, expand=1)        f2 = ttk.Frame(self.frame)        f2.pack(side=tk.TOP, fill=tk.BOTH, expand=1)        f = f2        self.forward = ttk.Button(f, text=\"Search +\", command=self.do_search)        self.forward.pack(side=tk.LEFT)        self.forward = ttk.Button(            f, text=\"Search -\", command=lambda x=self.do_search: x(other_strand=1)        )        self.forward.pack(side=tk.LEFT)        self.cancel = ttk.Button(f, text=\"Cancel\", command=self.exit)        self.cancel.pack(side=tk.LEFT)        self.current_color = \"cyan\"        self.colorb = ttk.Button(f, text=\"Color\", command=self.change_color)        self.colorb.pack(side=tk.LEFT)        self.config_color(self.current_color)",
        "documentation": "build the search window"
    },
    {
        "method_code": "def test_P84001(self):                xml = list(SeqIO.parse(\"SwissProt/P84001.xml\", \"uniprot-xml\"))[0]        self.assertEqual(xml.id, \"P84001\")        self.assertEqual(len(xml.annotations[\"comment_massspectrometry\"]), 1)        self.assertEqual(            xml.annotations[\"comment_massspectrometry\"][0],            \"undefined:9571|Electrospray\",        )",
        "documentation": "parse mass spec structured comment with unknown loc"
    },
    {
        "method_code": "def __init__(self, length: int) -> None:                dict.__init__(self)        self._length = int(length)",
        "documentation": "create an empty restricted dictionary"
    },
    {
        "method_code": "def test_pubmed2(self):                # To create the XML file, use        # >>> Bio.Entrez.einfo(db=\"pubmed\")        # Starting some time in 2010, the results returned by Bio.Entrez        # included some tags that are not part of the corresponding DTD.        from Bio.Entrez import Parser        with open(\"Entrez/einfo3.xml\", \"rb\") as stream:            self.assertRaises(Parser.ValidationError, Entrez.read, stream)",
        "documentation": "test validating the xml against the dtd"
    },
    {
        "method_code": "def isoelectric_point(self):                aa_content = self.count_amino_acids()        ie_point = IsoelectricPoint.IsoelectricPoint(self.sequence, aa_content)        return ie_point.pi()",
        "documentation": "calculate the isoelectric point us the module isoelectricpoint to calculate the pi of a protein"
    },
    {
        "method_code": "def _try_extracting_lyrics_from_non_data_lyrics_container(self, soup):                verse_div = soup.find(\"div\", class_=re.compile(\"Lyrics__Container\"))        if not verse_div:            if soup.find(                \"div\",                class_=re.compile(\"LyricsPlaceholder__Message\"),                string=\"This song is an instrumental\",            ):                self._log.debug(\"Detected instrumental\")                return \"[Instrumental]\"            else:                self._log.debug(\"Couldn't scrape page using known layouts\")                return None        lyrics_div = verse_div.parent        self.replace_br(lyrics_div)        ads = lyrics_div.find_all(            \"div\", class_=re.compile(\"InreadAd__Container\")        )        for ad in ads:            ad.replace_with(\"\\n\")        footers = lyrics_div.find_all(            \"div\", class_=re.compile(\"Lyrics__Footer\")        )        for footer in footers:            footer.replace_with(\"\")        return lyrics_div.get_text()",
        "documentation": "extract lyric from a div without attribute datalyricscontainer this is the second most common layout on geniuscom"
    },
    {
        "method_code": "def _parse_tree(self, text):                tokens = re.finditer(tokenizer, text.strip())        new_clade = self.new_clade        root_clade = new_clade()        current_clade = root_clade        entering_branch_length = False        lp_count = 0        rp_count = 0        for match in tokens:            token = match.group()            if token.startswith(\"'\"):                # quoted label; add characters to clade name                current_clade.name = token[1:-1]            elif token.startswith(\"[\"):                # comment                current_clade.comment = token[1:-1]                if self.comments_are_confidence:                    # Try to use this comment as a numeric support value                    current_clade.confidence = _parse_confidence(current_clade.comment)            elif token == \"(\":                # start a new clade, which is a child of the current clade                current_clade = new_clade(current_clade)                entering_branch_length = False                lp_count += 1            elif token == \",\":                # if the current clade is the root, then the external parentheses                # are missing and a new root should be created                if current_clade is root_clade:                    root_clade = new_clade()                    current_clade.parent = root_clade                # start a new child clade at the same level as the current clade                parent = self.process_clade(current_clade)                current_clade = new_clade(parent)                entering_branch_length = False            elif token == \")\":                # done adding children for this parent clade                parent = self.process_clade(current_clade)                if not parent:                    raise NewickError(\"Parenthesis mismatch.\")                current_clade = parent                entering_branch_length = False                rp_count += 1            elif token == \";\":                break            elif token.startswith(\":\"):                # branch length or confidence                value = float(token[1:])                if self.values_are_confidence:                    current_clade.confidence = value                else:                    current_clade.branch_length = value            elif token == \"\\n\":                pass            else:                # unquoted node label                current_clade.name = token        if lp_count != rp_count:            raise NewickError(                f\"Mismatch, {lp_count} open vs {rp_count} close parentheses.\"            )        # if ; token broke out of for loop, there should be no remaining tokens        try:            next_token = next(tokens)            raise NewickError(                f\"Text after semicolon in Newick tree: {next_token.group()}\"            )        except StopIteration:            pass        self.process_clade(current_clade)        self.process_clade(root_clade)        return Newick.Tree(root=root_clade, rooted=self.rooted)",
        "documentation": "parse the text representation into an tree object private"
    },
    {
        "method_code": "def write(obj, file, encoding=DEFAULT_ENCODING, indent=True):        def fix_single(tree):        if isinstance(tree, PX.Phylogeny):            return tree        if isinstance(tree, PX.Clade):            return tree.to_phylogeny()        if isinstance(tree, PX.BaseTree.Tree):            return PX.Phylogeny.from_tree(tree)        if isinstance(tree, PX.BaseTree.Clade):            return PX.Phylogeny.from_tree(PX.BaseTree.Tree(root=tree))        else:            raise ValueError(\"iterable must contain Tree or Clade types\")    if isinstance(obj, PX.Phyloxml):        pass    elif isinstance(obj, (PX.BaseTree.Tree, PX.BaseTree.Clade)):        obj = fix_single(obj).to_phyloxml()    elif hasattr(obj, \"__iter__\"):        obj = PX.Phyloxml({}, phylogenies=(fix_single(t) for t in obj))    else:        raise ValueError(            \"First argument must be a Phyloxml, Phylogeny, \"            \"Tree, or iterable of Trees or Phylogenies.\"        )    return Writer(obj).write(file, encoding=encoding, indent=indent)",
        "documentation": "write a phyloxml file parameter obj an instance of phyloxml phylogeny or basetreetree or an iterable of either of the latter two the object will be converted to a phyloxml object before serialization file either an open handle or a file name"
    },
    {
        "method_code": "def test_request_url_jlap_state(tmp_path, package_server, package_repository_base):        host, port = package_server.getsockname()    base = f\"http://{host}:{port}/test\"    url = f\"{base}/osx-64/repodata.json\"    cache = RepodataCache(base=tmp_path / \"cache\", repodata_fn=\"repodata.json\")    cache.state.set_has_format(\"jlap\", True)    cache.save(json.dumps({\"info\": {}}))    test_jlap = make_test_jlap(cache.cache_path_json.read_bytes(), 8)    test_jlap.terminate()    test_jlap_path = package_repository_base / \"osx-64\" / \"repodata.jlap\"    test_jlap.write(test_jlap_path)    temp_path = tmp_path / \"new_repodata.json\"    # correct hash must appear here and in test_jlap to reach desired condition    outdated_state = cache.load_state()    hasher = fetch.hash()    hasher.update(cache.cache_path_json.read_bytes())    outdated_state[fetch.NOMINAL_HASH] = hasher.hexdigest()    outdated_state[fetch.ON_DISK_HASH] = hasher.hexdigest()    # Simulate a peculiar \"request_url_jlap_state uses state object, but later    # re-fetches state from disk\" condition where the initial state and the    # on-disk state were inconsistent. This is done to avoid unnecessary reads    # of repodata.json. The failure will happen in the wild when another process    # writes repodata.json while we are downloading ours.    on_disk_state = json.loads(cache.cache_path_state.read_text())    on_disk_state[fetch.NOMINAL_HASH] = \"0\" * 64    on_disk_state[fetch.ON_DISK_HASH] = \"0\" * 64    cache.cache_path_state.write_text(json.dumps(on_disk_state))    result = fetch.request_url_jlap_state(        url,        outdated_state,        session=CondaSession(),        cache=cache,        temp_path=temp_path,    )    assert result is None",
        "documentation": "code coverage for case intended to catch repodatajson written while we were downloading it patch when this happens we do not write a new repodatajson and instruct the caller to defer to the ondisk cache"
    },
    {
        "method_code": "def _get_codon2codon_matrix(codon_table):        bases = (\"A\", \"T\", \"C\", \"G\")    codons = [        codon        for codon in list(codon_table.forward_table.keys()) + codon_table.stop_codons        if \"U\" not in codon    ]    # set up codon_dict considering stop codons    codon_dict = codon_table.forward_table.copy()    for stop in codon_table.stop_codons:        codon_dict[stop] = \"stop\"    # count site    num = len(codons)    G = {}  # graph for substitution    nonsyn_G = {}  # graph for nonsynonymous substitution    graph = {}    graph_nonsyn = {}    for i, codon in enumerate(codons):        graph[codon] = {}        graph_nonsyn[codon] = {}        for p in range(3):            for base in bases:                tmp_codon = codon[0:p] + base + codon[p + 1 :]                if codon_dict[codon] != codon_dict[tmp_codon]:                    graph_nonsyn[codon][tmp_codon] = 1                    graph[codon][tmp_codon] = 1                else:                    if codon != tmp_codon:                        graph_nonsyn[codon][tmp_codon] = 0.1                        graph[codon][tmp_codon] = 1    for codon1 in codons:        nonsyn_G[codon1] = {}        G[codon1] = {}        for codon2 in codons:            if codon1 == codon2:                nonsyn_G[codon1][codon2] = 0                G[codon1][codon2] = 0            else:                nonsyn_G[codon1][codon2] = _dijkstra(graph_nonsyn, codon1, codon2)                G[codon1][codon2] = _dijkstra(graph, codon1, codon2)    return G, nonsyn_G",
        "documentation": "get codon codon substitution matrix private element in the matrix are number of synonymous and nonsynonymous substitution required for the substitution"
    },
    {
        "method_code": "def write_footer(self, stream):                return",
        "documentation": "write the file footer to the output file"
    },
    {
        "method_code": "def get_main_info_str(info_dict: dict[str, Any]) -> str:        from ..common.compat import on_win    def flatten(lines: Iterable[str]) -> str:        return (\"\\n\" + 26 * \" \").join(map(str, lines))    def builder():        if info_dict[\"active_prefix_name\"]:            yield (\"active environment\", info_dict[\"active_prefix_name\"])            yield (\"active env location\", info_dict[\"active_prefix\"])        else:            yield (\"active environment\", info_dict[\"active_prefix\"])        if info_dict[\"conda_shlvl\"] >= 0:            yield (\"shell level\", info_dict[\"conda_shlvl\"])        yield (\"user config file\", info_dict[\"user_rc_path\"])        yield (\"populated config files\", flatten(info_dict[\"config_files\"]))        yield (\"conda version\", info_dict[\"conda_version\"])        yield (\"conda-build version\", info_dict[\"conda_build_version\"])        yield (\"python version\", info_dict[\"python_version\"])        yield (            \"solver\",            f\"{info_dict['solver']['name']}{' (default)' if info_dict['solver']['default'] else ''}\",        )        yield (            \"virtual packages\",            flatten(\"=\".join(pkg) for pkg in info_dict[\"virtual_pkgs\"]),        )        writable = \"writable\" if info_dict[\"root_writable\"] else \"read only\"        yield (\"base environment\", f\"{info_dict['root_prefix']}  ({writable})\")        yield (\"conda av data dir\", info_dict[\"av_data_dir\"])        yield (\"conda av metadata url\", info_dict[\"av_metadata_url_base\"])        yield (\"channel URLs\", flatten(info_dict[\"channels\"]))        yield (\"package cache\", flatten(info_dict[\"pkgs_dirs\"]))        yield (\"envs directories\", flatten(info_dict[\"envs_dirs\"]))        yield (\"platform\", info_dict[\"platform\"])        yield (\"user-agent\", info_dict[\"user_agent\"])        if on_win:            yield (\"administrator\", info_dict[\"is_windows_admin\"])        else:            yield (\"UID:GID\", f\"{info_dict['UID']}:{info_dict['GID']}\")        yield (\"netrc file\", info_dict[\"netrc_file\"])        yield (\"offline mode\", info_dict[\"offline\"])    return \"\\n\".join((\"\", *(f\"{key:>23} : {value}\" for key, value in builder()), \"\"))",
        "documentation": "return a printable string of the content of infodict param infodict the output of getinfodict return string to print"
    },
    {
        "method_code": "def write_file(self, records):                count = super().write_file(records, mincount=1, maxcount=1)        return count",
        "documentation": "write the complete file with the record and return the number of record"
    },
    {
        "method_code": "def test_ensembl_locus(self):                line = \"LOCUS       HG531_PATCH 1000000 bp DNA HTG 18-JUN-2011\\n\"        s = GenBank.Scanner.GenBankScanner()        c = GenBank._FeatureConsumer(True)        s._feed_first_line(c, line)        self.assertEqual(c.data.name, \"HG531_PATCH\")        self.assertEqual(c._expected_size, 1000000)        line = \"LOCUS       HG531_PATCH 759984 bp DNA HTG 18-JUN-2011\\n\"        s = GenBank.Scanner.GenBankScanner()        c = GenBank._FeatureConsumer(True)        s._feed_first_line(c, line)        self.assertEqual(c.data.name, \"HG531_PATCH\")        self.assertEqual(c._expected_size, 759984)        line = \"LOCUS       HG506_HG1000_1_PATCH 814959 bp DNA HTG 18-JUN-2011\\n\"        s = GenBank.Scanner.GenBankScanner()        c = GenBank._FeatureConsumer(True)        s._feed_first_line(c, line)        self.assertEqual(c.data.name, \"HG506_HG1000_1_PATCH\")        self.assertEqual(c._expected_size, 814959)        line = \"LOCUS       HG506_HG1000_1_PATCH 1219964 bp DNA HTG 18-JUN-2011\\n\"        s = GenBank.Scanner.GenBankScanner()        c = GenBank._FeatureConsumer(True)        s._feed_first_line(c, line)        self.assertEqual(c.data.name, \"HG506_HG1000_1_PATCH\")        self.assertEqual(c._expected_size, 1219964)",
        "documentation": "test the ensembl locus line"
    },
    {
        "method_code": "def mean(self):                data = list(self.data.values())        return sum(data) / len(data)",
        "documentation": "return the mean value for the data point float"
    },
    {
        "method_code": "def test_30_hmmscan_003(self):                tab_file = get_file(\"tab_30_hmmscan_003.out\")        qresults = parse(tab_file, FMT)        counter = 0        qresult = next(qresults)        counter += 1        self.assertEqual(1, len(qresult))        self.assertEqual(\"gi|4885477|ref|NP_005359.1|\", qresult.id)        self.assertEqual(\"-\", qresult.accession)        hit = qresult[0]        self.assertEqual(1, len(hit))        self.assertEqual(\"Globin\", hit.id)        self.assertEqual(\"PF00042.17\", hit.accession)        self.assertEqual(6e-21, hit.evalue)        self.assertEqual(74.6, hit.bitscore)        self.assertEqual(0.3, hit.bias)        self.assertEqual(1.3, hit.domain_exp_num)        self.assertEqual(1, hit.region_num)        self.assertEqual(0, hit.cluster_num)        self.assertEqual(0, hit.overlap_num)        self.assertEqual(1, hit.env_num)        self.assertEqual(1, hit.domain_obs_num)        self.assertEqual(1, hit.domain_reported_num)        self.assertEqual(1, hit.domain_included_num)        self.assertEqual(\"Globin\", hit.description)        hsp = hit.hsps[0]        self.assertEqual(9.2e-21, hsp.evalue)        self.assertEqual(74.0, hsp.bitscore)        self.assertEqual(0.2, hsp.bias)        # test if we've properly finished iteration        self.assertRaises(StopIteration, next, qresults)        self.assertEqual(1, counter)",
        "documentation": "test parsing hmmertab hmmscan single query single hit single hsp tabhmmscan"
    },
    {
        "method_code": "def __add_compounds(self):                for compound in self.pathway.compounds:            for g in compound.graphics:                # Modify transparency of compounds that don't participate                # in reactions                fillcolor = color_to_reportlab(g.bgcolor)                if not compound.is_reactant:                    fillcolor.alpha *= self.non_reactant_transparency                self.drawing.setStrokeColor(color_to_reportlab(g.fgcolor))                self.drawing.setFillColor(fillcolor)                self.__add_graphics(g)                if self.label_compounds:                    if not compound.is_reactant:                        t = 0.3                    else:                        t = 1                    self.drawing.setFillColor(colors.Color(0.2, 0.2, 0.2, t))                    self.__add_labels(g)",
        "documentation": "add compound element to the drawing of the map private"
    },
    {
        "method_code": "def compile_transition_rules(cls, from_state=None, to_state=None):                transition_rules = []        for rule in cls.priority():            if from_state in rule.FROM_STATES and to_state in rule.TO_STATES:                transition_rules.append(rule)        return transition_rules",
        "documentation": "return rule in policy that are valid for the specified state transition"
    },
    {
        "method_code": "def get_dask_client(    timeout: Optional[Union[int, float, str, timedelta]] = None,    **client_kwargs: Dict[str, Any],) -> Generator[Client, None, None]:        client_kwargs = _generate_client_kwargs(        async_client=False, timeout=timeout, **client_kwargs    )    with Client(**client_kwargs) as client:        yield client",
        "documentation": "yield a temporary synchronous dask client this is useful for parallelizing operation on dask collection such a a daskdataframe or daskbag without invoking this worker do not automatically get a client to connect to the full cluster therefore it will attempt perform work within the worker itself serially and potentially overwhelming the single worker when in an async context we recommend using getasyncdaskclient instead args timeout timeout after which to error out ha no effect in flow run context because the client ha already started default to the distributedcommtimeoutsconnect configuration value clientkwargs additional keyword argument to pas to distributedclient and overwrites inherited keyword argument from the task runner if any yield a temporary synchronous dask client example use getdaskclient to distribute work across worker python import dask from prefect import flow task from prefectdask import dasktaskrunner getdaskclient task def computetask with getdaskclienttimeouts a client df daskdatasetstimeseries partitionfreqw summarydf clientcomputedfdescriberesult return summarydf flowtaskrunnerdasktaskrunner def daskflow prefectfuture computetasksubmit return prefectfutureresult daskflow"
    },
    {
        "method_code": "def test_virtualmachine_site_context(self):                site = Site.objects.first()        cluster_type = ClusterType.objects.create(name=\"Cluster Type\")        cluster = Cluster.objects.create(name=\"Cluster\", type=cluster_type, site=site)        vm_role = DeviceRole.objects.first()        # Create a ConfigContext associated with the site        context = ConfigContext.objects.create(            name=\"context1\",            weight=100,            data={\"foo\": True}        )        context.sites.add(site)        # Create one VM assigned directly to the site, and one assigned via the cluster        vm1 = VirtualMachine.objects.create(name=\"VM 1\", site=site, role=vm_role)        vm2 = VirtualMachine.objects.create(name=\"VM 2\", cluster=cluster, role=vm_role)        # Check that their individually-rendered config contexts are identical        self.assertEqual(            vm1.get_config_context(),            vm2.get_config_context()        )        # Check that their annotated config contexts are identical        vms = VirtualMachine.objects.filter(pk__in=(vm1.pk, vm2.pk)).annotate_config_context_data()        self.assertEqual(            vms[0].get_config_context(),            vms[1].get_config_context()        )",
        "documentation": "check that config context associated with a site applies to a vm whether the vm is assigned directly to that site or via it cluster"
    },
    {
        "method_code": "def test_missing_features(self):                # Set a larger number of features than the file actually contains        # Offset of the features number byte:        # header + length of sequence + length of comment + overhangs        feature_byte = 112 + 1000 + len(\"Sample sequence A\") + 5        h = self.munge_buffer(feature_byte, 3)        with self.assertRaisesRegex(ValueError, \"Cannot read 1 bytes from handle\"):            SeqIO.read(h, \"xdna\")        h.close()",
        "documentation": "read a file with an incorrect number of feature"
    },
    {
        "method_code": "def test_alt_index_in_middle(self):                self.check_sff(\"Roche/E3MFGYR02_alt_index_in_middle.sff\")",
        "documentation": "test converting emfgyraltindexinmiddle into fastaqual"
    },
    {
        "method_code": "def test_Reference(self):                with open(EX_DOLLO) as handle:            tree = next(PhyloXMLIO.parse(handle))        reference = tree.clade[0, 0, 0, 0, 0, 0].references[0]        self.assertIsInstance(reference, PX.Reference)        self.assertEqual(reference.doi, \"10.1038/nature06614\")        self.assertIsNone(reference.desc)",
        "documentation": "instantiation of reference object"
    },
    {
        "method_code": "def handle_rest_api_exception(request, *args, **kwargs):        type_, error, traceback = sys.exc_info()    data = {        'error': str(error),        'exception': type_.__name__,        'netbox_version': settings.RELEASE.full_version,        'python_version': platform.python_version(),    }    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)",
        "documentation": "handle exception and return a useful error message for rest api request"
    },
    {
        "method_code": "def test_simple_rna(self):                s = Seq(\"GAUCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10))        self.assertIsNone(f.location.strand)        self.check(s, f, \"YWSMK\", \"6..10\")",
        "documentation": "feature on rna simple default strand"
    },
    {
        "method_code": "def start(self):                return min(loc.start for loc in self.parts)",
        "documentation": "start location left most minimum value regardless of strand read only return an integer like position object possibly a fuzzy position for the special case of a compoundlocation wrapping the origin of a circular genome this will return zero"
    },
    {
        "method_code": "def _ln_gamma_function(alpha):        if alpha <= 0:        raise ValueError    x = alpha    f = 0    if x < 7:        f = 1        z = x        while z < 7:            f *= z            z += 1        x = z        f = -log(f)    z = 1 / (x * x)    return (        f        + (x - 0.5) * log(x)        - x        + 0.918938533204673        + (            ((-0.000595238095238 * z + 0.000793650793651) * z - 0.002777777777778) * z            + 0.083333333333333        )        / x    )",
        "documentation": "compute the log of the gamma function for a given alpha private comment from z yang return lngammaalpha for alpha accurate to decimal place stirlings formula is used for the central polynomial part of the procedure pike mc hill id algorithm logarithm of the gamma function communication of the association for computing machinery"
    },
    {
        "method_code": "def __ne__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        return self.id != other.id",
        "documentation": "test for inequality"
    },
    {
        "method_code": "def draw(self, filename):                # Instantiate the drawing, first        # size x_max, y_max for now - we can add margins, later        if self.import_imagemap:            # We're drawing directly on the image, so we set the canvas to the            # same size as the image            if os.path.isfile(self.pathway.image):                imfilename = self.pathway.image            else:                imfilename = get_temp_imagefilename(self.pathway.image)            im = Image.open(imfilename)            cwidth, cheight = im.size        else:            # No image, so we set the canvas size to accommodate visible            # elements            cwidth, cheight = (self.pathway.bounds[1][0], self.pathway.bounds[1][1])        # Instantiate canvas        self.drawing = canvas.Canvas(            filename,            bottomup=0,            pagesize=(                cwidth * (1 + 2 * self.margins[0]),                cheight * (1 + 2 * self.margins[1]),            ),        )        self.drawing.setFont(self.fontname, self.fontsize)        # Transform the canvas to add the margins        self.drawing.translate(            self.margins[0] * self.pathway.bounds[1][0],            self.margins[1] * self.pathway.bounds[1][1],        )        # Add the map image, if required        if self.import_imagemap:            self.drawing.saveState()            self.drawing.scale(1, -1)            self.drawing.translate(0, -cheight)            self.drawing.drawImage(imfilename, 0, 0)            self.drawing.restoreState()        # Add the reactions, compounds and maps        # Maps go on first, to be overlaid by more information.        # By default, they're slightly transparent.        if self.show_maps:            self.__add_maps()        if self.show_reaction_entries:            self.__add_reaction_entries()        if self.show_orthologs:            self.__add_orthologs()        if self.show_compounds:            self.__add_compounds()        if self.show_genes:            self.__add_genes()        # TODO: complete draw_relations code        # if self.draw_relations:        #    self.__add_relations()        # Write the pathway map to PDF        self.drawing.save()",
        "documentation": "add the map element to the drawing"
    },
    {
        "method_code": "def test_global_hz_excess(        self, fname, enum_test=True, dememorization=10000, batches=20, iterations=5000    ):                return self._test_global_hz_both(            fname, 5, \".EG\", enum_test, dememorization, batches, iterations        )",
        "documentation": "use global hardyweinberg test for heterozygote excess return a triple with a list per population containing popname pval se switch some pop have a none if the info is not available se might be none for enumeration a list per locus containing locusname pval se switch some locus have a none if the info is not available se might be none for enumeration overall result pval se switch"
    },
    {
        "method_code": "def reversemap(obj):        return obj.__class__(reversed(item) for item in obj.items())",
        "documentation": "invert mapping object preserving type and ordering"
    },
    {
        "method_code": "def test_reverse(self):                self.mutable_s.reverse()        self.assertEqual(Seq.MutableSeq(\"GTACTACGTAGGAAAACT\"), self.mutable_s)",
        "documentation": "test using reverse method"
    },
    {
        "method_code": "def test_neighbor_search(self):                class RandomAtom:            def __init__(self):                self.coord = 100 * random(3)            def get_coord(self):                return self.coord        for i in range(20):            atoms = [RandomAtom() for j in range(100)]            ns = NeighborSearch(atoms)            hits = ns.search_all(5.0)            self.assertIsInstance(hits, list)            self.assertGreaterEqual(len(hits), 0)        x = array([250, 250, 250])  # Far away from our random atoms        self.assertEqual([], ns.search(x, 5.0, \"A\"))        self.assertEqual([], ns.search(x, 5.0, \"R\"))        self.assertEqual([], ns.search(x, 5.0, \"C\"))        self.assertEqual([], ns.search(x, 5.0, \"M\"))        self.assertEqual([], ns.search(x, 5.0, \"S\"))",
        "documentation": "neighborsearch find nearby randomly generated coordinate based on the self test in biopdbneighborsearch"
    },
    {
        "method_code": "def password_data(username, password):        return {        \"username\": username,        \"password\": hashlib.sha1(password.encode(\"utf-8\")).hexdigest(),        \"passwordMd5\": hashlib.md5(password.encode(\"utf-8\")).hexdigest(),    }",
        "documentation": "return a dict with username and it encoded password param username emby username param password emby password type username str type password str return dictionary with username and encoded password rtype dict"
    },
    {
        "method_code": "def set_atoms(self, fixed, moving):                if not len(fixed) == len(moving):            raise PDBException(\"Fixed and moving atom lists differ in size\")        length = len(fixed)        fixed_coord = np.zeros((length, 3))        moving_coord = np.zeros((length, 3))        for i in range(length):            fixed_coord[i] = fixed[i].get_coord()            moving_coord[i] = moving[i].get_coord()        sup = SVDSuperimposer()        sup.set(fixed_coord, moving_coord)        sup.run()        self.rms = sup.get_rms()        self.rotran = sup.get_rotran()",
        "documentation": "prepare translationrotation to minimize rmsd between atom put translaterotate the atom in fixed on the atom in moving in such a way that the rmsd is minimized param fixed list of fixed atom param moving list of moving atom type fixedmoving latom latom"
    },
    {
        "method_code": "def test_setitem_wrong_type(self):                # item assignment should fail if the object assigned is not a hit object        self.assertRaises(TypeError, self.qresult.__setitem__, \"hit4\", hsp111)        self.assertRaises(TypeError, self.qresult.__setitem__, \"hit5\", \"hit5\")",
        "documentation": "test queryresultsetitem wrong type"
    },
    {
        "method_code": "def _save_state(state):        try:        with open(config[\"statefile\"].as_filename(), \"wb\") as f:            pickle.dump(state, f)    except OSError as exc:        log.error(\"state file could not be written: {0}\", exc)",
        "documentation": "writes the state dictionary out to disk"
    },
    {
        "method_code": "def isiterable(obj: Any) -> bool:        try:        iter(obj)    except TypeError:        return False    else:        return not isinstance(obj, (str, bytes, io.IOBase))",
        "documentation": "return a boolean indicating if an object is iterable excludes type that are iterable but typically used a singleton str byte io object"
    },
    {
        "method_code": "def set_state_id_on_inserted_flow_runs_statement(        self,        fr_model,        frs_model,        inserted_flow_run_ids,        insert_flow_run_states,    ):                # postgres supports `UPDATE ... FROM` syntax        stmt = (            sa.update(fr_model)            .where(                fr_model.id.in_(inserted_flow_run_ids),                frs_model.flow_run_id == fr_model.id,                frs_model.id.in_([r[\"id\"] for r in insert_flow_run_states]),            )            .values(state_id=frs_model.id)            # no need to synchronize as these flow runs are entirely new            .execution_options(synchronize_session=False)        )        return stmt",
        "documentation": "given a list of flow run id and associated state set the stateid to the appropriate state for all flow run"
    },
    {
        "method_code": "def method_paths_from_routes(routes: Sequence[BaseRoute]) -> Set[str]:        method_paths = set()    for route in routes:        if isinstance(route, (APIRoute, StarletteRoute)):            for method in route.methods:                method_paths.add(f\"{method} {route.path}\")    return method_paths",
        "documentation": "generate a set of string describing the given route in the format method path for example get log"
    },
    {
        "method_code": "def _draw_title(self, cur_drawing, title, width, height):                title_string = String(width / 2, height - inch, title)        title_string.fontName = \"Helvetica-Bold\"        title_string.fontSize = self.title_size        title_string.textAnchor = \"middle\"        cur_drawing.add(title_string)",
        "documentation": "add the title of the figure to the drawing private"
    },
    {
        "method_code": "def test_genbank_read_invalid(self):                path = \"GenBank/NC_000932.faa\"        with open(path) as handle:            self.assertRaises(ValueError, GenBank.read, handle)",
        "documentation": "genbankread error on invalid file eg fasta file"
    },
    {
        "method_code": "def test_queues(self):                self.assertIn('netbox.tests.dummy_plugin.testing-low', settings.RQ_QUEUES)        self.assertIn('netbox.tests.dummy_plugin.testing-medium', settings.RQ_QUEUES)        self.assertIn('netbox.tests.dummy_plugin.testing-high', settings.RQ_QUEUES)",
        "documentation": "check that plugin queue are registered with the accurate name"
    },
    {
        "method_code": "def linked_data(prefix, ignore_channels=False):        from .core.prefix_data import PrefixData    from .models.dist import Dist    pd = PrefixData(prefix)    return {        Dist(prefix_record): prefix_record        for prefix_record in pd._prefix_records.values()    }",
        "documentation": "return a dictionary of the linked package in prefix"
    },
    {
        "method_code": "def _task_definitions_equal(self, taskdef_1, taskdef_2) -> bool:                if taskdef_1 == taskdef_2:            return True        if taskdef_1 is None or taskdef_2 is None:            return False        taskdef_1 = copy.deepcopy(taskdef_1)        taskdef_2 = copy.deepcopy(taskdef_2)        for taskdef in (taskdef_1, taskdef_2):            # Set defaults that AWS would set after registration            container_definitions = taskdef.get(\"containerDefinitions\", [])            essential = any(                container.get(\"essential\") for container in container_definitions            )            if not essential:                container_definitions[0].setdefault(\"essential\", True)            taskdef.setdefault(\"networkMode\", \"bridge\")        _drop_empty_keys_from_dict(taskdef_1)        _drop_empty_keys_from_dict(taskdef_2)        # Clear fields that change on registration for comparison        for field in ECS_POST_REGISTRATION_FIELDS:            taskdef_1.pop(field, None)            taskdef_2.pop(field, None)        return taskdef_1 == taskdef_2",
        "documentation": "compare two task definition since one may come from the aws api and have populated default we do our best to homogenize the definition without changing their meaning"
    },
    {
        "method_code": "def get_indiv(line):        def int_no_zero(val):                v = int(val)        if v == 0:            return None        return v    indiv_name, marker_line = line.split(\",\")    markers = marker_line.replace(\"\\t\", \" \").split(\" \")    markers = [marker for marker in markers if marker != \"\"]    if len(markers[0]) in [2, 4]:  # 2 digits per allele        marker_len = 2    else:        marker_len = 3    try:        allele_list = [            (int_no_zero(marker[0:marker_len]), int_no_zero(marker[marker_len:]))            for marker in markers        ]    except ValueError:  # Haploid        allele_list = [(int_no_zero(marker[0:marker_len]),) for marker in markers]    return indiv_name, allele_list, marker_len",
        "documentation": "extract the detail of the individual information on the line"
    },
    {
        "method_code": "def _fastq_solexa_convert_fastq_sanger(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # Map unexpected chars to null    mapping = \"\".join(        [chr(0) for ascii in range(59)]        + [            chr(33 + int(round(phred_quality_from_solexa(q))))            for q in range(-5, 62 + 1)        ]        + [chr(0) for ascii in range(127, 256)]    )    assert len(mapping) == 256    return _fastq_generic(in_file, out_file, mapping)",
        "documentation": "fast solexa fastq to sanger fastq conversion private avoids creating seqrecord and seq object in order to speed up this conversion"
    },
    {
        "method_code": "def _compute_sphere(self):                n = self.n_points        dl = np.pi * (3 - 5**0.5)        dz = 2.0 / n        longitude = 0        z = 1 - dz / 2        coords = np.zeros((n, 3), dtype=np.float32)        for k in range(n):            r = (1 - z * z) ** 0.5            coords[k, 0] = math.cos(longitude) * r            coords[k, 1] = math.sin(longitude) * r            coords[k, 2] = z            z -= dz            longitude += dl        return coords",
        "documentation": "return the d coordinate of n point on a sphere us the golden spiral algorithm to place point evenly on the sphere surface we compute this once and then move the sphere to the centroid of each atom a we compute the asas"
    },
    {
        "method_code": "def assertPathExists(self, nodes, **kwargs):                cablepath = self._get_cablepath(nodes, **kwargs)        self.assertIsNotNone(cablepath, msg='CablePath not found')        return cablepath",
        "documentation": "assert that a cablepath from origin to destination with a specific intermediate path exists return the first matching cablepath if found param node iterable of step with each step being either a single node or a list of node"
    },
    {
        "method_code": "def set_equal_probabilities(self):                # set initial state probabilities        new_initial_prob = 1.0 / len(self.transition_prob)        for state in self._state_alphabet:            self.initial_prob[state] = new_initial_prob        # set the transitions        new_trans_prob = 1.0 / len(self.transition_prob)        for key in self.transition_prob:            self.transition_prob[key] = new_trans_prob        # set the emissions        new_emission_prob = 1.0 / len(self.emission_prob)        for key in self.emission_prob:            self.emission_prob[key] = new_emission_prob",
        "documentation": "reset all probability to be an average value reset the value of all initial probability and all allowed transition and all allowed emission to be equal to divided by the number of possible element this is useful if you just want to initialize a markov model to starting value ie if you have no prior notion of what the probability should be or if you are just feeling too lazy to calculate them warning this will reset all currently set probability warning this just set all probability for transition and emission to total up to so it doesnt ensure that the sum of each set of transition add up to"
    },
    {
        "method_code": "def without_site(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if not v}",
        "documentation": "return only result from enzyme that dont cut the sequence"
    },
    {
        "method_code": "def clear_configuration_value_cache_for_key(self, key: str):                return self.queries.clear_configuration_value_cache_for_key(key=key)",
        "documentation": "remove a configuration key from the cache"
    },
    {
        "method_code": "def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "documentation": "return the path of a test file"
    },
    {
        "method_code": "def greatest(self, *values):",
        "documentation": "dialectspecific sqlalchemy binding"
    },
    {
        "method_code": "def _combine_args(first, *rest):        # Background: is_monophyletic takes a single list or iterable (like the    # same method in Bio.Nexus.Trees); root_with_outgroup and common_ancestor    # take separate arguments. This mismatch was in the initial release and I    # didn't notice the inconsistency until after Biopython 1.55. I can think    # of cases where either style is more convenient, so let's support both    # (for backward compatibility and consistency between methods).    if hasattr(first, \"__iter__\") and not (        isinstance(first, (TreeElement, dict, str, type))    ):        # terminals is an iterable of targets        if rest:            raise ValueError(                \"Arguments must be either a single list of \"                \"targets, or separately specified targets \"                \"(e.g. foo(t1, t2, t3)), but not both.\"            )        return first    # terminals is a single target -- wrap in a container    return itertools.chain([first], rest)",
        "documentation": "convert target or target argument to a single iterable private this help other function work like the builtin function max and min"
    },
    {
        "method_code": "def test_from_residue_level(self):                structure_residues = list(self.structure.get_residues())        struct_unfold = unfold_entities(structure_residues, \"S\")[0]        for res1, res2 in zip(structure_residues, struct_unfold.get_residues()):            assert res_full_id(res1) == res_full_id(res2)        model_unfold = unfold_entities(structure_residues, \"M\")[0]        for res1, res2 in zip(structure_residues, model_unfold.get_residues()):            assert res_full_id(res1) == res_full_id(res2)        residue_unfold = unfold_entities(structure_residues, \"R\")        for res1, res2 in zip(structure_residues, residue_unfold):            assert res_full_id(res1) == res_full_id(res2)        atom_unfold = unfold_entities(structure_residues, \"A\")        for at1, at2 in zip(self.structure.get_atoms(), atom_unfold):            assert at1 is at2",
        "documentation": "unfold from chain level to all level"
    },
    {
        "method_code": "def _is_http_error_most_400_codes(e: HTTPError) -> bool:        if e.response is None:  # 404 e.response is falsey        return False    status_code = e.response.status_code    return 400 <= status_code < 500 and status_code != 416",
        "documentation": "determine whether the httperror is an http error code except for"
    },
    {
        "method_code": "def _fastq_generic(    in_file: _TextIOSource,    out_file: _TextIOSource,    mapping: Union[Sequence[str], Mapping[int, Optional[Union[str, int]]]],) -> int:        # For real speed, don't even make SeqRecord and Seq objects!    count = 0    null = chr(0)    with as_handle(out_file, \"w\") as out_handle:        for title, seq, old_qual in FastqGeneralIterator(in_file):            count += 1            # map the qual...            qual = old_qual.translate(mapping)            if null in qual:                raise ValueError(\"Invalid character in quality string\")            out_handle.write(f\"@{title}\\n{seq}\\n+\\n{qual}\\n\")    return count",
        "documentation": "fastq helper function where cant have data loss by truncation private"
    },
    {
        "method_code": "def set_parent(self, parent):                self.parent = parent        for child in self.disordered_get_list():            child.set_parent(parent)",
        "documentation": "set the parent for the object and it child"
    },
    {
        "method_code": "def copy(self):                other = Array(alphabet=self._alphabet, data=self)        return other",
        "documentation": "create and return a copy of the array"
    },
    {
        "method_code": "def test_get_session_with_channel_settings_multiple(mocker):        mocker.patch(        \"conda.gateways.connection.session.get_channel_name_from_url\",        side_effect=[\"channel_one\", \"channel_two\"],    )    mock_context = mocker.patch(\"conda.gateways.connection.session.context\")    mock_context.channel_settings = (        {\"channel\": \"channel_one\", \"auth\": \"dummy_one\"},        {\"channel\": \"channel_two\", \"auth\": \"dummy_one\"},    )    mock_context.plugin_manager.get_auth_handler.return_value = ChannelAuthBase    url_one = \"https://localhost/test1\"    url_two = \"https://localhost/test2\"    session_obj_one = get_session(url_one)    session_obj_two = get_session(url_two)    get_session.cache_clear()  # ensuring cleanup    assert session_obj_one is not session_obj_two    storage_key_one = get_session_storage_key(session_obj_one.auth)    storage_key_two = get_session_storage_key(session_obj_two.auth)    assert storage_key_one in session_obj_one._thread_local.sessions    assert storage_key_two in session_obj_one._thread_local.sessions    assert type(session_obj_one) is CondaSession    assert type(session_obj_two) is CondaSession    # For session objects with a custom auth handler it will not be set to CondaHttpAuth    assert type(session_obj_one.auth) is not CondaHttpAuth    assert type(session_obj_two.auth) is not CondaHttpAuth    # Make sure we tried to retrieve our auth handler in this function    assert (        mocker.call(\"dummy_one\")        in mock_context.plugin_manager.get_auth_handler.mock_calls    )",
        "documentation": "test to make sure the getsession function work when channelsettings have been set on the context object and there exists more than one channel configured using the same type of auth handler it important that our cache key are set up so that we do not return the same condasession object for these two different channel"
    },
    {
        "method_code": "def __init__(self, adaptor, primary_id):                self._adaptor = adaptor        self._primary_id = primary_id        (            self._biodatabase_id,            self._taxon_id,            self.name,            accession,            version,            self._identifier,            self._division,            self.description,        ) = self._adaptor.execute_one(            \"SELECT biodatabase_id, taxon_id, name, accession, version,\"            \" identifier, division, description\"            \" FROM bioentry\"            \" WHERE bioentry_id = %s\",            (self._primary_id,),        )        if version and version != \"0\":            self.id = f\"{accession}.{version}\"        else:            self.id = accession        # We don't yet record any per-letter-annotations in the        # BioSQL database, but we should set this property up        # for completeness (and the __str__ method).        # We do NOT want to load the sequence from the DB here!        length = _retrieve_seq_len(adaptor, primary_id)        self._per_letter_annotations = _RestrictedDict(length=length)",
        "documentation": "create a dbseqrecord object argument adaptor a biosqlbioseqdatabaseadaptor object primaryid an internal integer id used by biosql you wouldnt normally create a dbseqrecord object yourself this is done for you when using a bioseqdatabase object"
    },
    {
        "method_code": "def album_for_id(self, album_id):                spotify_id = self._get_id(\"album\", album_id, self.id_regex)        if spotify_id is None:            return None        album_data = self._handle_response(            requests.get, self.album_url + spotify_id        )        if album_data[\"name\"] == \"\":            self._log.debug(\"Album removed from Spotify: {}\", album_id)            return None        artist, artist_id = self.get_artist(album_data[\"artists\"])        date_parts = [            int(part) for part in album_data[\"release_date\"].split(\"-\")        ]        release_date_precision = album_data[\"release_date_precision\"]        if release_date_precision == \"day\":            year, month, day = date_parts        elif release_date_precision == \"month\":            year, month = date_parts            day = None        elif release_date_precision == \"year\":            year = date_parts[0]            month = None            day = None        else:            raise ui.UserError(                \"Invalid `release_date_precision` returned \"                \"by {} API: '{}'\".format(                    self.data_source, release_date_precision                )            )        tracks_data = album_data[\"tracks\"]        tracks_items = tracks_data[\"items\"]        while tracks_data[\"next\"]:            tracks_data = self._handle_response(                requests.get, tracks_data[\"next\"]            )            tracks_items.extend(tracks_data[\"items\"])        tracks = []        medium_totals = collections.defaultdict(int)        for i, track_data in enumerate(tracks_items, start=1):            track = self._get_track(track_data)            track.index = i            medium_totals[track.medium] += 1            tracks.append(track)        for track in tracks:            track.medium_total = medium_totals[track.medium]        return AlbumInfo(            album=album_data[\"name\"],            album_id=spotify_id,            spotify_album_id=spotify_id,            artist=artist,            artist_id=artist_id,            spotify_artist_id=artist_id,            tracks=tracks,            albumtype=album_data[\"album_type\"],            va=len(album_data[\"artists\"]) == 1            and artist.lower() == \"various artists\",            year=year,            month=month,            day=day,            label=album_data[\"label\"],            mediums=max(medium_totals.keys()),            data_source=self.data_source,            data_url=album_data[\"external_urls\"][\"spotify\"],        )",
        "documentation": "fetch an album by it spotify id or url and return an albuminfo object or none if the album is not found param albumid spotify id or url for the album type albumid str return albuminfo object for album rtype beetsautotaghooksalbuminfo or none"
    },
    {
        "method_code": "def get_segid(self):                return self.segid",
        "documentation": "return the segment identifier"
    },
    {
        "method_code": "def cmd_swap(self, conn, i, j):                i = cast_arg(int, i)        j = cast_arg(int, j)        try:            track_i = self.playlist[i]            track_j = self.playlist[j]        except IndexError:            raise ArgumentIndexError()        self.playlist[j] = track_i        self.playlist[i] = track_j        # Update currently-playing song.        if self.current_index == i:            self.current_index = j        elif self.current_index == j:            self.current_index = i        self.playlist_version += 1        self._send_event(\"playlist\")",
        "documentation": "swap two track in the playlist"
    },
    {
        "method_code": "def _db_source_line(self):                if self.db_source:            output = Record.BASE_FORMAT % \"DBSOURCE\"            output += f\"{self.db_source}\\n\"        else:            output = \"\"        return output",
        "documentation": "output for dbsource line private"
    },
    {
        "method_code": "def test_atom_with_insertion(self):                chain = SeqIO.read(\"PDB/2n0n_M1.pdb\", \"pdb-atom\")        self.assertEqual(chain.seq, \"HAEGKFTSEF\")",
        "documentation": "read a pdb with residue insertion code"
    },
    {
        "method_code": "def update(self, values: Mapping[str, Any]):                for key, value in values.items():            self[key] = value",
        "documentation": "assign all value in the given dict"
    },
    {
        "method_code": "def __init__(self, name=None):                self.id = id  # Unique identifier for the set        self._next_id = 0  # Holds unique ids for graphs        self._graphs = {}  # Holds graphs, keyed by unique id        self.name = name",
        "documentation": "initialize argument name string identifying the graph set sensibly"
    },
    {
        "method_code": "def testRecord(self):                recLine = \"d7hbib_\\t7hbi\\tb:\\t1.001.001.001.001.001\"        rec = Dom.Record(recLine)        self.assertEqual(rec.sid, \"d7hbib_\")        self.assertEqual(rec.residues.pdbid, \"7hbi\")        self.assertEqual(rec.residues.fragments, ((\"b\", \"\", \"\"),))        self.assertEqual(rec.hierarchy, \"1.001.001.001.001.001\")",
        "documentation": "test one record in detail"
    },
    {
        "method_code": "def __getitem__(self, key):                if isinstance(key, numbers.Integral):            return self._get_row(key)        if isinstance(key, slice):            return self._get_rows(key)        sequences = list(self.sequences)        coordinates = self.coordinates.copy()        steps = np.diff(coordinates, 1)        aligned = sum(steps != 0, 0) > 1        # True for steps in which at least two sequences align, False if a gap        for i, sequence in enumerate(sequences):            row = steps[i, aligned]            if (row >= 0).all():                pass            elif (row <= 0).all():                steps[i, :] = -steps[i, :]                coordinates[i, :] = len(sequence) - coordinates[i, :]                sequences[i] = reverse_complement(sequence)                try:                    sequences[i].id = sequence.id                except AttributeError:                    pass            else:                raise ValueError(f\"Inconsistent steps in row {i}\")        gaps = steps.max(0)        if not ((steps == gaps) | (steps <= 0)).all():            raise ValueError(\"Unequal step sizes in alignment\")        m = sum(gaps)        if isinstance(key, tuple):            try:                row, col = key            except ValueError:                raise ValueError(\"only tuples of length 2 can be alignment indices\")        else:            raise TypeError(\"alignment indices must be integers, slices, or tuples\")        if isinstance(col, numbers.Integral):            if col < 0:                col += m            if col < 0 or col >= m:                raise IndexError(                    \"column index %d is out of bounds (%d columns)\" % (col, m)                )        steps = steps[row]        if isinstance(row, numbers.Integral):            sequence = sequences[row]            if isinstance(col, numbers.Integral):                return self._get_row_col(                    coordinates[row, 0], col, steps, gaps, sequence                )            coordinate = coordinates[row, :]            if isinstance(col, slice):                start_index, stop_index, step = col.indices(m)                if start_index < stop_index and step == 1:                    return self._get_row_cols_slice(                        coordinate, start_index, stop_index, steps, gaps, sequence                    )                # make an iterable if step != 1                col = range(start_index, stop_index, step)            return self._get_row_cols_iterable(coordinate, col, gaps, sequence)        if isinstance(row, slice):            sequences = sequences[row]            coordinates = coordinates[row]            if isinstance(col, numbers.Integral):                return self._get_rows_col(coordinates, col, steps, gaps, sequences)            if isinstance(col, slice):                start_index, stop_index, step = col.indices(m)                if start_index < stop_index and step == 1:                    return self._get_rows_cols_slice(                        coordinates,                        row,                        start_index,                        stop_index,                        steps,                        gaps,                    )                # make an iterable if step != 1                col = range(start_index, stop_index, step)            # try if we can use col as an iterable            return self._get_rows_cols_iterable(                coordinates, col, steps, gaps, sequences            )        raise TypeError(\"first index must be an integer or slice\")",
        "documentation": "return selfkey index of the form self return a copy of the alignment object self i self j self ij self iterable where iterable return integer return a new alignment object spanning the selected column selfk i selfk i selfk j selfk ij selfk iterable where iterable return integer selfk equivalent to selfk return a string with the aligned sequence including gap for the selected column where k represents the target and k represents the query sequence and self i return a string with the selected column in the alignment from bioalign import pairwisealigner aligner pairwisealigner alignment aligneralignaccggttt acgggtt alignment alignment printalignment target accggttt query acgggtt blankline alignment accggttt alignment acgggtt alignment accggttt alignment acgggtt alignment ccggt alignment cgggt alignment cc alignment agt alignment range agt alignment aa alignment g alignment doctestellipsis alignment object row x column at x printalignment target ccggttt query cgggtt blankline printalignment target cggttt query gggtt blankline printalignment target ggttt query gggtt blankline printalignment target ggtt query gggtt blankline printalignment target acgtt query agt blankline printalignment range target cgt query cggt blankline printalignment target ctg query tg blankline"
    },
    {
        "method_code": "def test_needle_file(self):                # Setup,        cline = NeedleCommandline(cmd=exes[\"needle\"])        cline.set_parameter(\"-asequence\", \"asis:ACCCGGGCGCGGT\")        cline.set_parameter(\"-bsequence\", \"asis:ACCCGAGCGCGGT\")        cline.set_parameter(\"-gapopen\", \"10\")        cline.set_parameter(\"-gapextend\", \"0.5\")        # EMBOSS would guess this, but let's be explicit:        cline.set_parameter(\"-snucleotide\", \"True\")        cline.set_parameter(\"-outfile\", \"Emboss/temp with space.needle\")        self.assertEqual(str(eval(repr(cline))), str(cline))        # Run the tool,        stdout, stderr = cline()        # Check it worked,        self.assertTrue(            stderr.strip().startswith(\"Needleman-Wunsch global alignment\"), stderr        )        self.assertEqual(stdout.strip(), \"\")        filename = cline.outfile        self.assertTrue(            os.path.isfile(filename),            f\"Missing output file {filename!r} from:\\n{cline}\",        )        # Check we can parse the output...        align = AlignIO.read(filename, \"emboss\")        self.assertEqual(len(align), 2)        self.assertEqual(align[0].seq, \"ACCCGGGCGCGGT\")        self.assertEqual(align[1].seq, \"ACCCGAGCGCGGT\")        # Clean up,        os.remove(filename)",
        "documentation": "run needle with the asis trick output to a file"
    },
    {
        "method_code": "def get_queue(self):                return [            PackageData(                pack[\"id\"],                pack[\"name\"],                pack[\"folder\"],                pack[\"site\"],                pack[\"password\"],                pack[\"queue\"],                pack[\"order\"],                pack[\"linksdone\"],                pack[\"sizedone\"],                pack[\"sizetotal\"],                pack[\"linkstotal\"],            )            for pack in self.pyload.files.get_info_data(Destination.QUEUE).values()        ]",
        "documentation": "return info about queue and package not about file see getqueuedata or getpackagedata instead return list of packageinfo"
    },
    {
        "method_code": "def test__conda_user_story__disable_notices(    capsys,    notices_cache_dir,    notices_mock_fetch_get_session,    disable_channel_notices,):        messages = (\"Test One\", \"Test Two\")    dummy_mesg = \"Dummy Mesg\"    messages_json = get_test_notices(messages)    add_resp_to_mock(notices_mock_fetch_get_session, 200, messages_json)    @notices.notices    def dummy(args, parser):        print(dummy_mesg)    dummy_args = DummyArgs()    dummy(dummy_args, None)    captured = capsys.readouterr()    notices_decorator_assert_message_in_stdout(        captured, messages=messages, dummy_mesg=dummy_mesg, not_in=True    )",
        "documentation": "a a conda user if i disable channel notification in my condarc file i do not want to see notification while running command like install update or create"
    },
    {
        "method_code": "def download_finished(self, pyfile):                file = pyfile.plugin.last_download        args = [pyfile.id, pyfile.name, file, pyfile.pluginname, pyfile.url, pyfile.package().name]        self.call_script(\"download_finished\", *args)",
        "documentation": "download successfully finished"
    },
    {
        "method_code": "def execute_and_fetchall(self, sql, args=None):                out = super().execute_and_fetchall(sql, args)        return [tuple(self._bytearray_to_str(v) for v in o) for o in out]",
        "documentation": "return a list of tuples of all row"
    },
    {
        "method_code": "def __init__(self):                self.init_alphabet()        self.sequence = \"\"",
        "documentation": "set up the alphabet"
    },
    {
        "method_code": "def test_str(self):                expected = (            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Diagram.Diagram'>: Test Diagram>\"            \"\\n1 tracks\"            \"\\nTrack 1: \"            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Track.Track'>: CDS Features>\"            \"\\n0 sets\"            \"\\n\"        )        self.assertEqual(expected, str(self.gdd))",
        "documentation": "test diagram info a string"
    },
    {
        "method_code": "def sync_data(self):                self.data = self.data_file.get_data()",
        "documentation": "synchronize context data from the designated datafile if any"
    },
    {
        "method_code": "def diff(self):                prechange_data = self.prechange_data_clean        postchange_data = self.postchange_data_clean        # Determine which attributes have changed        if self.action == ObjectChangeActionChoices.ACTION_CREATE:            changed_attrs = sorted(postchange_data.keys())        elif self.action == ObjectChangeActionChoices.ACTION_DELETE:            changed_attrs = sorted(prechange_data.keys())        else:            # TODO: Support deep (recursive) comparison            changed_data = shallow_compare_dict(prechange_data, postchange_data)            changed_attrs = sorted(changed_data.keys())        return {            'pre': {                k: prechange_data.get(k) for k in changed_attrs            },            'post': {                k: postchange_data.get(k) for k in changed_attrs            },        }",
        "documentation": "return a dictionary of pre and postchange value for attribute which have changed"
    },
    {
        "method_code": "def remove_items(lib, query, album, delete, force):        # Get the matching items.    items, albums = _do_query(lib, query, album)    objs = albums if album else items    # Confirm file removal if not forcing removal.    if not force:        # Prepare confirmation with user.        album_str = (            \" in {} album{}\".format(len(albums), \"s\" if len(albums) > 1 else \"\")            if album            else \"\"        )        if delete:            fmt = \"$path - $title\"            prompt = \"Really DELETE\"            prompt_all = \"Really DELETE {} file{}{}\".format(                len(items), \"s\" if len(items) > 1 else \"\", album_str            )        else:            fmt = \"\"            prompt = \"Really remove from the library?\"            prompt_all = \"Really remove {} item{}{} from the library?\".format(                len(items), \"s\" if len(items) > 1 else \"\", album_str            )        # Helpers for printing affected items        def fmt_track(t):            ui.print_(format(t, fmt))        def fmt_album(a):            ui.print_()            for i in a.items():                fmt_track(i)        fmt_obj = fmt_album if album else fmt_track        # Show all the items.        for o in objs:            fmt_obj(o)        # Confirm with user.        objs = ui.input_select_objects(            prompt, objs, fmt_obj, prompt_all=prompt_all        )    if not objs:        return    # Remove (and possibly delete) items.    with lib.transaction():        for obj in objs:            obj.remove(delete)",
        "documentation": "remove item matching query from lib if album then match and remove whole album if delete also remove file from disk"
    },
    {
        "method_code": "def __init__(self, model, radius=12, offset=0):                _AbstractHSExposure.__init__(            self, model, radius, offset, \"EXP_HSE_B_U\", \"EXP_HSE_B_D\"        )",
        "documentation": "initialize class param model the model that contains the residue type model lmodel param radius radius of the sphere centred at the ca atom type radius float param offset number of flanking residue that are ignored in the calculation of the number of neighbor type offset int"
    },
    {
        "method_code": "def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ) -> None:                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"FASTA\")        for line in self.stream:            if line[0] not in \"#!;\":                if not line.startswith(\">\"):                    raise ValueError(                        \"Expected FASTA record starting with '>' character.\\n\"                        \"If this line is a comment, please use '#', '!', or ';' as \"                        \"the first character, or use the 'fasta-pearson' \"                        \"format for parsing.\\n\"                        f\"Got: '{line}'\"                    )                self._line = line                break        else:            self._line = None",
        "documentation": "iterate over fasta record a seqrecord object argument source input stream opened in text mode or a path to a file alphabet optional alphabet not used leave a none this parser expects the data to be in fasta format a in blast line starting with or are interpreted a comment and ignored this iterator act like calling bioseqioparsehandle fastablast with no custom handling of the title line with openfastadupsfasta a handle for record in fastaiteratorhandle printrecordid alpha beta gamma alpha delta if you want to modify the record before writing for example to change the id of each record you can use a generator function a follows def modifyrecordsrecords for record in record recordid recordidupper yield record with openfastadupsfasta a handle for record in modifyrecordsfastaiteratorhandle printrecordid alpha beta gamma alpha delta"
    },
    {
        "method_code": "def issue_has_new_comment(issue, new_comment_interval_days, headers: dict) -> bool:        comments_url = issue[\"comments_url\"]    response = httpx.get(comments_url, headers=headers)    response.raise_for_status()    comments = response.json()    if comments:        latest_comment = max(comments, key=lambda comment: comment[\"created_at\"])        latest_comment_date = datetime.strptime(            latest_comment[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\"        )        if latest_comment_date > datetime.utcnow() - timedelta(            days=new_comment_interval_days        ):            print(f\"Issue #{issue['number']} has a new comment\")            return True    return False",
        "documentation": "check if an issue ha a new comment within the specified interval args issue dict the issue to check newcommentintervaldays int interval in day to check for new comment header dict http header for github api request return bool true if there is a new comment within the interval false otherwise"
    },
    {
        "method_code": "def _get_strand_from_desc(desc, is_protein, modify_desc=True):        if is_protein:        return \".\", desc    suffix = \"\"    if desc.endswith(\"[revcomp]\"):        suffix = \":[revcomp]\" if desc.endswith(\":[revcomp]\") else \"[revcomp]\"    if not suffix:        return \"+\", desc    if modify_desc:        return \"-\", desc[: -len(suffix)]    return \"-\", desc",
        "documentation": "determine the strand from the description private exonerate appends revcomp version or revcomp version to the query andor hit description string this function output if the description ha such modification or if not if the query andor hit is a protein sequence a is output instead aside from the strand the input description value is also returned it is returned unmodified if modifydesc is false otherwise the appended revcomp or revcomp is removed"
    },
    {
        "method_code": "def handle_cf_renamed(instance, created, **kwargs):        if not created and instance.name != instance._name:        instance.rename_object_data(old_name=instance._name, new_name=instance.name)",
        "documentation": "handle the renaming of custom field data on object when a customfield is renamed"
    },
    {
        "method_code": "def test_table12(self):                dna_table = unambiguous_dna_by_id[12]        nuc_table = generic_by_id[12]        self.assertEqual(generic_by_name[\"Alternative Yeast Nuclear\"].id, 12)        self.assertIn(\"Alternative Yeast Nuclear\", nuc_table.names)        self.assertEqual(len(dna_table.start_codons), 2)        self.assertEqual(len(dna_table.stop_codons), 3)        self.assertEqual(nuc_table.forward_table[\"CUG\"], \"S\")",
        "documentation": "check table alternative yeast nuclear ctg code for s instead of l"
    },
    {
        "method_code": "def tag(value, viewname=None):        return {        'tag': value,        'viewname': viewname,    }",
        "documentation": "display a tag optionally linked to a filtered list of object args value a tag instance viewname if provided the tag will be a hyperlink to the specified view url"
    },
    {
        "method_code": "def is_defined(cls):                return True",
        "documentation": "return if recognition sequence and cut are defined true if the sequence recognised and cut is constant ie the recognition site is not degenerated and the enzyme cut inside the site related method reisambiguous reisunknown"
    },
    {
        "method_code": "def raise_for_reserved_arguments(fn: Callable, reserved_arguments: Iterable[str]):        function_paremeters = inspect.signature(fn).parameters    for argument in reserved_arguments:        if argument in function_paremeters:            raise ReservedArgumentError(                f\"{argument!r} is a reserved argument name and cannot be used.\"            )",
        "documentation": "raise a reservedargumenterror if fn ha any parameter that conflict with the name contained in reservedarguments"
    },
    {
        "method_code": "def test_random_bam_ex1(self):                self.check_random(\"SamBam/ex1.bam\")",
        "documentation": "check random access to sambamexbam"
    },
    {
        "method_code": "def status(self):                return self.get(\"status\")",
        "documentation": "return the current status of the mpd"
    },
    {
        "method_code": "def prefect_db():        try:        with prefect_test_harness():            yield    except OSError as e:        if \"Directory not empty\" in str(e):            pass        else:            raise e",
        "documentation": "set up test harness for temporary db during test run"
    },
    {
        "method_code": "def test_read5(self):                filename = os.path.join(\"Prosite\", \"ps00488.txt\")        with open(filename) as handle:            record = Prosite.read(handle)        self.assertEqual(record.name, \"PAL_HISTIDASE\")        self.assertEqual(record.type, \"PATTERN\")        self.assertEqual(record.accession, \"PS00488\")        self.assertEqual(record.created, \"MAY-1991\")        self.assertEqual(record.data_update, \"DEC-2004\")        self.assertEqual(record.info_update, \"MAR-2006\")        self.assertEqual(record.pdoc, \"PDOC00424\")        self.assertEqual(            record.description, \"Phenylalanine and histidine ammonia-lyases signature.\"        )        self.assertEqual(            record.pattern,            \"[GS]-[STG]-[LIVM]-[STG]-[SAC]-S-G-[DH]-L-x-P-L-[SA]-x(2,3)-[SAGVT].\",        )        self.assertEqual(record.matrix, [])        self.assertEqual(record.rules, [])        self.assertEqual(record.nr_sp_release, \"49.3\")        self.assertEqual(record.nr_sp_seqs, 212425)        self.assertEqual(record.cc_taxo_range, \"A?EP?\")        self.assertEqual(record.cc_max_repeat, \"1\")        self.assertEqual(len(record.cc_site), 1)        self.assertEqual(record.cc_site[0], (6, \"active_site\"))        self.assertEqual(len(record.dr_positive), 127)        self.assertEqual(record.dr_positive[0], (\"Q8RFC2\", \"HUTH1_FUSNN\"))        self.assertEqual(record.dr_positive[1], (\"Q8RDU4\", \"HUTH2_FUSNN\"))        self.assertEqual(record.dr_positive[2], (\"Q9KWE4\", \"HUTH_AGRRH\"))        self.assertEqual(record.dr_positive[3], (\"Q8U8Z7\", \"HUTH_AGRT5\"))        self.assertEqual(record.dr_positive[4], (\"Q5NZX8\", \"HUTH_AZOSE\"))        self.assertEqual(record.dr_positive[5], (\"Q81Y45\", \"HUTH_BACAN\"))        self.assertEqual(record.dr_positive[6], (\"Q733H8\", \"HUTH_BACC1\"))        self.assertEqual(record.dr_positive[7], (\"Q81AC6\", \"HUTH_BACCR\"))        self.assertEqual(record.dr_positive[8], (\"Q9KBE6\", \"HUTH_BACHD\"))        self.assertEqual(record.dr_positive[9], (\"Q6HFE9\", \"HUTH_BACHK\"))        self.assertEqual(record.dr_positive[10], (\"Q5WAZ6\", \"HUTH_BACSK\"))        self.assertEqual(record.dr_positive[11], (\"P10944\", \"HUTH_BACSU\"))        self.assertEqual(record.dr_positive[12], (\"Q6G3U8\", \"HUTH_BARHE\"))        self.assertEqual(record.dr_positive[13], (\"Q6FZP9\", \"HUTH_BARQU\"))        self.assertEqual(record.dr_positive[14], (\"Q89GV3\", \"HUTH_BRAJA\"))        self.assertEqual(record.dr_positive[15], (\"Q579E8\", \"HUTH_BRUAB\"))        self.assertEqual(record.dr_positive[16], (\"Q8FVB4\", \"HUTH_BRUSU\"))        self.assertEqual(record.dr_positive[17], (\"Q62LJ6\", \"HUTH_BURMA\"))        self.assertEqual(record.dr_positive[18], (\"Q63SH6\", \"HUTH_BURPS\"))        self.assertEqual(record.dr_positive[19], (\"Q20502\", \"HUTH_CAEEL\"))        self.assertEqual(record.dr_positive[20], (\"P58082\", \"HUTH_CAUCR\"))        self.assertEqual(record.dr_positive[21], (\"Q7P188\", \"HUTH_CHRVO\"))        self.assertEqual(record.dr_positive[22], (\"Q891Q1\", \"HUTH_CLOTE\"))        self.assertEqual(record.dr_positive[23], (\"Q9RZ06\", \"HUTH_DEIRA\"))        self.assertEqual(record.dr_positive[24], (\"Q6AKP3\", \"HUTH_DESPS\"))        self.assertEqual(record.dr_positive[25], (\"Q5L310\", \"HUTH_GEOKA\"))        self.assertEqual(record.dr_positive[26], (\"Q7NCB3\", \"HUTH_GLOVI\"))        self.assertEqual(record.dr_positive[27], (\"Q5FRR8\", \"HUTH_GLUOX\"))        self.assertEqual(record.dr_positive[28], (\"Q9HQD5\", \"HUTH_HALSA\"))        self.assertEqual(record.dr_positive[29], (\"P42357\", \"HUTH_HUMAN\"))        self.assertEqual(record.dr_positive[30], (\"Q5QV30\", \"HUTH_IDILO\"))        self.assertEqual(record.dr_positive[31], (\"Q5X5I5\", \"HUTH_LEGPA\"))        self.assertEqual(record.dr_positive[32], (\"Q5ZVR0\", \"HUTH_LEGPH\"))        self.assertEqual(record.dr_positive[33], (\"Q5WWW8\", \"HUTH_LEGPL\"))        self.assertEqual(record.dr_positive[34], (\"P35492\", \"HUTH_MOUSE\"))        self.assertEqual(record.dr_positive[35], (\"Q7N296\", \"HUTH_PHOLL\"))        self.assertEqual(record.dr_positive[36], (\"Q6LQ56\", \"HUTH_PHOPR\"))        self.assertEqual(record.dr_positive[37], (\"Q9HU85\", \"HUTH_PSEAE\"))        self.assertEqual(record.dr_positive[38], (\"Q8VMR3\", \"HUTH_PSEFL\"))        self.assertEqual(record.dr_positive[39], (\"Q88CZ7\", \"HUTH_PSEPK\"))        self.assertEqual(record.dr_positive[40], (\"P21310\", \"HUTH_PSEPU\"))        self.assertEqual(record.dr_positive[41], (\"Q87UM1\", \"HUTH_PSESM\"))        self.assertEqual(record.dr_positive[42], (\"Q8XW29\", \"HUTH_RALSO\"))        self.assertEqual(record.dr_positive[43], (\"P21213\", \"HUTH_RAT\"))        self.assertEqual(record.dr_positive[44], (\"Q983I0\", \"HUTH_RHILO\"))        self.assertEqual(record.dr_positive[45], (\"O31197\", \"HUTH_RHIME\"))        self.assertEqual(record.dr_positive[46], (\"Q57RG6\", \"HUTH_SALCH\"))        self.assertEqual(record.dr_positive[47], (\"Q5PG61\", \"HUTH_SALPA\"))        self.assertEqual(record.dr_positive[48], (\"Q8Z896\", \"HUTH_SALTI\"))        self.assertEqual(record.dr_positive[49], (\"Q8ZQQ9\", \"HUTH_SALTY\"))        self.assertEqual(record.dr_positive[50], (\"Q5LRD8\", \"HUTH_SILPO\"))        self.assertEqual(record.dr_positive[51], (\"Q5HJY8\", \"HUTH_STAAC\"))        self.assertEqual(record.dr_positive[52], (\"P64415\", \"HUTH_STAAM\"))        self.assertEqual(record.dr_positive[53], (\"P64416\", \"HUTH_STAAN\"))        self.assertEqual(record.dr_positive[54], (\"Q6GKT7\", \"HUTH_STAAR\"))        self.assertEqual(record.dr_positive[55], (\"Q6GD82\", \"HUTH_STAAS\"))        self.assertEqual(record.dr_positive[56], (\"Q8NYY3\", \"HUTH_STAAW\"))        self.assertEqual(record.dr_positive[57], (\"Q93TX3\", \"HUTH_STIAU\"))        self.assertEqual(record.dr_positive[58], (\"Q82I33\", \"HUTH_STRAW\"))        self.assertEqual(record.dr_positive[59], (\"Q9EWW1\", \"HUTH_STRCO\"))        self.assertEqual(record.dr_positive[60], (\"P24221\", \"HUTH_STRGR\"))        self.assertEqual(record.dr_positive[61], (\"P58083\", \"HUTH_STRP1\"))        self.assertEqual(record.dr_positive[62], (\"Q8K5L5\", \"HUTH_STRP3\"))        self.assertEqual(record.dr_positive[63], (\"Q5X9K4\", \"HUTH_STRP6\"))        self.assertEqual(record.dr_positive[64], (\"Q8NZ46\", \"HUTH_STRP8\"))        self.assertEqual(record.dr_positive[65], (\"Q67JH4\", \"HUTH_SYMTH\"))        self.assertEqual(record.dr_positive[66], (\"Q9HLI6\", \"HUTH_THEAC\"))        self.assertEqual(record.dr_positive[67], (\"Q8RBH4\", \"HUTH_THETN\"))        self.assertEqual(record.dr_positive[68], (\"Q978N8\", \"HUTH_THEVO\"))        self.assertEqual(record.dr_positive[69], (\"Q9KSQ4\", \"HUTH_VIBCH\"))        self.assertEqual(record.dr_positive[70], (\"Q5E0C6\", \"HUTH_VIBF1\"))        self.assertEqual(record.dr_positive[71], (\"Q87Q77\", \"HUTH_VIBPA\"))        self.assertEqual(record.dr_positive[72], (\"Q8DA21\", \"HUTH_VIBVU\"))        self.assertEqual(record.dr_positive[73], (\"Q7MK58\", \"HUTH_VIBVY\"))        self.assertEqual(record.dr_positive[74], (\"Q8PLZ8\", \"HUTH_XANAC\"))        self.assertEqual(record.dr_positive[75], (\"Q8PAA7\", \"HUTH_XANCP\"))        self.assertEqual(record.dr_positive[76], (\"Q8ZA10\", \"HUTH_YERPE\"))        self.assertEqual(record.dr_positive[77], (\"Q664B8\", \"HUTH_YERPS\"))        self.assertEqual(record.dr_positive[78], (\"Q8YD09\", \"HUTIH_BRUME\"))        self.assertEqual(record.dr_positive[79], (\"P35510\", \"PAL1_ARATH\"))        self.assertEqual(record.dr_positive[80], (\"O23865\", \"PAL1_DAUCA\"))        self.assertEqual(record.dr_positive[81], (\"P14166\", \"PAL1_IPOBA\"))        self.assertEqual(record.dr_positive[82], (\"O49835\", \"PAL1_LITER\"))        self.assertEqual(record.dr_positive[83], (\"P35511\", \"PAL1_LYCES\"))        self.assertEqual(record.dr_positive[84], (\"P14717\", \"PAL1_ORYSA\"))        self.assertEqual(record.dr_positive[85], (\"Q01861\", \"PAL1_PEA\"))        self.assertEqual(record.dr_positive[86], (\"P24481\", \"PAL1_PETCR\"))        self.assertEqual(record.dr_positive[87], (\"P45731\", \"PAL1_POPKI\"))        self.assertEqual(record.dr_positive[88], (\"O64963\", \"PAL1_PRUAV\"))        self.assertEqual(record.dr_positive[89], (\"Q9M568\", \"PAL1_RUBID\"))        self.assertEqual(record.dr_positive[90], (\"P31425\", \"PAL1_SOLTU\"))        self.assertEqual(record.dr_positive[91], (\"P27991\", \"PAL1_SOYBN\"))        self.assertEqual(record.dr_positive[92], (\"P25872\", \"PAL1_TOBAC\"))        self.assertEqual(record.dr_positive[93], (\"P45724\", \"PAL2_ARATH\"))        self.assertEqual(record.dr_positive[94], (\"Q9SMK9\", \"PAL2_CICAR\"))        self.assertEqual(record.dr_positive[95], (\"Q42858\", \"PAL2_IPOBA\"))        self.assertEqual(record.dr_positive[96], (\"O49836\", \"PAL2_LITER\"))        self.assertEqual(record.dr_positive[97], (\"P53443\", \"PAL2_ORYSA\"))        self.assertEqual(record.dr_positive[98], (\"Q04593\", \"PAL2_PEA\"))        self.assertEqual(record.dr_positive[99], (\"P45728\", \"PAL2_PETCR\"))        self.assertEqual(record.dr_positive[100], (\"P19142\", \"PAL2_PHAVU\"))        self.assertEqual(record.dr_positive[101], (\"Q43052\", \"PAL2_POPKI\"))        self.assertEqual(record.dr_positive[102], (\"P31426\", \"PAL2_SOLTU\"))        self.assertEqual(record.dr_positive[103], (\"P35513\", \"PAL2_TOBAC\"))        self.assertEqual(record.dr_positive[104], (\"P45725\", \"PAL3_ARATH\"))        self.assertEqual(record.dr_positive[105], (\"P45729\", \"PAL3_PETCR\"))        self.assertEqual(record.dr_positive[106], (\"P19143\", \"PAL3_PHAVU\"))        self.assertEqual(record.dr_positive[107], (\"P45733\", \"PAL3_TOBAC\"))        self.assertEqual(record.dr_positive[108], (\"Q9SS45\", \"PAL4_ARATH\"))        self.assertEqual(record.dr_positive[109], (\"Q40910\", \"PAL4_POPKI\"))        self.assertEqual(record.dr_positive[110], (\"P26600\", \"PAL5_LYCES\"))        self.assertEqual(record.dr_positive[111], (\"O93967\", \"PALY_AMAMU\"))        self.assertEqual(record.dr_positive[112], (\"Q42609\", \"PALY_BROFI\"))        self.assertEqual(record.dr_positive[113], (\"P45726\", \"PALY_CAMSI\"))        self.assertEqual(record.dr_positive[114], (\"Q42667\", \"PALY_CITLI\"))        self.assertEqual(record.dr_positive[115], (\"O23924\", \"PALY_DIGLA\"))        self.assertEqual(record.dr_positive[116], (\"O04058\", \"PALY_HELAN\"))        self.assertEqual(record.dr_positive[117], (\"P27990\", \"PALY_MEDSA\"))        self.assertEqual(record.dr_positive[118], (\"P45727\", \"PALY_PERAE\"))        self.assertEqual(record.dr_positive[119], (\"P52777\", \"PALY_PINTA\"))        self.assertEqual(record.dr_positive[120], (\"P45730\", \"PALY_POPTR\"))        self.assertEqual(record.dr_positive[121], (\"P10248\", \"PALY_RHORB\"))        self.assertEqual(record.dr_positive[122], (\"P11544\", \"PALY_RHOTO\"))        self.assertEqual(record.dr_positive[123], (\"P45732\", \"PALY_STYHU\"))        self.assertEqual(record.dr_positive[124], (\"P45734\", \"PALY_TRISU\"))        self.assertEqual(record.dr_positive[125], (\"Q96V77\", \"PALY_USTMA\"))        self.assertEqual(record.dr_positive[126], (\"Q43210\", \"PALY_WHEAT\"))        self.assertEqual(len(record.dr_false_neg), 2)        self.assertEqual(record.dr_false_neg[0], (\"Q8EKJ4\", \"HUTH_SHEON\"))        self.assertEqual(record.dr_false_neg[1], (\"Q73Q56\", \"HUTH_TREDE\"))        self.assertEqual(len(record.dr_false_pos), 0)        self.assertEqual(len(record.dr_potential), 4)        self.assertEqual(record.dr_potential[0], (\"P07218\", \"PAL1_PHAVU\"))        self.assertEqual(record.dr_potential[1], (\"Q92195\", \"PALY_AGABI\"))        self.assertEqual(record.dr_potential[2], (\"P35512\", \"PALY_MALDO\"))        self.assertEqual(record.dr_potential[3], (\"P45735\", \"PALY_VITVI\"))        self.assertEqual(len(record.dr_unknown), 0)        self.assertEqual(record.pdb_structs[0], \"1B8F\")        self.assertEqual(record.pdb_structs[1], \"1EB4\")        self.assertEqual(record.pdb_structs[2], \"1GK2\")        self.assertEqual(record.pdb_structs[3], \"1GKJ\")        self.assertEqual(record.pdb_structs[4], \"1GKM\")        self.assertEqual(record.pdb_structs[5], \"1Y2M\")",
        "documentation": "parsing prosite record pstxt"
    },
    {
        "method_code": "def __iter__(self):                # TODO - Iterate over the cursor, much more efficient        return iter(self.adaptor.list_biodatabase_names())",
        "documentation": "iterate over namespaces subdatabases in the database"
    },
    {
        "method_code": "def add_files(self, package_id, links):                self.pyload.files.add_links(links, int(package_id))        self.pyload.log.info(            self._(\"Added {count:d} links to package #{package:d} \").format(                count=len(links), package=package_id            )        )        self.pyload.files.save()",
        "documentation": "add file to specific package param packageid package id param link list of url"
    },
    {
        "method_code": "def assertAnalysisFormat(self, analysis, expected):                dct = analysis.mapping        ls, nc = [], []        for k, v in dct.items():            if v:                ls.append((k, v))            else:                nc.append(k)        result = analysis.make_format(ls, \"\", [], \"\")        self.assertEqual(result.replace(\" \", \"\"), expected.replace(\" \", \"\"))",
        "documentation": "test makeformat test that the restrictionanalysis makeformatprintthat match some string"
    },
    {
        "method_code": "def __init__(self):                self.matrix = \"\"        self.gap_penalties = (None, None)        self.sc_match = None        self.sc_mismatch = None        self.num_hits = None        self.num_sequences = None        self.num_good_extends = None        self.num_seqs_better_e = None        self.hsps_no_gap = None        self.hsps_prelim_gapped = None        self.hsps_prelim_gapped_attemped = None        self.hsps_gapped = None        self.query_id = None        self.query_length = None        self.database_length = None        self.effective_hsp_length = None        self.effective_query_length = None        self.effective_database_length = None        self.effective_search_space = None        self.effective_search_space_used = None        self.frameshift = (None, None)        self.threshold = None        self.window_size = None        self.dropoff_1st_pass = (None, None)        self.gap_x_dropoff = (None, None)        self.gap_x_dropoff_final = (None, None)        self.gap_trigger = (None, None)        self.blast_cutoff = (None, None)",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def __iter__(self):                return iter(self.__next__, None)",
        "documentation": "iterate over the record"
    },
    {
        "method_code": "def evaluate_template(        self,        template: Union[str, functemplate.Template],        for_path: bool = False,    ) -> str:                # Perform substitution.        if isinstance(template, str):            t = functemplate.template(template)        else:            # Help out mypy            t = template        return t.substitute(            self.formatted(for_path=for_path), self._template_funcs()        )",
        "documentation": "evaluate a template a string or a template object using the object field if forpath is true then no new path separator will be added to the template"
    },
    {
        "method_code": "def _scrub(self, path):                for cls in self._mutagen_classes():            # Try opening the file with this type, but just skip in the            # event of any error.            try:                f = cls(util.syspath(path))            except Exception:                continue            if f.tags is None:                continue            # Remove the tag for this type.            try:                f.delete()            except NotImplementedError:                # Some Mutagen metadata subclasses (namely, ASFTag) do not                # support .delete(), presumably because it is impossible to                # remove them. In this case, we just remove all the tags.                for tag in f.keys():                    del f[tag]                f.save()            except (OSError, mutagen.MutagenError) as exc:                self._log.error(                    \"could not scrub {0}: {1}\", util.displayable_path(path), exc                )",
        "documentation": "remove all tag from a file"
    },
    {
        "method_code": "def startDocument(self):                self.startElementNS = self.startSeqXMLElement",
        "documentation": "set xml handler when an xml declaration is found"
    },
    {
        "method_code": "def _add_seqfeature_dbxref(self, seqfeature_id, dbxref_id, rank):                sql = (            \"INSERT INTO seqfeature_dbxref \"            '(seqfeature_id, dbxref_id, \"rank\") VALUES'            \"(%s, %s, %s)\"        )        self.adaptor.execute(sql, (seqfeature_id, dbxref_id, rank))        return (seqfeature_id, dbxref_id)",
        "documentation": "add db crossreference private insert a seqfeaturedbxref row and return the seqfeatureid and dbxrefid"
    },
    {
        "method_code": "def prepare_for_flow_run(        self,        flow_run: \"FlowRun\",        deployment: Optional[\"DeploymentResponse\"] = None,        flow: Optional[\"Flow\"] = None,    ):                super().prepare_for_flow_run(flow_run, deployment, flow)        self.image = self.image or get_prefect_image_name()        self.labels = self._convert_labels_to_docker_format(            {**self.labels, **CONTAINER_LABELS}        )        self.name = self._slugify_container_name()",
        "documentation": "prepares the agent for a flow run by setting the image label and name attribute"
    },
    {
        "method_code": "def _make_release_from_positions(self, positions):                tracks = [            self._make_track(\"TITLE%s\" % i, position)            for (i, position) in enumerate(positions, start=1)        ]        return self._make_release(tracks)",
        "documentation": "return a bag that mimic a discogsclientrelease with a tracklist where track have the specified position"
    },
    {
        "method_code": "def _set_hsp_align_len(self):                self._hsp.align_length = int(self._value)",
        "documentation": "record the length of the alignment private"
    },
    {
        "method_code": "def acquire_lock(        self, key: str, holder: Optional[str] = None, timeout: Optional[float] = None    ) -> bool:                holder = holder or self.generate_default_holder()        if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.acquire_lock(key, holder, timeout)",
        "documentation": "acquire a lock for a result record args key the key to acquire the lock for holder the holder of the lock if not provided a default holder based on the current host process and thread will be used timeout the timeout for the lock return bool true if the lock wa successfully acquired false otherwise"
    },
    {
        "method_code": "def album_candidates(    items: List[Item],    artist: str,    album: str,    va_likely: bool,    extra_tags: Dict,) -> Iterable[Tuple]:        if config[\"musicbrainz\"][\"enabled\"]:        # Base candidates if we have album and artist to match.        if artist and album:            yield from invoke_mb(                mb.match_album, artist, album, len(items), extra_tags            )        # Also add VA matches from MusicBrainz where appropriate.        if va_likely and album:            yield from invoke_mb(                mb.match_album, None, album, len(items), extra_tags            )    # Candidates from plugins.    yield from plugins.candidates(items, artist, album, va_likely, extra_tags)",
        "documentation": "search for album match item is a list of item object that make up the album artist and album are the respective name string which may be derived from the item list or may be entered by the user valikely is a boolean indicating whether the album is likely to be a various artist release extratags is an optional dictionary of additional tag used to further constrain the search"
    },
    {
        "method_code": "def six_frame_translations(seq, genetic_code=1):      # noqa for pep8 W291 trailing whitespace    from Bio.Seq import reverse_complement    from Bio.Seq import reverse_complement_rna    if \"u\" in seq.lower():        anti = reverse_complement_rna(seq)    else:        anti = reverse_complement(seq)    comp = anti[::-1]    length = len(seq)    frames = {}    for i in range(3):        fragment_length = 3 * ((length - i) // 3)        frames[i + 1] = translate(seq[i : i + fragment_length], genetic_code)        frames[-(i + 1)] = translate(anti[i : i + fragment_length], genetic_code)[::-1]    # create header    if length > 20:        short = f\"{seq[:10]} ... {seq[-10:]}\"    else:        short = seq    header = \"GC_Frame:\"    for nt in [\"a\", \"t\", \"g\", \"c\"]:        header += \" %s:%d\" % (nt, seq.count(nt.upper()))    gc = 100 * gc_fraction(seq, ambiguous=\"ignore\")    header += \"\\nSequence: %s, %d nt, %0.2f %%GC\\n\\n\\n\" % (        short.lower(),        length,        gc,    )    res = header    for i in range(0, length, 60):        subseq = seq[i : i + 60]        csubseq = comp[i : i + 60]        p = i // 3        res += \"%d/%d\\n\" % (i + 1, i / 3 + 1)        res += \"  \" + \"  \".join(frames[3][p : p + 20]) + \"\\n\"        res += \" \" + \"  \".join(frames[2][p : p + 20]) + \"\\n\"        res += \"  \".join(frames[1][p : p + 20]) + \"\\n\"        # seq        res += subseq.lower() + \"%5d %%\\n\" % int(gc)        res += csubseq.lower() + \"\\n\"        # - frames        res += \"  \".join(frames[-2][p : p + 20]) + \"\\n\"        res += \" \" + \"  \".join(frames[-1][p : p + 20]) + \"\\n\"        res += \"  \" + \"  \".join(frames[-3][p : p + 20]) + \"\\n\\n\"    return res",
        "documentation": "return pretty string showing the frame translation and gc content nice looking frame translation with gc content code from xbbtools similar to dna strider sixframe translation from biosequtils import sixframetranslations printsixframetranslationsauggccauuguaaugggccgcuga gcframe a t g c sequence auggccauug gggccgcuga nt gc blankline blankline g h c n g p l w p l w a a m a i v m g r auggccauuguaaugggccgcuga uaccgguaacauuacccggcgacu a m t i p r q h g n y h a a s p w q l p g s blankline blankline"
    },
    {
        "method_code": "def _validate(self):                for p in self.parameters:            # Check for missing required parameters:            if p.is_required and not (p.is_set):                raise ValueError(f\"Parameter {p.names[-1]} is not set.\")",
        "documentation": "make sure the required parameter have been set private no return value it either work or raise a valueerror this is a separate method called from str so that subclass may override it"
    },
    {
        "method_code": "def test_not_equal_comparsion(self):                self.assertNotEqual(Seq.Seq(\"TCAAA\"), Seq.Seq(\"TCAAAA\"))",
        "documentation": "test ne comparison method"
    },
    {
        "method_code": "def as_string(value: Any) -> str:        if value is None:        return \"\"    elif isinstance(value, memoryview):        return bytes(value).decode(\"utf-8\", \"ignore\")    elif isinstance(value, bytes):        return value.decode(\"utf-8\", \"ignore\")    else:        return str(value)",
        "documentation": "convert a value to a unicode object for matching with a query none becomes the empty string bytestrings are silently decoded"
    },
    {
        "method_code": "def __init__(self, to_process=keys_to_process):                self._to_process = to_process",
        "documentation": "initialize with the key we should deal with"
    },
    {
        "method_code": "def handle_407(response, **kwargs):  # pragma: no cover                # kwargs = {'verify': True, 'cert': None, 'proxies': {}, 'stream': False,        #           'timeout': (3.05, 60)}        if response.status_code != 407:            return response        # Consume content and release the original connection        # to allow our new request to reuse the same one.        response.content        response.close()        proxies = kwargs.pop(\"proxies\")        proxy_scheme = urlparse(response.url).scheme        if proxy_scheme not in proxies:            raise ProxyError(                dals(                    f                )            )        # fix-up proxy_url with username & password        proxy_url = proxies[proxy_scheme]        username, password = get_proxy_username_and_pass(proxy_scheme)        proxy_url = add_username_and_password(proxy_url, username, password)        proxy_authorization_header = _basic_auth_str(username, password)        proxies[proxy_scheme] = proxy_url        kwargs[\"proxies\"] = proxies        prep = response.request.copy()        extract_cookies_to_jar(prep._cookies, response.request, response.raw)        prep.prepare_cookies(prep._cookies)        prep.headers[\"Proxy-Authorization\"] = proxy_authorization_header        _response = response.connection.send(prep, **kwargs)        _response.history.append(response)        _response.request = prep        return _response",
        "documentation": "prompt the user for the proxy username and password and modifies the proxy in the session object to include it this method is modeled after requestsauthhttpdigestauthhandle requestsauthhttpproxyauth the previous condafetchhandleproxy it both add usernamepassword to the proxy url a well a adding a proxyauthorization header if any of this is incorrect please file an issue"
    },
    {
        "method_code": "def get_client(self, client_type: Union[str, ClientType]):                if isinstance(client_type, ClientType):            client_type = client_type.value        return _get_client_cached(ctx=self, client_type=client_type)",
        "documentation": "helper method to dynamically get a client type args clienttype the client service name return an authenticated client raise valueerror if the client is not supported"
    },
    {
        "method_code": "def count_terminals(self):                return sum(1 for clade in self.find_clades(terminal=True))",
        "documentation": "count the number of terminal leaf node within this tree"
    },
    {
        "method_code": "def __init__(self, id):                self._id = id        self.full_id = None        self.parent = None        self.child_list = []        self.child_dict = {}        # Dictionary that keeps additional properties        self.xtra = {}",
        "documentation": "initialize the class"
    },
    {
        "method_code": "def parse(handle):        return iter(Iterator(handle, RecordParser()))",
        "documentation": "iterate over genbank formatted entry a record object from bio import genbank with opengenbankncgb a handle for record in genbankparsehandle printrecordaccession nc to get seqrecord object use bioseqioparse formatgb instead"
    },
    {
        "method_code": "def _logvecadd(logvec1, logvec2):        assert len(logvec1) == len(logvec2), \"vectors aren't the same length\"    sumvec = np.zeros(len(logvec1))    for i in range(len(logvec1)):        sumvec[i] = logaddexp(logvec1[i], logvec2[i])    return sumvec",
        "documentation": "implement a log sum for two vector object private"
    },
    {
        "method_code": "def test_playlist_write(self):                tempdir = bytestring_path(mkdtemp())        the_playlist_file = path.join(tempdir, b\"playlist.m3u\")        m3ufile = M3UFile(the_playlist_file)        m3ufile.set_contents(            [                bytestring_path(\"/This/is/a/path/to_a_file.mp3\"),                bytestring_path(\"/This/is/another/path/to_a_file.mp3\"),            ]        )        m3ufile.write()        assert path.exists(the_playlist_file)        rmtree(tempdir)",
        "documentation": "test saving ascii path to a playlist file"
    },
    {
        "method_code": "def value(self, bypass_callback: bool = False) -> T:                return self.value_from(get_current_settings(), bypass_callback=bypass_callback)",
        "documentation": "get the current value of a setting example python from prefectsettings import prefectapiurl prefectapiurlvalue"
    },
    {
        "method_code": "def timeout(self):                if context.local_repodata_ttl > 1:            max_age = context.local_repodata_ttl        elif context.local_repodata_ttl == 1:            max_age = get_cache_control_max_age(self.state.cache_control)        else:            max_age = 0        max_age *= 10**9  # nanoseconds        now = time.time_ns()        refresh = self.state.get(\"refresh_ns\", 0)        return ((now - refresh) + max_age) / 1e9",
        "documentation": "return number of second until cache time out if already timed out"
    },
    {
        "method_code": "def test_long_names(self):                original = SeqIO.read(\"GenBank/iro.gb\", \"gb\")        self.assertEqual(len(original), 1326)        # Acceptability of LOCUS line with length > 80        # invalidates some of these tests        for name, seq_len, ok in [            (\"short\", 1, True),            (\"max_length_of_16\", 1000, True),            (\"overly_long_at_17\", 1000, True),            (\"excessively_long_at_22\", 99999, True),            (\"excessively_long_at_22\", 100000, True),            (\"pushing_the_limits_at_24\", 999, True),            (\"pushing_the_limits_at_24\", 1000, True),            (\"old_max_name_length_was_26\", 10, True),  # 2 digits            (\"old_max_name_length_was_26\", 9, True),        ]:  # 1 digit            # Make the length match the desired target            record = original[:]            # TODO - Implement Seq * int            record.seq = Seq(\"N\" * seq_len)            record.annotations[\"molecule_type\"] = original.annotations[\"molecule_type\"]            # Set the identifier to the desired name            record.id = record.name = name            # Attempt to output the record...            if not ok:                # e.g. ValueError:                # Locus identifier 'excessively_long_at_22' is too long                self.assertRaises(ValueError, record.format, \"gb\")                continue            with warnings.catch_warnings():                # e.g. BiopythonWarning: Stealing space from length                # field to allow long name in LOCUS line                warnings.simplefilter(\"ignore\", BiopythonWarning)                # output = record.format(\"gb\")                handle = StringIO()                self.assertEqual(1, SeqIO.write(record, handle, \"gb\"))            handle.seek(0)            line = handle.readline()            self.assertIn(f\" {name} \", line)            self.assertIn(\" %i bp \" % seq_len, line)            # Splitting based on whitespace rather than position due to            # updated GenBank specification            name_and_length = line.split()[1:3]            self.assertEqual(name_and_length, [name, str(seq_len)], line)            handle.seek(0)            with warnings.catch_warnings():                # e.g. BiopythonParserWarning: GenBank LOCUS line                # identifier over 16 characters                warnings.simplefilter(\"ignore\", BiopythonWarning)                new = SeqIO.read(handle, \"gb\")            self.assertEqual(name, new.name)            self.assertEqual(seq_len, len(new))",
        "documentation": "various genbank name which push the column based locus line"
    },
    {
        "method_code": "def assert_graph_is_connected(graph: Graph, incremental: bool = False) -> None:        nodes_by_id = {id: node for id, node in graph.nodes}    def assert_ordered_by_start_time(items: Iterable[Union[Edge, Node]]) -> None:        last_seen = pendulum.datetime(1978, 6, 4)        for item in items:            try:                node = nodes_by_id[item.id]            except KeyError:                if incremental:                    continue                raise            assert node.start_time >= last_seen            last_seen = pendulum.instance(node.start_time)    nodes = [node for _, node in graph.nodes]    assert_ordered_by_start_time(nodes)    for node in nodes:        # all root nodes should be in the root_node_ids list of the graph        if not node.parents:            assert node.id in graph.root_node_ids        # if I have parents, my parents should have a reference to me as a child        for parent_edge in node.parents:            try:                parent_node = nodes_by_id[parent_edge.id]            except KeyError:                if incremental:                    continue                raise            assert node.id in [child.id for child in parent_node.children]        assert_ordered_by_start_time(node.parents)        # if I have children, my children should have a reference to me as a parent        for child_edge in node.children:            try:                child = nodes_by_id[child_edge.id]            except KeyError:                if incremental:                    continue                raise            assert node.id in [parent.id for parent in child.parents]        assert_ordered_by_start_time(node.children)",
        "documentation": "thing we expect to be true about the node of all graph some thing may not be true of incremental graph so we may skip some test"
    },
    {
        "method_code": "def _validate_token_kwargs(cls, values):                authenticator = values.get(\"authenticator\")        token = values.get(\"token\")        if authenticator == \"oauth\" and not token:            raise ValueError(                \"If authenticator is set to `oauth`, `token` must be provided\"            )        return values",
        "documentation": "ensure an authorization value ha been provided by the user"
    },
    {
        "method_code": "def render_pep440_pre(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        if pieces[\"distance\"]:            # update the post release segment            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])            rendered = tag_version            if post_version is not None:                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])            else:                rendered += \".post0.dev%d\" % (pieces[\"distance\"])        else:            # no commits, use the tag as the version            rendered = pieces[\"closest-tag\"]    else:        # exception #1        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]    return rendered",
        "documentation": "tagpostndevdistance no dirty exception no tag postdevdistance"
    },
    {
        "method_code": "def _escape_filename(filename):        # Is adding the following helpful    # if os.path.isfile(filename):    #    # On Windows, if the file exists, we can ask for    #    # its alternative short name (DOS style 8.3 format)    #    # which has no spaces in it.  Note that this name    #    # is not portable between machines, or even folder!    #    try:    #        import win32api    #        short = win32api.GetShortPathName(filename)    #        assert os.path.isfile(short)    #        return short    #    except ImportError:    #        pass    if not isinstance(filename, str):        # for example the NCBI BLAST+ -outfmt argument can be an integer        return filename    if \" \" not in filename:        return filename    # We'll just quote it - works on Windows, Mac OS X etc    if filename.startswith('\"') and filename.endswith('\"'):        # Its already quoted        return filename    else:        return f'\"{filename}\"'",
        "documentation": "escape filename with space by adding quote private note this will not add quote if they are already included printescapefilenameexample with space example with space printescapefilenameexample with space example with space printescapefilename note the function is more generic than the name suggests since it is used to add quote around any string argument containing space"
    },
    {
        "method_code": "def cmd_move(self, conn, idx_from, idx_to):                idx_from = cast_arg(int, idx_from)        idx_to = cast_arg(int, idx_to)        try:            track = self.playlist.pop(idx_from)            self.playlist.insert(idx_to, track)        except IndexError:            raise ArgumentIndexError()        # Update currently-playing song.        if idx_from == self.current_index:            self.current_index = idx_to        elif idx_from < self.current_index <= idx_to:            self.current_index -= 1        elif idx_from > self.current_index >= idx_to:            self.current_index += 1        self.playlist_version += 1        self._send_event(\"playlist\")",
        "documentation": "move a track in the playlist"
    },
    {
        "method_code": "def __next_section(self, ls, into):                indentation = \"\\n\" + (self.NameWidth + self.Indent) * \" \"        linesize = self.linesize - self.MaxSize        pat = re.compile(r\"([\\w,\\s()]){1,%i}[,\\.]\" % linesize)        several, Join = \"\", \"\".join        for name, sites in sorted(ls):            stringsite = \"\"            output = Join((\", \".join(str(site) for site in sites), \".\"))            if len(output) > linesize:                #                #   cut where appropriate and add the indentation                #                output = [x.group() for x in re.finditer(pat, output)]                stringsite = indentation.join(output)            else:                stringsite = output            into = Join(                (into, str(name).ljust(self.NameWidth), \" :  \", stringsite, \"\\n\")            )        return into",
        "documentation": "next section private argument l is a tuplelist of tuple string int int into is a string to which the formatted l will be added format l a a string of line the form is enzyme position enzyme position position then add the formatted l to tot return tot"
    },
    {
        "method_code": "def cmd_status(self, conn):                yield (            \"repeat: \" + str(int(self.repeat)),            \"random: \" + str(int(self.random)),            \"consume: \" + str(int(self.consume)),            \"single: \" + str(int(self.single)),            \"playlist: \" + str(self.playlist_version),            \"playlistlength: \" + str(len(self.playlist)),            \"mixrampdb: \" + str(self.mixrampdb),        )        if self.volume > 0:            yield \"volume: \" + str(self.volume)        if not math.isnan(self.mixrampdelay):            yield \"mixrampdelay: \" + str(self.mixrampdelay)        if self.crossfade > 0:            yield \"xfade: \" + str(self.crossfade)        if self.current_index == -1:            state = \"stop\"        elif self.paused:            state = \"pause\"        else:            state = \"play\"        yield \"state: \" + state        if self.current_index != -1:  # i.e., paused or playing            current_id = self._item_id(self.playlist[self.current_index])            yield \"song: \" + str(self.current_index)            yield \"songid: \" + str(current_id)            if len(self.playlist) > self.current_index + 1:                # If there's a next song, report its index too.                next_id = self._item_id(self.playlist[self.current_index + 1])                yield \"nextsong: \" + str(self.current_index + 1)                yield \"nextsongid: \" + str(next_id)        if self.error:            yield \"error: \" + self.error",
        "documentation": "return some status information for use with an implementation of cmdstatus give a list of responselines for volume repeat random playlist playlistlength and xfade"
    },
    {
        "method_code": "def common_ancestor(self, node1, node2):                l1 = [self.root] + self.trace(self.root, node1)        l2 = [self.root] + self.trace(self.root, node2)        return [n for n in l1 if n in l2][-1]",
        "documentation": "return the common ancestor that connects two node nodeid commonancestorselfnodenode"
    },
    {
        "method_code": "def write(records, destination, fmt=\"XML\"):        fmt = fmt.upper()    if fmt == \"XML\":        Writer = _writers.XMLWriter    elif fmt == \"XML2\":        Writer = _writers.XML2Writer    else:        raise ValueError(f\"Unknown format {fmt}; expected 'XML' or 'XML2'\")    try:        stream = open(destination, \"wb\")    except TypeError:  # not a path, assume we received a stream        try:            destination.write(b\"\")        except TypeError:            # destination was opened in text mode            raise StreamModeError(                \"File must be opened in binary mode for writing.\"            ) from None        stream = destination    writer = Writer(stream)    try:        count = writer.write(records)    finally:        if stream is not destination:            stream.close()    return count",
        "documentation": "write blast record a an xml file and return the number of record argument record a bioblastrecords object destination file or filelike object to write to or filename a string the file object must have been opened for writing in binary mode and must be closed or flushed by the caller after this function return to ensure that all record are written fmt string describing the file format to write caseinsensitive currently only xml and xml are accepted return the number of record written a an integer"
    }
]