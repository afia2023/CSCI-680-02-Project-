[
    {
        "input_text": "summarize: def initialize_options(self):                self.offline = None",
        "labels_text": "Noop initialise option"
    },
    {
        "input_text": "summarize: def finalize_options(self):                pass",
        "labels_text": "Noop finalise option"
    },
    {
        "input_text": "summarize: def run(self):                this_dir = os.getcwd()        # change to the test dir and run the tests        os.chdir(\"Tests\")        sys.path.insert(0, \"\")        import run_tests        if self.offline:            run_tests.main([\"--offline\"])        else:            run_tests.main([])        # change back to the current directory        os.chdir(this_dir)",
        "labels_text": "Run the test"
    },
    {
        "input_text": "summarize: def can_import(module_name):        try:        return __import__(module_name)    except ImportError:        return None",
        "labels_text": "Check we can import the requested module"
    },
    {
        "input_text": "summarize: def get_version():        for line in open(\"Bio/__init__.py\"):        if line.startswith(\"__version__ = \"):            return ast.literal_eval(line.split(\"=\")[1].strip())    return \"Undefined\"",
        "labels_text": "Get version number from initpy"
    },
    {
        "input_text": "summarize: def open(filename, mode=\"rb\"):    r    if \"r\" in mode.lower():        return BgzfReader(filename, mode)    elif \"w\" in mode.lower() or \"a\" in mode.lower():        return BgzfWriter(filename, mode)    else:        raise ValueError(f\"Bad mode {mode!r}\")",
        "labels_text": "Open a BGZF file for reading writing or appending If text mode is requested in order to avoid multibyte character this is hard coded to use the latin encoding and r and n are passed a is without implementing universal new line mode If your data is in UTF or any other incompatible encoding you must use binary mode and decode the appropriate fragment yourself"
    },
    {
        "input_text": "summarize: def split_virtual_offset(virtual_offset):        start = virtual_offset >> 16    return start, virtual_offset ^ (start << 16)",
        "labels_text": "Divides a bit BGZF virtual offset into block start within block offset splitvirtualoffset True splitvirtualoffset True"
    },
    {
        "input_text": "summarize: def BgzfBlocks(handle):        if isinstance(handle, BgzfReader):        raise TypeError(\"Function BgzfBlocks expects a binary handle\")    data_start = 0    while True:        start_offset = handle.tell()        try:            block_length, data = _load_bgzf_block(handle)        except StopIteration:            break        data_len = len(data)        yield start_offset, block_length, data_start, data_len        data_start += data_len",
        "labels_text": "Low level debugging function to inspect BGZF block Expects a BGZF compressed file opened in binary read mode using the builtin open function Do not use a handle from this bgzf module or the gzip module open function which will decompress the file Returns the block start offset see virtual offset the block length add these for the start of the next block and the decompressed length of the block content limited to in BGZF a an iterator one tuple per BGZF block from builtins import open handle openSamBamexbam rb for value in BgzfBlockshandle printRaw start i raw length i data start i data length i value Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length handleclose Indirectly we can tell this file came from an old version of samtools because all the block except the final one and the dummy empty EOF marker block are byte Later version avoid splitting a read between two block and give the header it own block useful to speed up replacing the header You can see this in exrefreshbam created using samtools samtools view b exbam exrefreshbam handle openSamBamexrefreshbam rb for value in BgzfBlockshandle printRaw start i raw length i data start i data length i value Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length handleclose The above example ha no embedded SAM header thus the first block is very small at just byte of decompressed data while the next example doe a larger block of byte Notice that the rest of the block show the same size they contain the same read data handle openSamBamexheaderbam rb for value in BgzfBlockshandle printRaw start i raw length i data start i data length i value Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length Raw start raw length data start data length handleclose"
    },
    {
        "input_text": "summarize: def __next__(self):                line = self.readline()        if not line:            raise StopIteration        return line",
        "labels_text": "Return the next line"
    },
    {
        "input_text": "summarize: def __iter__(self):                return self",
        "labels_text": "Iterate over the line in the BGZF file"
    },
    {
        "input_text": "summarize: def close(self):                self._handle.close()        self._buffer = None        self._block_start_offset = None        self._buffers = None",
        "labels_text": "Close BGZF file"
    },
    {
        "input_text": "summarize: def seekable(self):                return True",
        "labels_text": "Return True indicating the BGZF support random access"
    },
    {
        "input_text": "summarize: def isatty(self):                return False",
        "labels_text": "Return True if connected to a TTY device"
    },
    {
        "input_text": "summarize: def fileno(self):                return self._handle.fileno()",
        "labels_text": "Return integer file descriptor"
    },
    {
        "input_text": "summarize: def __enter__(self):                return self",
        "labels_text": "Open a file operable with WITH statement"
    },
    {
        "input_text": "summarize: def __exit__(self, type, value, traceback):                self.close()",
        "labels_text": "Close a file with WITH statement"
    },
    {
        "input_text": "summarize: def flush(self):                while len(self._buffer) >= 65536:            self._write_block(self._buffer[:65535])            self._buffer = self._buffer[65535:]        self._write_block(self._buffer)        self._buffer = b\"\"        self._handle.flush()",
        "labels_text": "Flush data explicitally"
    },
    {
        "input_text": "summarize: def close(self):                if self._buffer:            self.flush()        self._handle.write(_bgzf_eof)        self._handle.flush()        self._handle.close()",
        "labels_text": "Flush data write byte BGZF EOF marker and close BGZF file samtools will look for a magic EOF marker just a byte empty BGZF block and if it is missing warns the BAM file may be truncated In addition to samtools writing this block so too doe bgzip so this implementation doe too"
    },
    {
        "input_text": "summarize: def tell(self):                return make_virtual_offset(self._handle.tell(), len(self._buffer))",
        "labels_text": "Return a BGZF bit virtual offset"
    },
    {
        "input_text": "summarize: def seekable(self):                # Not seekable, but we do support tell...        return False",
        "labels_text": "Return True indicating the BGZF support random access"
    },
    {
        "input_text": "summarize: def isatty(self):                return False",
        "labels_text": "Return True if connected to a TTY device"
    },
    {
        "input_text": "summarize: def fileno(self):                return self._handle.fileno()",
        "labels_text": "Return integer file descriptor"
    },
    {
        "input_text": "summarize: def __enter__(self):                return self",
        "labels_text": "Open a file operable with WITH statement"
    },
    {
        "input_text": "summarize: def __exit__(self, type, value, traceback):                self.close()",
        "labels_text": "Close a file with WITH statement"
    },
    {
        "input_text": "summarize: def as_handle(handleish, mode=\"r\", **kwargs):    r    try:        with open(handleish, mode, **kwargs) as fp:            yield fp    except TypeError:        yield handleish",
        "labels_text": "Context manager to ensure we are using a handle Context manager for argument that can be passed to SeqIO and AlignIO read write and parse method either file object or pathlike object string pathlibPath instance or more generally anything that can be handled by the builtin open function When given a pathlike object return an open file handle to that path with provided mode which will be closed when the manager exit All other input are returned and are not closed Arguments handleish Either a file handle or pathlike object anything which can be passed to the builtin open function such a str byte pathlibPath and osDirEntry object mode Mode to open handleish used only if handleish is a string kwargs Further argument to pas to open Examples from Bio import File import o with Fileashandleseqsfasta w a fp fpwritetestnACGT fpclosed True handle openseqsfasta w with Fileashandlehandle a fp fpwritetestnACGT fpclosed False fpclose osremoveseqsfasta tidy up"
    },
    {
        "input_text": "summarize: def __iter__(self):                raise NotImplementedError",
        "labels_text": "Return identifier offset length in byte tuples The length can be zero where it is not implemented or not possible for a particular file format"
    },
    {
        "input_text": "summarize: def get(self, offset):                # Most file formats with self contained records can be handled by        # parsing StringIO(self.get_raw(offset).decode())        raise NotImplementedError",
        "labels_text": "Return parsed object for this entry"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                # Should be done by each sub-class (if possible)        raise NotImplementedError(\"Not available for this file format.\")",
        "labels_text": "Return the raw record from the file a a byte string if implemented If the key is not found a KeyError exception is raised This may not have been implemented for all file format"
    },
    {
        "input_text": "summarize: def __repr__(self):                return self._repr",
        "labels_text": "Return a string representation of the File object"
    },
    {
        "input_text": "summarize: def __str__(self):                # TODO - How best to handle the __str__ for SeqIO and SearchIO?        if self:            return f\"{{{list(self.keys())[0]!r} : {self._obj_repr}(...), ...}}\"        else:            return \"{}\"",
        "labels_text": "Create a string representation of the File object"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._offsets)",
        "labels_text": "Return the number of record"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self._offsets)",
        "labels_text": "Iterate over the key"
    },
    {
        "input_text": "summarize: def get_raw(self, key):                # Pass the offset to the proxy        return self._proxy.get_raw(self._offsets[key])",
        "labels_text": "Return the raw record from the file a a byte string If the key is not found a KeyError exception is raised"
    },
    {
        "input_text": "summarize: def close(self):                self._proxy._handle.close()",
        "labels_text": "Close the file handle being used to read the data Once called further use of the index wont work The sole purpose of this method is to allow explicit handle closure for example if you wish to delete the file on Windows you must first close all open handle to that file"
    },
    {
        "input_text": "summarize: def __len__(self):                return self._length",
        "labels_text": "Return the number of record indexed"
    },
    {
        "input_text": "summarize: def __iter__(self):                for row in self._con.execute(            \"SELECT key FROM offset_data ORDER BY file_number, offset;\"        ):            yield str(row[0])",
        "labels_text": "Iterate over the key"
    },
    {
        "input_text": "summarize: def close(self):                proxies = self._proxies        while proxies:            proxies.popitem()[1]._handle.close()",
        "labels_text": "Close any open file handle"
    },
    {
        "input_text": "summarize: def __init__(self):                self.classes = set()        self.xs = []        self.ys = []        self.k = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def equal_weight(x, y):        # everything gets 1 vote    return 1",
        "labels_text": "Return integer one dummy method for equally weighting"
    },
    {
        "input_text": "summarize: def train(xs, ys, k, typecode=None):        knn = kNN()    knn.classes = set(ys)    knn.xs = np.asarray(xs, typecode)    knn.ys = ys    knn.k = k    return knn",
        "labels_text": "Train a k nearest neighbor classifier on a training set x is a list of observation and y is a list of the class assignment Thus x and y should contain the same number of element k is the number of neighbor that should be examined when doing the classification"
    },
    {
        "input_text": "summarize: def classify(knn, x, weight_fn=None, distance_fn=None):        if weight_fn is None:        weight_fn = equal_weight    weights = calculate(knn, x, weight_fn=weight_fn, distance_fn=distance_fn)    most_class = None    most_weight = None    for klass, weight in weights.items():        if most_class is None or weight > most_weight:            most_class = klass            most_weight = weight    return most_class",
        "labels_text": "Classify an observation into a class If not specified weightfn will give all neighbor equal weight distancefn is an optional function that take two point and return the distance between them If distancefn is None the default the Euclidean distance is used"
    },
    {
        "input_text": "summarize: def __init__(self):                self.beta = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def calculate(lr, x):        # Insert a constant term for x.    x = np.asarray([1.0] + x)    # Calculate the probability.  p = e^(beta X) / (1+e^(beta X))    ebetaX = np.exp(np.dot(lr.beta, x))    p = ebetaX / (1 + ebetaX)    return [1 - p, p]",
        "labels_text": "Calculate the probability for each class Arguments lr is a LogisticRegression object x is the observed data Returns a list of the probability that it fit each class"
    },
    {
        "input_text": "summarize: def classify(lr, x):        probs = calculate(lr, x)    if probs[0] > probs[1]:        return 0    return 1",
        "labels_text": "Classify an observation into a class"
    },
    {
        "input_text": "summarize: def itemindex(values):        d = {}    entries = enumerate(values[::-1])    n = len(values) - 1    for index, key in entries:        d[key] = n - index    return d",
        "labels_text": "Return a dictionary of value with their sequence offset a key"
    },
    {
        "input_text": "summarize: def __init__(        self, states, alphabet, p_initial=None, p_transition=None, p_emission=None    ):                self.states = states        self.alphabet = alphabet        self.p_initial = p_initial        self.p_transition = p_transition        self.p_emission = p_emission",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                from io import StringIO        handle = StringIO()        save(self, handle)        handle.seek(0)        return handle.read()",
        "labels_text": "Create a string representation of the MarkovModel object"
    },
    {
        "input_text": "summarize: def _readline_and_check_start(handle, start):        line = handle.readline()    if not line.startswith(start):        raise ValueError(f\"I expected {start!r} but got {line!r}\")    return line",
        "labels_text": "Read the first line and evaluate that begisn with the correct start PRIVATE"
    },
    {
        "input_text": "summarize: def _argmaxes(vector, allowance=None):        return [np.argmax(vector)]",
        "labels_text": "Return index of the maximum value aong the vector PRIVATE"
    },
    {
        "input_text": "summarize: def _normalize(matrix):        if len(matrix.shape) == 1:        matrix = matrix / sum(matrix)    elif len(matrix.shape) == 2:        # Normalize by rows.        for i in range(len(matrix)):            matrix[i, :] = matrix[i, :] / sum(matrix[i, :])    else:        raise ValueError(\"I cannot handle matrixes of that shape\")    return matrix",
        "labels_text": "Normalize matrix object PRIVATE"
    },
    {
        "input_text": "summarize: def _uniform_norm(shape):        matrix = np.ones(shape)    return _normalize(matrix)",
        "labels_text": "Normalize a uniform matrix PRIVATE"
    },
    {
        "input_text": "summarize: def _random_norm(shape):        matrix = np.random.random(shape)    return _normalize(matrix)",
        "labels_text": "Normalize a random matrix PRIVATE"
    },
    {
        "input_text": "summarize: def _logsum(matrix):        if len(matrix.shape) > 1:        vec = np.reshape(matrix, (np.prod(matrix.shape),))    else:        vec = matrix    sum = LOG0    for num in vec:        sum = logaddexp(sum, num)    return sum",
        "labels_text": "Implement logsum for a matrix object PRIVATE"
    },
    {
        "input_text": "summarize: def _logvecadd(logvec1, logvec2):        assert len(logvec1) == len(logvec2), \"vectors aren't the same length\"    sumvec = np.zeros(len(logvec1))    for i in range(len(logvec1)):        sumvec[i] = logaddexp(logvec1[i], logvec2[i])    return sumvec",
        "labels_text": "Implement a log sum for two vector object PRIVATE"
    },
    {
        "input_text": "summarize: def _exp_logsum(numbers):        sum = _logsum(numbers)    return np.exp(sum)",
        "labels_text": "Return the exponential of a logsum PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self):                self.classes = []        self.alphas = []        self.feature_fns = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def calculate(me, observation):        scores = []    assert len(me.feature_fns) == len(me.alphas)    for klass in me.classes:        lprob = 0.0        for fn, alpha in zip(me.feature_fns, me.alphas):            lprob += fn(observation, klass) * alpha        scores.append(lprob)    return scores",
        "labels_text": "Calculate the log of the probability for each class me is a MaxEntropy object that ha been trained observation is a vector representing the observed data The return value is a list of unnormalized log probability for each class"
    },
    {
        "input_text": "summarize: def classify(me, observation):        scores = calculate(me, observation)    max_score, klass = scores[0], me.classes[0]    for i in range(1, len(scores)):        if scores[i] > max_score:            max_score, klass = scores[i], me.classes[i]    return klass",
        "labels_text": "Classify an observation into a class"
    },
    {
        "input_text": "summarize: def _eval_feature_fn(fn, xs, classes):        values = {}    for i in range(len(xs)):        for j in range(len(classes)):            f = fn(xs[i], classes[j])            if f != 0:                values[(i, j)] = f    return values",
        "labels_text": "Evaluate a feature function on every instance of the training set and class PRIVATE fn is a callback function that take two parameter a training instance and a class Return a dictionary of training set index class index nonzero value Values of are not stored in the dictionary"
    },
    {
        "input_text": "summarize: def _calc_f_sharp(N, nclasses, features):        # f#(x, y) = SUM_i feature(x, y)    f_sharp = np.zeros((N, nclasses))    for feature in features:        for (i, j), f in feature.items():            f_sharp[i][j] += f    return f_sharp",
        "labels_text": "Calculate a matrix of f sharp value PRIVATE"
    },
    {
        "input_text": "summarize: def _contents(items):        term = 1.0 / len(items)    counts = {}    for item in items:        counts[item] = counts.get(item, 0) + term    return counts",
        "labels_text": "Return a dictionary where the key is the item and the value is the probability associated PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self):                self.classes = []        self.p_conditional = None        self.p_prior = []        self.dimensionality = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def classify(nb, observation):        # The class is the one with the highest probability.    probs = calculate(nb, observation, scale=False)    max_prob = max_class = None    for klass in nb.classes:        if max_prob is None or probs[klass] > max_prob:            max_prob, max_class = probs[klass], klass    return max_class",
        "labels_text": "Classify an observation into a class"
    },
    {
        "input_text": "summarize: def __call__(self, *args, **keywds):                        keywds = self.decode(*args, **keywds)            return _align(**keywds)",
        "labels_text": "Call the alignment instance already created"
    },
    {
        "input_text": "summarize: def __getattr__(self, attr):                # The following 'magic' is needed to rewrite the class docstring        # dynamically:        wrapper = self.alignment_function(attr)        wrapper_type = type(wrapper)        wrapper_dict = wrapper_type.__dict__.copy()        wrapper_dict[\"__doc__\"] = wrapper.__doc__        new_alignment_function = type(\"alignment_function\", (object,), wrapper_dict)        return new_alignment_function(attr)",
        "labels_text": "Call alignmentfunction to check and decode the attribute"
    },
    {
        "input_text": "summarize: def rint(x, precision=_PRECISION):        return int(x * precision + 0.5)",
        "labels_text": "Print number with declared precision"
    },
    {
        "input_text": "summarize: def __init__(self, match=1, mismatch=0):                self.match = match        self.mismatch = mismatch",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __call__(self, charA, charB):                if charA == charB:            return self.match        return self.mismatch",
        "labels_text": "Call a match function instance already created"
    },
    {
        "input_text": "summarize: def __init__(self, score_dict, symmetric=1):                if isinstance(score_dict, substitution_matrices.Array):            score_dict = dict(score_dict)  # Access to dict is much faster        self.score_dict = score_dict        self.symmetric = symmetric",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __call__(self, charA, charB):                if self.symmetric and (charA, charB) not in self.score_dict:            # If the score dictionary is symmetric, then look up the            # score both ways.            charB, charA = charA, charB        return self.score_dict[(charA, charB)]",
        "labels_text": "Call a dictionary match instance already created"
    },
    {
        "input_text": "summarize: def __call__(self, index, length):                return calc_affine_penalty(            length, self.open, self.extend, self.penalize_extend_when_opening        )",
        "labels_text": "Call a gap function instance already created"
    },
    {
        "input_text": "summarize: def calc_affine_penalty(length, open, extend, penalize_extend_when_opening):        if length <= 0:        return 0.0    penalty = open + extend * length    if not penalize_extend_when_opening:        penalty -= extend    return penalty",
        "labels_text": "Calculate a penalty score for the gap function"
    },
    {
        "input_text": "summarize: def _maketrans(complement_mapping):        keys = \"\".join(complement_mapping.keys()).encode(\"ASCII\")    values = \"\".join(complement_mapping.values()).encode(\"ASCII\")    return bytes.maketrans(keys + keys.lower(), values + values.lower())",
        "labels_text": "Make a python string translation table PRIVATE Arguments complementmapping a dictionary such a ambiguousdnacomplement and ambiguousrnacomplement from DataIUPACData Returns a translation table a byte object of length for use with the python string translate method to use in a reverse complement Compatible with lower case and upper case sequence For internal use only"
    },
    {
        "input_text": "summarize: def __init__(self):                assert self[:0] == b\"\"",
        "labels_text": "Check if getitem return a byteslike object"
    },
    {
        "input_text": "summarize: def decode(self, encoding=\"utf-8\"):                return bytes(self).decode(encoding)",
        "labels_text": "Decode the data a byte using the codec registered for encoding encoding The encoding with which to decode the byte"
    },
    {
        "input_text": "summarize: def count(self, sub, start=None, end=None):                return bytes(self).count(sub, start, end)",
        "labels_text": "Return the number of nonoverlapping occurrence of sub in datastartend Optional argument start and end are interpreted a in slice notation This method behaves a the count method of Python string"
    },
    {
        "input_text": "summarize: def find(self, sub, start=None, end=None):                return bytes(self).find(sub, start, end)",
        "labels_text": "Return the lowest index in data where subsection sub is found Return the lowest index in data where subsection sub is found such that sub is contained within datastartend Optional argument start and end are interpreted a in slice notation Return on failure"
    },
    {
        "input_text": "summarize: def rfind(self, sub, start=None, end=None):                return bytes(self).rfind(sub, start, end)",
        "labels_text": "Return the highest index in data where subsection sub is found Return the highest index in data where subsection sub is found such that sub is contained within datastartend Optional argument start and end are interpreted a in slice notation Return on failure"
    },
    {
        "input_text": "summarize: def index(self, sub, start=None, end=None):                return bytes(self).index(sub, start, end)",
        "labels_text": "Return the lowest index in data where subsection sub is found Return the lowest index in data where subsection sub is found such that sub is contained within datastartend Optional argument start and end are interpreted a in slice notation Raises ValueError when the subsection is not found"
    },
    {
        "input_text": "summarize: def rindex(self, sub, start=None, end=None):                return bytes(self).rindex(sub, start, end)",
        "labels_text": "Return the highest index in data where subsection sub is found Return the highest index in data where subsection sub is found such that sub is contained within datastartend Optional argument start and end are interpreted a in slice notation Raise ValueError when the subsection is not found"
    },
    {
        "input_text": "summarize: def startswith(self, prefix, start=None, end=None):                return bytes(self).startswith(prefix, start, end)",
        "labels_text": "Return True if data start with the specified prefix False otherwise With optional start test data beginning at that position With optional end stop comparing data at that position prefix can also be a tuple of byte to try"
    },
    {
        "input_text": "summarize: def endswith(self, suffix, start=None, end=None):                return bytes(self).endswith(suffix, start, end)",
        "labels_text": "Return True if data end with the specified suffix False otherwise With optional start test data beginning at that position With optional end stop comparing data at that position suffix can also be a tuple of byte to try"
    },
    {
        "input_text": "summarize: def split(self, sep=None, maxsplit=-1):                return bytes(self).split(sep, maxsplit)",
        "labels_text": "Return a list of the section in the data using sep a the delimiter sep The delimiter according which to split the data None the default value mean split on ASCII whitespace character space tab return newline formfeed vertical tab maxsplit Maximum number of split to do the default value mean no limit"
    },
    {
        "input_text": "summarize: def rsplit(self, sep=None, maxsplit=-1):                return bytes(self).rsplit(sep, maxsplit)",
        "labels_text": "Return a list of the section in the data using sep a the delimiter sep The delimiter according which to split the data None the default value mean split on ASCII whitespace character space tab return newline formfeed vertical tab maxsplit Maximum number of split to do the default value mean no limit Splitting is done starting at the end of the data and working to the front"
    },
    {
        "input_text": "summarize: def strip(self, chars=None):                return bytes(self).strip(chars)",
        "labels_text": "Strip leading and trailing character contained in the argument If the argument is omitted or None strip leading and trailing ASCII whitespace"
    },
    {
        "input_text": "summarize: def lstrip(self, chars=None):                return bytes(self).lstrip(chars)",
        "labels_text": "Strip leading character contained in the argument If the argument is omitted or None strip leading ASCII whitespace"
    },
    {
        "input_text": "summarize: def rstrip(self, chars=None):                return bytes(self).rstrip(chars)",
        "labels_text": "Strip trailing character contained in the argument If the argument is omitted or None strip trailing ASCII whitespace"
    },
    {
        "input_text": "summarize: def removeprefix(self, prefix):                # Want to do just this, but need Python 3.9+        # return bytes(self).removeprefix(prefix)        data = bytes(self)        try:            return data.removeprefix(prefix)        except AttributeError:            if data.startswith(prefix):                return data[len(prefix) :]            else:                return data",
        "labels_text": "Remove the prefix if present"
    },
    {
        "input_text": "summarize: def removesuffix(self, suffix):                # Want to do just this, but need Python 3.9+        # return bytes(self).removesuffix(suffix)        data = bytes(self)        try:            return data.removesuffix(suffix)        except AttributeError:            if data.startswith(suffix):                return data[: -len(suffix)]            else:                return data",
        "labels_text": "Remove the suffix if present"
    },
    {
        "input_text": "summarize: def upper(self):                return bytes(self).upper()",
        "labels_text": "Return a copy of data with all ASCII character converted to uppercase"
    },
    {
        "input_text": "summarize: def lower(self):                return bytes(self).lower()",
        "labels_text": "Return a copy of data with all ASCII character converted to lowercase"
    },
    {
        "input_text": "summarize: def isupper(self):                return bytes(self).isupper()",
        "labels_text": "Return True if all ASCII character in data are uppercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def islower(self):                return bytes(self).islower()",
        "labels_text": "Return True if all ASCII character in data are lowercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def replace(self, old, new):                return bytes(self).replace(old, new)",
        "labels_text": "Return a copy with all occurrence of substring old replaced by new"
    },
    {
        "input_text": "summarize: def translate(self, table, delete=b\"\"):                return bytes(self).translate(table, delete)",
        "labels_text": "Return a copy with each character mapped by the given translation table table Translation table which must be a byte object of length All character occurring in the optional argument delete are removed The remaining character are mapped through the given translation table"
    },
    {
        "input_text": "summarize: def defined(self):                return True",
        "labels_text": "Return True if the sequence is defined False if undefined or partially defined Zerolength sequence are always considered to be defined"
    },
    {
        "input_text": "summarize: def defined_ranges(self):                length = len(self)        if length > 0:            return ((0, length),)        else:            return ()",
        "labels_text": "Return a tuple of the range where the sequence content is defined The return value ha the format start end start end"
    },
    {
        "input_text": "summarize: def __str__(self):                return self._data.decode(\"ASCII\")",
        "labels_text": "Return the full sequence a a python string"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data == other._data        elif isinstance(other, str):            return self._data == other.encode(\"ASCII\")        else:            return self._data == other",
        "labels_text": "Compare the sequence to another sequence or a string Sequences are equal to each other if their sequence content is identical from BioSeq import Seq MutableSeq seq SeqACGT seq SeqACGT mutableseq MutableSeqACGT seq seq True seq mutableseq True seq ACGT True Note that the sequence object themselves are not identical to each other idseq idseq False seq is seq False Sequences can also be compared to string byte and bytearray object seq ACGT True seq bACGT True seq bytearraybACGT True"
    },
    {
        "input_text": "summarize: def __lt__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data < other._data        elif isinstance(other, str):            return self._data < other.encode(\"ASCII\")        else:            return self._data < other",
        "labels_text": "Implement the lessthan operand"
    },
    {
        "input_text": "summarize: def __le__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data <= other._data        elif isinstance(other, str):            return self._data <= other.encode(\"ASCII\")        else:            return self._data <= other",
        "labels_text": "Implement the lessthan or equal operand"
    },
    {
        "input_text": "summarize: def __gt__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data > other._data        elif isinstance(other, str):            return self._data > other.encode(\"ASCII\")        else:            return self._data > other",
        "labels_text": "Implement the greaterthan operand"
    },
    {
        "input_text": "summarize: def __ge__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self._data >= other._data        elif isinstance(other, str):            return self._data >= other.encode(\"ASCII\")        else:            return self._data >= other",
        "labels_text": "Implement the greaterthan or equal operand"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._data)",
        "labels_text": "Return the length of the sequence"
    },
    {
        "input_text": "summarize: def __iter__(self):                return self._data.decode(\"ASCII\").__iter__()",
        "labels_text": "Return an iterable of the sequence"
    },
    {
        "input_text": "summarize: def __getitem__(self, index):                if isinstance(index, numbers.Integral):            # Return a single letter as a string            return chr(self._data[index])        else:            # Return the (sub)sequence as another Seq/MutableSeq object            return self.__class__(self._data[index])",
        "labels_text": "Return a subsequence a a single letter or a a sequence object If the index is an integer a single letter is returned a a Python string seq SeqACTCGACGTCG seq A Otherwise a new sequence object of the same class is returned seq SeqACG mutableseq MutableSeqACTCGACGTCG mutableseq MutableSeqACG"
    },
    {
        "input_text": "summarize: def __add__(self, other):                if isinstance(other, _SeqAbstractBaseClass):            return self.__class__(self._data + other._data)        elif isinstance(other, str):            return self.__class__(self._data + other.encode(\"ASCII\"))        else:            # If other is a SeqRecord, then SeqRecord's __radd__ will handle            # this. If not, returning NotImplemented will trigger a TypeError.            return NotImplemented",
        "labels_text": "Add a sequence or string to this sequence from BioSeq import Seq MutableSeq SeqMELKI LV SeqMELKILV MutableSeqMELKI LV MutableSeqMELKILV"
    },
    {
        "input_text": "summarize: def __radd__(self, other):                if isinstance(other, str):            return self.__class__(other.encode(\"ASCII\") + self._data)        else:            return NotImplemented",
        "labels_text": "Add a sequence string on the left from BioSeq import Seq MutableSeq LV SeqMELKI SeqLVMELKI LV MutableSeqMELKI MutableSeqLVMELKI Adding two sequence object is handled via the add method"
    },
    {
        "input_text": "summarize: def __mul__(self, other):                if not isinstance(other, numbers.Integral):            raise TypeError(f\"can't multiply {self.__class__.__name__} by non-int type\")        # we would like to simply write        # data = self._data * other        # here, but currently that causes a bug on PyPy if self._data is a        # bytearray and other is a numpy integer. Using this workaround:        data = self._data.__mul__(other)        return self.__class__(data)",
        "labels_text": "Multiply sequence by integer from BioSeq import Seq MutableSeq SeqATG SeqATGATG MutableSeqATG MutableSeqATGATG"
    },
    {
        "input_text": "summarize: def __rmul__(self, other):                if not isinstance(other, numbers.Integral):            raise TypeError(f\"can't multiply {self.__class__.__name__} by non-int type\")        # we would like to simply write        # data = self._data * other        # here, but currently that causes a bug on PyPy if self._data is a        # bytearray and other is a numpy integer. Using this workaround:        data = self._data.__mul__(other)        return self.__class__(data)",
        "labels_text": "Multiply integer by sequence from BioSeq import Seq SeqATG SeqATGATG"
    },
    {
        "input_text": "summarize: def __imul__(self, other):                if not isinstance(other, numbers.Integral):            raise TypeError(f\"can't multiply {self.__class__.__name__} by non-int type\")        # we would like to simply write        # data = self._data * other        # here, but currently that causes a bug on PyPy if self._data is a        # bytearray and other is a numpy integer. Using this workaround:        data = self._data.__mul__(other)        return self.__class__(data)",
        "labels_text": "Multiply the sequence object by other and assign from BioSeq import Seq seq SeqATG seq seq SeqATGATG Note that this is different from inplace multiplication The seq variable is reassigned to the multiplication result but any variable pointing to seq will remain unchanged seq SeqATG seq seq idseq idseq True seq seq SeqATGATG seq SeqATG idseq idseq False"
    },
    {
        "input_text": "summarize: def count(self, sub, start=None, end=None):                if isinstance(sub, MutableSeq):            sub = sub._data        elif isinstance(sub, Seq):            sub = bytes(sub)        elif isinstance(sub, str):            sub = sub.encode(\"ASCII\")        elif not isinstance(sub, (bytes, bytearray)):            raise TypeError(                \"a Seq, MutableSeq, str, bytes, or bytearray object is required, not '%s'\"                % type(sub)            )        return self._data.count(sub, start, end)",
        "labels_text": "Return a nonoverlapping count like that of a python string The number of occurrence of substring argument sub in the subsequence given by startend is returned a an integer Optional argument start and end are interpreted a in slice notation Arguments sub a string or another Seq object to look for start optional integer slice start end optional integer slice end eg from BioSeq import Seq myseq SeqAAAATGA printmyseqcountA printmyseqcountATG printmyseqcountSeqAT printmyseqcountAT HOWEVER please note because the count method of Seq and MutableSeq object like that of Python string do a nonoverlapping search this may not give the answer you expect AAAAcountAA printSeqAAAAcountAA For an overlapping search use the countoverlap method printSeqAAAAcountoverlapAA"
    },
    {
        "input_text": "summarize: def __contains__(self, item):                if isinstance(item, _SeqAbstractBaseClass):            item = bytes(item)        elif isinstance(item, str):            item = item.encode(\"ASCII\")        return item in self._data",
        "labels_text": "Return True if item is a subsequence of the sequence and False otherwise eg from BioSeq import Seq MutableSeq mydna SeqATATGAAATTTGAAAA AAA in mydna True SeqAAA in mydna True MutableSeqAAA in mydna True"
    },
    {
        "input_text": "summarize: def find(self, sub, start=None, end=None):                if isinstance(sub, _SeqAbstractBaseClass):            sub = bytes(sub)        elif isinstance(sub, str):            sub = sub.encode(\"ASCII\")        elif not isinstance(sub, (bytes, bytearray)):            raise TypeError(                \"a Seq, MutableSeq, str, bytes, or bytearray object is required, not '%s'\"                % type(sub)            )        return self._data.find(sub, start, end)",
        "labels_text": "Return the lowest index in the sequence where subsequence sub is found With optional argument start and end return the lowest index in the sequence such that the subsequence sub is contained within the sequence region startend Arguments sub a string or another Seq or MutableSeq object to search for start optional integer slice start end optional integer slice end Returns if the subsequence is NOT found eg Locating the first typical start codon AUG in an RNA sequence from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myrnafindAUG The next typical start codon can then be found by starting the search at position myrnafindAUG See the search method to find the location of multiple subsequence at the same time"
    },
    {
        "input_text": "summarize: def rfind(self, sub, start=None, end=None):                if isinstance(sub, _SeqAbstractBaseClass):            sub = bytes(sub)        elif isinstance(sub, str):            sub = sub.encode(\"ASCII\")        elif not isinstance(sub, (bytes, bytearray)):            raise TypeError(                \"a Seq, MutableSeq, str, bytes, or bytearray object is required, not '%s'\"                % type(sub)            )        return self._data.rfind(sub, start, end)",
        "labels_text": "Return the highest index in the sequence where subsequence sub is found With optional argument start and end return the highest index in the sequence such that the subsequence sub is contained within the sequence region startend Arguments sub a string or another Seq or MutableSeq object to search for start optional integer slice start end optional integer slice end Returns if the subsequence is NOT found eg Locating the last typical start codon AUG in an RNA sequence from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myrnarfindAUG The location of the typical start codon before that can be found by ending the search at position myrnarfindAUG end See the search method to find the location of multiple subsequence at the same time"
    },
    {
        "input_text": "summarize: def index(self, sub, start=None, end=None):                if isinstance(sub, MutableSeq):            sub = sub._data        elif isinstance(sub, Seq):            sub = bytes(sub)        elif isinstance(sub, str):            sub = sub.encode(\"ASCII\")        elif not isinstance(sub, (bytes, bytearray)):            raise TypeError(                \"a Seq, MutableSeq, str, bytes, or bytearray object is required, not '%s'\"                % type(sub)            )        return self._data.index(sub, start, end)",
        "labels_text": "Return the lowest index in the sequence where subsequence sub is found With optional argument start and end return the lowest index in the sequence such that the subsequence sub is contained within the sequence region startend Arguments sub a string or another Seq or MutableSeq object to search for start optional integer slice start end optional integer slice end Raises a ValueError if the subsequence is NOT found eg Locating the first typical start codon AUG in an RNA sequence from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myrnaindexAUG The next typical start codon can then be found by starting the search at position myrnaindexAUG This method performs the same search a the find method However if the subsequence is not found find return while index raise a ValueError myrnaindexT Traceback most recent call last ValueError myrnafindT See the search method to find the location of multiple subsequence at the same time"
    },
    {
        "input_text": "summarize: def startswith(self, prefix, start=None, end=None):                if isinstance(prefix, tuple):            prefix = tuple(                bytes(p) if isinstance(p, _SeqAbstractBaseClass) else p.encode(\"ASCII\")                for p in prefix            )        elif isinstance(prefix, _SeqAbstractBaseClass):            prefix = bytes(prefix)        elif isinstance(prefix, str):            prefix = prefix.encode(\"ASCII\")        return self._data.startswith(prefix, start, end)",
        "labels_text": "Return True if the sequence start with the given prefix False otherwise Return True if the sequence start with the specified prefix a string or another Seq object False otherwise With optional start test sequence beginning at that position With optional end stop comparing sequence at that position prefix can also be a tuple of string to try eg from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myrnastartswithGUC True myrnastartswithAUG False myrnastartswithAUG True myrnastartswithUCC UCA UCG True"
    },
    {
        "input_text": "summarize: def endswith(self, suffix, start=None, end=None):                if isinstance(suffix, tuple):            suffix = tuple(                bytes(p) if isinstance(p, _SeqAbstractBaseClass) else p.encode(\"ASCII\")                for p in suffix            )        elif isinstance(suffix, _SeqAbstractBaseClass):            suffix = bytes(suffix)        elif isinstance(suffix, str):            suffix = suffix.encode(\"ASCII\")        return self._data.endswith(suffix, start, end)",
        "labels_text": "Return True if the sequence end with the given suffix False otherwise Return True if the sequence end with the specified suffix a string or another Seq object False otherwise With optional start test sequence beginning at that position With optional end stop comparing sequence at that position suffix can also be a tuple of string to try eg from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myrnaendswithUUG True myrnaendswithAUG False myrnaendswithAUG True myrnaendswithUCC UCA UUG True"
    },
    {
        "input_text": "summarize: def split(self, sep=None, maxsplit=-1):                if isinstance(sep, _SeqAbstractBaseClass):            sep = bytes(sep)        elif isinstance(sep, str):            sep = sep.encode(\"ASCII\")        return [Seq(part) for part in self._data.split(sep, maxsplit)]",
        "labels_text": "Return a list of subsequence when splitting the sequence by separator sep Return a list of the subsequence in the sequence a Seq object using sep a the delimiter string If maxsplit is given at most maxsplit split are done If maxsplit is omitted all split are made For consistency with the split method of Python string any whitespace tab space newlines is a separator if sep is None the default value eg from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myaa myrnatranslate myaa SeqVMAIVMGRKGARL for pep in myaasplit pep SeqVMAIVMGR SeqKGAR SeqL for pep in myaasplit pep SeqVMAIVMGR SeqKGARL See also the rsplit method which split the sequence starting from the end for pep in myaarsplit pep SeqVMAIVMGRKGAR SeqL"
    },
    {
        "input_text": "summarize: def rsplit(self, sep=None, maxsplit=-1):                if isinstance(sep, _SeqAbstractBaseClass):            sep = bytes(sep)        elif isinstance(sep, str):            sep = sep.encode(\"ASCII\")        return [Seq(part) for part in self._data.rsplit(sep, maxsplit)]",
        "labels_text": "Return a list of subsequence by splitting the sequence from the right Return a list of the subsequence in the sequence a Seq object using sep a the delimiter string If maxsplit is given at most maxsplit split are done If maxsplit is omitted all split are made For consistency with the rsplit method of Python string any whitespace tab space newlines is a separator if sep is None the default value eg from BioSeq import Seq myrna SeqGUCAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAGUUG myaa myrnatranslate myaa SeqVMAIVMGRKGARL for pep in myaarsplit pep SeqVMAIVMGR SeqKGAR SeqL for pep in myaarsplit pep SeqVMAIVMGRKGAR SeqL See also the split method which split the sequence starting from the beginning for pep in myaasplit pep SeqVMAIVMGR SeqKGARL"
    },
    {
        "input_text": "summarize: def upper(self, inplace=False):                data = self._data.upper()        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        else:            return self.__class__(data)",
        "labels_text": "Return the sequence in upper case An uppercase copy of the sequence is returned if inplace is False the default value from BioSeq import Seq MutableSeq myseq SeqVHLTPeeK myseq SeqVHLTPeeK myseqlower Seqvhltpeek mysequpper SeqVHLTPEEK myseq SeqVHLTPeeK The sequence is modified inplace and returned if inplace is True myseq MutableSeqVHLTPeeK myseq MutableSeqVHLTPeeK myseqlower MutableSeqvhltpeek mysequpper MutableSeqVHLTPEEK myseq MutableSeqVHLTPeeK myseqlowerinplaceTrue MutableSeqvhltpeek myseq MutableSeqvhltpeek mysequpperinplaceTrue MutableSeqVHLTPEEK myseq MutableSeqVHLTPEEK As Seq object are immutable a TypeError is raised if upper is called on a Seq object with inplaceTrue See also the lower method"
    },
    {
        "input_text": "summarize: def lower(self, inplace=False):                data = self._data.lower()        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        else:            return self.__class__(data)",
        "labels_text": "Return the sequence in lower case An lowercase copy of the sequence is returned if inplace is False the default value from BioSeq import Seq MutableSeq myseq SeqVHLTPeeK myseq SeqVHLTPeeK myseqlower Seqvhltpeek mysequpper SeqVHLTPEEK myseq SeqVHLTPeeK The sequence is modified inplace and returned if inplace is True myseq MutableSeqVHLTPeeK myseq MutableSeqVHLTPeeK myseqlower MutableSeqvhltpeek mysequpper MutableSeqVHLTPEEK myseq MutableSeqVHLTPeeK myseqlowerinplaceTrue MutableSeqvhltpeek myseq MutableSeqvhltpeek mysequpperinplaceTrue MutableSeqVHLTPEEK myseq MutableSeqVHLTPEEK As Seq object are immutable a TypeError is raised if lower is called on a Seq object with inplaceTrue See also the upper method"
    },
    {
        "input_text": "summarize: def isupper(self):                return self._data.isupper()",
        "labels_text": "Return True if all ASCII character in data are uppercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def islower(self):                return self._data.islower()",
        "labels_text": "Return True if all ASCII character in data are lowercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def complement(self, inplace=False):                ttable = _dna_complement_table        try:            data = self._data.translate(ttable)        except UndefinedSequenceError:            # complement of an undefined sequence is an undefined sequence            # of the same length            return self        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        return self.__class__(data)",
        "labels_text": "Return the complement a a DNA sequence SeqCGAcomplement SeqGCT Any U in the sequence is treated a a T SeqCGAUTcomplement SeqGCTAA In contrast complementrna return an RNA sequence SeqCGAUTcomplementrna SeqGCUAA The sequence is modified inplace and returned if inplace is True myseq MutableSeqCGA myseq MutableSeqCGA myseqcomplement MutableSeqGCT myseq MutableSeqCGA myseqcomplementinplaceTrue MutableSeqGCT myseq MutableSeqGCT As Seq object are immutable a TypeError is raised if complementrna is called on a Seq object with inplaceTrue"
    },
    {
        "input_text": "summarize: def complement_rna(self, inplace=False):                try:            data = self._data.translate(_rna_complement_table)        except UndefinedSequenceError:            # complement of an undefined sequence is an undefined sequence            # of the same length            return self        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        return self.__class__(data)",
        "labels_text": "Return the complement a an RNA sequence SeqCGAcomplementrna SeqGCU Any T in the sequence is treated a a U SeqCGAUTcomplementrna SeqGCUAA In contrast complement return a DNA sequence by default SeqCGAcomplement SeqGCT The sequence is modified inplace and returned if inplace is True myseq MutableSeqCGA myseq MutableSeqCGA myseqcomplementrna MutableSeqGCU myseq MutableSeqCGA myseqcomplementrnainplaceTrue MutableSeqGCU myseq MutableSeqGCU As Seq object are immutable a TypeError is raised if complementrna is called on a Seq object with inplaceTrue"
    },
    {
        "input_text": "summarize: def reverse_complement(self, inplace=False):                try:            data = self._data.translate(_dna_complement_table)        except UndefinedSequenceError:            # reverse complement of an undefined sequence is an undefined sequence            # of the same length            return self        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[::-1] = data            return self        return self.__class__(data[::-1])",
        "labels_text": "Return the reverse complement a a DNA sequence SeqCGAreversecomplement SeqTCG Any U in the sequence is treated a a T SeqCGAUTreversecomplement SeqAATCG In contrast reversecomplementrna return an RNA sequence SeqCGAreversecomplementrna SeqUCG The sequence is modified inplace and returned if inplace is True myseq MutableSeqCGA myseq MutableSeqCGA myseqreversecomplement MutableSeqTCG myseq MutableSeqCGA myseqreversecomplementinplaceTrue MutableSeqTCG myseq MutableSeqTCG As Seq object are immutable a TypeError is raised if reversecomplement is called on a Seq object with inplaceTrue"
    },
    {
        "input_text": "summarize: def reverse_complement_rna(self, inplace=False):                try:            data = self._data.translate(_rna_complement_table)        except UndefinedSequenceError:            # reverse complement of an undefined sequence is an undefined sequence            # of the same length            return self        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[::-1] = data            return self        return self.__class__(data[::-1])",
        "labels_text": "Return the reverse complement a an RNA sequence SeqCGAreversecomplementrna SeqUCG Any T in the sequence is treated a a U SeqCGAUTreversecomplementrna SeqAAUCG In contrast reversecomplement return a DNA sequence SeqCGAreversecomplement SeqTCG The sequence is modified inplace and returned if inplace is True myseq MutableSeqCGA myseq MutableSeqCGA myseqreversecomplementrna MutableSeqUCG myseq MutableSeqCGA myseqreversecomplementrnainplaceTrue MutableSeqUCG myseq MutableSeqUCG As Seq object are immutable a TypeError is raised if reversecomplementrna is called on a Seq object with inplaceTrue"
    },
    {
        "input_text": "summarize: def transcribe(self, inplace=False):                data = self._data.replace(b\"T\", b\"U\").replace(b\"t\", b\"u\")        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        return self.__class__(data)",
        "labels_text": "Transcribe a DNA sequence into RNA and return the RNA sequence a a new Seq object Following the usual convention the sequence is interpreted a the coding strand of the DNA double helix not the template strand This mean we can get the RNA sequence just by switching T to U from BioSeq import Seq codingdna SeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG codingdna SeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG codingdnatranscribe SeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG The sequence is modified inplace and returned if inplace is True sequence MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG sequence MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG sequencetranscribe MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG sequence MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG sequencetranscribeinplaceTrue MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG sequence MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG As Seq object are immutable a TypeError is raised if transcribe is called on a Seq object with inplaceTrue Trying to transcribe an RNA sequence ha no effect If you have a nucleotide sequence which might be DNA or RNA or even a mixture calling the transcribe method will ensure any T becomes U Trying to transcribe a protein sequence will replace any T for Threonine with U for Selenocysteine which ha no biologically plausible rational from BioSeq import Seq myprotein SeqMAIVMGRT myproteintranscribe SeqMAIVMGRU"
    },
    {
        "input_text": "summarize: def back_transcribe(self, inplace=False):                data = self._data.replace(b\"U\", b\"T\").replace(b\"u\", b\"t\")        if inplace:            if not isinstance(self._data, bytearray):                raise TypeError(\"Sequence is immutable\")            self._data[:] = data            return self        return self.__class__(data)",
        "labels_text": "Return the DNA sequence from an RNA sequence by creating a new Seq object from BioSeq import Seq messengerrna SeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG messengerrna SeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG messengerrnabacktranscribe SeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG The sequence is modified inplace and returned if inplace is True sequence MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG sequence MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG sequencebacktranscribe MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG sequence MutableSeqAUGGCCAUUGUAAUGGGCCGCUGAAAGGGUGCCCGAUAG sequencebacktranscribeinplaceTrue MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG sequence MutableSeqATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG As Seq object are immutable a TypeError is raised if transcribe is called on a Seq object with inplaceTrue Trying to backtranscribe DNA ha no effect If you have a nucleotide sequence which might be DNA or RNA or even a mixture calling the backtranscribe method will ensure any U becomes T Trying to backtranscribe a protein sequence will replace any U for Selenocysteine with T for Threonine which is biologically meaningless from BioSeq import Seq myprotein SeqMAIVMGRU myproteinbacktranscribe SeqMAIVMGRT"
    },
    {
        "input_text": "summarize: def defined(self):                if isinstance(self._data, (bytes, bytearray)):            return True        else:            return self._data.defined",
        "labels_text": "Return True if the sequence is defined False if undefined or partially defined Zerolength sequence are always considered to be defined"
    },
    {
        "input_text": "summarize: def defined_ranges(self):                if isinstance(self._data, (bytes, bytearray)):            length = len(self)            if length > 0:                return ((0, length),)            else:                return ()        else:            return self._data.defined_ranges",
        "labels_text": "Return a tuple of the range where the sequence content is defined The return value ha the format start end start end"
    },
    {
        "input_text": "summarize: def __hash__(self):                return hash(self._data)",
        "labels_text": "Hash of the sequence a a string for comparison See Seq object comparison documentation method eq in particular a this ha changed in Biopython Older version would hash on object identity"
    },
    {
        "input_text": "summarize: def __delitem__(self, index):                # Could be deleting a single letter, or a slice        del self._data[index]",
        "labels_text": "Delete a subsequence of single letter myseq MutableSeqACTCGACGTCG del myseq myseq MutableSeqCTCGACGTCG"
    },
    {
        "input_text": "summarize: def append(self, c):                self._data.append(ord(c.encode(\"ASCII\")))",
        "labels_text": "Add a subsequence to the mutable sequence object myseq MutableSeqACTCGACGTCG myseqappendA myseq MutableSeqACTCGACGTCGA No return value"
    },
    {
        "input_text": "summarize: def insert(self, i, c):                self._data.insert(i, ord(c.encode(\"ASCII\")))",
        "labels_text": "Add a subsequence to the mutable sequence object at a given index myseq MutableSeqACTCGACGTCG myseqinsertA myseq MutableSeqAACTCGACGTCG myseqinsertG myseq MutableSeqAACTCGACGGTCG No return value"
    },
    {
        "input_text": "summarize: def pop(self, i=(-1)):                c = self._data[i]        del self._data[i]        return chr(c)",
        "labels_text": "Remove a subsequence of a single letter at given index myseq MutableSeqACTCGACGTCG myseqpop G myseq MutableSeqACTCGACGTC myseqpop C myseq MutableSeqACTCGACGT Returns the last character of the sequence"
    },
    {
        "input_text": "summarize: def remove(self, item):                codepoint = ord(item)        try:            self._data.remove(codepoint)        except ValueError:            raise ValueError(\"value not found in MutableSeq\") from None",
        "labels_text": "Remove a subsequence of a single letter from mutable sequence myseq MutableSeqACTCGACGTCG myseqremoveC myseq MutableSeqATCGACGTCG myseqremoveA myseq MutableSeqTCGACGTCG No return value"
    },
    {
        "input_text": "summarize: def reverse(self):                self._data.reverse()",
        "labels_text": "Modify the mutable sequence to reverse itself No return value"
    },
    {
        "input_text": "summarize: def extend(self, other):                if isinstance(other, MutableSeq):            self._data.extend(other._data)        elif isinstance(other, Seq):            self._data.extend(bytes(other))        elif isinstance(other, str):            self._data.extend(other.encode(\"ASCII\"))        else:            raise TypeError(\"expected a string, Seq or MutableSeq\")",
        "labels_text": "Add a sequence to the original mutable sequence object myseq MutableSeqACTCGACGTCG myseqextendA myseq MutableSeqACTCGACGTCGA myseqextendTTT myseq MutableSeqACTCGACGTCGATTT No return value"
    },
    {
        "input_text": "summarize: def __init__(self, length):                self._length = length        super().__init__()",
        "labels_text": "Initialize the object with the sequence length The calling function is responsible for ensuring that the length is greater than zero"
    },
    {
        "input_text": "summarize: def upper(self):                # An upper case copy of an undefined sequence is an undefined        # sequence of the same length        return _UndefinedSequenceData(self._length)",
        "labels_text": "Return an upper case copy of the sequence"
    },
    {
        "input_text": "summarize: def lower(self):                # A lower case copy of an undefined sequence is an undefined        # sequence of the same length        return _UndefinedSequenceData(self._length)",
        "labels_text": "Return a lower case copy of the sequence"
    },
    {
        "input_text": "summarize: def isupper(self):                # Character case is irrelevant for an undefined sequence        raise UndefinedSequenceError(\"Sequence content is undefined\")",
        "labels_text": "Return True if all ASCII character in data are uppercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def islower(self):                # Character case is irrelevant for an undefined sequence        raise UndefinedSequenceError(\"Sequence content is undefined\")",
        "labels_text": "Return True if all ASCII character in data are lowercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def replace(self, old, new):                # Replacing substring old by new in an undefined sequence will result        # in an undefined sequence of the same length, if old and new have the        # number of characters.        if len(old) != len(new):            raise UndefinedSequenceError(\"Sequence content is undefined\")        return _UndefinedSequenceData(self._length)",
        "labels_text": "Return a copy with all occurrence of substring old replaced by new"
    },
    {
        "input_text": "summarize: def defined(self):                return False",
        "labels_text": "Return False a the sequence is not defined and ha a nonzero length"
    },
    {
        "input_text": "summarize: def defined_ranges(self):                return ()",
        "labels_text": "Return a tuple of the range where the sequence content is defined As the sequence content of an UndefinedSequenceData object is fully undefined the return value is always an empty tuple"
    },
    {
        "input_text": "summarize: def __init__(self, length, data):                self._length = length        self._data = data        super().__init__()",
        "labels_text": "Initialize with the sequence length and defined sequence segment The calling function is responsible for ensuring that the length is greater than zero"
    },
    {
        "input_text": "summarize: def upper(self):                data = {start: seq.upper() for start, seq in self._data.items()}        return _PartiallyDefinedSequenceData(self._length, data)",
        "labels_text": "Return an upper case copy of the sequence"
    },
    {
        "input_text": "summarize: def lower(self):                data = {start: seq.lower() for start, seq in self._data.items()}        return _PartiallyDefinedSequenceData(self._length, data)",
        "labels_text": "Return a lower case copy of the sequence"
    },
    {
        "input_text": "summarize: def isupper(self):                # Character case is irrelevant for an undefined sequence        raise UndefinedSequenceError(\"Sequence content is only partially defined\")",
        "labels_text": "Return True if all ASCII character in data are uppercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def islower(self):                # Character case is irrelevant for an undefined sequence        raise UndefinedSequenceError(\"Sequence content is only partially defined\")",
        "labels_text": "Return True if all ASCII character in data are lowercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def translate(self, table, delete=b\"\"):                items = self._data.items()        data = {start: seq.translate(table, delete) for start, seq in items}        return _PartiallyDefinedSequenceData(self._length, data)",
        "labels_text": "Return a copy with each character mapped by the given translation table table Translation table which must be a byte object of length All character occurring in the optional argument delete are removed The remaining character are mapped through the given translation table"
    },
    {
        "input_text": "summarize: def defined(self):                return False",
        "labels_text": "Return False a the sequence is not fully defined and ha a nonzero length"
    },
    {
        "input_text": "summarize: def defined_ranges(self):                return tuple((start, start + len(seq)) for start, seq in self._data.items())",
        "labels_text": "Return a tuple of the range where the sequence content is defined The return value ha the format start end start end"
    },
    {
        "input_text": "summarize: def transcribe(dna):        if isinstance(dna, Seq):        return dna.transcribe()    elif isinstance(dna, MutableSeq):        return Seq(dna).transcribe()    else:        return dna.replace(\"T\", \"U\").replace(\"t\", \"u\")",
        "labels_text": "Transcribe a DNA sequence into RNA Following the usual convention the sequence is interpreted a the coding strand of the DNA double helix not the template strand This mean we can get the RNA sequence just by switching T to U If given a string return a new string object Given a Seq or MutableSeq return a new Seq object eg transcribeACTGN ACUGN"
    },
    {
        "input_text": "summarize: def back_transcribe(rna):        if isinstance(rna, Seq):        return rna.back_transcribe()    elif isinstance(rna, MutableSeq):        return Seq(rna).back_transcribe()    else:        return rna.replace(\"U\", \"T\").replace(\"u\", \"t\")",
        "labels_text": "Return the RNA sequence backtranscribed into DNA If given a string return a new string object Given a Seq or MutableSeq return a new Seq object eg backtranscribeACUGN ACTGN"
    },
    {
        "input_text": "summarize: def _test():        print(\"Running doctests...\")    import doctest    doctest.testmod(optionflags=doctest.IGNORE_EXCEPTION_DETAIL)    print(\"Done\")",
        "labels_text": "Run the BioSeq module doctests PRIVATE"
    },
    {
        "input_text": "summarize: def _get_strand(self):                warnings.warn(            \"Please use .location.strand rather than .strand\",            BiopythonDeprecationWarning,        )        return self.location.strand",
        "labels_text": "Get function for the strand property PRIVATE"
    },
    {
        "input_text": "summarize: def _set_strand(self, value):                warnings.warn(            \"Please use .location.strand rather than .strand\",            BiopythonDeprecationWarning,        )        try:            self.location.strand = value        except AttributeError:            if self.location is None:                if value is not None:                    raise ValueError(\"Can't set strand without a location.\") from None            else:                raise",
        "labels_text": "Set function for the strand property PRIVATE"
    },
    {
        "input_text": "summarize: def _get_ref(self):                warnings.warn(            \"Please use .location.ref rather than .ref\",            BiopythonDeprecationWarning,        )        try:            return self.location.ref        except AttributeError:            return None",
        "labels_text": "Get function for the reference property PRIVATE"
    },
    {
        "input_text": "summarize: def _set_ref(self, value):                warnings.warn(            \"Please use .location.ref rather than .ref\",            BiopythonDeprecationWarning,        )        try:            self.location.ref = value        except AttributeError:            if self.location is None:                if value is not None:                    raise ValueError(\"Can't set ref without a location.\") from None            else:                raise",
        "labels_text": "Set function for the reference property PRIVATE"
    },
    {
        "input_text": "summarize: def _get_ref_db(self):                warnings.warn(            \"Please use .location.ref_db rather than .ref_db\",            BiopythonDeprecationWarning,        )        try:            return self.location.ref_db        except AttributeError:            return None",
        "labels_text": "Get function for the database reference property PRIVATE"
    },
    {
        "input_text": "summarize: def _set_ref_db(self, value):                warnings.warn(            \"Please use .location.ref_db rather than .ref_db\",            BiopythonDeprecationWarning,        )        self.location.ref_db = value",
        "labels_text": "Set function for the database reference property PRIVATE"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                return (            isinstance(other, SeqFeature)            and self.id == other.id            and self.type == other.type            and self.location == other.location            and self.qualifiers == other.qualifiers        )",
        "labels_text": "Check if two SeqFeature object should be considered equal"
    },
    {
        "input_text": "summarize: def __repr__(self):                answer = f\"{self.__class__.__name__}({self.location!r}\"        if self.type:            answer += f\", type={self.type!r}\"        if self.id and self.id != \"<unknown id>\":            answer += f\", id={self.id!r}\"        if self.qualifiers:            answer += \", qualifiers=...\"        answer += \")\"        return answer",
        "labels_text": "Represent the feature a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                out = f\"type: {self.type}\\n\"        out += f\"location: {self.location}\\n\"        if self.id and self.id != \"<unknown id>\":            out += f\"id: {self.id}\\n\"        out += \"qualifiers:\\n\"        for qual_key in sorted(self.qualifiers):            out += f\"    Key: {qual_key}, Value: {self.qualifiers[qual_key]}\\n\"        return out",
        "labels_text": "Return the full feature a a python string"
    },
    {
        "input_text": "summarize: def _shift(self, offset):                return SeqFeature(            location=self.location._shift(offset),            type=self.type,            id=self.id,            qualifiers=self.qualifiers.copy(),        )",
        "labels_text": "Return a copy of the feature with it location shifted PRIVATE The annotation qualifier are copied"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return SeqFeature(            location=self.location._flip(length),            type=self.type,            id=self.id,            qualifiers=self.qualifiers.copy(),        )",
        "labels_text": "Return a copy of the feature with it location flipped PRIVATE The argument length give the length of the parent sequence For example a location strand with parent length becomes after flipping strand Strandless None or unknown strand remain like that just their end point are changed The annotation qualifier are copied"
    },
    {
        "input_text": "summarize: def extract(self, parent_sequence, references=None):                if self.location is None:            raise ValueError(                \"The feature's .location is None. Check the \"                \"sequence file for a valid location.\"            )        return self.location.extract(parent_sequence, references=references)",
        "labels_text": "Extract the feature sequence from supplied parent sequence The parentsequence can be a Seq like object or a string and will generally return an object of the same type The exception to this is a MutableSeq a the parent sequence will return a Seq object This should cope with complex location including complement join and fuzzy position Even mixed strand feature should work This also cover feature on protein sequence eg domain although here reverse strand feature are not permitted If the location refers to other record they must be supplied in the optional dictionary reference from BioSeq import Seq from BioSeqFeature import SeqFeature SimpleLocation seq SeqMKQHKAMIVALIVICITAVVAAL f SeqFeatureSimpleLocation typedomain fextractseq SeqVALIVIC If the SimpleLocation is None eg when parsing invalid locus location in the GenBank parser extract will raise a ValueError from BioSeq import Seq from BioSeqFeature import SeqFeature seq SeqMKQHKAMIVALIVICITAVVAAL f SeqFeatureNone typedomain fextractseq Traceback most recent call last ValueError The feature location is None Check the sequence file for a valid location Note currently only compound feature of type join are supported"
    },
    {
        "input_text": "summarize: def __bool__(self):                return True",
        "labels_text": "Boolean value of an instance of this class True This behavior is for backwards compatibility since until the len method wa added a SeqFeature always evaluated a True Note that in comparison Seq object string list etc will all evaluate to False if they have length zero WARNING The SeqFeature may in future evaluate to False when it length is zero in order to better match normal python behavior"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.location)",
        "labels_text": "Return the length of the region where the feature is located from BioSeq import Seq from BioSeqFeature import SeqFeature SimpleLocation seq SeqMKQHKAMIVALIVICITAVVAAL f SeqFeatureSimpleLocation typedomain lenf fextractseq SeqVALIVIC lenfextractseq This is a proxy for taking the length of the feature location lenflocation For simple feature this is the same a the region spanned end position minus start position using Pythonic counting However for a compound location eg a CDS a the join of several exon the gap are not counted eg intron This ensures that lenf match lenfextractparentseq and also make sure thing work properly with feature wrapping the origin etc"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.location)",
        "labels_text": "Iterate over the parent position within the feature The iteration order is strand aware and can be thought of a moving along the feature using the parent sequence coordinate from BioSeqFeature import SeqFeature SimpleLocation f SeqFeatureSimpleLocation strand typedomain lenf for i in f printi listf This is a proxy for iterating over the location listflocation"
    },
    {
        "input_text": "summarize: def __contains__(self, value):                return value in self.location",
        "labels_text": "Check if an integer position is within the feature from BioSeqFeature import SeqFeature SimpleLocation f SeqFeatureSimpleLocation strand typedomain lenf i for i in range if i in f For example to see which feature include a SNP position you could use this from Bio import SeqIO record SeqIOreadGenBankNCgb gb for f in recordfeatures if in f print s ftype flocation source gene tRNA join Note that for a feature defined a a join of several subfeatures eg the union of several exon the gap are not checked eg intron In this example the tRNA location is defined in the GenBank file a complementjoin so that position fall in the gap for f in recordfeatures if in f print s ftype flocation source gene Note that additional care may be required with fuzzy location for example just before a BeforePosition from BioSeqFeature import SeqFeature SimpleLocation from BioSeqFeature import BeforePosition f SeqFeatureSimpleLocationBeforePosition typedomain lenf i for i in range if i in f Note that is is a proxy for testing membership on the location i for i in range if i in flocation"
    },
    {
        "input_text": "summarize: def __init__(self):                self.location = []        self.authors = \"\"        self.consrtm = \"\"        self.title = \"\"        self.journal = \"\"        self.medline_id = \"\"        self.pubmed_id = \"\"        self.comment = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                # TODO - Update this is __init__ later accepts values        return f\"{self.__class__.__name__}(title={self.title!r}, ...)\"",
        "labels_text": "Represent the Reference object a a string for debugging"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                return (            self.authors == other.authors            and self.consrtm == other.consrtm            and self.title == other.title            and self.journal == other.journal            and self.medline_id == other.medline_id            and self.pubmed_id == other.pubmed_id            and self.comment == other.comment            and self.location == other.location        )",
        "labels_text": "Check if two Reference object should be considered equal Note prior to Biopython the location wa not compared a until then eq for the SimpleLocation class wa not defined"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}(...)\"",
        "labels_text": "Represent the Location object a a string for debugging"
    },
    {
        "input_text": "summarize: def _get_strand(self):                return self._strand",
        "labels_text": "Get function for the strand property PRIVATE"
    },
    {
        "input_text": "summarize: def _set_strand(self, value):                if value not in [+1, -1, 0, None]:            raise ValueError(f\"Strand should be +1, -1, 0 or None, not {value!r}\")        self._strand = value",
        "labels_text": "Set function for the strand property PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                optional = \"\"        if self.strand is not None:            optional += f\", strand={self.strand!r}\"        if self.ref is not None:            optional += f\", ref={self.ref!r}\"        if self.ref_db is not None:            optional += f\", ref_db={self.ref_db!r}\"        return f\"{self.__class__.__name__}({self.start!r}, {self.end!r}{optional})\"",
        "labels_text": "Represent the SimpleLocation object a a string for debugging"
    },
    {
        "input_text": "summarize: def __add__(self, other):                if isinstance(other, SimpleLocation):            return CompoundLocation([self, other])        elif isinstance(other, int):            return self._shift(other)        else:            # This will allow CompoundLocation's __radd__ to be called:            return NotImplemented",
        "labels_text": "Combine location with another SimpleLocation object or shift it You can add two feature location to make a join CompoundLocation from BioSeqFeature import SimpleLocation f SimpleLocation f SimpleLocation combined f f printcombined join This is thus equivalent to from BioSeqFeature import CompoundLocation join CompoundLocationf f printjoin join You can also use sum in this way join sumf f printjoin join Furthermore you can combine a SimpleLocation with a CompoundLocation in this way Separately adding an integer will give a new SimpleLocation with it start and end offset by that amount For example printf printf print f This can be useful when editing annotation"
    },
    {
        "input_text": "summarize: def __radd__(self, other):                if isinstance(other, int):            return self._shift(other)        else:            return NotImplemented",
        "labels_text": "Return a SimpleLocation object by shifting the location by an integer amount"
    },
    {
        "input_text": "summarize: def __sub__(self, other):                if isinstance(other, int):            return self._shift(-other)        else:            return NotImplemented",
        "labels_text": "Subtracting an integer will shift the start and end by that amount from BioSeqFeature import SimpleLocation f SimpleLocation printf printf This can be useful when editing annotation You can also add an integer to a feature location which shift in the opposite direction"
    },
    {
        "input_text": "summarize: def __nonzero__(self):                return True",
        "labels_text": "Return True regardless of the length of the feature This behavior is for backwards compatibility since until the len method wa added a SimpleLocation always evaluated a True Note that in comparison Seq object string list etc will all evaluate to False if they have length zero WARNING The SimpleLocation may in future evaluate to False when it length is zero in order to better match normal python behavior"
    },
    {
        "input_text": "summarize: def __len__(self):                return int(self._end) - int(self._start)",
        "labels_text": "Return the length of the region described by the SimpleLocation object Note that extra care may be needed for fuzzy location eg from BioSeqFeature import SimpleLocation from BioSeqFeature import BeforePosition AfterPosition loc SimpleLocationBeforePosition AfterPosition lenloc"
    },
    {
        "input_text": "summarize: def __contains__(self, value):                if not isinstance(value, int):            raise ValueError(                \"Currently we only support checking for integer \"                \"positions being within a SimpleLocation.\"            )        if value < self._start or value >= self._end:            return False        else:            return True",
        "labels_text": "Check if an integer position is within the SimpleLocation object Note that extra care may be needed for fuzzy location eg from BioSeqFeature import SimpleLocation from BioSeqFeature import BeforePosition AfterPosition loc SimpleLocationBeforePosition AfterPosition lenloc i for i in range if i in loc"
    },
    {
        "input_text": "summarize: def __iter__(self):                if self.strand == -1:            yield from range(self._end - 1, self._start - 1, -1)        else:            yield from range(self._start, self._end)",
        "labels_text": "Iterate over the parent position within the SimpleLocation object from BioSeqFeature import SimpleLocation from BioSeqFeature import BeforePosition AfterPosition loc SimpleLocationBeforePosition AfterPosition lenloc for i in loc printi listloc i for i in range if i in loc Note this is strand aware loc SimpleLocationBeforePosition AfterPosition strand listloc"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if not isinstance(other, SimpleLocation):            return False        return (            self._start == other.start            and self._end == other.end            and self._strand == other.strand            and self.ref == other.ref            and self.ref_db == other.ref_db        )",
        "labels_text": "Implement equality by comparing all the location attribute"
    },
    {
        "input_text": "summarize: def _shift(self, offset):                # TODO - What if offset is a fuzzy position?        if self.ref or self.ref_db:            return self        return SimpleLocation(            start=self._start + offset,            end=self._end + offset,            strand=self.strand,        )",
        "labels_text": "Return a copy of the SimpleLocation shifted by an offset PRIVATE Returns self when location is relative to an external reference"
    },
    {
        "input_text": "summarize: def parts(self):                return [self]",
        "labels_text": "Read only list of section always one the SimpleLocation object This is a convenience property allowing you to write code handling both SimpleLocation object with one part and more complex CompoundLocation object with multiple part interchangeably"
    },
    {
        "input_text": "summarize: def start(self):                return self._start",
        "labels_text": "Start location left most minimum value regardless of strand Read only return an integer like position object possibly a fuzzy position"
    },
    {
        "input_text": "summarize: def end(self):                return self._end",
        "labels_text": "End location right most maximum value regardless of strand Read only return an integer like position object possibly a fuzzy position"
    },
    {
        "input_text": "summarize: def __init__(self, parts, operator=\"join\"):                self.operator = operator        self.parts = list(parts)        for loc in self.parts:            if not isinstance(loc, SimpleLocation):                raise ValueError(                    \"CompoundLocation should be given a list of \"                    \"SimpleLocation objects, not %s\" % loc.__class__                )        if len(parts) < 2:            raise ValueError(                f\"CompoundLocation should have at least 2 parts, not {parts!r}\"            )",
        "labels_text": "Initialize the class from BioSeqFeature import SimpleLocation CompoundLocation f SimpleLocation strand f SimpleLocation strand f CompoundLocationf f lenf lenf lenf lenlistf True printfoperator join in f False in f True fstrand Notice that the strand of the compound location is computed automatically in the case of mixed strand on the sublocations the overall strand is set to None f CompoundLocationSimpleLocation strand SimpleLocation strand printfstrand None lenf listf The example above doing listf iterates over the coordinate within the feature This allows you to use max and min on the location to find the range covered minf maxf More generally you can use the compound location start and end which give the full span covered start end full sequence length fstart minf True fend maxf True This is consistent with the behavior of the SimpleLocation for a single region where again the start and end do not necessarily give the biological start and end but rather the minimal and maximal coordinate boundary Note that adding location provides a more intuitive method of construction f SimpleLocation strand SimpleLocation strand lenf listf"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"%s{%s}\" % (self.operator, \", \".join(str(loc) for loc in self.parts))",
        "labels_text": "Return a representation of the CompoundLocation object with python counting"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}({self.parts!r}, {self.operator!r})\"",
        "labels_text": "Represent the CompoundLocation object a string for debugging"
    },
    {
        "input_text": "summarize: def _set_strand(self, value):                # Should this be allowed/encouraged?        for loc in self.parts:            loc.strand = value",
        "labels_text": "Set function for the strand property PRIVATE"
    },
    {
        "input_text": "summarize: def __radd__(self, other):                if isinstance(other, SimpleLocation):            return CompoundLocation([other] + self.parts, self.operator)        elif isinstance(other, int):            return self._shift(other)        else:            raise NotImplementedError",
        "labels_text": "Add a feature to the left"
    },
    {
        "input_text": "summarize: def __contains__(self, value):                for loc in self.parts:            if value in loc:                return True        return False",
        "labels_text": "Check if an integer position is within the CompoundLocation object"
    },
    {
        "input_text": "summarize: def __nonzero__(self):                return True",
        "labels_text": "Return True regardless of the length of the feature This behavior is for backwards compatibility since until the len method wa added a SimpleLocation always evaluated a True Note that in comparison Seq object string list etc will all evaluate to False if they have length zero WARNING The SimpleLocation may in future evaluate to False when it length is zero in order to better match normal python behavior"
    },
    {
        "input_text": "summarize: def __len__(self):                return sum(len(loc) for loc in self.parts)",
        "labels_text": "Return the length of the CompoundLocation object"
    },
    {
        "input_text": "summarize: def __iter__(self):                for loc in self.parts:            yield from loc",
        "labels_text": "Iterate over the parent position within the CompoundLocation object"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if not isinstance(other, CompoundLocation):            return False        if len(self.parts) != len(other.parts):            return False        if self.operator != other.operator:            return False        for self_part, other_part in zip(self.parts, other.parts):            if self_part != other_part:                return False        return True",
        "labels_text": "Check if all part of CompoundLocation are equal to all part of other CompoundLocation"
    },
    {
        "input_text": "summarize: def _shift(self, offset):                return CompoundLocation(            [loc._shift(offset) for loc in self.parts], self.operator        )",
        "labels_text": "Return a copy of the CompoundLocation shifted by an offset PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                if all(loc.strand is None for loc in self.parts):            return CompoundLocation(                [loc._flip(length) for loc in self.parts[::-1]], self.operator            )        else:            return CompoundLocation(                [loc._flip(length) for loc in self.parts], self.operator            )",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE Note that the order of the part is NOT reversed unless all part have strandNone since the order ha meaning for stranded feature Consider a CDS on the forward strand with exon small medium and large in length Once we change the frame of reference to the reverse complement strand the start codon is still part of the small exon and the stop codon still part of the large exon so the part order remains the same Here is an artificial example were the feature map to the two upper case region and the lower case run of n are not used from BioSeq import Seq from BioSeqFeature import SimpleLocation dna SeqnnnnnAGCATCCTGCTGTACnnnnnnnnGAGAMTGCCATGCCCCTGGAGTGAnnnnn small SimpleLocation strand large SimpleLocation strand location small large printsmall printlarge printlocation join for part in locationparts printlenpart As you can see this is a silly example where each exon is a word printsmallextractdnatranslate SILLY printlargeextractdnatranslate EXAMPLE printlocationextractdnatranslate SILLYEXAMPLE for part in locationparts printpartextractdnatranslate SILLY EXAMPLE Now let look at this from the reverse strand frame of reference flippeddna dnareversecomplement flippedlocation locationfliplendna printflippedlocationextractflippeddnatranslate SILLYEXAMPLE for part in flippedlocationparts printpartextractflippeddnatranslate SILLY EXAMPLE The key point here is the first part of the CompoundFeature is still the small exon while the second part is still the large exon for part in flippedlocationparts printlenpart printflippedlocation join Notice the part are not reversed However there wa a bug here in older version of Biopython which would have given join and the translation would have wrongly been EXAMPLESILLY instead When all the part have strand None the order of the part is reversed In principle this doe not change the meaning of the location but improves the representation of feature location spanning the origin in circular molecule loc SimpleLocation None SimpleLocation None printloc join printlocflip join join would not be properly represented in SnapGene Benchling etc"
    },
    {
        "input_text": "summarize: def start(self):                return min(loc.start for loc in self.parts)",
        "labels_text": "Start location left most minimum value regardless of strand Read only return an integer like position object possibly a fuzzy position For the special case of a CompoundLocation wrapping the origin of a circular genome this will return zero"
    },
    {
        "input_text": "summarize: def end(self):                return max(loc.end for loc in self.parts)",
        "labels_text": "End location right most maximum value regardless of strand Read only return an integer like position object possibly a fuzzy position For the special case of a CompoundLocation wrapping the origin of a circular genome this will match the genome length"
    },
    {
        "input_text": "summarize: def ref(self):                return None",
        "labels_text": "Not present in CompoundLocation dummy method for API compatibility"
    },
    {
        "input_text": "summarize: def ref_db(self):                return None",
        "labels_text": "Not present in CompoundLocation dummy method for API compatibility"
    },
    {
        "input_text": "summarize: def extract(self, parent_sequence, references=None):                # This copes with mixed strand features & all on reverse:        parts = [            loc.extract(parent_sequence, references=references) for loc in self.parts        ]        f_seq = functools.reduce(lambda x, y: x + y, parts)        return f_seq",
        "labels_text": "Extract the sequence from supplied parent sequence using the CompoundLocation object The parentsequence can be a Seq like object or a string and will generally return an object of the same type The exception to this is a MutableSeq a the parent sequence will return a Seq object If the location refers to other record they must be supplied in the optional dictionary reference from BioSeq import Seq from BioSeqFeature import SimpleLocation CompoundLocation seq SeqMKQHKAMIVALIVICITAVVAAL fl SimpleLocation fl SimpleLocation fl CompoundLocationflfl flextractseq SeqQHKAMILIVIC"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}(...)\"",
        "labels_text": "Represent the Position object a a string for debugging"
    },
    {
        "input_text": "summarize: def __new__(cls, position, extension=0):                if extension != 0:            raise AttributeError(f\"Non-zero extension {extension} for exact position.\")        return int.__new__(cls, position)",
        "labels_text": "Create an ExactPosition object"
    },
    {
        "input_text": "summarize: def __str__(self):                return str(int(self))",
        "labels_text": "Return a representation of the ExactPosition object with python counting"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i)\" % (self.__class__.__name__, int(self))",
        "labels_text": "Represent the ExactPosition object a a string for debugging"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                # By default preserve any subclass        return self.__class__(int(self) + offset)",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                # By default preserve any subclass        return self.__class__(length - int(self))",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}()\"",
        "labels_text": "Represent the UnknownPosition object a a string for debugging"
    },
    {
        "input_text": "summarize: def __hash__(self):                return hash(None)",
        "labels_text": "Return the hash value of the UnknownPosition object"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return self",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __new__(cls, position, left, right):                if not (position == left or position == right):            raise RuntimeError(                \"WithinPosition: %r should match left %r or \"                \"right %r\" % (position, left, right)            )        obj = int.__new__(cls, position)        obj._left = left        obj._right = right        return obj",
        "labels_text": "Create a WithinPosition object"
    },
    {
        "input_text": "summarize: def __getnewargs__(self):                return (int(self), self._left, self._right)",
        "labels_text": "Return the argument accepted by new Necessary to allow pickling and unpickling of class instance"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i, left=%i, right=%i)\" % (            self.__class__.__name__,            int(self),            self._left,            self._right,        )",
        "labels_text": "Represent the WithinPosition object a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"({self._left}.{self._right})\"",
        "labels_text": "Return a representation of the WithinPosition object with python counting"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self.__class__(            int(self) + offset, self._left + offset, self._right + offset        )",
        "labels_text": "Return a copy of the position object with it location shifted"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return self.__class__(            length - int(self), length - self._right, length - self._left        )",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __new__(cls, position, left, right):                assert position == left or position == right        # TODO - public API for getting left/right, especially the unknown one        obj = int.__new__(cls, position)        obj._left = left        obj._right = right        return obj",
        "labels_text": "Create a new instance in BetweenPosition object"
    },
    {
        "input_text": "summarize: def __getnewargs__(self):                return (int(self), self._left, self._right)",
        "labels_text": "Return the argument accepted by new Necessary to allow pickling and unpickling of class instance"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i, left=%i, right=%i)\" % (            self.__class__.__name__,            int(self),            self._left,            self._right,        )",
        "labels_text": "Represent the BetweenPosition object a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"({self._left}^{self._right})\"",
        "labels_text": "Return a representation of the BetweenPosition object with python counting"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self.__class__(            int(self) + offset, self._left + offset, self._right + offset        )",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return self.__class__(            length - int(self), length - self._right, length - self._left        )",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __new__(cls, position, extension=0):                if extension != 0:            raise AttributeError(f\"Non-zero extension {extension} for exact position.\")        return int.__new__(cls, position)",
        "labels_text": "Create a new instance in BeforePosition object"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i)\" % (self.__class__.__name__, int(self))",
        "labels_text": "Represent the location a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"<{int(self)}\"",
        "labels_text": "Return a representation of the BeforePosition object with python counting"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self.__class__(int(self) + offset)",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return AfterPosition(length - int(self))",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __new__(cls, position, extension=0):                if extension != 0:            raise AttributeError(f\"Non-zero extension {extension} for exact position.\")        return int.__new__(cls, position)",
        "labels_text": "Create a new instance of the AfterPosition object"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i)\" % (self.__class__.__name__, int(self))",
        "labels_text": "Represent the location a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\">{int(self)}\"",
        "labels_text": "Return a representation of the AfterPosition object with python counting"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self.__class__(int(self) + offset)",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return BeforePosition(length - int(self))",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __new__(cls, position, choices):                if position not in choices:            raise ValueError(                f\"OneOfPosition: {position!r} should match one of {choices!r}\"            )        obj = int.__new__(cls, position)        obj.position_choices = choices        return obj",
        "labels_text": "Initialize with a set of possible position choice is a list of Position derived object specifying possible location position is an integer specifying the default behavior"
    },
    {
        "input_text": "summarize: def __getnewargs__(self):                return (int(self), self.position_choices)",
        "labels_text": "Return the argument accepted by new Necessary to allow pickling and unpickling of class instance"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(%i, choices=%r)\" % (            self.__class__.__name__,            int(self),            self.position_choices,        )",
        "labels_text": "Represent the OneOfPosition object a a string for debugging"
    },
    {
        "input_text": "summarize: def __str__(self):                out = \"one-of(\"        for position in self.position_choices:            out += f\"{position},\"        # replace the last comma with the closing parenthesis        return out[:-1] + \")\"",
        "labels_text": "Return a representation of the OneOfPosition object with python counting"
    },
    {
        "input_text": "summarize: def __add__(self, offset):                return self.__class__(            int(self) + offset, [p + offset for p in self.position_choices]        )",
        "labels_text": "Return a copy of the position object with it location shifted PRIVATE"
    },
    {
        "input_text": "summarize: def _flip(self, length):                return self.__class__(            length - int(self), [p._flip(length) for p in self.position_choices[::-1]]        )",
        "labels_text": "Return a copy of the location after the parent is reversed PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, length: int) -> None:                dict.__init__(self)        self._length = int(length)",
        "labels_text": "Create an EMPTY restricted dictionary"
    },
    {
        "input_text": "summarize: def __iter__(self) -> Iterable[Union[\"Seq\", \"MutableSeq\"]]:                return iter(self.seq)",
        "labels_text": "Iterate over the letter in the sequence For example using BioSeqIO to read in a protein FASTA file from Bio import SeqIO record SeqIOreadFastaloveliesbleedingpro fasta for amino in record printamino if amino L break X A G L printrecordseq L This is just a shortcut for iterating over the sequence directly for amino in recordseq printamino if amino L break X A G L printrecordseq L Note that this doe not facilitate iteration together with any perletterannotation However you can achieve that using the python zip function on the record or it sequence and the relevant perletterannotation from Bio import SeqIO rec SeqIOreadQualitysolexafakedfastq fastqsolexa print s recid recseq slxa ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTNNNNNN printlistrecletterannotations solexaquality for nuc qual in ziprec recletterannotationssolexaquality if qual print i nuc qual A C G T A You may agree that using ziprecseq is more explicit than using ziprec a shown above"
    },
    {
        "input_text": "summarize: def __contains__(self, char: str) -> bool:                return char in self.seq",
        "labels_text": "Implement the in keyword search the sequence eg from Bio import SeqIO record SeqIOreadFastasweetpeanu fasta GAATTC in record False AAA in record True This essentially act a a proxy for using in on the sequence GAATTC in recordseq False AAA in recordseq True Note that you can also use Seq object a the query from BioSeq import Seq SeqAAA in record True See also the Seq object contains method"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return (            f\"{self.__class__.__name__}(seq={self.seq!r}, id={self.id!r},\"            f\" name={self.name!r}, description={self.description!r},\"            f\" dbxrefs={self.dbxrefs!r})\"        )",
        "labels_text": "Return a concise summary of the record for debugging string The python built in function repr work by calling the object repr method eg from BioSeq import Seq from BioSeqRecord import SeqRecord rec SeqRecordSeqMASRGVNKVILVGNLGQDPEVRYMPNGGAVANITLATSESWRDKAT GEMKEQTEWHRVVLFGKLAEVASEYLRKGSQVYIEGQLRTRKWTDQ SGQDRYTTEVVVNVGGTMQMLGGRQGGGAPAGGNIGGGQPQGGWGQ PQQPQGGNQFSGGAQSRPQQSAPAAPSNEPPMDFDDDIPF idNP nameb descriptionssDNAbinding protein dbxrefsASAP GI GeneID printreprrec SeqRecordseqSeqMASRGVNKVILVGNLGQDPEVRYMPNGGAVANITLATSESWRDKATGEMKEQTEIPF idNP nameb descriptionssDNAbinding protein dbxrefsASAP GI GeneID At the python prompt you can also use this shorthand rec SeqRecordseqSeqMASRGVNKVILVGNLGQDPEVRYMPNGGAVANITLATSESWRDKATGEMKEQTEIPF idNP nameb descriptionssDNAbinding protein dbxrefsASAP GI GeneID Note that long sequence are shown truncated Also note that any annotation letterannotations and feature are not shown a they would lead to a very long string"
    },
    {
        "input_text": "summarize: def format(self, format: str) -> str:        r        # See also the __format__ method        return self.__format__(format)",
        "labels_text": "Return the record a a string in the specified file format The format should be a lower case string supported a an output format by BioSeqIO which is used to turn the SeqRecord into a string eg from BioSeq import Seq from BioSeqRecord import SeqRecord record SeqRecordSeqMKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF idYP nameHokC descriptiontoxic membrane protein recordformatfasta YP toxic membrane proteinnMKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVFn printrecordformatfasta YP toxic membrane protein MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF BLANKLINE The Python print function automatically appends a new line meaning in this example a blank line is shown If you look at the string representation you can see there is a trailing new line shown a slash n which is important when writing to a file or if concatenating multiple sequence string together Note that this method will NOT work on every possible file format supported by BioSeqIO eg some are for multiple sequence only and binary format are not supported"
    },
    {
        "input_text": "summarize: def __len__(self) -> int:                return len(self.seq)",
        "labels_text": "Return the length of the sequence For example using BioSeqIO to read in a FASTA nucleotide file from Bio import SeqIO record SeqIOreadFastasweetpeanu fasta lenrecord lenrecordseq"
    },
    {
        "input_text": "summarize: def __lt__(self, other: Any) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the lessthan operand not implemented"
    },
    {
        "input_text": "summarize: def __le__(self, other: Any) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the lessthanorequalto operand not implemented"
    },
    {
        "input_text": "summarize: def __eq__(self, other: object) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the equalto operand not implemented"
    },
    {
        "input_text": "summarize: def __ne__(self, other: object) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the notequalto operand not implemented"
    },
    {
        "input_text": "summarize: def __gt__(self, other: Any) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the greaterthan operand not implemented"
    },
    {
        "input_text": "summarize: def __ge__(self, other: Any) -> NoReturn:                raise NotImplementedError(_NO_SEQRECORD_COMPARISON)",
        "labels_text": "Define the greaterthanorequalto operand not implemented"
    },
    {
        "input_text": "summarize: def __bool__(self) -> bool:                return True",
        "labels_text": "Boolean value of an instance of this class True This behaviour is for backwards compatibility since until the len method wa added a SeqRecord always evaluated a True Note that in comparison a Seq object will evaluate to False if it ha a zero length sequence WARNING The SeqRecord may in future evaluate to False when it sequence is of zero length in order to better match the Seq object behaviour"
    },
    {
        "input_text": "summarize: def count(self, sub, start=None, end=None):                return self.seq.count(sub, start, end)",
        "labels_text": "Return the number of nonoverlapping occurrence of sub in seqstartend Optional argument start and end are interpreted a in slice notation This method behaves a the count method of Python string"
    },
    {
        "input_text": "summarize: def upper(self) -> \"SeqRecord\":                return type(self)(            self.seq.upper(),            id=self.id,            name=self.name,            description=self.description,            dbxrefs=self.dbxrefs[:],            features=self.features[:],            annotations=self.annotations.copy(),            letter_annotations=self.letter_annotations.copy(),        )",
        "labels_text": "Return a copy of the record with an upper case sequence All the annotation is preserved unchanged eg from BioSeq import Seq from BioSeqRecord import SeqRecord record SeqRecordSeqacgtACGT idTest description Made up for this example recordletterannotationsphredquality printrecordupperformatfastq Test Made up for this example ACGTACGT BLANKLINE Naturally there is a matching lower method printrecordlowerformatfastq Test Made up for this example acgtacgt BLANKLINE"
    },
    {
        "input_text": "summarize: def lower(self) -> \"SeqRecord\":                return type(self)(            self.seq.lower(),            id=self.id,            name=self.name,            description=self.description,            dbxrefs=self.dbxrefs[:],            features=self.features[:],            annotations=self.annotations.copy(),            letter_annotations=self.letter_annotations.copy(),        )",
        "labels_text": "Return a copy of the record with a lower case sequence All the annotation is preserved unchanged eg from Bio import SeqIO record SeqIOreadFastaasterpro fasta printrecordformatfasta gidbjBAA SAMIPF GGHVNPAVTFGAFVGGNITLLRGIVYIIAQLLGSTVACLLLKFVTNDMAVGVFSLSAGVG VTNALVFEIVMTFGLVYTVYATAIDPKKGSLGTIAPIAIGFIVGANI BLANKLINE printrecordlowerformatfasta gidbjBAA SAMIPF gghvnpavtfgafvggnitllrgivyiiaqllgstvaclllkfvtndmavgvfslsagvg vtnalvfeivmtfglvytvyataidpkkgslgtiapiaigfivgani BLANKLINE To take a more annotation rich example from Bio import SeqIO old SeqIOreadEMBLTRBGembl embl lenoldfeatures new oldlower lenoldfeatures lennewfeatures True oldannotationsorganism newannotationsorganism True olddbxrefs newdbxrefs True"
    },
    {
        "input_text": "summarize: def isupper(self):                return self.seq.isupper()",
        "labels_text": "Return True if all ASCII character in the record sequence are uppercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def islower(self):                return self.seq.islower()",
        "labels_text": "Return True if all ASCII character in the record sequence are lowercase If there are no cased character the method return False"
    },
    {
        "input_text": "summarize: def function_with_previous(func: F) -> _FunctionWithPrevious[F]:        function_with_previous = cast(_FunctionWithPrevious[F], func)    # Make sure the cast isn't a lie.    function_with_previous.previous = None    return function_with_previous",
        "labels_text": "Decorate a function a having an attribute named previous"
    },
    {
        "input_text": "summarize: def __init__(self, *args):                super().__init__(*args)",
        "labels_text": "Initialise class"
    },
    {
        "input_text": "summarize: def __init__(self, alignment):                self.alignment = alignment        self.ic_vector = []",
        "labels_text": "Initialize with the alignment to calculate information on icvector attribute A list of ic content for each column number"
    },
    {
        "input_text": "summarize: def _pair_replacement(self, seq1, seq2, weight1, weight2, dictionary, letters):                # loop through each residue in the sequences        for residue1, residue2 in zip(seq1, seq2):            if residue1 in letters and residue2 in letters:                dictionary[(residue1, residue2)] += weight1 * weight2",
        "labels_text": "Compare two sequence and generate info on the replacement seen PRIVATE Arguments seq seq The two sequence to compare weight weight The relative weight of seq and seq dictionary The dictionary containing the starting replacement info that we will modify letter A list of character to include when calculating replacement"
    },
    {
        "input_text": "summarize: def _get_all_letters(self):                set_letters = set()        for record in self.alignment:            set_letters.update(record.seq)        list_letters = sorted(set_letters)        all_letters = \"\".join(list_letters)        return all_letters",
        "labels_text": "Return a string containing the expected letter in the alignment PRIVATE"
    },
    {
        "input_text": "summarize: def get_column(self, col):                # TODO - Deprecate this and implement slicing?        return self.alignment[:, col]",
        "labels_text": "Return column of alignment"
    },
    {
        "input_text": "summarize: def get_residue(self, pos):                return self.pssm[pos][0]",
        "labels_text": "Return the residue letter at the specified position"
    },
    {
        "input_text": "summarize: def _count_replacement(codons, G):        if len(codons) == 1:        return 0, 0    elif len(codons) == 2:        codons = list(codons)        return floor(G[codons[0]][codons[1]])    else:        subgraph = {            codon1: {codon2: G[codon1][codon2] for codon2 in codons if codon1 != codon2}            for codon1 in codons        }        return _prim(subgraph)",
        "labels_text": "Count replacement needed for a given codonset PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, target, bedN=12):                if bedN < 3 or bedN > 12:            raise ValueError(\"bedN must be between 3 and 12\")        super().__init__(target)        self.bedN = bedN",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name bedN number of column in the BED file This must be between and default value is"
    },
    {
        "input_text": "summarize: def __init__(self, name, comment, fields):                self.name = name        self.comment = comment        self[:] = fields",
        "labels_text": "Create an AutoSQL table describing the column of an extended BED format"
    },
    {
        "input_text": "summarize: def from_string(cls, data):                return cls.from_bytes(data.encode() + b\"\\0\")",
        "labels_text": "Return an AutoSQLTable initialized using the string object data"
    },
    {
        "input_text": "summarize: def __init__(        self,        target,        targets=None,        compress=True,        blockSize=256,        itemsPerSlot=512,    ):                super().__init__(            target,            bedN=3,            declaration=declaration,            targets=targets,            compress=compress,            blockSize=blockSize,            itemsPerSlot=itemsPerSlot,        )",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name target A list of SeqRecord object with the chromosome in the order a they appear in the alignment The sequence content in each SeqRecord may be undefined but the sequence length must be defined a in this example SeqRecordSeqNone length idchr If target is None the default value the alignment must have an attribute target providing the list of SeqRecord object compress If True default compress data using zlib If False do not compress data Use compressFalse for faster searching blockSize Number of item to bundle in rtree See UCSCs bedToBigBed program for more information Default value is itemsPerSlot Number of data point bundled at lowest level See UCSCs bedToBigBed program for more information Use itemsPerSlot for faster searching Default value is"
    },
    {
        "input_text": "summarize: def __init__(self, source):                self.reference = None        super().__init__(source)",
        "labels_text": "Create an AlignmentIterator object Arguments source input file stream or path to input file"
    },
    {
        "input_text": "summarize: def __init__(        self,        target,        targets=None,        compress=True,        extraIndex=(),        cds=False,        fa=False,        mask=None,        wildcard=\"N\",    ):                super().__init__(            target,            bedN=12,            declaration=declaration,            targets=targets,            compress=compress,            extraIndex=extraIndex,        )        self.cds = cds        self.fa = fa        self.mask = mask        self.wildcard = wildcard",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name target A list of SeqRecord object with the chromosome in the order a they appear in the alignment The sequence content in each SeqRecord may be undefined but the sequence length must be defined a in this example SeqRecordSeqNone length idchr If target is None the default value the alignment must have an attribute target providing the list of SeqRecord object compress If True default compress data using zlib If False do not compress data extraIndex List of string with the name of extra column to be indexed Default value is an empty list cd If True look for a query feature of type CDS and write it in NCBI style in the PSL file default False fa If True include the query sequence in the PSL file default False mask Specify if repeat region in the target sequence are masked and should be reported in the repMatches field instead of in the match field Acceptable value are None no masking default lower masking by lowercase character upper masking by uppercase character wildcard Report alignment to the wildcard character in the target or query sequence in the nCount field instead of in the match misMatches or repMatches field Default value is N"
    },
    {
        "input_text": "summarize: def write_header(self, stream, alignments):                try:            metadata = alignments.metadata            program = metadata[\"Program\"]        except (AttributeError, KeyError):            program = \"Biopython\"            version = Bio.__version__        else:            version = metadata.get(\"Version\", \"\")        line = f\"{program} {version} multiple sequence alignment\\n\"        stream.write(line)        stream.write(\"\\n\")        stream.write(\"\\n\")",
        "labels_text": "Use this to write the file header"
    },
    {
        "input_text": "summarize: def __init__(self, target, fmt=\"vulgar\"):                super().__init__(target)        if fmt == \"vulgar\":            self.format_alignment = self._format_alignment_vulgar        elif fmt == \"cigar\":            self.format_alignment = self._format_alignment_cigar        else:            raise ValueError(                \"argument fmt should be 'vulgar' or 'cigar' (received %s)\" % fmt            )",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name fmt write alignment in the vulgar Verbose Useful Labelled Gapped Alignment Report format fmtvulgar or in the cigar Compact Idiosyncratic Gapped Alignment Report format fmtcigar Default value is vulgar"
    },
    {
        "input_text": "summarize: def write_header(self, stream, alignments):                try:            metadata = alignments.metadata        except AttributeError:            commandline = \"\"            hostname = \"\"        else:            commandline = metadata.get(\"Command line\", \"\")            hostname = metadata.get(\"Hostname\", \"\")        stream.write(f\"Command line: [{commandline}]\\n\")        stream.write(f\"Hostname: [{hostname}]\\n\")",
        "labels_text": "Write the header"
    },
    {
        "input_text": "summarize: def write_footer(self, stream):                stream.write(\"-- completed exonerate analysis\\n\")",
        "labels_text": "Write the footer"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            stream = self._stream        except AttributeError:            raise StopIteration from None        alignment = self._read_next_alignment(stream)        if alignment is None:            self._len = self._index            raise StopIteration        self._index += 1        return alignment",
        "labels_text": "Return the next alignment"
    },
    {
        "input_text": "summarize: def __len__(self):                try:            length = self._len        except AttributeError:            index = self._index            self.rewind()            length = 0            for alignment in self:                length += 1            self.rewind()            while self._index < index:                next(self)            self._len = length        return length",
        "labels_text": "Return the number of alignment The number of alignment is cached If not yet calculated the iterator is rewound to the beginning and the number of alignment is calculated by iterating over the alignment The iterator is then returned to it original position in the file"
    },
    {
        "input_text": "summarize: def _read_header(self, stream):                return",
        "labels_text": "Read the file header and store it in metadata"
    },
    {
        "input_text": "summarize: def _read_next_alignment(self, stream):",
        "labels_text": "Read one Alignment from the stream and return it"
    },
    {
        "input_text": "summarize: def write_header(self, stream, alignments):                return",
        "labels_text": "Write the file header to the output file"
    },
    {
        "input_text": "summarize: def write_footer(self, stream):                return",
        "labels_text": "Write the file footer to the output file"
    },
    {
        "input_text": "summarize: def format_alignment(self, alignment):                raise NotImplementedError(\"This method should be implemented\")",
        "labels_text": "Format a single alignment a a string alignment an Alignment object"
    },
    {
        "input_text": "summarize: def write_single_alignment(self, stream, alignments):                count = 0        for alignment in alignments:            if count == 1:                raise ValueError(                    f\"Alignment files in the {self.fmt} format can contain a single alignment only.\"                )            line = self.format_alignment(alignment)            stream.write(line)            count += 1        return count",
        "labels_text": "Write a single alignment to the output file and return alignment A list or iterator returning Alignment object stream Output file stream"
    },
    {
        "input_text": "summarize: def write_multiple_alignments(self, stream, alignments):                count = 0        for alignment in alignments:            line = self.format_alignment(alignment)            stream.write(line)            count += 1        return count",
        "labels_text": "Write alignment to the output file and return the number of alignment alignment A list or iterator returning Alignment object stream Output file stream"
    },
    {
        "input_text": "summarize: def write_file(self, stream, alignments):                self.write_header(stream, alignments)        count = self.write_alignments(stream, alignments)        self.write_footer(stream)        return count",
        "labels_text": "Write the alignment to the file strenm and return the number of alignment alignment A list or iterator returning Alignment object stream Output file stream"
    },
    {
        "input_text": "summarize: def write(self, alignments):                stream = self._stream        try:            count = self.write_file(stream, alignments)        finally:            if stream is not self._target:                stream.close()        return count",
        "labels_text": "Write a file with the alignment and return the number of alignment alignment A list or iterator returning Alignment object"
    },
    {
        "input_text": "summarize: def __init__(self, target, metadata=None, identifiers=None):                super().__init__(target)        self._metadata = metadata        self._identifiers = identifiers",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name metadata metadata to be included in the output If metadata is None then the alignment object to be written must have an attribute metadata identifier list of the IDs of the sequence included in the alignment Sequences will be numbered according to their index in this list If identifier is None then the alignment object to be written must have an attribute identifier"
    },
    {
        "input_text": "summarize: def __init__(self, target, interleave=None):                super().__init__(target)        self.interleave = interleave",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name interleave if None default interleave if column if True use interleaved format if False do not use interleaved format"
    },
    {
        "input_text": "summarize: def write_file(self, stream, alignments):                count = super().write_file(stream, alignments)        if count != 1:            raise ValueError(\"Expected to write 1 alignment; wrote %d\" % count)        return count",
        "labels_text": "Write a file with the alignment and return the number of alignment alignment A list or iterator returning Alignment object"
    },
    {
        "input_text": "summarize: def format_alignment(self, alignment, interleave=None):                stream = StringIO()        self.write_alignment(alignment, stream, interleave)        stream.seek(0)        return stream.read()",
        "labels_text": "Return a string with a single alignment in the Nexus format Creates an empty Nexus object add the sequence and then get Nexus to prepare the output alignment An Alignment object interleave if None default interleave if column if True use interleaved format if False do not use interleaved format"
    },
    {
        "input_text": "summarize: def write_alignments(self, stream, alignments):                count = 0        interleave = self.interleave        for alignment in alignments:            self.write_alignment(alignment, stream, interleave=interleave)            count += 1        return count",
        "labels_text": "Write alignment to the output file and return the number of alignment alignment A list or iterator returning Alignment object"
    },
    {
        "input_text": "summarize: def __init__(self, target, header=True, mask=None, wildcard=\"N\"):                super().__init__(target)        self.header = header        if wildcard is not None:            if mask == \"upper\":                wildcard = ord(wildcard.lower())            else:                wildcard = ord(wildcard.upper())        self.wildcard = wildcard        self.mask = mask",
        "labels_text": "Create an AlignmentWriter object Arguments target output stream or file name header If True default write the PSL header consisting of five line containing the PSL format version and a header for each column If False suppress the PSL header resulting in a simple tabdelimited file mask Specify if repeat region in the target sequence are masked and should be reported in the repMatches field of the PSL file instead of in the match field Acceptable value are None no masking default lower masking by lowercase character upper masking by uppercase character wildcard Report alignment to the wildcard character in the target or query sequence in the nCount field of the PSL file instead of in the match misMatches or repMatches field Default value is N"
    },
    {
        "input_text": "summarize: def write_header(self, stream, alignments):                if not self.header:            return        try:            metadata = alignments.metadata        except AttributeError:            version = \"3\"        else:            version = metadata.get(\"psLayout version\", \"3\")        # fmt: off        stream.write(            f  # noqa: W191, E101        )",
        "labels_text": "Write the PSL header"
    },
    {
        "input_text": "summarize: def __init__(self, target, md=False):                super().__init__(target)        self.md = md",
        "labels_text": "Create an AlignmentWriter object Arguments md If True calculate the MD tag from the alignment and include it in the output If False default do not include the MD tag in the output"
    },
    {
        "input_text": "summarize: def _format_long_text(prefix, text):                if text is None:            return \"\"        return (            textwrap.fill(                text,                width=79,                break_long_words=False,                initial_indent=prefix,                subsequent_indent=prefix,            )            + \"\\n\"        )",
        "labels_text": "Format the text a wrapped line PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                # A doctest for __repr__ would be nice, but __class__ comes out differently        # if run via the __main__ trick.        return \"<%s instance (%i records of length %i) at %x>\" % (            self.__class__,            len(self._records),            self.get_alignment_length(),            id(self),        )",
        "labels_text": "Return a representation of the object for debugging The representation cannot be used with eval to recreate the object which is usually possible with simple python object For example BioAlignMultipleSeqAlignment instance record of length at acc The hex string is the memory address of the object see helpid This provides a simple way to visually distinguish alignment of the same size"
    },
    {
        "input_text": "summarize: def __format__(self, format_spec):                if format_spec:            from io import StringIO            from Bio import AlignIO            handle = StringIO()            AlignIO.write([self], handle, format_spec)            return handle.getvalue()        else:            # Follow python convention and default to using __str__            return str(self)",
        "labels_text": "Return the alignment a a string in the specified file format The format should be a lower case string supported a an output format by BioAlignIO such a fasta clustal phylip stockholm etc which is used to turn the alignment into a string eg from BioSeq import Seq from BioSeqRecord import SeqRecord from BioAlign import MultipleSeqAlignment a SeqRecordSeqACTGCTAGCTAG idAlpha description b SeqRecordSeqACTCTAGCTAG idBeta description c SeqRecordSeqACTGCTAGATAG idGamma description align MultipleSeqAlignmenta b c printformatalign fasta Alpha ACTGCTAGCTAG Beta ACTCTAGCTAG Gamma ACTGCTAGATAG BLANKLINE printformatalign phylip Alpha ACTGCTAGCT AG Beta ACTCTAGCT AG Gamma ACTGCTAGAT AG BLANKLINE"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self._records)",
        "labels_text": "Iterate over alignment row a SeqRecord object eg from BioSeq import Seq from BioSeqRecord import SeqRecord from BioAlign import MultipleSeqAlignment a SeqRecordSeqACTGCTAGCTAG idAlpha b SeqRecordSeqACTCTAGCTAG idBeta c SeqRecordSeqACTGCTAGATAG idGamma align MultipleSeqAlignmenta b c for record in align printrecordid printrecordseq Alpha ACTGCTAGCTAG Beta ACTCTAGCTAG Gamma ACTGCTAGATAG"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._records)",
        "labels_text": "Return the number of sequence in the alignment Use lenalignment to get the number of sequence ie the number of row and alignmentgetalignmentlength to get the length of the longest sequence ie the number of column This is easy to remember if you think of the alignment a being like a list of SeqRecord object"
    },
    {
        "input_text": "summarize: def get_alignment_length(self):                max_length = 0        for record in self._records:            if len(record.seq) > max_length:                max_length = len(record.seq)        return max_length",
        "labels_text": "Return the maximum length of the alignment All object in the alignment should hopefully have the same length This function will go through and find this length by finding the maximum length of sequence in the alignment from BioSeq import Seq from BioSeqRecord import SeqRecord from BioAlign import MultipleSeqAlignment a SeqRecordSeqACTGCTAGCTAG idAlpha b SeqRecordSeqACTCTAGCTAG idBeta c SeqRecordSeqACTGCTAGATAG idGamma align MultipleSeqAlignmenta b c aligngetalignmentlength If you want to know the number of sequence in the alignment use lenalign instead lenalign"
    },
    {
        "input_text": "summarize: def append(self, record):                if self._records:            self._append(record, self.get_alignment_length())        else:            self._append(record)",
        "labels_text": "Add one more SeqRecord object to the alignment a a new row This must have the same length a the original alignment unless this is the first record from Bio import AlignIO align AlignIOreadClustalwopuntiaaln clustal printalign Alignment with row and column TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF lenalign Well now construct a dummy record to append a an example from BioSeq import Seq from BioSeqRecord import SeqRecord dummy SeqRecordSeqN iddummy Now append this to the alignment alignappenddummy printalign Alignment with row and column TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAGAGA gigbAFAF NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN dummy lenalign"
    },
    {
        "input_text": "summarize: def __delitem__(self, index):                if not isinstance(index, int) and not isinstance(index, slice):            raise TypeError(\"Invalid index type.\")        del self._records[index]",
        "labels_text": "Delete SeqRecord by index or multiple SeqRecords by slice"
    },
    {
        "input_text": "summarize: def sort(self, key=None, reverse=False):                if key is None:            self._records.sort(key=lambda r: r.id, reverse=reverse)        else:            self._records.sort(key=key, reverse=reverse)",
        "labels_text": "Sort the row SeqRecord object of the alignment in place This sort the row alphabetically using the SeqRecord object id by default The sorting can be controlled by supplying a key function which must map each SeqRecord to a sort value This is useful if you want to add two alignment which use the same record identifier but in a different order For example from BioSeq import Seq from BioSeqRecord import SeqRecord from BioAlign import MultipleSeqAlignment align MultipleSeqAlignment SeqRecordSeqACGT idHuman SeqRecordSeqACGG idMouse SeqRecordSeqACGC idChicken align MultipleSeqAlignment SeqRecordSeqCGGT idMouse SeqRecordSeqCGTT idHuman SeqRecordSeqCGCT idChicken If you simple try and add these without sorting you get this printalign align Alignment with row and column ACGTCGGT unknown id ACGGCGTT unknown id ACGCCGCT Chicken Consult the SeqRecord documentation which explains why you get a default value when annotation like the identifier doesnt match up However if we sort the alignment first then add them we get the desired result alignsort alignsort printalign align Alignment with row and column ACGCCGCT Chicken ACGTCGTT Human ACGGCGGT Mouse As an example using a different sort order you could sort on the GC content of each sequence from BioSeqUtils import gcfraction printalign Alignment with row and column ACGC Chicken ACGT Human ACGG Mouse alignsortkey lambda record gcfractionrecordseq printalign Alignment with row and column ACGT Human ACGC Chicken ACGG Mouse There is also a reverse argument so if you wanted to sort by ID but backwards alignsortreverseTrue printalign Alignment with row and column ACGG Mouse ACGT Human ACGC Chicken"
    },
    {
        "input_text": "summarize: def infer_coordinates(cls, lines):                warnings.warn(            \"The method infer_coordinates is deprecated; please use the \"            \"method parse_printed_alignment instead. This method is much \"            \"faster than infer_coordinates, and returns both the sequences \"            \"after removal of the gaps and the coordinates.\",            BiopythonDeprecationWarning,        )        lines = [line.encode() for line in lines]        seqdata, coordinates = cls.parse_printed_alignment(lines)        return coordinates",
        "labels_text": "Infer the coordinate from a printed alignment DEPRECATED This method is primarily employed in Biopythons alignment parser though it may be useful for other purpose For an alignment consisting of N sequence printed a N line with the same number of column where gap are represented by dash this method will calculate the sequence coordinate that define the alignment The coordinate are returned a a NumPy array of integer and can be used to create an Alignment object This is an example for the alignment of three sequence TAGGCATACGTG AACGTACGT and ACGCATACTTG with gap in the second and third sequence from BioAlign import Alignment line TAGGCATACGTG AACGTACGT ACGCATACTTG sequence linereplace for line in line sequence TAGGCATACGTG AACGTACGT ACGCATACTTG coordinate Alignmentinfercoordinateslines printcoordinates alignment Alignmentsequences coordinate"
    },
    {
        "input_text": "summarize: def parse_printed_alignment(cls, lines):                parser = _aligncore.PrintedAlignmentParser(b\"\\0\")        sequences = []        for line in lines:            nbytes, sequence = parser.feed(line)            sequences.append(sequence)        shape = parser.shape        coordinates = np.empty(shape, np.int64)        parser.fill(coordinates)        return sequences, coordinates",
        "labels_text": "Infer the sequence and coordinate from a printed alignment This method is primarily employed in Biopythons alignment parser though it may be useful for other purpose For an alignment consisting of N sequence printed a N line with the same number of column where gap are represented by dash this method will calculate the sequence coordinate that define the alignment It return the tuple sequence coordinate where sequence is the list of N sequence after removing the gap and the coordinate is a D NumPy array of integer Together the sequence and coordinate can be used to create an Alignment object This is an example for the alignment of three sequence TAGGCATACGTG AACGTACGT and ACGCATACTTG with gap in the second and third sequence Note that the input sequence are byte object from BioAlign import Alignment from BioSeq import Seq line bTAGGCATACGTG bAACGTACGT bACGCATACTTG sequence coordinate Alignmentparseprintedalignmentlines sequence bTAGGCATACGTG bAACGTACGT bACGCATACTTG printcoordinates sequence Seqsequence for sequence in sequence sequence SeqTAGGCATACGTG SeqAACGTACGT SeqACGCATACTTG alignment Alignmentsequences coordinate printalignment TAGGCATACGTG AACGTACGT ACGCATACTTG BLANKLINE"
    },
    {
        "input_text": "summarize: def target(self):                n = len(self.sequences)        if n > 2:            # also allow alignments in which only the target alignment is defined            raise ValueError(                \"self.target is defined for pairwise alignments only (found alignment of %d sequences)\"                % n            )        return self.sequences[0]",
        "labels_text": "Return selfsequences for a pairwise alignment"
    },
    {
        "input_text": "summarize: def target(self, value):                n = len(self.sequences)        if n != 2:            raise ValueError(                \"self.target is defined for pairwise alignments only (found alignment of %d sequences)\"                % n            )        self.sequences[0] = value",
        "labels_text": "For a pairwise alignment set selfsequences"
    },
    {
        "input_text": "summarize: def query(self):                n = len(self.sequences)        if n != 2:            raise ValueError(                \"self.query is defined for pairwise alignments only (found alignment of %d sequences)\"                % n            )        return self.sequences[1]",
        "labels_text": "Return selfsequences for a pairwise alignment"
    },
    {
        "input_text": "summarize: def query(self, value):                n = len(self.sequences)        if n != 2:            raise ValueError(                \"self.query is defined for pairwise alignments only (found alignment of %d sequences)\"                % n            )        self.sequences[1] = value",
        "labels_text": "For a pairwise alignment set selfsequences"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                for left, right in zip_longest(self.sequences, other.sequences):            try:                left = left.seq            except AttributeError:                pass            try:                right = right.seq            except AttributeError:                pass            if left != right:                return False        return np.array_equal(self.coordinates, other.coordinates)",
        "labels_text": "Check if two Alignment object specify the same alignment"
    },
    {
        "input_text": "summarize: def __ne__(self, other):                for left, right in zip_longest(self.sequences, other.sequences):            try:                left = left.seq            except AttributeError:                pass            try:                right = right.seq            except AttributeError:                pass            if left != right:                return True        return not np.array_equal(self.coordinates, other.coordinates)",
        "labels_text": "Check if two Alignment object have different alignment"
    },
    {
        "input_text": "summarize: def _get_rows(self, key):                sequences = self.sequences[key]        coordinates = self.coordinates[key].copy()        alignment = Alignment(sequences, coordinates)        if np.array_equal(self.coordinates, coordinates):            try:                alignment.score = self.score            except AttributeError:                pass            try:                alignment.column_annotations = self.column_annotations            except AttributeError:                pass        return alignment",
        "labels_text": "Return selfkey where key is a slice object PRIVATE This method is called by getitem for invocation of the form selfrows where row is a slice object Return value is an Alignment object"
    },
    {
        "input_text": "summarize: def _get_row_col(self, j, col, steps, gaps, sequence):                indices = gaps.cumsum()        index = indices.searchsorted(col, side=\"right\")        if steps[index]:            offset = col - indices[index]            j += sum(steps[: index + 1]) + offset            return sequence[j]        else:            return \"-\"",
        "labels_text": "Return the sequence content at alignment column j PRIVATE This method is called by getitem for invocation of the form selfrow col where both row and col are integer Return value is a string of length"
    },
    {
        "input_text": "summarize: def _get_rows_col(self, coordinates, col, steps, gaps, sequences):                indices = gaps.cumsum()        j = indices.searchsorted(col, side=\"right\")        offset = indices[j] - col        line = \"\"        for sequence, coordinate, step in zip(sequences, coordinates, steps):            if step[j] == 0:                line += \"-\"            else:                index = coordinate[j] + step[j] - offset                line += sequence[index]        return line",
        "labels_text": "Return the alignment content of multiple row and one column PRIVATE This method is called by getitem for invocation of the form selfrows col where row is a slice object and col is an integer Return value is a string"
    },
    {
        "input_text": "summarize: def __format__(self, format_spec):                return self.format(format_spec)",
        "labels_text": "Return the alignment a a string in the specified file format Wrapper for selfformat"
    },
    {
        "input_text": "summarize: def format(self, fmt=\"\", *args, **kwargs):                if fmt == \"\":            return self._format_pretty()        module = _load(fmt)        if module.AlignmentIterator.mode == \"b\":            raise ValueError(f\"{fmt} is a binary file format\")        try:            writer = module.AlignmentWriter(None, *args, **kwargs)        except AttributeError:            raise ValueError(                f\"Formatting alignments has not yet been implemented for the {fmt} format\"            ) from None        return writer.format_alignment(self)",
        "labels_text": "Return the alignment a a string in the specified file format Arguments fmt File format Acceptable value are an empty string to create a humanreadable representation of the alignment or any of the alignment file format supported by BioAlign some have not yet been implemented All other argument are passed to the formatspecific writer function mask PSL format only Specify if repeat region in the target sequence are masked and should be reported in the repMatches field of the PSL file instead of in the match field Acceptable value are None no masking default lower masking by lowercase character upper masking by uppercase character wildcard PSL format only Report alignment to the wildcard character in the target or query sequence in the nCount field of the PSL file instead of in the match misMatches or repMatches field Default value is N md SAM format only If True calculate the MD tag from the alignment and include it in the output If False default do not include the MD tag in the output"
    },
    {
        "input_text": "summarize: def __str__(self):                return self.format()",
        "labels_text": "Return a humanreadable string representation of the alignment For sequence alignment each line ha at most column The first column show the possibly truncated sequence name which may be the id attribute of a SeqRecord or otherwise target or query for pairwise alignment The next column show the sequence coordinate using zerobased counting a usual in Python The remaining column shown the sequence using dash to represent gap At the end of the alignment the end coordinate are shown on the right of the sequence again in zerobased coordinate Pairwise alignment have an additional line between the two sequence showing whether the sequence match or mismatch or if there is a gap The coordinate shown for this line are the column index which can be useful when extracting a subalignment For example from BioAlign import PairwiseAligner aligner PairwiseAligner seqA TTAACCCCATTTG seqB AAGCCCCTTT seqC AAAGGGGCTT alignment aligneralignseqA seqB lenalignments alignment alignment printalignment target TTAACCCCATTTG query AAGCCCCTTT BLANKLINE Note that seqC is the reverse complement of seqB Aligning it to the reverse strand give the same alignment but the query coordinate are switched alignment aligneralignseqA seqC strand lenalignments alignment alignment printalignment target TTAACCCCATTTG query AAGCCCCTTT BLANKLINE"
    },
    {
        "input_text": "summarize: def __repr__(self):                if self.coordinates is None:            return \"<%s object at 0x%x>\" % (                self.__class__.__name__,                id(self),            )        n, m = self.shape        return \"<%s object (%i rows x %i columns) at 0x%x>\" % (            self.__class__.__name__,            n,            m,            id(self),        )",
        "labels_text": "Return a representation of the alignment including it shape The representation cannot be used with eval to recreate the object which is usually possible with simple python object For example Alignment object row x column at xd The hex string is the memory address of the object and can be used to distinguish different Alignment object See helpid for more information import numpy a np from BioAlign import Alignment alignment AlignmentACCGT ACGT coordinate nparray printalignment target ACCGT query ACGT BLANKLINE alignment doctestELLIPSIS Alignment object row x column at x"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.sequences)",
        "labels_text": "Return the number of sequence in the alignment"
    },
    {
        "input_text": "summarize: def shape(self):                n = len(self.coordinates)        m = self.length        return (n, m)",
        "labels_text": "Return the shape of the alignment a a tuple of two integer value The first integer value is the number of sequence in the alignment a returned by lenalignment which is always for pairwise alignment The second integer value is the number of column in the alignment when it is printed and is equal to the sum of the number of match number of mismatch and the total length of gap in the target and query Sequence section beyond the aligned segment are not included in the number of column For example from Bio import Align aligner AlignPairwiseAligner alignermode global alignment aligneralignGACCTG CGATCG alignment alignment printalignment target GACCTG query CGATCG BLANKLINE lenalignment alignmentshape alignermode local alignment aligneralignGACCTG CGATCG alignment alignment printalignment target GACCTG query GATCG BLANKLINE lenalignment alignmentshape"
    },
    {
        "input_text": "summarize: def sort(self, key=None, reverse=False):                sequences = self.sequences        if key is None:            try:                values = [sequence.id for sequence in sequences]            except AttributeError:                values = sequences        else:            values = [key(sequence) for sequence in sequences]        indices = sorted(range(len(sequences)), key=values.__getitem__, reverse=reverse)        self.sequences = [sequences[index] for index in indices]        self.coordinates = self.coordinates.take(indices, 0)",
        "labels_text": "Sort the sequence of the alignment in place By default this sort the sequence alphabetically using their id attribute if available or by their sequence content otherwise For example from BioAlign import PairwiseAligner aligner PairwiseAligner alignergapscore alignment aligneralignAATAA AAGAA lenalignments alignment alignment printalignment target AATAA query AAGAA BLANKLINE alignmentsort printalignment target AAGAA query AATAA BLANKLINE Alternatively a key function can be supplied that map each sequence to a sort value For example you could sort on the GC content of each sequence from BioSeqUtils import gcfraction alignmentsortkeygcfraction printalignment target AATAA query AAGAA BLANKLINE You can reverse the sort order by passing reverseTrue alignmentsortkeygcfraction reverseTrue printalignment target AAGAA query AATAA BLANKLINE The sequence are now sorted by decreasing GC content value"
    },
    {
        "input_text": "summarize: def counts(self):                gaps = identities = mismatches = 0        for i, seq1 in enumerate(self):            for j, seq2 in enumerate(self):                if i == j:                    # Don't count seq1 vs seq2 and seq2 vs seq1                    break                for a, b in zip(seq1, seq2):                    if a == \"-\" or b == \"-\":                        gaps += 1                    elif a == b:                        identities += 1                    else:                        mismatches += 1        return AlignmentCounts(gaps, identities, mismatches)",
        "labels_text": "Return number of identity mismatch and gap of a pairwise alignment aligner PairwiseAlignermodeglobal matchscore mismatchscore for alignment in aligneralignTACCG ACG printScore f alignmentscore c alignmentcounts namedtuple printfcgaps gap cidentities identity cmismatches mismatch printalignment Score gap identity mismatch target TACCG query ACG BLANKLINE Score gap identity mismatch target TACCG query ACG BLANKLINE This classifies each pair of letter in a pairwise alignment into gap perfect match or mismatch It ha been defined a a method not a property so that it may in future take optional argument allowing the behavior to be customized These three value are returned a a namedtuple This is calculated for all the pair of sequence in the alignment"
    },
    {
        "input_text": "summarize: def __iter__(self):                self.rewind()        return self",
        "labels_text": "Iterate over the alignment a Alignment object This method SHOULD NOT be overridden by any subclass"
    },
    {
        "input_text": "summarize: def __next__(self):",
        "labels_text": "Return the next alignment"
    },
    {
        "input_text": "summarize: def rewind(self):",
        "labels_text": "Rewind the iterator to let it loop over the alignment from the beginning"
    },
    {
        "input_text": "summarize: def __len__(self):",
        "labels_text": "Return the number of alignment"
    },
    {
        "input_text": "summarize: def __init__(self, seqA, seqB, score, paths):                self.sequences = [seqA, seqB]        self.score = score        self._paths = paths        self._index = -1",
        "labels_text": "Initialize a new PairwiseAlignments object Arguments seqA The first sequence a a plain string without gap seqB The second sequence a a plain string without gap score The alignment score path An iterator over the path in the traceback matrix each path defines one alignment You would normally obtain a PairwiseAlignments object by calling aligneralignseqA seqB where aligner is a PairwiseAligner object or a CodonAligner object"
    },
    {
        "input_text": "summarize: def score(self, seqA, seqB, strand=\"+\"):                if isinstance(seqA, (Seq, MutableSeq, SeqRecord)):            seqA = bytes(seqA)        if strand == \"-\":            seqB = reverse_complement(seqB)        if isinstance(seqB, (Seq, MutableSeq, SeqRecord)):            seqB = bytes(seqB)        return super().score(seqA, seqB, strand)",
        "labels_text": "Return the alignment score of two sequence using PairwiseAligner"
    },
    {
        "input_text": "summarize: def __init__(self, codon_table=None, anchor_len=10):                super().__init__()        if codon_table is None:            codon_table = CodonTable.generic_by_id[1]        elif not isinstance(codon_table, CodonTable.CodonTable):            raise TypeError(\"Input table is not a CodonTable object\")        self.codon_table = codon_table",
        "labels_text": "Initialize a CodonAligner for a specific genetic code Arguments codontable a CodonTable object representing the genetic code If codontable is None the standard genetic code is used"
    },
    {
        "input_text": "summarize: def write(alignments, target, fmt, *args, **kwargs):        if isinstance(alignments, Alignment):        alignments = [alignments]    module = _load(fmt)    try:        writer = module.AlignmentWriter    except AttributeError:        raise ValueError(            f\"File writing has not yet been implemented for the {fmt} format\"        )    return writer(target, *args, **kwargs).write(alignments)",
        "labels_text": "Write alignment to a file Arguments alignment An Alignments object an iterator of Alignment object or a single Alignment target File or filelike object to write to or filename a string fmt String describing the file format caseinsensitive Note if providing a file or filelike object your code should close the target after calling this function or call flush to ensure the data get flushed to disk Returns the number of alignment written a an integer"
    },
    {
        "input_text": "summarize: def parse(source, fmt):        module = _load(fmt)    alignments = module.AlignmentIterator(source)    return alignments",
        "labels_text": "Parse an alignment file and return an iterator over alignment Arguments source File or filelike object to read from or filename a string fmt String describing the file format caseinsensitive Typical usage opening a file to read in and looping over the alignment from Bio import Align filename Exonerateexnmnercigarexn for alignment in Alignparsefilename exonerate printNumber of sequence in alignment lenalignment printAlignment score alignmentscore Number of sequence in alignment Alignment score Number of sequence in alignment Alignment score Number of sequence in alignment Alignment score For lazyloading file format such a bigMaf for which the file content is read on demand only ensure that the file remains open while extracting alignment data You can use the BioAlignread function when the file contains only one alignment"
    },
    {
        "input_text": "summarize: def read(handle, fmt):        with parse(handle, fmt) as alignments:        try:            alignment = next(alignments)        except StopIteration:            raise ValueError(\"No alignments found in file\") from None        try:            next(alignments)            raise ValueError(\"More than one alignment found in file\")        except StopIteration:            pass    return alignment",
        "labels_text": "Parse a file containing one alignment and return it Arguments source File or filelike object to read from or filename a string fmt String describing the file format caseinsensitive This function is for use parsing alignment file containing exactly one alignment For example reading a Clustal file from Bio import Align alignment AlignreadClustalwopuntiaaln clustal printAlignment shape alignmentshape Alignment shape for sequence in alignmentsequences printsequenceid lensequence gigbAFAF gigbAFAF gigbAFAF gigbAFAF gigbAFAF gigbAFAF gigbAFAF If the file contains no record or more than one record an exception is raised For example from Bio import Align filename Exonerateexnmnercigarexn alignment Alignreadfilename exonerate Traceback most recent call last ValueError More than one alignment found in file Use the BioAlignparse function if you want to read a file containing more than one alignment"
    },
    {
        "input_text": "summarize: def transpose(self, axes=None):                other = np.ndarray.transpose(self, axes)        other._alphabet = self._alphabet        return other",
        "labels_text": "Transpose the array"
    },
    {
        "input_text": "summarize: def alphabet(self):                return self._alphabet",
        "labels_text": "Return the alphabet property"
    },
    {
        "input_text": "summarize: def copy(self):                other = Array(alphabet=self._alphabet, data=self)        return other",
        "labels_text": "Create and return a copy of the array"
    },
    {
        "input_text": "summarize: def get(self, key, value=None):                try:            return self[key]        except IndexError:            return value",
        "labels_text": "Return the value of the key if found return value otherwise"
    },
    {
        "input_text": "summarize: def keys(self):                dims = len(self.shape)        alphabet = self._alphabet        if dims == 1:            return tuple(alphabet)        elif dims == 2:            return tuple((c1, c2) for c2 in alphabet for c1 in alphabet)        else:            raise RuntimeError(\"array has unexpected shape %s\" % self.shape)",
        "labels_text": "Return a tuple with the key associated with the array"
    },
    {
        "input_text": "summarize: def values(self):                dims = len(self.shape)        alphabet = self._alphabet        if dims == 1:            return tuple(self)        elif dims == 2:            n1, n2 = self.shape            return tuple(                np.ndarray.__getitem__(self, (i1, i2))                for i2 in range(n2)                for i1 in range(n1)            )        else:            raise RuntimeError(\"array has unexpected shape %s\" % self.shape)",
        "labels_text": "Return a tuple with the value stored in the array"
    },
    {
        "input_text": "summarize: def update(self, E=None, **F):                if E is not None:            try:                alphabet = E.keys()            except AttributeError:                for key, value in E:                    self[key] = value            else:                for key in E:                    self[key] = E[key]        for key in F:            self[key] = F[key]",
        "labels_text": "Update the array from dictiterable E and F"
    },
    {
        "input_text": "summarize: def format(self, fmt=\"\"):                if fmt == \"\":            if np.issubdtype(self.dtype, np.integer):                fmt = \"%i\"            else:                fmt = \"%.1f\"        n = len(self.shape)        if n == 1:            return self._format_1D(fmt)        elif n == 2:            return self._format_2D(fmt)        else:            raise RuntimeError(\"Array has unexpected rank %d\" % n)",
        "labels_text": "Return a string representation of the array The argument fmt specifies the number format to be used By default the number format is i if the array contains integer number and f otherwise"
    },
    {
        "input_text": "summarize: def __init__(self, handle, seq_count=None):                self.handle = handle        self.records_per_alignment = seq_count",
        "labels_text": "Create an AlignmentIterator object Arguments handle input file count optional expected number of record per alignment Recommend for fasta file format Note when subclassing there should be a single nonoptional argument the handle and optional count IN THAT ORDER you can add additional optional argument"
    },
    {
        "input_text": "summarize: def __next__(self):                raise NotImplementedError(\"This object should be subclassed\")",
        "labels_text": "Return the next alignment in the file This method should be replaced by any derived class to do something useful"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.__next__, None)",
        "labels_text": "Iterate over the entry a MultipleSeqAlignment object Example usage for concatenated PHYLIP file with openmanyphyr a myFile for alignment in PhylipIteratormyFile printNew alignment for record in alignment printrecordid printrecordseq"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def write_file(self, alignments):                raise NotImplementedError(\"This object should be subclassed\")",
        "labels_text": "Use this to write an entire file containing the given alignment Arguments alignment A list or iterator returning MultipleSeqAlignment object In general this method can only be called once per file This method should be replaced by any derived class to do something useful It should return the number of alignment"
    },
    {
        "input_text": "summarize: def clean(self, text):                return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
        "labels_text": "Use this to avoid getting newlines in the output"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def write_file(self, alignments):                self.write_header()        count = 0        for alignment in alignments:            self.write_alignment(alignment)            count += 1        self.write_footer()        return count",
        "labels_text": "Use this to write an entire file containing the given alignment Arguments alignment A list or iterator returning MultipleSeqAlignment object In general this method can only be called once per file"
    },
    {
        "input_text": "summarize: def write_header(self):",
        "labels_text": "Use this to write any header This method should be replaced by any derived class to do something useful"
    },
    {
        "input_text": "summarize: def write_footer(self):",
        "labels_text": "Use this to write any footer This method should be replaced by any derived class to do something useful"
    },
    {
        "input_text": "summarize: def write_alignment(self, alignment):                raise NotImplementedError(\"This object should be subclassed\")",
        "labels_text": "Use this to write a single alignment This method should be replaced by any derived class to do something useful"
    },
    {
        "input_text": "summarize: def write_header(self):                self.handle.write(\"##maf version=1 scoring=none\\n\")        self.handle.write(\"# generated by Biopython\\n\\n\")",
        "labels_text": "Write the MAF header"
    },
    {
        "input_text": "summarize: def close(self):                self._con.close()        self._record_count = 0",
        "labels_text": "Close the file handle being used to read the data Once called further use of the index wont work The sole purpose of this method is to allow explicit handle closure for example if you wish to delete the file on Windows you must first close all open handle to that file"
    },
    {
        "input_text": "summarize: def _region2bin(start, end):                bins = [0, 1]        bins.extend(range(1 + (start >> 26), 2 + ((end - 1) >> 26)))        bins.extend(range(9 + (start >> 23), 10 + ((end - 1) >> 23)))        bins.extend(range(73 + (start >> 20), 74 + ((end - 1) >> 20)))        bins.extend(range(585 + (start >> 17), 586 + ((end - 1) >> 17)))        return set(bins)",
        "labels_text": "Find bin that a region may belong to PRIVATE Converts a region to a list of bin that it may belong to including largest and smallest bin"
    },
    {
        "input_text": "summarize: def _get_record(self, offset):                self._maf_fp.seek(offset)        return next(self._mafiter)",
        "labels_text": "Retrieve a single MAF record located at the offset provided PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"MafIO.MafIndex(%r, target_seqname=%r)\" % (            self._maf_fp.name,            self._target_seqname,        )",
        "labels_text": "Return a string representation of the index"
    },
    {
        "input_text": "summarize: def __len__(self):                return self._record_count",
        "labels_text": "Return the number of record in the index"
    },
    {
        "input_text": "summarize: def _identifier_split(identifier):        id, loc, strand = identifier.split(\":\")    start, end = map(int, loc.split(\"-\"))    start -= 1    return id, start, end, strand",
        "labels_text": "Return name start end string tuple from an identifier PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, *args, **kwargs):                super().__init__(*args, **kwargs)        self._wrote_header = False        self._wrote_first = False",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def write_file(self, alignments):                align_iter = iter(alignments)  # Could have been a list        try:            alignment = next(align_iter)        except StopIteration:            # Nothing to write!            return 0        # Check there is only one alignment...        try:            next(align_iter)            raise ValueError(\"We can only write one Alignment to a Nexus file.\")        except StopIteration:            pass        # Good.  Actually write the single alignment,        self.write_alignment(alignment)        return 1",
        "labels_text": "Use this to write an entire file containing the given alignment Arguments alignment A list or iterator returning MultipleSeqAlignment object This should hold ONE and only one alignment"
    },
    {
        "input_text": "summarize: def _split_id(self, line):                seq_id = line[: self.id_width].strip()        seq = line[self.id_width :].strip().replace(\" \", \"\")        return seq_id, seq",
        "labels_text": "Extract the sequence ID from a Phylip line PRIVATE Returning a tuple containing sequenceid sequenceresidues The first character in the line are are the sequence id the remainder are sequence data"
    },
    {
        "input_text": "summarize: def _split_id(self, line):                seq_id, sequence = line.split(None, 1)        sequence = sequence.strip().replace(\" \", \"\")        return seq_id, sequence",
        "labels_text": "Extract the sequence ID from a Phylip line PRIVATE Returns a tuple containing sequenceid sequenceresidues For relaxed format split at the first whitespace character"
    },
    {
        "input_text": "summarize: def sanitize_name(name, width=None):        name = name.strip()    for char in \"[](),\":        name = name.replace(char, \"\")    for char in \":;\":        name = name.replace(char, \"|\")    if width is not None:        name = name[:width]    return name",
        "labels_text": "Sanitise sequence identifier for output Removes the banned character and replaces the character with The name is truncated to width character if specified"
    },
    {
        "input_text": "summarize: def _identifier_split(self, identifier):                if \"/\" in identifier:            name, start_end = identifier.rsplit(\"/\", 1)            if start_end.count(\"-\") == 1:                try:                    start, end = start_end.split(\"-\")                    return name, int(start), int(end)                except ValueError:                    # Non-integers after final '/' - fall through                    pass        return identifier, None, None",
        "labels_text": "Return name start end string tuple from an identifier PRIVATE"
    },
    {
        "input_text": "summarize: def _get_meta_data(self, identifier, meta_dict):                name, start, end = self._identifier_split(identifier)        if name == identifier:            identifier_keys = [identifier]        else:            identifier_keys = [identifier, name]        answer = {}        for identifier_key in identifier_keys:            try:                for feature_key in meta_dict[identifier_key]:                    answer[feature_key] = meta_dict[identifier_key][feature_key]            except KeyError:                pass        return answer",
        "labels_text": "Take an identifier and return dict of all metadata matching it PRIVATE For example given QPNCAMJE will return all match to this or QPNCAMJE which the identifier without it startend suffix In the example below the suffix is required to match the AC but must be removed to match the OS and OC metadata STOCKHOLM GS QPNCAMJE AC QPN QPNCAMJE NKA GS QPNCAMJE OS Campylobacter jejuni GS QPNCAMJE OC Bacteria This function will return an empty dictionary if no data is found"
    },
    {
        "input_text": "summarize: def __init__(self, returncode, cmd, stdout=\"\", stderr=\"\"):                self.returncode = returncode        self.cmd = cmd        self.stdout = stdout        self.stderr = stderr",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"ApplicationError(%i, %s, %s, %s)\" % (            self.returncode,            self.cmd,            self.stdout,            self.stderr,        )",
        "labels_text": "Represent the error a a string"
    },
    {
        "input_text": "summarize: def _validate(self):                for p in self.parameters:            # Check for missing required parameters:            if p.is_required and not (p.is_set):                raise ValueError(f\"Parameter {p.names[-1]} is not set.\")",
        "labels_text": "Make sure the required parameter have been set PRIVATE No return value it either work or raise a ValueError This is a separate method called from str so that subclass may override it"
    },
    {
        "input_text": "summarize: def __str__(self):                self._validate()        commandline = f\"{_escape_filename(self.program_name)} \"        for parameter in self.parameters:            if parameter.is_set:                # This will include a trailing space:                commandline += str(parameter)        return commandline.strip()",
        "labels_text": "Make the commandline string with the currently set option eg from BioEmbossApplications import WaterCommandline cline WaterCommandlinegapopen gapextend clineasequence asisACCCGGGCGCGGT clinebsequence asisACCCGAGCGCGGT clineoutfile tempwatertxt printcline water outfiletempwatertxt asequenceasisACCCGGGCGCGGT bsequenceasisACCCGAGCGCGGT gapopen gapextend strcline water outfiletempwatertxt asequenceasisACCCGGGCGCGGT bsequenceasisACCCGAGCGCGGT gapopen gapextend"
    },
    {
        "input_text": "summarize: def __repr__(self):                answer = f\"{self.__class__.__name__}(cmd={self.program_name!r}\"        for parameter in self.parameters:            if parameter.is_set:                if isinstance(parameter, _Switch):                    answer += f\", {parameter.names[-1]}=True\"                else:                    answer += f\", {parameter.names[-1]}={parameter.value!r}\"        answer += \")\"        return answer",
        "labels_text": "Return a representation of the command line object for debugging eg from BioEmbossApplications import WaterCommandline cline WaterCommandlinegapopen gapextend clineasequence asisACCCGGGCGCGGT clinebsequence asisACCCGAGCGCGGT clineoutfile tempwatertxt printcline water outfiletempwatertxt asequenceasisACCCGGGCGCGGT bsequenceasisACCCGAGCGCGGT gapopen gapextend cline WaterCommandlinecmdwater outfiletempwatertxt asequenceasisACCCGGGCGCGGT bsequenceasisACCCGAGCGCGGT gapopen gapextend"
    },
    {
        "input_text": "summarize: def _get_parameter(self, name):                for parameter in self.parameters:            if name in parameter.names:                if isinstance(parameter, _Switch):                    return parameter.is_set                else:                    return parameter.value        raise ValueError(f\"Option name {name} was not found.\")",
        "labels_text": "Get a commandline option value PRIVATE"
    },
    {
        "input_text": "summarize: def _clear_parameter(self, name):                cleared_option = False        for parameter in self.parameters:            if name in parameter.names:                parameter.value = None                parameter.is_set = False                cleared_option = True        if not cleared_option:            raise ValueError(f\"Option name {name} was not found.\")",
        "labels_text": "Reset or clear a commandline option value PRIVATE"
    },
    {
        "input_text": "summarize: def _check_value(self, value, name, check_function):                if check_function is not None:            is_good = check_function(value)  # May raise an exception            if is_good not in [0, 1, True, False]:                raise ValueError(                    f\"Result of check_function: {is_good!r} is of an unexpected value\"                )            if not is_good:                raise ValueError(                    f\"Invalid parameter value {value!r} for parameter {name}\"                )",
        "labels_text": "Check whether the given value is valid PRIVATE No return value it either work or raise a ValueError This us the passed function checkfunction which can either return a bad good value or raise an error Either way this function will raise an error if the value is not valid or finish silently otherwise"
    },
    {
        "input_text": "summarize: def __setattr__(self, name, value):                if name in [\"parameters\", \"program_name\"]:  # Allowed attributes            self.__dict__[name] = value        else:            self.set_parameter(name, value)",
        "labels_text": "Set attribute name to value PRIVATE This code implement a workaround for a user interface issue Without this setattr attributebased assignment of parameter will silently accept invalid parameter leading to known instance of the user assuming that parameter for the application are set when they are not from BioEmbossApplications import WaterCommandline cline WaterCommandlinegapopen gapextend stdoutTrue clineasequence afasta clinebsequence bfasta clinecsequence cfasta Traceback most recent call last ValueError Option name csequence wa not found printcline water stdout asequenceafasta bsequencebfasta gapopen gapextend This workaround us a whitelist of object attribute and set the object attribute list a normal for these Other attribute are assumed to be parameter and passed to the selfsetparameter method for validation and assignment"
    },
    {
        "input_text": "summarize: def __str__(self):                assert not hasattr(self, \"value\")        if self.is_set:            return f\"{self.names[0]} \"        else:            return \"\"",
        "labels_text": "Return the value of this option for the commandline Includes a trailing space"
    },
    {
        "input_text": "summarize: def _test():        import doctest    doctest.testmod(verbose=1)",
        "labels_text": "Run the BioApplication module doctests PRIVATE"
    },
    {
        "input_text": "summarize: def _validate_incompatibilities(self, incompatibles):                for a in incompatibles:            if self._get_parameter(a):                for b in incompatibles[a]:                    if self._get_parameter(b):                        raise ValueError(f\"Options {a} and {b} are incompatible.\")",
        "labels_text": "Validate parameter for incompatibility PRIVATE Used by the validate method"
    },
    {
        "input_text": "summarize: def _test():        import doctest    doctest.testmod(verbose=1)",
        "labels_text": "Run the BioBlastApplications module doctests PRIVATE"
    },
    {
        "input_text": "summarize: def fmt_(value, format_spec=\"%s\", default_str=\"<unknown>\"):        if value is None:        return default_str    return format_spec % value",
        "labels_text": "Ensure the given value format to a string correctly"
    },
    {
        "input_text": "summarize: def __init__(self):                self.application = \"\"        self.version = \"\"        self.date = \"\"        self.reference = \"\"        self.query = \"\"        self.query_letters = None        self.database = \"\"        self.database_sequences = None        self.database_letters = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.title = \"\"        self.score = None        self.bits = None        self.e = None        self.num_alignments = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"{self.title:<66} {self.score:>5}  {self.e}\"",
        "labels_text": "Return the description a a string"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.items = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def append_item(self, item):                if len(self.items) == 0:            self.title = str(item)        self.items.append(item)",
        "labels_text": "Add a description extended record"
    },
    {
        "input_text": "summarize: def __init__(self):                self.id = None        self.title = None        self.accession = None        self.taxid = None        self.sciname = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"{self.id} {self.title}\"",
        "labels_text": "Return the description identifier and title a a string"
    },
    {
        "input_text": "summarize: def __init__(self):                self.title = \"\"        self.hit_id = \"\"        self.hit_def = \"\"        self.length = None        self.hsps = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                lines = self.title.split(\"\\n\")        lines.append(f\"Length = {self.length}\\n\")        return \"\\n           \".join(lines)",
        "labels_text": "Return the BLAST alignment a a formatted string"
    },
    {
        "input_text": "summarize: def __init__(self):                self.alignment = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.number = None        self.reused_seqs = []        self.new_seqs = []        self.alignments = []        self.multiple_alignment = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.database_name = []        self.posted_date = []        self.num_letters_in_database = []        self.num_sequences_in_database = []        self.ka_params = (None, None, None)        self.gapped = 0        self.ka_params_gap = (None, None, None)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                Header.__init__(self)        DatabaseReport.__init__(self)        Parameters.__init__(self)        self.descriptions = []        self.alignments = []        self.multiple_alignment = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                Header.__init__(self)        DatabaseReport.__init__(self)        Parameters.__init__(self)        self.rounds = []        self.converged = 0",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, debug=0):                self._tag = []        self._value = \"\"        self._debug = debug        self._debug_ignore_list = []        self._method_name_level = 1        self._method_map = None",
        "labels_text": "Initialize the parser Arguments debug integer amount of debug information to print"
    },
    {
        "input_text": "summarize: def characters(self, ch):                self._value += ch",
        "labels_text": "Found some text Arguments ch character read"
    },
    {
        "input_text": "summarize: def reset(self):                self._records = []        self._header = Header()        self._parameters = Parameters()        self._parameters.filter = None",
        "labels_text": "Reset all the data allowing reuse of the BlastParser object"
    },
    {
        "input_text": "summarize: def _start_blast_record(self):                self._blast = Blast()",
        "labels_text": "Start interaction PRIVATE"
    },
    {
        "input_text": "summarize: def _set_header_application(self):                self._header.application = self._value.upper()",
        "labels_text": "BLAST program eg blastp blastn etc PRIVATE Save this to put on each blast record object"
    },
    {
        "input_text": "summarize: def _set_header_reference(self):                self._header.reference = self._value",
        "labels_text": "Record any article reference describing the algorithm PRIVATE Save this to put on each blast record object"
    },
    {
        "input_text": "summarize: def _set_header_database(self):                self._header.database = self._value",
        "labels_text": "Record the database searched PRIVATE Save this to put on each blast record object"
    },
    {
        "input_text": "summarize: def _set_header_query_id(self):                self._header.query_id = self._value",
        "labels_text": "Record the identifier of the query PRIVATE Important in old pre BLAST for recent version IterationqueryID is enough"
    },
    {
        "input_text": "summarize: def _set_header_query(self):                self._header.query = self._value",
        "labels_text": "Record the definition line of the query PRIVATE Important in old pre BLAST for recent version Iterationquerydef is enough"
    },
    {
        "input_text": "summarize: def _set_header_query_letters(self):                self._header.query_letters = int(self._value)",
        "labels_text": "Record the length of the query PRIVATE Important in old pre BLAST for recent version Iterationquerylen is enough"
    },
    {
        "input_text": "summarize: def _set_record_query_id(self):                self._blast.query_id = self._value",
        "labels_text": "Record the identifier of the query PRIVATE"
    },
    {
        "input_text": "summarize: def _set_record_query_def(self):                self._blast.query = self._value",
        "labels_text": "Record the definition line of the query PRIVATE"
    },
    {
        "input_text": "summarize: def _set_record_query_letters(self):                self._blast.query_letters = int(self._value)",
        "labels_text": "Record the length of the query PRIVATE"
    },
    {
        "input_text": "summarize: def _set_record_hits(self):                self._blast.num_hits = int(self._value)",
        "labels_text": "Hits to the database sequence one for every sequence PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_matrix(self):                self._parameters.matrix = self._value",
        "labels_text": "Matrix used M on legacy BLAST PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_expect(self):                # NOTE: In old text output there was a line:        # Number of sequences better than 1.0e-004: 1        # As far as I can see, parameters.num_seqs_better_e        # would take the value of 1, and the expectation        # value was not recorded.        #        # Anyway we should NOT record this against num_seqs_better_e        self._parameters.expect = self._value",
        "labels_text": "Expect value cutoff PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_sc_match(self):                self._parameters.sc_match = int(self._value)",
        "labels_text": "Match score for nucleotidenucleotide comparison r PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_sc_mismatch(self):                self._parameters.sc_mismatch = int(self._value)",
        "labels_text": "Mismatch penalty for nucleotidenucleotide comparison r PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_gap_penalties(self):                self._parameters.gap_penalties = int(self._value)",
        "labels_text": "Gap existence cost G PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_gap_extend(self):                self._parameters.gap_penalties = (            self._parameters.gap_penalties,            int(self._value),        )",
        "labels_text": "Gap extension cose E PRIVATE"
    },
    {
        "input_text": "summarize: def _set_parameters_filter(self):                self._parameters.filter = self._value",
        "labels_text": "Record filtering option F PRIVATE"
    },
    {
        "input_text": "summarize: def _end_hit(self):                # Cleanup        self._blast.multiple_alignment = None        self._hit = None        self._descr = None",
        "labels_text": "Clear variable PRIVATE"
    },
    {
        "input_text": "summarize: def set_hit_id(self):                self._hit.hit_id = self._value        self._hit.title = self._value + \" \"",
        "labels_text": "Record the identifier of the database sequence PRIVATE"
    },
    {
        "input_text": "summarize: def set_hit_def(self):                self._hit.hit_def = self._value        self._hit.title += self._value        self._descr.title = self._hit.title",
        "labels_text": "Record the definition line of the database sequence PRIVATE"
    },
    {
        "input_text": "summarize: def set_hit_accession(self):                self._hit.accession = self._value        self._descr.accession = self._value",
        "labels_text": "Record the accession value of the database sequence PRIVATE"
    },
    {
        "input_text": "summarize: def set_hit_len(self):                self._hit.length = int(self._value)",
        "labels_text": "Record the length of the hit"
    },
    {
        "input_text": "summarize: def _set_hsp_score(self):                self._hsp.score = float(self._value)        if self._descr.score is None:            self._descr.score = float(self._value)",
        "labels_text": "Record the raw score of HSP PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_bit_score(self):                self._hsp.bits = float(self._value)        if self._descr.bits is None:            self._descr.bits = float(self._value)",
        "labels_text": "Record the Bit score of HSP PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_e_value(self):                self._hsp.expect = float(self._value)        if self._descr.e is None:            self._descr.e = float(self._value)",
        "labels_text": "Record the expect value of the HSP PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_query_start(self):                self._hsp.query_start = int(self._value)",
        "labels_text": "Offset of query at the start of the alignment oneoffset PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_query_end(self):                self._hsp.query_end = int(self._value)",
        "labels_text": "Offset of query at the end of the alignment oneoffset PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_hit_from(self):                self._hsp.sbjct_start = int(self._value)",
        "labels_text": "Offset of the database at the start of the alignment oneoffset PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_hit_to(self):                self._hsp.sbjct_end = int(self._value)",
        "labels_text": "Offset of the database at the end of the alignment oneoffset PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_query_frame(self):                v = int(self._value)        self._hsp.frame = (v,)        if self._header.application == \"BLASTN\":            self._hsp.strand = (\"Plus\" if v > 0 else \"Minus\",)",
        "labels_text": "Frame of the query if applicable PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_hit_frame(self):                v = int(self._value)        if len(self._hsp.frame) == 0:            self._hsp.frame = (0, v)        else:            self._hsp.frame += (v,)        if self._header.application == \"BLASTN\":            self._hsp.strand += (\"Plus\" if v > 0 else \"Minus\",)",
        "labels_text": "Frame of the database sequence if applicable PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_query_strand(self):                self._hsp.strand = (self._value,)        if self._header.application == \"BLASTN\":            self._hsp.frame = (1 if self._value == \"Plus\" else -1,)",
        "labels_text": "Frame of the query if applicable PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_hit_strand(self):                self._hsp.strand += (self._value,)        if self._header.application == \"BLASTN\":            self._hsp.frame += (1 if self._value == \"Plus\" else -1,)",
        "labels_text": "Frame of the database sequence if applicable PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_identity(self):                v = int(self._value)        self._hsp.identities = v        if self._hsp.positives is None:            self._hsp.positives = v",
        "labels_text": "Record the number of identity in the alignment PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_positive(self):                self._hsp.positives = int(self._value)",
        "labels_text": "Record the number of positive conservative substitution in the alignment PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_gaps(self):                self._hsp.gaps = int(self._value)",
        "labels_text": "Record the number of gap in the alignment PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_align_len(self):                self._hsp.align_length = int(self._value)",
        "labels_text": "Record the length of the alignment PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_query_seq(self):                self._hsp.query = self._value",
        "labels_text": "Record the alignment string for the query PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_subject_seq(self):                self._hsp.sbjct = self._value",
        "labels_text": "Record the alignment string for the database PRIVATE"
    },
    {
        "input_text": "summarize: def _set_hsp_midline(self):                self._hsp.match = self._value  # do NOT strip spaces!        assert len(self._hsp.match) == len(self._hsp.query)        assert len(self._hsp.match) == len(self._hsp.sbjct)",
        "labels_text": "Record the middle line a normally seen in BLAST report PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_db_num(self):                self._blast.num_sequences_in_database = int(self._value)",
        "labels_text": "Record the number of sequence in the database PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_db_len(self):                self._blast.num_letters_in_database = int(self._value)",
        "labels_text": "Record the number of letter in the database PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_hsp_len(self):                self._blast.effective_hsp_length = int(self._value)",
        "labels_text": "Record the effective HSP length PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_eff_space(self):                self._blast.effective_search_space = float(self._value)",
        "labels_text": "Record the effective search space PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_kappa(self):                self._blast.ka_params = float(self._value)",
        "labels_text": "KarlinAltschul parameter K PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_lambda(self):                self._blast.ka_params = (float(self._value), self._blast.ka_params)",
        "labels_text": "KarlinAltschul parameter Lambda PRIVATE"
    },
    {
        "input_text": "summarize: def _set_statistics_entropy(self):                self._blast.ka_params = self._blast.ka_params + (float(self._value),)",
        "labels_text": "KarlinAltschul parameter H PRIVATE"
    },
    {
        "input_text": "summarize: def _start_hit_descr_item(self):                self._hit_descr_item = DescriptionExtItem()",
        "labels_text": "XML v Start hit description item"
    },
    {
        "input_text": "summarize: def _end_hit_descr_item(self):                self._descr.append_item(self._hit_descr_item)        if not self._hit.title:            self._hit.title = str(self._hit_descr_item)        self._hit_descr_item = None",
        "labels_text": "XML v Start hit description item"
    },
    {
        "input_text": "summarize: def _end_description_id(self):                self._hit_descr_item.id = self._value        if not self._hit.hit_id:            self._hit.hit_id = self._value",
        "labels_text": "XML v The identifier of the database sequencePRIVATE"
    },
    {
        "input_text": "summarize: def _end_description_accession(self):                self._hit_descr_item.accession = self._value        if not getattr(self._hit, \"accession\", None):            self._hit.accession = self._value",
        "labels_text": "XML v The accession value of the database sequence PRIVATE"
    },
    {
        "input_text": "summarize: def _end_description_title(self):                self._hit_descr_item.title = self._value",
        "labels_text": "XML v The hit description title PRIVATE"
    },
    {
        "input_text": "summarize: def read(handle, debug=0):        iterator = parse(handle, debug)    try:        record = next(iterator)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(iterator)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    return record",
        "labels_text": "Return a single Blast record assumes just one query Uses the BlastParser internally This function is for use when there is one and only one BLAST result in your XML file Use the BioBlastNCBIXMLparse function if you expect more than one BLAST record ie if you have more than one query sequence"
    },
    {
        "input_text": "summarize: def __init__(self):                parser = expat.ParserCreate()        parser.SetParamEntityParsing(expat.XML_PARAM_ENTITY_PARSING_ALWAYS)        parser.ExternalEntityRefHandler = self._externalEntityRefHandler        self.parser = parser        self.start_methods = {}        self.end_methods = {}        XMLHandler._dtd_methods = self.start_methods, self.end_methods",
        "labels_text": "Initialize the parser and parse the BLAST XML DTD file"
    },
    {
        "input_text": "summarize: def parseFile(self, filename):                directory = Entrez.__path__[0]        path = os.path.join(directory, \"DTDs\", filename)        parser = self.parser.ExternalEntityParserCreate(None)        parser.ElementDeclHandler = self._elementDeclHandler        with open(path, \"rb\") as stream:            parser.ParseFile(stream)",
        "labels_text": "Parse a DTD file"
    },
    {
        "input_text": "summarize: def __init__(self, parser):                self.parser = parser        self.start_methods = {}        self.end_methods = {}        XMLHandler._schema_methods = self.start_methods, self.end_methods",
        "labels_text": "Initialize the XML Schema parser"
    },
    {
        "input_text": "summarize: def __init__(self, parser):                parser.XmlDeclHandler = self._xmlDeclHandler        parser.SetParamEntityParsing(expat.XML_PARAM_ENTITY_PARSING_ALWAYS)        self._parser = parser",
        "labels_text": "Initialize the expat parser"
    },
    {
        "input_text": "summarize: def _startElementHandler(self, name, attributes):                method = self._start_methods.get(name)        if method is None:            raise ValueError(                \"Failed to find method for %s (%s)\" % (name, self._start_methods.keys())            )        method(self, name, attributes)",
        "labels_text": "Found XML start tag Arguments name name of the tag attribute tag attribute"
    },
    {
        "input_text": "summarize: def _endElementHandler(self, name):                method = self._end_methods.get(name)        if method is None:            raise ValueError(\"Failed to find method for %s\" % name)        method(self, name)",
        "labels_text": "Found XML end tag Arguments name tag name"
    },
    {
        "input_text": "summarize: def _characterDataHandler(self, characters):                self._characters += characters",
        "labels_text": "Found some text Arguments character character read"
    },
    {
        "input_text": "summarize: def __init__(self, stream):                self.stream = stream",
        "labels_text": "Init the stuff"
    },
    {
        "input_text": "summarize: def __init__(self, message):                self.msg = message",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"Failed to parse the XML data (%s). Please make sure that the input data \"            \"are in XML format.\" % self.msg        )",
        "labels_text": "Return a string summary of the exception"
    },
    {
        "input_text": "summarize: def __init__(self, message):                self.msg = message",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"Failed to parse the XML data (%s). Please make sure that the input data \"            \"are not corrupted.\" % self.msg        )",
        "labels_text": "Return a string summary of the exception"
    },
    {
        "input_text": "summarize: def __init__(self):                self.query = None",
        "labels_text": "Initialize the Record object"
    },
    {
        "input_text": "summarize: def keys(self):                return [hit.target.id for hit in self]",
        "labels_text": "Return a list of the targetid of each hit"
    },
    {
        "input_text": "summarize: def index(self, key):                for i, hit in enumerate(self):            if hit.target.id == key:                return i        raise ValueError(f\"'{key}' not found\")",
        "labels_text": "Return the index of the hit for which the targetid is equal to the key"
    },
    {
        "input_text": "summarize: def parse(source):        return Records(source)",
        "labels_text": "Parse an XML file containing BLAST output and return a BioBlastRecords object This return an iterator object iterating over it return BioBlastRecord object one by one The source can be a file stream or the path to an XML file containing the BLAST output If a file stream source must be in binary mode This allows the parser to detect the encoding from the XML fileand to use it to convert any text in the XML to the correct Unicode string The qblast function in BioBlast return a file stream in binary mode For file please use mode rb when opening the file a in from Bio import Blast stream openBlastwntsxml rb opened in binary mode record Blastparsestream for record in record printrecordqueryid recordquerydescription Query gi Homo sapiens winglesstype MMTV integration site family member WNT transcript variant mRNA Query gi Homo sapiens winglesstype MMTV integration site family member A WNTA mRNA Query gi Homo sapiens winglesstype MMTV integration site family member WNT mRNA Query gi Homo sapiens winglesstype MMTV integration site family member A WNTA transcript variant mRNA Query gi Homo sapiens winglesstype MMTV integration site family member WNT mRNA streamclose"
    },
    {
        "input_text": "summarize: def __init__(self, **kwds):                self.start = int(kwds[\"start\"])        self.enzyme = kwds[\"enzyme\"]        self.cuts_in = kwds[\"cuts_in\"]        self.blocked_in = kwds[\"blocked_in\"]",
        "labels_text": "Initialize a DifferentialCutsite Each member a listed in the class description should be included a a keyword"
    },
    {
        "input_text": "summarize: def sort(self, order=None):                n = len(self) + 1        indices = np.ones(n, dtype=\"intc\")        if order is None:            order = np.ones(n, dtype=\"d\")        elif isinstance(order, np.ndarray):            order = np.require(order, dtype=\"d\", requirements=\"C\")        else:            order = np.array(order, dtype=\"d\")        _cluster.Tree.sort(self, indices, order)        return indices",
        "labels_text": "Sort the hierarchical clustering tree Sort the hierarchical clustering tree by switching the left and right subnode of node such that the element in the lefttoright order of the tree tend to have increasing order value Return the index of the element in the lefttoright order in the hierarchical clustering tree such that the element with index indicesi occurs at position i in the dendrogram"
    },
    {
        "input_text": "summarize: def cut(self, nclusters=None):                n = len(self) + 1        indices = np.ones(n, dtype=\"intc\")        if nclusters is None:            nclusters = n        _cluster.Tree.cut(self, indices, nclusters)        return indices",
        "labels_text": "Create cluster by cutting the hierarchical clustering tree Divide the element in a hierarchical clustering result mytree into cluster and return an array with the number of the cluster to which each element wa assigned Keyword argument nclusters The desired number of cluster"
    },
    {
        "input_text": "summarize: def kmedoids(distance, nclusters=2, npass=1, initialid=None):        distance = __check_distancematrix(distance)    nitems = len(distance)    clusterid, npass = __check_initialid(initialid, npass, nitems)    error, nfound = _cluster.kmedoids(distance, nclusters, npass, clusterid)    return clusterid, error, nfound",
        "labels_text": "Perform kmedoids clustering This function performs kmedoids clustering and return the cluster assignment the withincluster sum of distance of the optimal kmedoids clustering solution and the number of time the optimal solution wa found Keyword argument distance The distance matrix between the item There are three way in which you can pas a distance matrix a D NumPy array in which only the leftlower part of the array will be accessed a D NumPy array containing the distance consecutively a list of row containing the lowertriangular part of the distance matrix Examples are from numpy import array option distance array option distance array option distance array array array These three correspond to the same distance matrix nclusters number of cluster the k in kmedoids npass the number of time the kmedoids clustering algorithm is performed each time with a different random initial condition initialid the initial clustering from which the algorithm should start If initialid is not given the routine carry out npass repetition of the EM algorithm each time starting from a different random initial clustering If initialid is given the routine carry out the EM algorithm only once starting from the initial clustering specified by initialid and without randomizing the order in which item are assigned to cluster ie using the same order a in the data matrix In that case the kmedoids algorithm is fully deterministic Return value clusterid array containing the index of the cluster to which each item wa assigned in the best kmedoids clustering solution that wa found in the npass run note that the index of a cluster is the index of the item that is the medoid of the cluster error the withincluster sum of distance for the returned kmedoids clustering solution nfound the number of time this solution wa found"
    },
    {
        "input_text": "summarize: def treecluster(self, transpose=False, method=\"m\", dist=\"e\"):                if transpose:            weight = self.gweight        else:            weight = self.eweight        return treecluster(self.data, self.mask, weight, transpose, method, dist)",
        "labels_text": "Apply hierarchical clustering and return a Tree object The pairwise single complete centroid and average linkage hierarchical clustering method are available Keyword argument transpose if False row are clustered if True column are clustered dist specifies the distance function to be used dist e Euclidean distance dist b City Block distance dist c Pearson correlation dist a absolute value of the correlation dist u uncentered correlation dist x absolute uncentered correlation dist s Spearmans rank correlation dist k Kendalls tau method specifies which linkage method is used method s Single pairwise linkage method m Complete maximum pairwise linkage default method c Centroid linkage method a Average pairwise linkage See the description of the Tree class for more information about the Tree object returned by this method"
    },
    {
        "input_text": "summarize: def kcluster(        self,        nclusters=2,        transpose=False,        npass=1,        method=\"a\",        dist=\"e\",        initialid=None,    ):                if transpose:            weight = self.gweight        else:            weight = self.eweight        return kcluster(            self.data,            nclusters,            self.mask,            weight,            transpose,            npass,            method,            dist,            initialid,        )",
        "labels_text": "Apply kmeans or kmedian clustering This method return a tuple clusterid error nfound Keyword argument nclusters number of cluster the k in kmeans transpose if False gene row are clustered if True sample column are clustered npass number of time the kmeans clustering algorithm is performed each time with a different random initial condition method specifies how the center of a cluster is found method a arithmetic mean method m median dist specifies the distance function to be used dist e Euclidean distance dist b City Block distance dist c Pearson correlation dist a absolute value of the correlation dist u uncentered correlation dist x absolute uncentered correlation dist s Spearmans rank correlation dist k Kendalls tau initialid the initial clustering from which the algorithm should start If initialid is None the routine carry out npass repetition of the EM algorithm each time starting from a different random initial clustering If initialid is given the routine carry out the EM algorithm only once starting from the given initial clustering and without randomizing the order in which item are assigned to cluster ie using the same order a in the data matrix In that case the kmeans algorithm is fully deterministic Return value clusterid array containing the number of the cluster to which each genesample wa assigned in the best kmeans clustering solution that wa found in the npass run error the withincluster sum of distance for the returned kmeans clustering solution nfound the number of time this solution wa found"
    },
    {
        "input_text": "summarize: def somcluster(        self, transpose=False, nxgrid=2, nygrid=1, inittau=0.02, niter=1, dist=\"e\"    ):                if transpose:            weight = self.gweight        else:            weight = self.eweight        return somcluster(            self.data,            self.mask,            weight,            transpose,            nxgrid,            nygrid,            inittau,            niter,            dist,        )",
        "labels_text": "Calculate a selforganizing map on a rectangular grid The somcluster method return a tuple clusterid celldata Keyword argument transpose if False gene row are clustered if True sample column are clustered nxgrid the horizontal dimension of the rectangular SOM map nygrid the vertical dimension of the rectangular SOM map inittau the initial value of tau the neighborbood function niter the number of iteration dist specifies the distance function to be used dist e Euclidean distance dist b City Block distance dist c Pearson correlation dist a absolute value of the correlation dist u uncentered correlation dist x absolute uncentered correlation dist s Spearmans rank correlation dist k Kendalls tau Return value clusterid array with two column while the number of row is equal to the number of gene or the number of sample depending on whether gene or sample are being clustered Each row in the array contains the x and y coordinate of the cell in the rectangular SOM grid to which the gene or sample wa assigned celldata an array with dimension nxgrid nygrid number of sample if gene are being clustered or nxgrid nygrid number of gene if sample are being clustered Each item ix iy of this array is a D vector containing the gene expression data for the centroid of the cluster in the SOM grid cell with coordinate ix iy"
    },
    {
        "input_text": "summarize: def clustercentroids(self, clusterid=None, method=\"a\", transpose=False):                return clustercentroids(self.data, self.mask, clusterid, method, transpose)",
        "labels_text": "Calculate the cluster centroid and return a tuple cdata cmask The centroid is defined a either the mean or the median over all item for each dimension Keyword argument data nrows x ncolumns array containing the expression data mask nrows x ncolumns array of integer showing which data are missing If maski j then datai j is missing transpose if False gene row cluster are considered if True sample column cluster are considered clusterid array containing the cluster number for each gene or sample The cluster number should be nonnegative method specifies how the centroid is calculated method a arithmetic mean over each dimension default method m median over each dimension Return value cdata D array containing the cluster centroid If transpose is False then the dimension of cdata are nclusters x ncolumns If transpose is True then the dimension of cdata are nrows x nclusters cmask D array of integer describing which item in cdata if any are missing"
    },
    {
        "input_text": "summarize: def clusterdistance(        self, index1=0, index2=0, method=\"a\", dist=\"e\", transpose=False    ):                if transpose:            weight = self.gweight        else:            weight = self.eweight        return clusterdistance(            self.data, self.mask, weight, index1, index2, method, dist, transpose        )",
        "labels_text": "Calculate the distance between two cluster Keyword argument index D array identifying which genessamples belong to the first cluster If the cluster contains only one gene then index can also be written a a single integer index D array identifying which genessamples belong to the second cluster If the cluster contains only one gene then index can also be written a a single integer transpose if False gene row are clustered if True sample column are clustered dist specifies the distance function to be used dist e Euclidean distance dist b City Block distance dist c Pearson correlation dist a absolute value of the correlation dist u uncentered correlation dist x absolute uncentered correlation dist s Spearmans rank correlation dist k Kendalls tau method specifies how the distance between two cluster is defined method a the distance between the arithmetic mean of the two cluster method m the distance between the median of the two cluster method s the smallest pairwise distance between member of the two cluster method x the largest pairwise distance between member of the two cluster method v average of the pairwise distance between member of the two cluster transpose if False cluster of row are considered if True cluster of column are considered"
    },
    {
        "input_text": "summarize: def distancematrix(self, transpose=False, dist=\"e\"):                if transpose:            weight = self.gweight        else:            weight = self.eweight        return distancematrix(self.data, self.mask, weight, transpose, dist)",
        "labels_text": "Calculate the distance matrix and return it a a list of array Keyword argument transpose if False calculate the distance between gene row if True calculate the distance between sample column dist specifies the distance function to be used dist e Euclidean distance dist b City Block distance dist c Pearson correlation dist a absolute value of the correlation dist u uncentered correlation dist x absolute uncentered correlation dist s Spearmans rank correlation dist k Kendalls tau Return value The distance matrix is returned a a list of D array containing the distance matrix between the gene expression data The number of column in each row is equal to the row number Hence the first row ha zero length An example of the return value is matrix array array array This corresponds to the distance matrix"
    },
    {
        "input_text": "summarize: def read(handle):        return Record(handle)",
        "labels_text": "Read gene expression data from the file handle and return a Record The file should be in the file format defined for Michael Eisens ClusterTreeView program"
    },
    {
        "input_text": "summarize: def get_aln_length(self):                return self.get_alignment_length() // 3",
        "labels_text": "Get alignment length"
    },
    {
        "input_text": "summarize: def toMultipleSeqAlignment(self):                alignments = [SeqRecord(rec.seq.toSeq(), id=rec.id) for rec in self._records]        return MultipleSeqAlignment(alignments)",
        "labels_text": "Convert the CodonAlignment to a MultipleSeqAlignment Return a MultipleSeqAlignment containing all the SeqRecord in the CodonAlignment using Seq to store sequence"
    },
    {
        "input_text": "summarize: def from_msa(cls, align):                rec = [SeqRecord(CodonSeq(str(i.seq)), id=i.id) for i in align._records]        return cls(rec)",
        "labels_text": "Convert a MultipleSeqAlignment to CodonAlignment Function to convert a MultipleSeqAlignment to CodonAlignment It is the user responsibility to ensure all the requirement needed by CodonAlignment is met"
    },
    {
        "input_text": "summarize: def _count_replacement(codon_set, G):        from math import floor    if len(codon_set) == 1:        return 0, 0    elif len(codon_set) == 2:        codons = list(codon_set)        return floor(G[codons[0]][codons[1]])    else:        codons = list(codon_set)        return _prim(G)",
        "labels_text": "Count replacement needed for a given codonset PRIVATE"
    },
    {
        "input_text": "summarize: def _get_subgraph(codons, G):        subgraph = {}    for i in codons:        subgraph[i] = {}        for j in codons:            if i != j:                subgraph[i][j] = G[i][j]    return subgraph",
        "labels_text": "Get the subgraph that contains all codon in list PRIVATE"
    },
    {
        "input_text": "summarize: def get_codon_num(self):                return len(self.rf_table)",
        "labels_text": "Return the number of codon in the CodonSeq"
    },
    {
        "input_text": "summarize: def toSeq(self):                return Seq(str(self))",
        "labels_text": "Convert DNA to seq object"
    },
    {
        "input_text": "summarize: def full_translate(self, codon_table=None, stop_symbol=\"*\"):                if codon_table is None:            codon_table = CodonTable.generic_by_id[1]        full_rf_table = self.get_full_rf_table()        return self.translate(            codon_table=codon_table,            stop_symbol=stop_symbol,            rf_table=full_rf_table,            ungap_seq=False,        )",
        "labels_text": "Apply full translation with gap considered"
    },
    {
        "input_text": "summarize: def ungap(self, gap=\"-\"):                if len(gap) != 1 or not isinstance(gap, str):            raise ValueError(f\"Unexpected gap character, {gap!r}\")        return CodonSeq(str(self).replace(gap, \"\"), rf_table=self.rf_table)",
        "labels_text": "Return a copy of the sequence without the gap character"
    },
    {
        "input_text": "summarize: def from_seq(cls, seq, rf_table=None):                if rf_table is None:            return cls(str(seq))        else:            return cls(str(seq), rf_table=rf_table)",
        "labels_text": "Get codon sequence from sequence data"
    },
    {
        "input_text": "summarize: def _codons2re(codons):        reg = \"\"    for i in zip(*codons):        if len(set(i)) == 1:            reg += \"\".join(set(i))        else:            reg += \"[\" + \"\".join(set(i)) + \"]\"    return reg",
        "labels_text": "Generate regular expression based on a given list of codon PRIVATE"
    },
    {
        "input_text": "summarize: def query_coverage(self):                s = self.query_aln.replace(\"=\", \"\")        return len(s)",
        "labels_text": "Return the length of the query covered in the alignment"
    },
    {
        "input_text": "summarize: def hit_coverage(self):                s = self.hit_aln.replace(\"=\", \"\")        return len(s)",
        "labels_text": "Return the length of the hit covered in the alignment"
    },
    {
        "input_text": "summarize: def make_back_table(table, default_stop_codon):        # Do the sort so changes in the hash implementation won't affect    # the result when one amino acid is coded by more than one codon.    back_table = {}    for key in sorted(table):        back_table[table[key]] = key    back_table[None] = default_stop_codon    return back_table",
        "labels_text": "Back a backtable naive single codon mapping ONLY RETURNS A SINGLE CODON chosen from the possible alternative based on their sort order"
    },
    {
        "input_text": "summarize: def __init__(self, id, names, table, start_codons, stop_codons):                self.id = id        self.names = names        self.forward_table = table        self.back_table = make_back_table(table, stop_codons[0])        self.start_codons = start_codons        self.stop_codons = stop_codons",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}(id={self.id!r}, names={self.names!r}, ...)\"",
        "labels_text": "Represent the NCBI codon table class a a string for debugging"
    },
    {
        "input_text": "summarize: def __getattr__(self, name):                return getattr(self._codon_table, name)",
        "labels_text": "Forward attribute lookup to the original table"
    },
    {
        "input_text": "summarize: def __contains__(self, codon):                try:            self.__getitem__(codon)            return True        except (KeyError, TranslationError):            return False",
        "labels_text": "Check if codon work a key for ambiguous forwardtable Only return True if forwardtablecodon return a value"
    },
    {
        "input_text": "summarize: def get(self, codon, failobj=None):                try:            return self.__getitem__(codon)        except KeyError:            return failobj",
        "labels_text": "Implement get for dictionarylike behaviour"
    },
    {
        "input_text": "summarize: def __init__(self):                self.comments = \"\"        self.primers = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __len__(self):                return self.size",
        "labels_text": "Length of the primer product ie product size"
    },
    {
        "input_text": "summarize: def read(handle):        iterator = parse(handle)    try:        record = next(iterator)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(iterator)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    return record",
        "labels_text": "Parse primer output into a BioEmbossPrimerRecord object This is for when there is one and only one target sequence If designing primer for multiple sequence use the parse function"
    },
    {
        "input_text": "summarize: def __init__(self):                self.primer_info = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                output = \"\"        for name, primer1, primer2 in self.primer_info:            output += f\"{name} {primer1} {primer2}\\n\"        return output",
        "labels_text": "Summarize the primersearch input record a a string"
    },
    {
        "input_text": "summarize: def add_primer_set(self, primer_name, first_primer_seq, second_primer_seq):                self.primer_info.append((primer_name, first_primer_seq, second_primer_seq))",
        "labels_text": "Add primer information to the record"
    },
    {
        "input_text": "summarize: def __init__(self):                self.amplifiers = {}",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.hit_info = \"\"        self.length = 0",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, tag, attributes, key):                self.tag = tag        self.key = key        self.attributes = attributes",
        "labels_text": "Create a NoneElement"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if other is None:            return True        elif other.__eq__(None):            return True        else:            return False",
        "labels_text": "Define equality with other None object"
    },
    {
        "input_text": "summarize: def __ne__(self, other):                if other is None:            return False        elif other.__eq__(None):            return False        else:            return True",
        "labels_text": "Define nonequality"
    },
    {
        "input_text": "summarize: def __repr__(self):                try:            attributes = self.attributes        except AttributeError:            return \"NoneElement\"        return \"NoneElement(attributes=%r)\" % attributes",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def __new__(cls, value, *args, **kwargs):                return int.__new__(cls, value)",
        "labels_text": "Create an IntegerElement"
    },
    {
        "input_text": "summarize: def __init__(self, value, tag, attributes, key):                self.tag = tag        self.attributes = attributes        self.key = key",
        "labels_text": "Initialize an IntegerElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = int.__repr__(self)        try:            attributes = self.attributes        except AttributeError:            return text        return f\"IntegerElement({text}, attributes={attributes!r})\"",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def __new__(cls, value, *args, **kwargs):                return str.__new__(cls, value)",
        "labels_text": "Create a StringElement"
    },
    {
        "input_text": "summarize: def __init__(self, value, tag, attributes, key):                self.tag = tag        self.attributes = attributes        self.key = key",
        "labels_text": "Initialize a StringElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = str.__repr__(self)        attributes = self.attributes        if not attributes:            return text        return f\"StringElement({text}, attributes={attributes!r})\"",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def __init__(self, tag, attributes, allowed_tags, key=None):                self.tag = tag        if key is None:            self.key = tag        else:            self.key = key        self.attributes = attributes        self.allowed_tags = allowed_tags",
        "labels_text": "Create a ListElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = list.__repr__(self)        attributes = self.attributes        if not attributes:            return text        return f\"ListElement({text}, attributes={attributes!r})\"",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def store(self, value):                key = value.key        if self.allowed_tags is not None and key not in self.allowed_tags:            raise ValueError(\"Unexpected item '%s' in list\" % key)        del value.key        self.append(value)",
        "labels_text": "Append an element to the list checking tag"
    },
    {
        "input_text": "summarize: def __init__(self, tag, attrs, allowed_tags, repeated_tags=None, key=None):                self.tag = tag        if key is None:            self.key = tag        else:            self.key = key        self.attributes = attrs        self.allowed_tags = allowed_tags        self.repeated_tags = repeated_tags        if repeated_tags:            for key in repeated_tags:                self[key] = []",
        "labels_text": "Create a DictionaryElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = dict.__repr__(self)        attributes = self.attributes        if not attributes:            return text        return f\"DictElement({text}, attributes={attributes!r})\"",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def store(self, value):                key = value.key        tag = value.tag        if self.allowed_tags is not None and tag not in self.allowed_tags:            raise ValueError(\"Unexpected item '%s' in dictionary\" % key)        del value.key        if self.repeated_tags and key in self.repeated_tags:            self[key].append(value)        else:            self[key] = value",
        "labels_text": "Add an entry to the dictionary checking tag"
    },
    {
        "input_text": "summarize: def __init__(self, tag, attributes, allowed_tags, first_tag, key=None):                self.tag = tag        if key is None:            self.key = tag        else:            self.key = key        self.attributes = attributes        self.allowed_tags = allowed_tags        self.first_tag = first_tag",
        "labels_text": "Create an OrderedListElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = list.__repr__(self)        attributes = self.attributes        if not attributes:            return text        return f\"OrderedListElement({text}, attributes={attributes!r})\"",
        "labels_text": "Return a string representation of the object"
    },
    {
        "input_text": "summarize: def store(self, value):                key = value.key        if self.allowed_tags is not None and key not in self.allowed_tags:            raise ValueError(\"Unexpected item '%s' in list\" % key)        if key == self.first_tag:            self.append([])        self[-1].append(value)",
        "labels_text": "Append an element to the list checking tag"
    },
    {
        "input_text": "summarize: def __new__(cls, value, *args, **kwargs):                return str.__new__(cls, value)",
        "labels_text": "Create an ErrorElement"
    },
    {
        "input_text": "summarize: def __init__(self, value, tag):                self.tag = tag        self.key = tag",
        "labels_text": "Initialize an ErrorElement"
    },
    {
        "input_text": "summarize: def __repr__(self):                text = str.__repr__(self)        return f\"ErrorElement({text})\"",
        "labels_text": "Return the error message a a string"
    },
    {
        "input_text": "summarize: def __init__(self, message):                self.msg = message",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"Failed to parse the XML data (%s). Please make sure that the input data \"            \"are in XML format.\" % self.msg        )",
        "labels_text": "Return a string summary of the exception"
    },
    {
        "input_text": "summarize: def __init__(self, message):                self.msg = message",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"Failed to parse the XML data (%s). Please make sure that the input data \"            \"are not corrupted.\" % self.msg        )",
        "labels_text": "Return a string summary of the exception"
    },
    {
        "input_text": "summarize: def __init__(self, name):                self.name = name",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"Failed to find tag '%s' in the DTD. To skip all tags that \"            \"are not represented in the DTD, please call Bio.Entrez.read \"            \"or Bio.Entrez.parse with validate=False.\" % self.name        )",
        "labels_text": "Return a string summary of the exception"
    },
    {
        "input_text": "summarize: def __init__(cls, *args, **kwargs):                from Bio import Entrez        try:            cls.directory = Entrez.local_cache  # use default directory for local cache        except PermissionError:            cls._directory = Entrez.local_cache  # no local cache        del Entrez",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def directory(cls):                return cls._directory",
        "labels_text": "Directory for caching XSD and DTD file"
    },
    {
        "input_text": "summarize: def xmlDeclHandler(self, version, encoding, standalone):                self.parser.CharacterDataHandler = self.characterDataHandler        self.parser.ExternalEntityRefHandler = self.externalEntityRefHandler        self.parser.StartNamespaceDeclHandler = self.startNamespaceDeclHandler        self.parser.EndNamespaceDeclHandler = self.endNamespaceDeclHandler        self.parser.StartElementHandler = self.handleMissingDocumentDefinition",
        "labels_text": "Set XML handler when an XML declaration is found"
    },
    {
        "input_text": "summarize: def handleMissingDocumentDefinition(self, tag, attrs):                raise ValueError(            \"As the XML data contained neither a Document Type Definition (DTD) nor an XML Schema, Bio.Entrez is unable to parse these data. We recommend using a generic XML parser from the Python standard library instead, for example ElementTree.\"        )",
        "labels_text": "Raise an Exception if neither a DTD nor an XML Schema is found"
    },
    {
        "input_text": "summarize: def endNamespaceDeclHandler(self, prefix):                if prefix != \"xsi\":            self.namespace_level[prefix] -= 1            if self.namespace_level[prefix] == 0:                for key, value in self.namespace_prefix.items():                    if value == prefix:                        break                else:                    raise RuntimeError(\"Failed to find namespace prefix\")                del self.namespace_prefix[key]",
        "labels_text": "Handle end of an XML namespace declaration"
    },
    {
        "input_text": "summarize: def startSkipElementHandler(self, name, attrs):                self.level += 1",
        "labels_text": "Handle start of an XML skip element"
    },
    {
        "input_text": "summarize: def endRawElementHandler(self, name):                self.level -= 1        if self.level == 0:            self.parser.EndElementHandler = self.endStringElementHandler        if self.namespace_prefix:            try:                uri, name = name.split()            except ValueError:                pass        tag = \"</%s>\" % name        self.data.append(tag)",
        "labels_text": "Handle end of an XML raw element"
    },
    {
        "input_text": "summarize: def endSkipElementHandler(self, name):                self.level -= 1        if self.level == 0:            self.parser.StartElementHandler = self.startElementHandler            self.parser.EndElementHandler = self.endElementHandler",
        "labels_text": "Handle end of an XML skip element"
    },
    {
        "input_text": "summarize: def endElementHandler(self, name):                element = self.element        self.element = element.parent        del element.parent",
        "labels_text": "Handle end of an XML element"
    },
    {
        "input_text": "summarize: def characterDataHandlerRaw(self, content):                self.data.append(content)",
        "labels_text": "Handle character data asis raw"
    },
    {
        "input_text": "summarize: def characterDataHandlerEscape(self, content):                content = escape(content)        self.data.append(content)",
        "labels_text": "Handle character data by encoding it"
    },
    {
        "input_text": "summarize: def skipCharacterDataHandler(self, content):",
        "labels_text": "Handle character data by skipping it"
    },
    {
        "input_text": "summarize: def open_dtd_file(self, filename):                if DataHandler.local_dtd_dir is not None:            path = os.path.join(DataHandler.local_dtd_dir, filename)            try:                handle = open(path, \"rb\")            except FileNotFoundError:                pass            else:                return handle        path = os.path.join(DataHandler.global_dtd_dir, filename)        try:            handle = open(path, \"rb\")        except FileNotFoundError:            pass        else:            return handle        return None",
        "labels_text": "Open specified DTD file"
    },
    {
        "input_text": "summarize: def open_xsd_file(self, filename):                if DataHandler.local_xsd_dir is not None:            path = os.path.join(DataHandler.local_xsd_dir, filename)            try:                handle = open(path, \"rb\")            except FileNotFoundError:                pass            else:                return handle        path = os.path.join(DataHandler.global_xsd_dir, filename)        try:            handle = open(path, \"rb\")        except FileNotFoundError:            pass        else:            return handle        return None",
        "labels_text": "Open specified XSD file"
    },
    {
        "input_text": "summarize: def save_dtd_file(self, filename, text):                if DataHandler.local_dtd_dir is None:            return        path = os.path.join(DataHandler.local_dtd_dir, filename)        try:            handle = open(path, \"wb\")        except OSError:            warnings.warn(f\"Failed to save {filename} at {path}\")        else:            handle.write(text)            handle.close()",
        "labels_text": "Save DTD file to cache"
    },
    {
        "input_text": "summarize: def save_xsd_file(self, filename, text):                if DataHandler.local_xsd_dir is None:            return        path = os.path.join(DataHandler.local_xsd_dir, filename)        try:            handle = open(path, \"wb\")        except OSError:            warnings.warn(f\"Failed to save {filename} at {path}\")        else:            handle.write(text)            handle.close()",
        "labels_text": "Save XSD file to cache"
    },
    {
        "input_text": "summarize: def epost(db, **keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/epost.fcgi\"    variables = {\"db\": db}    variables.update(keywds)    request = _build_request(cgi, variables, post=True)    return _open(request)",
        "labels_text": "Post a file of identifier for future use Posts a file containing a list of UIs for future use in the user environment to use with subsequent search strategy See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterEPost return Handle to the result raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def efetch(db, **keywords):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"    variables = {\"db\": db}    variables.update(keywords)    request = _build_request(cgi, variables)    return _open(request)",
        "labels_text": "Fetch Entrez result which are returned a a handle EFetch retrieves record in the requested format from a list or set of one or more UIs or from user environment See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterEFetch Short example from Bio import Entrez Entrezemail YourNameHereexampleorg handle Entrezefetchdbnucleotide idAY rettypegb retmodetext printhandlereadlinestrip LOCUS AY bp DNA linear PLN APR handleclose This will automatically use an HTTP POST rather than HTTP GET if there are over identifier a recommended by the NCBI Warning The NCBI changed the default retmode in Feb so many database which previously returned text output now give XML return Handle to the result raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def esearch(db, term, **keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"    variables = {\"db\": db, \"term\": term}    variables.update(keywds)    request = _build_request(cgi, variables)    return _open(request)",
        "labels_text": "Run an Entrez search and return a handle to the result ESearch search and retrieves primary IDs for use in EFetch ELink and ESummary and term translation and optionally retains result for future use in the user environment See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterESearch Short example from Bio import Entrez Entrezemail YourNameHereexampleorg handle Entrezesearch dbnucleotide retmax idtypeacc termopuntiaORGN accD Publication Date record Entrezreadhandle handleclose intrecordCount True EF in recordIdList True EF in recordIdList True return Handle to the result which are always in XML format raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def elink(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\"    variables = {}    variables.update(keywds)    request = _build_request(cgi, variables, join_ids=False)    return _open(request)",
        "labels_text": "Check for linked external article and return a handle ELink check for the existence of an external or Related Articles link from a list of one or more primary IDs retrieves IDs and relevancy score for link to Entrez database or Related Articles creates a hyperlink to the primary LinkOut provider for a specific ID and database or list LinkOut URLs and attribute for multiple IDs See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterELink Note that ELink treat the id parameter differently than the other tool when multiple value are given You should generally pas multiple UIDs a a list of string or integer This will provide a onetoone mapping from source database UIDs to destination database UIDs in the result If multiple source UIDs are passed a a single commadelimited string all destination UIDs will be mixed together in the result This example find article related to the Biopython application note entry in the PubMed database from Bio import Entrez Entrezemail YourNameHereexampleorg pmid handle Entrezelinkdbfrompubmed idpmid linknamepubmedpubmed record Entrezreadhandle handleclose printrecordLinkSetDbLinkName pubmedpubmed linked linkId for link in recordLinkSetDbLink in linked True This is explained in much more detail in the Biopython Tutorial return Handle to the result by default in XML format raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def einfo(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi\"    variables = {}    variables.update(keywds)    request = _build_request(cgi, variables)    return _open(request)",
        "labels_text": "Return a summary of the Entrez database a a result handle EInfo provides field name index term count last update and available link for each Entrez database See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterEInfo Short example from Bio import Entrez Entrezemail YourNameHereexampleorg record EntrezreadEntrezeinfo pubmed in recordDbList True return Handle to the result by default in XML format raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def esummary(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"    variables = {}    variables.update(keywds)    request = _build_request(cgi, variables)    return _open(request)",
        "labels_text": "Retrieve document summary a a result handle ESummary retrieves document summary from a list of primary IDs or from the user environment See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterESummary This example discovers more about entry in the structure database from Bio import Entrez Entrezemail YourNameHereexampleorg handle Entrezesummarydbstructure id record Entrezreadhandle handleclose printrecordId printrecordPdbDescr CRYSTAL STRUCTURE OF E COLI ACONITASE B return Handle to the result by default in XML format raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def espell(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/espell.fcgi\"    variables = {}    variables.update(keywds)    request = _build_request(cgi, variables)    return _open(request)",
        "labels_text": "Retrieve spelling suggestion a a result handle ESpell retrieves spelling suggestion if available See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterESpell Short example from Bio import Entrez Entrezemail YourNameHereexampleorg record EntrezreadEntrezespelltermbiopythooon printrecordQuery biopythooon printrecordCorrectedQuery biopython return Handle to the result by default in XML format raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def ecitmatch(**keywds):        cgi = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/ecitmatch.cgi\"    variables = _update_ecitmatch_variables(keywds)    request = _build_request(cgi, variables, ecitmatch=True)    return _open(request)",
        "labels_text": "Retrieve PMIDs for input citation string returned a a handle ECitMatch retrieves PubMed IDs PMIDs that correspond to a set of input citation string See the online documentation for an explanation of the parameter httpwwwncbinlmnihgovbooksNBKchapterECitMatch Short example from Bio import Entrez Entrezemail YourNameHereexampleorg citation journaltitle proc natl acad sci u s a year volume firstpage authorname mann bj key citation handle Entrezecitmatchdbpubmed bdatacitation printhandlereadstripsplit proc natl acad sci u s a mann bj citation handleclose return Handle to the result by default in plain text raise urlliberrorURLError If there a network error"
    },
    {
        "input_text": "summarize: def read(source, validate=True, escape=False, ignore_errors=False):        from .Parser import DataHandler    handler = DataHandler(validate, escape, ignore_errors)    record = handler.read(source)    return record",
        "labels_text": "Parse an XML file from the NCBI Entrez Utilities into python object This function parses an XML file created by NCBIs Entrez Utilities returning a multilevel data structure of Python list and dictionary Most XML file returned by NCBIs Entrez Utilities can be parsed by this function provided it DTD is available Biopython includes the DTDs for most commonly used Entrez Utilities The argument source must be a file or filelike object opened in binary mode or a filename The parser detects the encoding from the XML file and us it to convert all text in the XML to the correct Unicode string The function in BioEntrez to access NCBI Entrez will automatically return XML data in binary mode For file use mode rb when opening the file a in from Bio import Entrez path Entrezesearchxml stream openpath rb opened in binary mode record Entrezreadstream printrecordQueryTranslation biopythonAll Fields streamclose Alternatively you can use the filename directly a in record Entrezreadpath printrecordQueryTranslation biopythonAll Fields which is safer a the file stream will automatically be closed after the record ha been read or if an error occurs If validate is True default the parser will validate the XML file against the DTD and raise an error if the XML file contains tag that are not represented in the DTD If validate is False the parser will simply skip such tag If escape is True all character that are not valid HTML are replaced by HTML escape character to guarantee that the returned string are valid HTML fragment For example a lessthan sign is replaced by lt If escape is False default the string is returned a is If ignoreerrors is False default any error message in the XML file will raise a RuntimeError If ignoreerrors is True error message will be stored a ErrorElement item without raising an exception Whereas the data structure seems to consist of generic Python list dictionary string and so on each of these is actually a class derived from the base type This allows u to store the attribute if any of each element in a dictionary myelementattributes and the tag name in myelementtag"
    },
    {
        "input_text": "summarize: def parse(source, validate=True, escape=False, ignore_errors=False):        from .Parser import DataHandler    handler = DataHandler(validate, escape, ignore_errors)    records = handler.parse(source)    return records",
        "labels_text": "Parse an XML file from the NCBI Entrez Utilities into python object This function parses an XML file created by NCBIs Entrez Utilities returning a multilevel data structure of Python list and dictionary This function is suitable for XML file that in Python can be represented a a list of individual record Whereas read read the complete file and return a single Python list parse is a generator function that return the record one by one This function is therefore particularly useful for parsing large file Most XML file returned by NCBIs Entrez Utilities can be parsed by this function provided it DTD is available Biopython includes the DTDs for most commonly used Entrez Utilities The argument source must be a file or filelike object opened in binary mode or a filename The parser detects the encoding from the XML file and us it to convert all text in the XML to the correct Unicode string The function in BioEntrez to access NCBI Entrez will automatically return XML data in binary mode For file use mode rb when opening the file a in from Bio import Entrez path Entrezpubmedxml stream openpath rb opened in binary mode record Entrezparsestream for record in record printrecordMedlineCitationArticleJournalTitle Social justice San Francisco Calif Biochimica et biophysica acta streamclose Alternatively you can use the filename directly a in record Entrezparsepath for record in record printrecordMedlineCitationArticleJournalTitle Social justice San Francisco Calif Biochimica et biophysica acta which is safer a the file stream will automatically be closed after all the record have been read or if an error occurs If validate is True default the parser will validate the XML file against the DTD and raise an error if the XML file contains tag that are not represented in the DTD If validate is False the parser will simply skip such tag If escape is True all character that are not valid HTML are replaced by HTML escape character to guarantee that the returned string are valid HTML fragment For example a lessthan sign is replaced by lt If escape is False default the string is returned a is If ignoreerrors is False default any error message in the XML file will raise a RuntimeError If ignoreerrors is True error message will be stored a ErrorElement item without raising an exception Whereas the data structure seems to consist of generic Python list dictionary string and so on each of these is actually a class derived from the base type This allows u to store the attribute if any of each element in a dictionary myelementattributes and the tag name in myelementtag"
    },
    {
        "input_text": "summarize: def _format_ids(ids):        if isinstance(ids, int):        # Single integer, just convert to str        return str(ids)    if isinstance(ids, str):        # String which represents one or more IDs joined by commas        # Remove any whitespace around commas if they are present        return \",\".join(id.strip() for id in ids.split(\",\"))    # Not a string or integer, assume iterable    return \",\".join(map(str, ids))",
        "labels_text": "Convert one or more UIDs to a single commadelimited string Input may be a single ID a an integer or string an iterable of stringsints or a string of IDs already separated by comma"
    },
    {
        "input_text": "summarize: def _has_api_key(request):        if request.method == \"POST\":        return b\"api_key=\" in request.data    return \"api_key=\" in request.full_url",
        "labels_text": "Check if a Request ha the apikey parameter set to set the rate limit Works with GET or POST request"
    },
    {
        "input_text": "summarize: def parse(handle):        while True:        record = __read(handle)        if not record:            break        yield record",
        "labels_text": "Parse cell line record This function is for parsing cell line file containing multiple record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def read(handle):        record = __read(handle)    # We should have reached the end of the record by now    remainder = handle.read()    if remainder:        raise ValueError(\"More than one cell line record found\")    return record",
        "labels_text": "Read one cell line record This function is for parsing cell line file containing exactly one record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def __repr__(self):                if self[\"ID\"]:            if self[\"AC\"]:                return f\"{self.__class__.__name__} ({self['ID']}, {self['AC']})\"            else:                return f\"{self.__class__.__name__} ({self['ID']})\"        else:            return f\"{self.__class__.__name__} ( )\"",
        "labels_text": "Return the canonical string representation of the Record object"
    },
    {
        "input_text": "summarize: def parse(handle):        while True:        record = __read(handle)        if not record:            break        yield record",
        "labels_text": "Parse ENZYME record This function is for parsing ENZYME file containing multiple record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def read(handle):        record = __read(handle)    # We should have reached the end of the record by now    remainder = handle.read()    if remainder:        raise ValueError(\"More than one ENZYME record found\")    return record",
        "labels_text": "Read one ENZYME record This function is for parsing ENZYME file containing exactly one record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def __init__(self):                dict.__init__(self)        self[\"ID\"] = \"\"        self[\"DE\"] = \"\"        self[\"AN\"] = []        self[\"CA\"] = \"\"        self[\"CF\"] = \"\"        self[\"CC\"] = []  # one comment per line        self[\"PR\"] = []        self[\"DR\"] = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                if self[\"ID\"]:            if self[\"DE\"]:                return f\"{self.__class__.__name__} ({self['ID']}, {self['DE']})\"            else:                return f\"{self.__class__.__name__} ({self['ID']})\"        else:            return f\"{self.__class__.__name__} ( )\"",
        "labels_text": "Return the canonical string representation of the Record object"
    },
    {
        "input_text": "summarize: def __str__(self):                output = [            \"ID: \" + self[\"ID\"],            \"DE: \" + self[\"DE\"],            \"AN: \" + repr(self[\"AN\"]),            \"CA: '\" + self[\"CA\"] + \"'\",            \"CF: \" + self[\"CF\"],            \"CC: \" + repr(self[\"CC\"]),            \"PR: \" + repr(self[\"PR\"]),            \"DR: %d Records\" % len(self[\"DR\"]),        ]        return \"\\n\".join(output)",
        "labels_text": "Return a readable string representation of the Record object"
    },
    {
        "input_text": "summarize: def read(handle):        record = __read(handle)    # We should have reached the end of the record by now    line = handle.readline()    if line:        raise ValueError(\"More than one Prodoc record found\")    return record",
        "labels_text": "Read in a record from a file with exactly one Prodoc record"
    },
    {
        "input_text": "summarize: def parse(handle):        while True:        record = __read(handle)        if not record:            return        yield record",
        "labels_text": "Iterate over the record in a Prodoc file"
    },
    {
        "input_text": "summarize: def __init__(self):                self.accession = \"\"        self.prosite_refs = []        self.text = \"\"        self.references = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.number = \"\"        self.authors = \"\"        self.citation = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def parse(handle):        while True:        record = __read(handle)        if not record:            break        yield record",
        "labels_text": "Parse Prosite record This function is for parsing Prosite file containing multiple record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def read(handle):        record = __read(handle)    # We should have reached the end of the record by now    remainder = handle.read()    if remainder:        raise ValueError(\"More than one Prosite record found\")    return record",
        "labels_text": "Read one Prosite record This function is for parsing Prosite file containing exactly one record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def __init__(self):                self.n_match = None        self.n_seq = None        self.capped = None        self.warning = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def scan(seq=\"\", mirror=\"https://prosite.expasy.org\", output=\"xml\", **keywords):        parameters = {\"seq\": seq, \"output\": output}    for key, value in keywords.items():        if value is not None:            parameters[key] = value    command = urlencode(parameters)    url = f\"{mirror}/cgi-bin/prosite/scanprosite/PSScan.cgi?{command}\"    handle = urlopen(url)    return handle",
        "labels_text": "Execute a ScanProsite search Arguments mirror The ScanProsite mirror to be used default httpsprositeexpasyorg seq The query sequence or UniProtKB SwissProt TrEMBL accession output Format of the search result default xml Further search parameter can be passed a keywords see the documentation for programmatic access to ScanProsite at httpsprositeexpasyorgscanprositescanprositedochtml for a description of such parameter This function return a handle to the search result returned by ScanProsite Search result in the XML format can be parsed into a Python object by using the BioExPASyScanPrositeread function"
    },
    {
        "input_text": "summarize: def read(handle):        content_handler = ContentHandler()    saxparser = Parser()    saxparser.setContentHandler(content_handler)    saxparser.parse(handle)    record = content_handler.record    return record",
        "labels_text": "Parse search result returned by ScanProsite into a Python object"
    },
    {
        "input_text": "summarize: def __init__(self):                ExpatParser.__init__(self)        self.firsttime = True",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.element = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def startElement(self, name, attrs):                self.element.append(name)        self.content = \"\"        if self.element == [\"matchset\"]:            self.record = Record()            self.record.n_match = int(attrs[\"n_match\"])            self.record.n_seq = int(attrs[\"n_seq\"])        elif self.element == [\"matchset\", \"match\"]:            match = {}            self.record.append(match)",
        "labels_text": "Define the beginning of a record and store the search record"
    },
    {
        "input_text": "summarize: def endElement(self, name):                assert name == self.element.pop()        if self.element == [\"matchset\", \"match\"]:            match = self.record[-1]            if name in ContentHandler.integers:                match[name] = int(self.content)            elif name in ContentHandler.strings:                match[name] = self.content            else:                # Unknown type, treat it as a string                match[name] = self.content",
        "labels_text": "Define the end of the search record"
    },
    {
        "input_text": "summarize: def characters(self, content):                self.content += content",
        "labels_text": "Store the record content"
    },
    {
        "input_text": "summarize: def get_prodoc_entry(    id, cgi=\"https://prosite.expasy.org/cgi-bin/prosite/get-prodoc-entry\"):        return _open(f\"{cgi}?{id}\")",
        "labels_text": "Get a text handle to a PRODOC entry at ExPASy in HTML format from Bio import ExPASy import o with ExPASygetprodocentryPDOC a inhandle html inhandleread with openmyprodocrecordhtml w a outhandle length outhandlewritehtml osremovemyprodocrecordhtml tidy up For a nonexisting key XXX ExPASy return an HTMLformatted page containing this text There is currently no PROSITE entry for"
    },
    {
        "input_text": "summarize: def get_prosite_entry(    id, cgi=\"https://prosite.expasy.org/cgi-bin/prosite/get-prosite-entry\"):        return _open(f\"{cgi}?{id}\")",
        "labels_text": "Get a text handle to a PROSITE entry at ExPASy in HTML format from Bio import ExPASy import o with ExPASygetprositeentryPS a inhandle html inhandleread with openmyprositerecordhtml w a outhandle length outhandlewritehtml osremovemyprositerecordhtml tidy up For a nonexisting key XXX ExPASy return an HTMLformatted page containing this text There is currently no PROSITE entry for"
    },
    {
        "input_text": "summarize: def get_sprot_raw(id):        try:        handle = _open(f\"http://www.uniprot.org/uniprot/{id}.txt\")    except HTTPError as exception:        if exception.code in (400, 404):            raise ValueError(f\"Failed to find SwissProt entry '{id}'\") from None        else:            raise    return handle",
        "labels_text": "Get a text handle to a raw SwissProt entry at ExPASy For an ID of XXX fetch httpwwwuniprotorguniprotXXXtxt a per the httpswwwexpasyorgexpasyurlshtml documentation from Bio import ExPASy from Bio import SwissProt with ExPASygetsprotrawO a handle record SwissProtreadhandle printrecordentryname CHSBROFI This function raise a ValueError if the identifier doe not exist ExPASygetsprotrawDOESNOTEXIST Traceback most recent call last ValueError Failed to find SwissProt entry DOESNOTEXIST"
    },
    {
        "input_text": "summarize: def _open(url):        handle = urlopen(url)    text_handle = io.TextIOWrapper(handle, encoding=\"UTF-8\")    text_handle.url = handle.url    return text_handle",
        "labels_text": "Open URL and convert to text assuming UTF encoding PRIVATE"
    },
    {
        "input_text": "summarize: def _indent_genbank(information, indent):        # split the info into lines based on line breaks    info_parts = information.split(\"\\n\")    # the first line will have no indent    output_info = info_parts[0] + \"\\n\"    for info_part in info_parts[1:]:        output_info += \" \" * indent + info_part + \"\\n\"    return output_info",
        "labels_text": "Write out information with the specified indent PRIVATE Unlike wrappedgenbank this function make no attempt to wrap line it assumes that the information already ha newlines in the appropriate place and will add the specified indent to the start of each line"
    },
    {
        "input_text": "summarize: def _definition_line(self):                output = Record.BASE_FORMAT % \"DEFINITION\"        output += _wrapped_genbank(self.definition + \".\", Record.GB_BASE_INDENT)        return output",
        "labels_text": "Provide output for the DEFINITION line PRIVATE"
    },
    {
        "input_text": "summarize: def _accession_line(self):                if self.accession:            output = Record.BASE_FORMAT % \"ACCESSION\"            acc_info = \"\"            for accession in self.accession:                acc_info += f\"{accession} \"            # strip off an extra space at the end            acc_info = acc_info.rstrip()            output += _wrapped_genbank(acc_info, Record.GB_BASE_INDENT)        else:            output = \"\"        return output",
        "labels_text": "Output for the ACCESSION line PRIVATE"
    },
    {
        "input_text": "summarize: def _version_line(self):                if self.version:            output = Record.BASE_FORMAT % \"VERSION\"            output += self.version            output += \"  GI:\"            output += f\"{self.gi}\\n\"        else:            output = \"\"        return output",
        "labels_text": "Output for the VERSION line PRIVATE"
    },
    {
        "input_text": "summarize: def _nid_line(self):                if self.nid:            output = Record.BASE_FORMAT % \"NID\"            output += f\"{self.nid}\\n\"        else:            output = \"\"        return output",
        "labels_text": "Output for the NID line Use of NID is obsolete in GenBank file PRIVATE"
    },
    {
        "input_text": "summarize: def _pid_line(self):                if self.pid:            output = Record.BASE_FORMAT % \"PID\"            output += f\"{self.pid}\\n\"        else:            output = \"\"        return output",
        "labels_text": "Output for PID line Presumedly PID usage is also obsolete PRIVATE"
    },
    {
        "input_text": "summarize: def _keywords_line(self):                output = \"\"        if self.keywords:            output += Record.BASE_FORMAT % \"KEYWORDS\"            keyword_info = \"\"            for keyword in self.keywords:                keyword_info += f\"{keyword}; \"            # replace the ; at the end with a period            keyword_info = keyword_info[:-2]            keyword_info += \".\"            output += _wrapped_genbank(keyword_info, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for the KEYWORDS line PRIVATE"
    },
    {
        "input_text": "summarize: def _db_source_line(self):                if self.db_source:            output = Record.BASE_FORMAT % \"DBSOURCE\"            output += f\"{self.db_source}\\n\"        else:            output = \"\"        return output",
        "labels_text": "Output for DBSOURCE line PRIVATE"
    },
    {
        "input_text": "summarize: def _segment_line(self):                output = \"\"        if self.segment:            output += Record.BASE_FORMAT % \"SEGMENT\"            output += _wrapped_genbank(self.segment, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for the SEGMENT line PRIVATE"
    },
    {
        "input_text": "summarize: def _source_line(self):                output = Record.BASE_FORMAT % \"SOURCE\"        output += _wrapped_genbank(self.source, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for SOURCE line on where the sample came from PRIVATE"
    },
    {
        "input_text": "summarize: def _comment_line(self):                output = \"\"        if self.comment:            output += Record.BASE_FORMAT % \"COMMENT\"            output += _indent_genbank(self.comment, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for the COMMENT line PRIVATE"
    },
    {
        "input_text": "summarize: def _features_line(self):                output = \"\"        if len(self.features) > 0:            output += Record.BASE_FEATURE_FORMAT % \"FEATURES\"            output += \"Location/Qualifiers\\n\"        return output",
        "labels_text": "Output for the FEATURES line PRIVATE"
    },
    {
        "input_text": "summarize: def _origin_line(self):                output = \"\"        # only output the ORIGIN line if we have a sequence        if self.sequence:            output += Record.BASE_FORMAT % \"ORIGIN\"            if self.origin:                output += _wrapped_genbank(self.origin, Record.GB_BASE_INDENT)            else:                output += \"\\n\"        return output",
        "labels_text": "Output for the ORIGIN line PRIVATE"
    },
    {
        "input_text": "summarize: def _contig_line(self):                output = \"\"        if self.contig:            output += Record.BASE_FORMAT % \"CONTIG\"            output += _wrapped_genbank(                self.contig, Record.GB_BASE_INDENT, split_char=\",\"            )        return output",
        "labels_text": "Output for CONTIG location information from RefSeq PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self):                self.number = \"\"        self.bases = \"\"        self.authors = \"\"        self.consrtm = \"\"        self.title = \"\"        self.journal = \"\"        self.medline_id = \"\"        self.pubmed_id = \"\"        self.remark = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                output = self._reference_line()        output += self._authors_line()        output += self._consrtm_line()        output += self._title_line()        output += self._journal_line()        output += self._medline_line()        output += self._pubmed_line()        output += self._remark_line()        return output",
        "labels_text": "Convert the reference to a GenBank format string"
    },
    {
        "input_text": "summarize: def _reference_line(self):                output = Record.BASE_FORMAT % \"REFERENCE\"        if self.number:            if self.bases:                output += \"%-3s\" % self.number                output += f\"{self.bases}\"            else:                output += f\"{self.number}\"        output += \"\\n\"        return output",
        "labels_text": "Output for REFERENCE line PRIVATE"
    },
    {
        "input_text": "summarize: def _authors_line(self):                output = \"\"        if self.authors:            output += Record.INTERNAL_FORMAT % \"AUTHORS\"            output += _wrapped_genbank(self.authors, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for AUTHORS information PRIVATE"
    },
    {
        "input_text": "summarize: def _consrtm_line(self):                output = \"\"        if self.consrtm:            output += Record.INTERNAL_FORMAT % \"CONSRTM\"            output += _wrapped_genbank(self.consrtm, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for CONSRTM information PRIVATE"
    },
    {
        "input_text": "summarize: def _title_line(self):                output = \"\"        if self.title:            output += Record.INTERNAL_FORMAT % \"TITLE\"            output += _wrapped_genbank(self.title, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for TITLE information PRIVATE"
    },
    {
        "input_text": "summarize: def _journal_line(self):                output = \"\"        if self.journal:            output += Record.INTERNAL_FORMAT % \"JOURNAL\"            output += _wrapped_genbank(self.journal, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for JOURNAL information PRIVATE"
    },
    {
        "input_text": "summarize: def _medline_line(self):                output = \"\"        if self.medline_id:            output += Record.INTERNAL_FORMAT % \"MEDLINE\"            output += self.medline_id + \"\\n\"        return output",
        "labels_text": "Output for MEDLINE information PRIVATE"
    },
    {
        "input_text": "summarize: def _pubmed_line(self):                output = \"\"        if self.pubmed_id:            output += Record.OTHER_INTERNAL_FORMAT % \"PUBMED\"            output += self.pubmed_id + \"\\n\"        return output",
        "labels_text": "Output for PUBMED information PRIVATE"
    },
    {
        "input_text": "summarize: def _remark_line(self):                output = \"\"        if self.remark:            output += Record.INTERNAL_FORMAT % \"REMARK\"            output += _wrapped_genbank(self.remark, Record.GB_BASE_INDENT)        return output",
        "labels_text": "Output for REMARK information PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, key=\"\", location=\"\"):                self.key = key        self.location = location        self.qualifiers = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"Feature(key={self.key!r}, location={self.location!r})\"",
        "labels_text": "Representation of the object for debugging or logging"
    },
    {
        "input_text": "summarize: def __str__(self):                output = Record.INTERNAL_FEATURE_FORMAT % self.key        output += _wrapped_genbank(            self.location, Record.GB_FEATURE_INDENT, split_char=\",\"        )        for qualifier in self.qualifiers:            output += str(qualifier)        return output",
        "labels_text": "Return feature a a GenBank format string"
    },
    {
        "input_text": "summarize: def __init__(self, key=\"\", value=\"\"):                self.key = key        self.value = value",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"Qualifier(key={self.key!r}, value={self.value!r})\"",
        "labels_text": "Representation of the object for debugging or logging"
    },
    {
        "input_text": "summarize: def __init__(self, debug=0):                assert len(self.RECORD_START) == self.HEADER_WIDTH        for marker in self.SEQUENCE_HEADERS:            assert marker == marker.rstrip()        assert len(self.FEATURE_QUALIFIER_SPACER) == self.FEATURE_QUALIFIER_INDENT        self.debug = debug        self.handle = None        self.line = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def set_handle(self, handle):                self.handle = handle        self.line = \"\"",
        "labels_text": "Set the handle attribute"
    },
    {
        "input_text": "summarize: def _feed_first_line(self, consumer, line):",
        "labels_text": "Handle the LOCUSID line passing data to the consumer PRIVATE This should be implemented by the EMBL GenBank specific subclass Used by the parserecords and parse method"
    },
    {
        "input_text": "summarize: def _feed_header_lines(self, consumer, lines):",
        "labels_text": "Handle the header line list of string passing data to the consumer PRIVATE This should be implemented by the EMBL GenBank specific subclass Used by the parserecords and parse method"
    },
    {
        "input_text": "summarize: def _feed_feature_table(consumer, feature_tuples):                consumer.start_feature_table()        for feature_key, location_string, qualifiers in feature_tuples:            consumer.feature_key(feature_key)            consumer.location(location_string)            for q_key, q_value in qualifiers:                if q_value is None:                    consumer.feature_qualifier(q_key, q_value)                else:                    consumer.feature_qualifier(q_key, q_value.replace(\"\\n\", \" \"))",
        "labels_text": "Handle the feature table list of tuples passing data to the consumer PRIVATE Used by the parserecords and parse method"
    },
    {
        "input_text": "summarize: def _feed_misc_lines(self, consumer, lines):",
        "labels_text": "Handle any line between feature and sequence list of string passing data to the consumer PRIVATE This should be implemented by the EMBL GenBank specific subclass Used by the parserecords and parse method"
    },
    {
        "input_text": "summarize: def parse(self, handle, do_features=True):                from Bio.GenBank import _FeatureConsumer        from Bio.GenBank.utils import FeatureValueCleaner        consumer = _FeatureConsumer(            use_fuzziness=1, feature_cleaner=FeatureValueCleaner()        )        if self.feed(handle, consumer, do_features):            return consumer.data        else:            return None",
        "labels_text": "Return a SeqRecord with SeqFeatures if dofeaturesTrue See also the method parserecords for use on multirecord file"
    },
    {
        "input_text": "summarize: def __init__(self, to_process=keys_to_process):                self._to_process = to_process",
        "labels_text": "Initialize with the key we should deal with"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}({self._to_process!r})\"",
        "labels_text": "Return a string representation of the class"
    },
    {
        "input_text": "summarize: def clean_value(self, key_name, value):                if key_name in self._to_process:            try:                cleaner = getattr(self, f\"_clean_{key_name}\")            except AttributeError:                raise AssertionError(f\"No function to clean key: {key_name}\") from None            value = cleaner(value)        return value",
        "labels_text": "Clean the specified value and return it If the value is not specified to be dealt with the original value will be returned"
    },
    {
        "input_text": "summarize: def _clean_translation(self, value):                translation_parts = value.split()        return \"\".join(translation_parts)",
        "labels_text": "Concatenate a translation value to one long protein string PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, handle, parser=None):                self.handle = handle        self._parser = parser",
        "labels_text": "Initialize the iterator Arguments handle A handle with GenBank entry to iterate through parser An optional parser to pas the entry through before returning them If None then the raw entry will be returned"
    },
    {
        "input_text": "summarize: def __next__(self):                if self._parser is None:            lines = []            while True:                line = self.handle.readline()                if not line:                    return None  # Premature end of file?                lines.append(line)                if line.rstrip() == \"//\":                    break            return \"\".join(lines)        try:            return self._parser.parse(self.handle)        except StopIteration:            return None",
        "labels_text": "Return the next GenBank record from the handle Will return None if we ran out of record"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.__next__, None)",
        "labels_text": "Iterate over the record"
    },
    {
        "input_text": "summarize: def __init__(self, debug_level=0, use_fuzziness=1, feature_cleaner=None):                self._scanner = GenBankScanner(debug_level)        self.use_fuzziness = use_fuzziness        if feature_cleaner:            self._cleaner = feature_cleaner        else:            self._cleaner = _cleaner",
        "labels_text": "Initialize a GenBank parser and Feature consumer Arguments debuglevel An optional argument that specie the amount of debugging information the parser should spit out By default we have no debugging info the fastest way to do thing but if you want you can set this a high a two and see exactly where a parse fails usefuzziness Specify whether or not to use fuzzy representation The default is use fuzziness featurecleaner A class which will be used to clean out the value of feature This class must implement the function cleanvalue GenBankutils ha a standard cleaner class which is used by default"
    },
    {
        "input_text": "summarize: def parse(self, handle):                _consumer = _FeatureConsumer(self.use_fuzziness, self._cleaner)        self._scanner.feed(handle, _consumer)        return _consumer.data",
        "labels_text": "Parse the specified handle"
    },
    {
        "input_text": "summarize: def __init__(self, debug_level=0):                self._scanner = GenBankScanner(debug_level)",
        "labels_text": "Initialize the parser Arguments debuglevel An optional argument that specie the amount of debugging information the parser should spit out By default we have no debugging info the fastest way to do thing but if you want you can set this a high a two and see exactly where a parse fails"
    },
    {
        "input_text": "summarize: def parse(self, handle):                _consumer = _RecordConsumer()        self._scanner.feed(handle, _consumer)        return _consumer.data",
        "labels_text": "Parse the specified handle into a GenBank record"
    },
    {
        "input_text": "summarize: def _split_keywords(keyword_string):                # process the keywords into a python list        if keyword_string == \"\" or keyword_string == \".\":            keywords = \"\"        elif keyword_string[-1] == \".\":            keywords = keyword_string[:-1]        else:            keywords = keyword_string        keyword_list = keywords.split(\";\")        return [x.strip() for x in keyword_list]",
        "labels_text": "Split a string of keywords into a nice clean list PRIVATE"
    },
    {
        "input_text": "summarize: def _split_accessions(accession_string):                # first replace all line feeds with spaces        # Also, EMBL style accessions are split with ';'        accession = accession_string.replace(\"\\n\", \" \").replace(\";\", \" \")        return [x.strip() for x in accession.split() if x.strip()]",
        "labels_text": "Split a string of accession number into a list PRIVATE"
    },
    {
        "input_text": "summarize: def _clean_location(location_string):                # Originally this imported string.whitespace and did a replace        # via a loop.  It's simpler to just split on whitespace and rejoin        # the string - and this avoids importing string too.  See Bug 2684.        return \"\".join(location_string.split())",
        "labels_text": "Clean whitespace out of a location string PRIVATE The location parser isnt a fan of whitespace so we clean it out before feeding it into the parser"
    },
    {
        "input_text": "summarize: def _remove_newlines(text):                # get rid of newlines in the qualifier value        newlines = [\"\\n\", \"\\r\"]        for ws in newlines:            text = text.replace(ws, \"\")        return text",
        "labels_text": "Remove any newlines in the passed text returning the new string PRIVATE"
    },
    {
        "input_text": "summarize: def _normalize_spaces(text):                # get rid of excessive spaces        return \" \".join(x for x in text.split(\" \") if x)",
        "labels_text": "Replace multiple space in the passed text with single space PRIVATE"
    },
    {
        "input_text": "summarize: def _remove_spaces(text):                return text.replace(\" \", \"\")",
        "labels_text": "Remove all space from the passed text PRIVATE"
    },
    {
        "input_text": "summarize: def _convert_to_python_numbers(start, end):                new_start = start - 1        new_end = end        return new_start, new_end",
        "labels_text": "Convert a start and end range to python notation PRIVATE In GenBank start and end are defined in biological coordinate where is the first base and i j mean to include both i and j In python is the first base and i j mean to include i but not j So to convert biological to python coordinate we need to subtract from the start and leave the end and thing should be converted happily"
    },
    {
        "input_text": "summarize: def locus(self, locus_name):                self.data.name = locus_name",
        "labels_text": "Set the locus name is set a the name of the Sequence"
    },
    {
        "input_text": "summarize: def size(self, content):                self._expected_size = int(content)",
        "labels_text": "Record the sequence length"
    },
    {
        "input_text": "summarize: def residue_type(self, type):                self._seq_type = type.strip()",
        "labels_text": "Record the sequence type SEMIOBSOLETE This reflects the fact that the topology linearcircular and molecule type eg DNA v RNA were a single field in early file Current GenBankEMBL file have two field"
    },
    {
        "input_text": "summarize: def topology(self, topology):                if topology:            if topology not in [\"linear\", \"circular\"]:                raise ParserFailureError(                    f\"Unexpected topology {topology!r} should be linear or circular\"                )            self.data.annotations[\"topology\"] = topology",
        "labels_text": "Validate and record sequence topology The topology argument should be linear or circular string"
    },
    {
        "input_text": "summarize: def definition(self, definition):                if self.data.description:            # Append to any existing description            # e.g. EMBL files with two DE lines.            self.data.description += \" \" + definition        else:            self.data.description = definition",
        "labels_text": "Set the definition a the description of the sequence"
    },
    {
        "input_text": "summarize: def project(self, content):                content = content.replace(\"GenomeProject:\", \"Project:\")        self.data.dbxrefs.extend(p for p in content.split() if p)",
        "labels_text": "Handle the information from the PROJECT line a a list of project eg PROJECT GenomeProject or PROJECT GenomeProject GenomeProject This is stored a dbxrefs in the SeqRecord to be consistent with the projected switch of this line to DBLINK in future GenBank version Note the NCBI plan to replace GenomeProject with the shorter Project a part of this transition"
    },
    {
        "input_text": "summarize: def dblink(self, content):                # During the transition period with both PROJECT and DBLINK lines,        # we don't want to add the same cross reference twice.        while \": \" in content:            content = content.replace(\": \", \":\")        if content.strip() not in self.data.dbxrefs:            self.data.dbxrefs.append(content.strip())",
        "labels_text": "Store DBLINK cross reference a dbxrefs in our record object This line type is expected to replace the PROJECT line in eg During transition PROJECT GenomeProject DBLINK Project Trace Assembly Archive Once the project line is dropped DBLINK Project Trace Assembly Archive Note GenomeProject Project Well have to see some real example to be sure but based on the above example we can expect one reference per line Note that at some point the NCBI have included an extra space eg DBLINK Project"
    },
    {
        "input_text": "summarize: def taxonomy(self, content):                lineage = self._split_taxonomy(content)        try:            self.data.annotations[\"taxonomy\"].extend(lineage)        except KeyError:            self.data.annotations[\"taxonomy\"] = lineage",
        "labels_text": "Record another line of the taxonomy lineage"
    },
    {
        "input_text": "summarize: def reference_num(self, content):                # if we have a current reference that hasn't been added to        # the list of references, add it.        if self._cur_reference is not None:            self.data.annotations[\"references\"].append(self._cur_reference)        else:            self.data.annotations[\"references\"] = []        self._cur_reference = Reference()",
        "labels_text": "Signal the beginning of a new reference object"
    },
    {
        "input_text": "summarize: def remark(self, content):                if self._cur_reference.comment:            self._cur_reference.comment += \" \" + content        else:            self._cur_reference.comment = content",
        "labels_text": "Deal with a reference comment"
    },
    {
        "input_text": "summarize: def features_line(self, content):                self.start_feature_table()",
        "labels_text": "Get ready for the feature table when we reach the FEATURE line"
    },
    {
        "input_text": "summarize: def start_feature_table(self):                # make sure we've added on our last reference object        if self._cur_reference is not None:            self.data.annotations[\"references\"].append(self._cur_reference)            self._cur_reference = None",
        "labels_text": "Indicate weve got to the start of the feature table"
    },
    {
        "input_text": "summarize: def feature_qualifier_name(self, content_list):                raise NotImplementedError(\"Use the feature_qualifier method instead.\")",
        "labels_text": "Use featurequalifier instead OBSOLETE"
    },
    {
        "input_text": "summarize: def feature_qualifier_description(self, content):                raise NotImplementedError(\"Use the feature_qualifier method instead.\")",
        "labels_text": "Use featurequalifier instead OBSOLETE"
    },
    {
        "input_text": "summarize: def sequence(self, content):                assert \" \" not in content        self._seq_data.append(content.upper())",
        "labels_text": "Add up sequence information a we get it To try and make thing speedier this put all of the string into a list of string and then us stringjoin later to put them together Supposedly this is a big time saving"
    },
    {
        "input_text": "summarize: def topology(self, topology):                if topology:            if topology not in [\"linear\", \"circular\"]:                raise ParserFailureError(                    f\"Unexpected topology {topology!r} should be linear or circular\"                )            self.data.topology = topology",
        "labels_text": "Validate and record sequence topology The topology argument should be linear or circular string"
    },
    {
        "input_text": "summarize: def reference_num(self, content):                # check if we have a reference to add        if self._cur_reference is not None:            self.data.references.append(self._cur_reference)        from . import Record        self._cur_reference = Record.Reference()        self._cur_reference.number = content",
        "labels_text": "Grab the reference number and signal the start of a new reference"
    },
    {
        "input_text": "summarize: def primary_ref_line(self, content):                self.data.primary.append(content)",
        "labels_text": "Save reference data for the PRIMARY line"
    },
    {
        "input_text": "summarize: def features_line(self, content):                self.start_feature_table()",
        "labels_text": "Get ready for the feature table when we reach the FEATURE line"
    },
    {
        "input_text": "summarize: def start_feature_table(self):                # we need to add on the last reference        if self._cur_reference is not None:            self.data.references.append(self._cur_reference)",
        "labels_text": "Signal the start of the feature table"
    },
    {
        "input_text": "summarize: def feature_key(self, content):                # first add on feature information if we've got any        self._add_feature()        from . import Record        self._cur_feature = Record.Feature()        self._cur_feature.key = content",
        "labels_text": "Grab the key of the feature and signal the start of a new feature"
    },
    {
        "input_text": "summarize: def _add_feature(self):                if self._cur_feature is not None:            # if we have a left over qualifier, add it to the qualifiers            # on the current feature            if self._cur_qualifier is not None:                self._cur_feature.qualifiers.append(self._cur_qualifier)            self._cur_qualifier = None            self.data.features.append(self._cur_feature)",
        "labels_text": "Add a feature to the record with relevant check PRIVATE This doe all of the appropriate checking to make sure we havent left any info behind and that we are only adding info if it exists"
    },
    {
        "input_text": "summarize: def contig_location(self, content):                self.data.contig = self._clean_location(content)",
        "labels_text": "Signal that we have contig information to add to the record"
    },
    {
        "input_text": "summarize: def sequence(self, content):                assert \" \" not in content        self._seq_data.append(content.upper())",
        "labels_text": "Add sequence information to a list of sequence string This remove space in the data and uppercase the sequence and then add it to a list of sequence Later on well join this list together to make the final sequence This is faster than adding on the new string every time"
    },
    {
        "input_text": "summarize: def record_end(self, content):                # add together all of the sequence parts to create the        # final sequence string        self.data.sequence = \"\".join(self._seq_data)        # add on the last feature        self._add_feature()",
        "labels_text": "Signal the end of the record and do any necessary cleanup"
    },
    {
        "input_text": "summarize: def parse(handle):        return iter(Iterator(handle, RecordParser()))",
        "labels_text": "Iterate over GenBank formatted entry a Record object from Bio import GenBank with openGenBankNCgb a handle for record in GenBankparsehandle printrecordaccession NC To get SeqRecord object use BioSeqIOparse formatgb instead"
    },
    {
        "input_text": "summarize: def read(handle):        iterator = parse(handle)    try:        record = next(iterator)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(iterator)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    return record",
        "labels_text": "Read a handle containing a single GenBank entry a a Record object from Bio import GenBank with openGenBankNCgb a handle record GenBankreadhandle printrecordaccession NC To get a SeqRecord object use BioSeqIOread formatgb instead"
    },
    {
        "input_text": "summarize: def __init__(self):                self.entity_type = \"\"        self.entity_id = \"\"        self.entity_attributes = {}        self.col_defs = {}        self.table_rows = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def out_block(text, prefix=\"\"):        output = \"\"    for j in range(0, len(text), 80):        output += f\"{prefix}{text[j : j + 80]}\\n\"    output += \"\\n\"    return output",
        "labels_text": "Format text in block of char with an additional optional prefix"
    },
    {
        "input_text": "summarize: def __init__(self):                self._sub_components = []",
        "labels_text": "Initialize a chromosome component Attributes subcomponents Any component which are contained under this parent component This attribute should be accessed through the add and remove function"
    },
    {
        "input_text": "summarize: def add(self, component):                if not isinstance(component, _ChromosomeComponent):            raise TypeError(f\"Expected a _ChromosomeComponent object, got {component}\")        self._sub_components.append(component)",
        "labels_text": "Add a subcomponent to the list of component under this item"
    },
    {
        "input_text": "summarize: def remove(self, component):                try:            self._sub_components.remove(component)        except ValueError:            raise ValueError(                f\"Component {component} not found in sub_components.\"            ) from None",
        "labels_text": "Remove the specified component from the subcomponents Raises a ValueError if the component is not registered a a subcomponent"
    },
    {
        "input_text": "summarize: def draw(self):                raise AssertionError(\"Subclasses must implement.\")",
        "labels_text": "Draw the specified component"
    },
    {
        "input_text": "summarize: def __init__(self, output_format=\"pdf\"):                _ChromosomeComponent.__init__(self)        # customizable attributes        self.page_size = letter        self.title_size = 20        # Do we need this given we don't draw a legend?        # If so, should be a public API...        self._legend_height = 0  # 2 * inch        self.output_format = output_format",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _draw_title(self, cur_drawing, title, width, height):                title_string = String(width / 2, height - inch, title)        title_string.fontName = \"Helvetica-Bold\"        title_string.fontSize = self.title_size        title_string.textAnchor = \"middle\"        cur_drawing.add(title_string)",
        "labels_text": "Write out the title of the organism figure PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_legend(self, cur_drawing, start_y, width):",
        "labels_text": "Draw a legend for the figure PRIVATE Subclasses should implement this see also selflegendheight to provide specialized legend"
    },
    {
        "input_text": "summarize: def subcomponent_size(self):                total_sub = 0        for sub_component in self._sub_components:            total_sub += sub_component.scale        return total_sub",
        "labels_text": "Return the scaled size of all subcomponents of this component"
    },
    {
        "input_text": "summarize: def __init__(self):                _ChromosomeComponent.__init__(self)        self.start_x_position = -1        self.end_x_position = -1        self.start_y_position = -1        self.end_y_position = -1        # --- attributes for configuration        self.scale = 1        self.fill_color = None        self.label = None        self.label_size = 6        self.chr_percent = 0.25",
        "labels_text": "Initialize a ChromosomeSegment Attributes startxposition endxposition Defines the x range we have to draw thing in startyposition endyposition Defines the y range we have to draw thing in Configuration Attributes scale A scaling value for the component By default this is set at ie ha the same scale a everything else Higher value give more size to the component smaller value give less fillcolor A color to fill in the segment with Colors are available in reportlablibcolors label A label to place on the chromosome segment This should be a text string specifying what is to be included in the label labelsize The size of the label chrpercent The percentage of area that the chromosome segment take up"
    },
    {
        "input_text": "summarize: def _draw_subcomponents(self, cur_drawing):",
        "labels_text": "Draw any subcomponents of the chromosome segment PRIVATE This should be overridden in derived class if there are subcomponents to be drawn"
    },
    {
        "input_text": "summarize: def _overdraw_subcomponents(self, cur_drawing):",
        "labels_text": "Draw any subcomponents of the chromosome segment over the main part PRIVATE This should be overridden in derived class if there are subcomponents to be drawn"
    },
    {
        "input_text": "summarize: def __init__(self, inverted=0):                ChromosomeSegment.__init__(self)        self._inverted = inverted",
        "labels_text": "Initialize a segment at the end of a chromosome See ChromosomeSegment for all of the attribute that can be customized in a TelomereSegments Arguments inverted Whether or not the telomere should be inverted ie drawn on the bottom of a chromosome"
    },
    {
        "input_text": "summarize: def draw(self, cur_diagram):",
        "labels_text": "Draw nothing to the current diagram dummy method The segment spacer ha no actual image in the diagram so this method therefore doe nothing but is defined to match the expected API of the other segment object"
    },
    {
        "input_text": "summarize: def __init__(self, a=1, b=0.33, v_init=0.85, v_final=0.5, jitter=0.05):                # Initialize attributes        self.a = a        self.b = b        self.v_init = v_init        self.v_final = v_final        self.jitter = jitter",
        "labels_text": "Initialize a logarithmic spiral path through HSV colour space Arguments a Parameter a for the spiral control the initial spiral direction a b parameter b for the spiral control the rate at which the spiral revolves around the axis b vinit initial value of V brightness for the spiral vinit in vfinal final value of V brightness for the spiral vfinal in jitter the degree of V brightness jitter to add to each selected colour The amount of jitter will be selected from a uniform random distribution jitter jitter and V will be maintained in"
    },
    {
        "input_text": "summarize: def get_colors(k, **kwargs):        cs = ColorSpiral(**kwargs)    return cs.get_colors(k)",
        "labels_text": "Return k colour selected by the ColorSpiral object a a generator Arguments k the number of colour to return kwargs passthrough argument to the ColorSpiral object"
    },
    {
        "input_text": "summarize: def get_color_dict(l, **kwargs):  # noqa: E741        cs = ColorSpiral(**kwargs)    colors = cs.get_colors(len(l))    dict = {}    for item in l:        dict[item] = next(colors)    return dict",
        "labels_text": "Return a dictionary of colour using the provided value a key Returns a dictionary keyed by the member of iterable l with a colour assigned to each member Arguments l an iterable representing class to be coloured kwargs passthrough argument to the ColorSpiral object"
    },
    {
        "input_text": "summarize: def _draw_title(self, cur_drawing, title, width, height):                title_string = String(width / 2, height - inch, title)        title_string.fontName = \"Helvetica-Bold\"        title_string.fontSize = self.title_size        title_string.textAnchor = \"middle\"        cur_drawing.add(title_string)",
        "labels_text": "Add a title to the page we are outputting PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, segment_names, color_scheme=RAINBOW_COLORS):                self._names = segment_names        self._count_info = {}        self._label_info = {}        self._scale_info = {}        for name in self._names:            self._count_info[name] = 0            self._label_info[name] = None            self._scale_info[name] = 1        self._color_scheme = color_scheme",
        "labels_text": "Initialize a representation of chromosome count Arguments segmentnames An ordered list of all segment name along the chromosome The count and other information will be added to these colorscheme A coloring scheme to use in the count This should be a dictionary mapping count range to color specified in reportlablibcolors"
    },
    {
        "input_text": "summarize: def add_count(self, segment_name, count=1):                try:            self._count_info[segment_name] += count        except KeyError:            raise KeyError(f\"Segment name {segment_name} not found.\") from None",
        "labels_text": "Add count to the given segment name Arguments segmentname The name of the segment we should add count to If the name is not present a KeyError will be raised count The count to add the current segment This default to a single count"
    },
    {
        "input_text": "summarize: def scale_segment_value(self, segment_name, scale_value=None):                try:            self._count_info[segment_name] /= scale_value        except KeyError:            raise KeyError(f\"Segment name {segment_name} not found.\") from None",
        "labels_text": "Divide the count for a segment by some kind of scale value This is useful if segment arent represented by raw count but are instead count divided by some number"
    },
    {
        "input_text": "summarize: def add_label(self, segment_name, label):                if segment_name in self._label_info:            self._label_info[segment_name] = label        else:            raise KeyError(f\"Segment name {segment_name} not found.\")",
        "labels_text": "Add a label to a specific segment Raises a KeyError is the specified segment name is not found"
    },
    {
        "input_text": "summarize: def set_scale(self, segment_name, scale):                if segment_name in self._label_info:            self._scale_info[segment_name] = scale        else:            raise KeyError(f\"Segment name {segment_name} not found.\")",
        "labels_text": "Set the scale for a specific chromosome segment By default all segment have the same scale this allows scaling by the size of the segment Raises a KeyError is the specified segment name is not found"
    },
    {
        "input_text": "summarize: def get_segment_info(self):                order_info = []        for seg_name in self._names:            order_info.append((self._count_info[seg_name], self._label_info[seg_name]))        return order_info",
        "labels_text": "Retrieve the color and label info about the segment Returns a list consisting of two tuples specifying the count and label name for each segment The list is ordered according to the original listing of name Labels are set a None if no label wa specified"
    },
    {
        "input_text": "summarize: def _color_from_count(self, count):                for count_start, count_end in self._color_scheme:            if count >= count_start and count <= count_end:                return self._color_scheme[(count_start, count_end)]        # if we got here we didn't find a color for the count        raise ValueError(f\"Count value {count} was not found in the color scheme.\")",
        "labels_text": "Translate the given count into a color using the color scheme PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, output_format=\"pdf\"):                self.distributions = []        # customizable attributes        self.number_of_columns = 1        self.page_size = letter        self.title_size = 20        self.output_format = output_format",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _draw_title(self, cur_drawing, title, width, height):                title_string = String(width / 2, height - inch, title)        title_string.fontName = \"Helvetica-Bold\"        title_string.fontSize = self.title_size        title_string.textAnchor = \"middle\"        cur_drawing.add(title_string)",
        "labels_text": "Add the title of the figure to the drawing PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_legend(self, cur_drawing, start_y, width):",
        "labels_text": "Add a legend to the figure PRIVATE Subclasses can implement to provide a specialized legend"
    },
    {
        "input_text": "summarize: def __init__(self, display_info=None):                if display_info is None:            display_info = []        self.display_info = display_info        self.x_axis_title = \"\"        self.y_axis_title = \"\"        self.chart_title = \"\"        self.chart_title_size = 10        self.padding_percent = 0.15",
        "labels_text": "Initialize a Bar Chart display of distribution info Attributes displayinfo the information to be displayed in the distribution This should be ordered a a list of list where each internal list is a data set to display in the bar chart"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def draw(self, cur_drawing, start_x, start_y, end_x, end_y):",
        "labels_text": "Draw a line distribution into the current drawing"
    },
    {
        "input_text": "summarize: def darken(color, factor=0.7):        newcol = color_to_reportlab(color)    for a in [\"red\", \"green\", \"blue\"]:        setattr(newcol, a, factor * getattr(newcol, a))    return newcol",
        "labels_text": "Return darkened color a a ReportLab RGB color Take a passed color and return a Reportlab color that is darker by the factor indicated in the parameter"
    },
    {
        "input_text": "summarize: def get_temp_imagefilename(url):        img = urlopen(url).read()    im = Image.open(BytesIO(img))    # im.transpose(Image.FLIP_TOP_BOTTOM)    f = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")    fname = f.name    f.close()    im.save(fname, \"PNG\")    return fname",
        "labels_text": "Return filename of temporary file containing downloaded image Create a new temporary file to hold the image file at the passed URL and return the filename"
    },
    {
        "input_text": "summarize: def __add_maps(self):                for m in self.pathway.maps:            for g in m.graphics:                self.drawing.setStrokeColor(\"#888888\")                self.drawing.setFillColor(\"#DDDDDD\")                self.__add_graphics(g)                if self.label_maps:                    self.drawing.setFillColor(\"#888888\")                    self.__add_labels(g)",
        "labels_text": "Add map to the drawing of the map PRIVATE We do this first a theyre regional label to be overlaid by information Also we want to set the color to something subtle Were using Hex color because thats what KGML us and Reportlab doesnt mind"
    },
    {
        "input_text": "summarize: def __add_genes(self):                for gene in self.pathway.genes:            for g in gene.graphics:                self.drawing.setStrokeColor(color_to_reportlab(g.fgcolor))                self.drawing.setFillColor(color_to_reportlab(g.bgcolor))                self.__add_graphics(g)                if self.label_compounds:                    self.drawing.setFillColor(darken(g.fgcolor))                    self.__add_labels(g)",
        "labels_text": "Add gene element to the drawing of the map PRIVATE"
    },
    {
        "input_text": "summarize: def deduplicate(points):        assert len(points) % 2 == 0    if len(points) < 2:        return points    newpoints = points[0:2]    for x, y in zip(islice(points, 2, None, 2), islice(points, 3, None, 2)):        if x != newpoints[-2] or y != newpoints[-1]:            newpoints.append(x)            newpoints.append(y)    return newpoints",
        "labels_text": "Remove adjacent duplicate point This is important for use with the Polygon class since reportlab ha a bug with duplicate point Arguments point list of point x y x y Returns a list in the same format with consecutive duplicate removed"
    },
    {
        "input_text": "summarize: def angle2trig(theta):        c = cos(theta * pi / 180)    s = sin(theta * pi / 180)    return (c, s, -s, c)",
        "labels_text": "Convert angle to a reportlab ready tuple Arguments theta Angle in degree counter clockwise from horizontal Returns a representation of the passed angle in a format suitable for ReportLab rotation ie costheta sintheta sintheta costheta tuple"
    },
    {
        "input_text": "summarize: def is_in_bounds(self, value):                if value >= self.start and value <= self.end:            return 1        return 0",
        "labels_text": "Check if given value is within the region selected for drawing Arguments value A base position"
    },
    {
        "input_text": "summarize: def __len__(self):                return self.length",
        "labels_text": "Return the length of the region to be drawn"
    },
    {
        "input_text": "summarize: def draw_graph_set(self, set):                # print('draw graph set')        elements = []  # Holds graph elements        # Distribution dictionary for how to draw the graph        style_methods = {            \"line\": self.draw_line_graph,            \"heat\": self.draw_heat_graph,            \"bar\": self.draw_bar_graph,        }        for graph in set.get_graphs():            elements += style_methods[graph.style](graph)        return elements, []",
        "labels_text": "Return list of graph element and list of their label Arguments set GraphSet object"
    },
    {
        "input_text": "summarize: def canvas_angle(self, base):                angle = self.sweep * 2 * pi * (base - self.start) / self.length        return (angle, cos(angle), sin(angle))",
        "labels_text": "Given basepair position return angle cosine sin PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_sigil_box(        self, bottom, center, top, startangle, endangle, strand, **kwargs    ):                if strand == 1:            inner_radius = center            outer_radius = top        elif strand == -1:            inner_radius = bottom            outer_radius = center        else:            inner_radius = bottom            outer_radius = top        return self._draw_arc(            inner_radius, outer_radius, startangle, endangle, **kwargs        )",
        "labels_text": "Draw BOX sigil PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_sigil_big_arrow(        self, bottom, center, top, startangle, endangle, strand, **kwargs    ):                if strand == -1:            orientation = \"left\"        else:            orientation = \"right\"        return self._draw_arc_arrow(            bottom, top, startangle, endangle, orientation=orientation, **kwargs        )",
        "labels_text": "Draw BIGARROW sigil like ARROW but straddle the axis PRIVATE"
    },
    {
        "input_text": "summarize: def get_artemis_colorscheme(self):                return self._artemis_colorscheme",
        "labels_text": "Return the Artemis color scheme a a dictionary"
    },
    {
        "input_text": "summarize: def artemis_color(self, value):                try:            value = int(value)        except ValueError:            if value.count(\".\"):  # dot-delimited                value = int(value.split(\".\", 1)[0])  # Use only first integer            else:                raise        if value in self._artemis_colorscheme:            return self._artemis_colorscheme[value][0]        else:            raise ValueError(\"Artemis color out of range: %d\" % value)",
        "labels_text": "Artemis color integer to ReportLab Color object Arguments value An int representing a functional class in the Artemis color scheme see wwwsangeracuk for a description or a string from a GenBank feature annotation for the color which may be dot delimited in which case the first value is used Takes an int representing a functional class in the Artemis color scheme and return the appropriate colorsColor object"
    },
    {
        "input_text": "summarize: def get_colorscheme(self):                return self._colorscheme",
        "labels_text": "Return the userdefined color scheme a a dictionary"
    },
    {
        "input_text": "summarize: def scheme_color(self, value):                if value in self._colorscheme:            return self._colorscheme[value][0]        else:            raise ValueError(\"Scheme color out of range: %d\" % value)",
        "labels_text": "Map a userdefined color integer to a ReportLab Color object value An int representing a single color in the userdefined color scheme Takes an int representing a userdefined color and return the appropriate colorsColor object"
    },
    {
        "input_text": "summarize: def int255_color(self, values):                red, green, blue = values        factor = 1 / 255.0        red, green, blue = red * factor, green * factor, blue * factor        return colors.Color(red, green, blue)",
        "labels_text": "Map integer red green blue tuple to a ReportLab Color object value A tuple of red green blue intensity a integer in the range Takes a tuple of red green blue intensity value in the range and return an appropriate colorsColor object"
    },
    {
        "input_text": "summarize: def float1_color(self, values):                red, green, blue = values        return colors.Color(red, green, blue)",
        "labels_text": "Map float red green blue tuple to a ReportLab Color object value A tuple of red green blue intensity a float in the range Takes a tuple of red green blue intensity value in the range and return an appropriate colorsColor object"
    },
    {
        "input_text": "summarize: def __init__(        self, featureA, featureB, color=colors.lightgreen, border=None, flip=False    ):                # Initialize attributes        self.featureA = featureA        self.featureB = featureB        self.color = color  # default color to draw the feature        self.border = border        self.flip = flip",
        "labels_text": "Create a new cross link Arguments featureA and featureB should GenomeDiagram feature object or tuples track object start end and currently must be on different track The color and border argument should be ReportLab colour object or for border use a boolean False for no border otherwise it default to the same a the main colour The flip argument draw an inverted cross link useful for showing a mapping where one sequence ha been reversed It is conventional to also use a different colour eg red for simple link blue for any flipped link"
    },
    {
        "input_text": "summarize: def startA(self):                try:            return self.featureA.start        except AttributeError:            track, start, end = self.featureA            return start",
        "labels_text": "Start position of Feature A"
    },
    {
        "input_text": "summarize: def endA(self):                try:            return self.featureA.end        except AttributeError:            track, start, end = self.featureA            return end",
        "labels_text": "End position of Feature A"
    },
    {
        "input_text": "summarize: def startB(self):                try:            return self.featureB.start        except AttributeError:            track, start, end = self.featureB            return start",
        "labels_text": "Start position of Feature B"
    },
    {
        "input_text": "summarize: def endB(self):                try:            return self.featureB.end        except AttributeError:            track, start, end = self.featureB            return end",
        "labels_text": "End position of Feature B"
    },
    {
        "input_text": "summarize: def _first_defined(*args):        for arg in args:        if arg is not None:            return arg    return None",
        "labels_text": "Return the first nonnull argument PRIVATE"
    },
    {
        "input_text": "summarize: def set_all_tracks(self, attr, value):                for track in self.tracks.values():            if hasattr(track, attr):                # If the feature has the attribute set it to the passed value                setattr(track, attr, value)",
        "labels_text": "Set the passed attribute of all track in the set to the passed value Arguments attr An attribute of the Track class value The value to set that attribute setalltracksself attr value"
    },
    {
        "input_text": "summarize: def write(self, filename=\"test1.ps\", output=\"PS\", dpi=72):                return _write(self.drawing, filename, output, dpi=dpi)",
        "labels_text": "Write the drawn diagram to a specified file in a specified format Arguments filename a string indicating the name of the output file or a handle to write to output a string indicating output format one of PS PDF SVG or provided the ReportLab renderPM module is installed one of the bitmap format JPG BMP GIF PNG TIFF or TIFF The format can be given in upper or lower case dpi an integer Resolution dot per inch for bitmap format Returns No return value writeself filenametestps outputPS dpi"
    },
    {
        "input_text": "summarize: def write_to_string(self, output=\"PS\", dpi=72):                # The ReportLab drawToString method, which this function used to call,        # originally just used a StringIO handle with the drawToFile method.        #        # TODO - Rename this method to include keyword bytes?        from io import BytesIO        handle = BytesIO()        self.write(handle, output, dpi)        return handle.getvalue()",
        "labels_text": "Return a byte string containing the diagram in the requested format Arguments output a string indicating output format one of PS PDF SVG JPG BMP GIF PNG TIFF or TIFF a specified for the write method dpi Resolution dot per inch for bitmap format Returns Return the completed drawing a a byte string in a prescribed format"
    },
    {
        "input_text": "summarize: def del_track(self, track_level):                del self.tracks[track_level]",
        "labels_text": "Remove the track to be drawn at a particular level on the diagram Arguments tracklevel an integer The level of the track on the diagram to delete deltrackself tracklevel"
    },
    {
        "input_text": "summarize: def get_tracks(self):                return list(self.tracks.values())",
        "labels_text": "Return a list of the track contained in the diagram"
    },
    {
        "input_text": "summarize: def move_track(self, from_level, to_level):                aux = self.tracks[from_level]        del self.tracks[from_level]        self.add_track(aux, to_level)",
        "labels_text": "Move a track from one level on the diagram to another Arguments fromlevel an integer The level at which the track to be moved is found tolevel an integer The level to move the track to"
    },
    {
        "input_text": "summarize: def renumber_tracks(self, low=1, step=1):                track = low  # Start numbering from here        levels = self.get_levels()        conversion = {}  # Holds new set of levels        for level in levels:  # Starting at low...            conversion[track] = self.tracks[level]  # Add old tracks to new set            conversion[track].track_level = track            track += step  # step interval        self.tracks = conversion",
        "labels_text": "Renumber all track consecutively Optionally from a passed lowest number Arguments low an integer The track number to start from step an integer The track interval for separation of track"
    },
    {
        "input_text": "summarize: def get_levels(self):                return sorted(self.tracks)",
        "labels_text": "Return a sorted list of level occupied by track in the diagram"
    },
    {
        "input_text": "summarize: def get_drawn_levels(self):                return sorted(key for key in self.tracks if not self.tracks[key].hide)",
        "labels_text": "Return a sorted list of level occupied by track These track are not explicitly hidden"
    },
    {
        "input_text": "summarize: def range(self):                lows, highs = [], []        for track in self.tracks.values():  # Get ranges for each track            low, high = track.range()            lows.append(low)            highs.append(high)        return min(lows), max(highs)",
        "labels_text": "Return lowest and highest base number from track feature Returned type is a tuple"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                return self.tracks[key]",
        "labels_text": "Return the track contained at the level of the passed key"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [f\"\\n<{self.__class__}: {self.name}>\"]        outstr.append(\"%d tracks\" % len(self.tracks))        for level in self.get_levels():            outstr.append(\"Track %d: %s\\n\" % (level, self.tracks[level]))        outstr = \"\\n\".join(outstr)        return outstr",
        "labels_text": "Return a formatted string describing the diagram"
    },
    {
        "input_text": "summarize: def set_feature(self, feature):                self._feature = feature        self.__process_feature()",
        "labels_text": "Define the BioSeqFeature object to be wrapped"
    },
    {
        "input_text": "summarize: def get_feature(self):                return self._feature",
        "labels_text": "Return the unwrapped BioSeqFeature object"
    },
    {
        "input_text": "summarize: def set_colour(self, colour):                color = self._colortranslator.translate(colour)        self.color = color",
        "labels_text": "Backwards compatible variant of setcolorself color using UK spelling"
    },
    {
        "input_text": "summarize: def set_color(self, color):                # TODO - Make this into the set method for a color property?        color = self._colortranslator.translate(color)        self.color = color",
        "labels_text": "Set the color in which the feature will be drawn Arguments color The color to draw the feature either a colorsColor object an RGB tuple of float or an integer corresponding a color in colorstxt"
    },
    {
        "input_text": "summarize: def __getattr__(self, name):                return getattr(self._feature, name)",
        "labels_text": "Get attribute by name If the Feature class doesnt have the attribute called for check in selffeature for it"
    },
    {
        "input_text": "summarize: def __init__(self, set_id=None, name=None, parent=None):                self.parent = parent        self.id = id  # Unique id for the set        self.next_id = 0  # counter for unique feature ids        self.features = {}  # Holds features, keyed by ID        self.name = name",
        "labels_text": "Create the object Arguments setid Unique id for the set name String identifying the feature set"
    },
    {
        "input_text": "summarize: def del_feature(self, feature_id):                del self.features[feature_id]",
        "labels_text": "Delete a feature Arguments featureid Unique id of the feature to delete Remove a feature from the set indicated by it id"
    },
    {
        "input_text": "summarize: def set_all_features(self, attr, value):                for feature in self.features.values():            if hasattr(feature, attr):                # If the feature has the attribute, set it to the passed value                setattr(feature, attr, value)",
        "labels_text": "Set an attribute of all the feature Arguments attr An attribute of the Feature class value The value to set that attribute to Set the passed attribute of all feature in the set to the passed value"
    },
    {
        "input_text": "summarize: def get_ids(self):                return list(self.features.keys())",
        "labels_text": "Return a list of all id for the feature set"
    },
    {
        "input_text": "summarize: def range(self):                lows, highs = [], []        for feature in self.features.values():            for start, end in feature.locations:                lows.append(start)                highs.append(end)        if len(lows) != 0 and len(highs) != 0:  # Default in case there is            return (min(lows), max(highs))  # nothing in the set        return 0, 0",
        "labels_text": "Return the lowest and highest base or mark number a a tuple"
    },
    {
        "input_text": "summarize: def to_string(self, verbose=0):                if not verbose:  # Short account only required            return f\"{self}\"        else:  # Long account desired            outstr = [f\"\\n<{self.__class__}: {self.name}>\"]            outstr.append(\"%d features\" % len(self.features))            for key in self.features:                outstr.append(f\"feature: {self.features[key]}\")            return \"\\n\".join(outstr)",
        "labels_text": "Return a formatted string with information about the set Arguments verbose Boolean indicating whether a short default or complete account of the set is required"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.features)",
        "labels_text": "Return the number of feature in the set"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                return self.features[key]",
        "labels_text": "Return a feature keyed by id"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [            \"\\n<%s: %s %d features>\" % (self.__class__, self.name, len(self.features))        ]        return \"\\n\".join(outstr)",
        "labels_text": "Return a formatted string with information about the feature set"
    },
    {
        "input_text": "summarize: def set_data(self, data):                for pos, val in data:  # Fill data dictionary            self.data[pos] = val",
        "labels_text": "Add data a a list of position value tuples"
    },
    {
        "input_text": "summarize: def get_data(self):                data = []        for xval in self.data:            yval = self.data[xval]            data.append((xval, yval))        data.sort()        return data",
        "labels_text": "Return data a a list of sorted position value tuples"
    },
    {
        "input_text": "summarize: def add_point(self, point):                pos, val = point        self.data[pos] = val",
        "labels_text": "Add a single point to the set of data a a position value tuple"
    },
    {
        "input_text": "summarize: def quartiles(self):                data = sorted(self.data.values())        datalen = len(data)        return (            data[0],            data[datalen // 4],            data[datalen // 2],            data[3 * datalen // 4],            data[-1],        )",
        "labels_text": "Return minimum lowerQ medianQ upperQ maximum value a tuple"
    },
    {
        "input_text": "summarize: def range(self):                positions = sorted(self.data)  # i.e. dict keys        # Return first and last positions in graph        # print(len(self.data))        return (positions[0], positions[-1])",
        "labels_text": "Return range of data a start end tuple Returns the range of the data ie it start and end point on the genome a a start end tuple"
    },
    {
        "input_text": "summarize: def mean(self):                data = list(self.data.values())        return sum(data) / len(data)",
        "labels_text": "Return the mean value for the data point float"
    },
    {
        "input_text": "summarize: def stdev(self):                data = list(self.data.values())        m = self.mean()        runtotal = 0.0        for entry in data:            runtotal += (entry - m) ** 2        # This is sample standard deviation; population stdev would involve        # division by len(data), rather than len(data)-1        return sqrt(runtotal / (len(data) - 1))",
        "labels_text": "Return the sample standard deviation for the data float"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.data)",
        "labels_text": "Return the number of point in the data set"
    },
    {
        "input_text": "summarize: def __init__(self, name=None):                self.id = id  # Unique identifier for the set        self._next_id = 0  # Holds unique ids for graphs        self._graphs = {}  # Holds graphs, keyed by unique id        self.name = name",
        "labels_text": "Initialize Arguments name String identifying the graph set sensibly"
    },
    {
        "input_text": "summarize: def del_graph(self, graph_id):                del self._graphs[graph_id]",
        "labels_text": "Remove a graph from the set indicated by it id"
    },
    {
        "input_text": "summarize: def get_graphs(self):                return [self._graphs[id] for id in sorted(self._graphs)]",
        "labels_text": "Return list of all graph in the graph set sorted by id Sorting is to ensure reliable stacking"
    },
    {
        "input_text": "summarize: def get_ids(self):                return list(self._graphs.keys())",
        "labels_text": "Return a list of all id for the graph set"
    },
    {
        "input_text": "summarize: def range(self):                lows, highs = [], []        for graph in self._graphs.values():            low, high = graph.range()            lows.append(low)            highs.append(high)        return (min(lows), max(highs))",
        "labels_text": "Return the lowest and highest base or mark number a a tuple"
    },
    {
        "input_text": "summarize: def data_quartiles(self):                data = []        for graph in self._graphs.values():            data += list(graph.data.values())        data.sort()        datalen = len(data)        return (            data[0],            data[datalen / 4],            data[datalen / 2],            data[3 * datalen / 4],            data[-1],        )",
        "labels_text": "Return minimum lowerQ medianQ upperQ maximum value a a tuple"
    },
    {
        "input_text": "summarize: def to_string(self, verbose=0):                if not verbose:            return f\"{self}\"        else:            outstr = [f\"\\n<{self.__class__}: {self.name}>\"]            outstr.append(\"%d graphs\" % len(self._graphs))            for key in self._graphs:                outstr.append(f\"{self._graphs[key]}\")            return \"\\n\".join(outstr)",
        "labels_text": "Return a formatted string with information about the set Arguments verbose Flag indicating whether a short or complete account of the set is required"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._graphs)",
        "labels_text": "Return the number of graph in the set"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                return self._graphs[key]",
        "labels_text": "Return a graph keyed by id"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [f\"\\n<{self.__class__}: {self.name}>\"]        outstr.append(\"%d graphs\" % len(self._graphs))        outstr = \"\\n\".join(outstr)        return outstr",
        "labels_text": "Return a formatted string with information about the feature set"
    },
    {
        "input_text": "summarize: def draw_graph_set(self, set):                # print('draw graph set')        elements = []  # Holds graph elements        # Distribution dictionary for how to draw the graph        style_methods = {            \"line\": self.draw_line_graph,            \"heat\": self.draw_heat_graph,            \"bar\": self.draw_bar_graph,        }        for graph in set.get_graphs():            elements += style_methods[graph.style](graph)        return elements, []",
        "labels_text": "Draw graph set Arguments set GraphSet object Returns tuple list of graph element list of graph label"
    },
    {
        "input_text": "summarize: def _draw_sigil_box(self, bottom, center, top, x1, x2, strand, **kwargs):                if strand == 1:            y1 = center            y2 = top        elif strand == -1:            y1 = bottom            y2 = center        else:            y1 = bottom            y2 = top        return draw_box((x1, y1), (x2, y2), **kwargs)",
        "labels_text": "Draw BOX sigil PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_sigil_octo(self, bottom, center, top, x1, x2, strand, **kwargs):                if strand == 1:            y1 = center            y2 = top        elif strand == -1:            y1 = bottom            y2 = center        else:            y1 = bottom            y2 = top        return draw_cut_corner_box((x1, y1), (x2, y2), **kwargs)",
        "labels_text": "Draw OCTO sigil a box with the corner cut off PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_sigil_arrow(self, bottom, center, top, x1, x2, strand, **kwargs):                if strand == 1:            y1 = center            y2 = top            orientation = \"right\"        elif strand == -1:            y1 = bottom            y2 = center            orientation = \"left\"        else:            y1 = bottom            y2 = top            orientation = \"right\"  # backward compatibility        return draw_arrow((x1, y1), (x2, y2), orientation=orientation, **kwargs)",
        "labels_text": "Draw ARROW sigil PRIVATE"
    },
    {
        "input_text": "summarize: def _draw_sigil_big_arrow(self, bottom, center, top, x1, x2, strand, **kwargs):                if strand == -1:            orientation = \"left\"        else:            orientation = \"right\"        return draw_arrow((x1, bottom), (x2, top), orientation=orientation, **kwargs)",
        "labels_text": "Draw BIGARROW sigil like ARROW but straddle the axis PRIVATE"
    },
    {
        "input_text": "summarize: def add_set(self, set):                set.id = self._next_id  # Assign unique id to set        set.parent = self  # Make set's parent this track        self._sets[self._next_id] = set  # Add set, keyed by unique id        self._next_id += 1",
        "labels_text": "Add a preexisting FeatureSet or GraphSet object to the track"
    },
    {
        "input_text": "summarize: def del_set(self, set_id):                del self._sets[set_id]",
        "labels_text": "Remove the set with the passed id from the track"
    },
    {
        "input_text": "summarize: def get_sets(self):                return list(self._sets.values())",
        "labels_text": "Return the set contained in this track"
    },
    {
        "input_text": "summarize: def get_ids(self):                return list(self._sets.keys())",
        "labels_text": "Return the id of all set contained in this track"
    },
    {
        "input_text": "summarize: def to_string(self, verbose=0):                if not verbose:  # Return the short description            return f\"{self}\"  # Use __str__ method instead        else:  # Return the long description            outstr = [f\"\\n<{self.__class__}: {self.name}>\"]            outstr.append(\"%d sets\" % len(self._sets))            for key in self._sets:                outstr.append(f\"set: {self._sets[key]}\")            return \"\\n\".join(outstr)",
        "labels_text": "Return a formatted string with information about the track Arguments verbose Boolean indicating whether a short or complete account of the track is required"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                return self._sets[key]",
        "labels_text": "Return the set with the passed id"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [f\"\\n<{self.__class__}: {self.name}>\"]        outstr.append(\"%d sets\" % len(self._sets))        return \"\\n\".join(outstr)",
        "labels_text": "Return a formatted string with information about the Track"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model, sequence):                self._mm = markov_model        self._seq = sequence",
        "labels_text": "Initialize to calculate forward and backward probability Arguments markovmodel The current Markov model we are working with sequence A training sequence containing a set of emission"
    },
    {
        "input_text": "summarize: def _forward_recursion(self, cur_state, sequence_pos, forward_vars):                raise NotImplementedError(\"Subclasses must implement\")",
        "labels_text": "Calculate the forward recursion value PRIVATE"
    },
    {
        "input_text": "summarize: def _backward_recursion(self, cur_state, sequence_pos, forward_vars):                raise NotImplementedError(\"Subclasses must implement\")",
        "labels_text": "Calculate the backward recursion value PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model, sequence):                AbstractDPAlgorithms.__init__(self, markov_model, sequence)        self._s_values = {}",
        "labels_text": "Initialize the scaled approach to calculating probability Arguments markovmodel The current Markov model we are working with sequence A TrainingSequence object that must have a set of emission to work with"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model, sequence):                raise NotImplementedError(\"Haven't coded this yet...\")",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _gen_random_array(n):        randArray = [random.random() for _ in range(n)]    total = sum(randArray)    return [x / total for x in randArray]",
        "labels_text": "Return an array of n random number summing to PRIVATE"
    },
    {
        "input_text": "summarize: def _calculate_emissions(emission_probs):        # loop over all of the state-symbol duples, mapping states to    # lists of emitted symbols    emissions = defaultdict(list)    for state, symbol in emission_probs:        emissions[state].append(symbol)    return emissions",
        "labels_text": "Calculate which symbol can be emitted in each state PRIVATE"
    },
    {
        "input_text": "summarize: def _calculate_from_transitions(trans_probs):        transitions = defaultdict(list)    for from_state, to_state in trans_probs:        transitions[from_state].append(to_state)    return transitions",
        "labels_text": "Calculate which from transition are allowed for each state PRIVATE This look through all of the transprobs and us this dictionary to determine allowed transition It convert this information into a dictionary whose key are source state and whose value are list of destination state reachable from the source state via a transition"
    },
    {
        "input_text": "summarize: def _calculate_to_transitions(trans_probs):        transitions = defaultdict(list)    for from_state, to_state in trans_probs:        transitions[to_state].append(from_state)    return transitions",
        "labels_text": "Calculate which to transition are allowed for each state PRIVATE This look through all of the transprobs and us this dictionary to determine allowed transition It convert this information into a dictionary whose key are destination state and whose value are list of source state from which the destination is reachable via a transition"
    },
    {
        "input_text": "summarize: def _all_blank(self, first_alphabet, second_alphabet):                all_blank = {}        for first_state in first_alphabet:            for second_state in second_alphabet:                all_blank[(first_state, second_state)] = 0        return all_blank",
        "labels_text": "Return a dictionary with all count set to zero PRIVATE This us the letter in the first and second alphabet to create a dictionary with key of two tuples organized a letter of first alphabet letter of second alphabet The value are all set to"
    },
    {
        "input_text": "summarize: def _all_pseudo(self, first_alphabet, second_alphabet):                all_counts = {}        for first_state in first_alphabet:            for second_state in second_alphabet:                all_counts[(first_state, second_state)] = self.DEFAULT_PSEUDO        return all_counts",
        "labels_text": "Return a dictionary with all count set to a default value PRIVATE This take the letter in first alphabet and second alphabet and creates a dictionary with key of two tuples organized a letter of first alphabet letter of second alphabet The value are all set to the value of the class attribute DEFAULTPSEUDO"
    },
    {
        "input_text": "summarize: def set_random_initial_probabilities(self):                initial_freqs = _gen_random_array(len(self._state_alphabet))        for state in self._state_alphabet:            self.initial_prob[state] = initial_freqs.pop()        return self.initial_prob",
        "labels_text": "Set all initial state probability to a randomly generated distribution Returns the dictionary containing the initial probability"
    },
    {
        "input_text": "summarize: def set_random_probabilities(self):                self.set_random_initial_probabilities()        self.set_random_transition_probabilities()        self.set_random_emission_probabilities()",
        "labels_text": "Set all probability to randomly generated number Resets probability of all initial state transition and emission to random value"
    },
    {
        "input_text": "summarize: def destroy_transition(self, from_state, to_state):                try:            del self.transition_prob[(from_state, to_state)]            del self.transition_pseudo[(from_state, to_state)]        except KeyError:            raise KeyError(                f\"Transition from {from_state} to {to_state} is already disallowed.\"            )",
        "labels_text": "Restrict transition between the two state Raises KeyError if the transition is not currently allowed"
    },
    {
        "input_text": "summarize: def set_transition_score(self, from_state, to_state, probability):                if (from_state, to_state) in self.transition_prob:            self.transition_prob[(from_state, to_state)] = probability        else:            raise KeyError(                f\"Transition from {from_state} to {to_state} is not allowed.\"            )",
        "labels_text": "Set the probability of a transition between two state Raises KeyError if the transition is not allowed"
    },
    {
        "input_text": "summarize: def set_transition_pseudocount(self, from_state, to_state, count):                if (from_state, to_state) in self.transition_pseudo:            self.transition_pseudo[(from_state, to_state)] = count        else:            raise KeyError(                f\"Transition from {from_state} to {to_state} is not allowed.\"            )",
        "labels_text": "Set the default pseudocount for a transition To avoid computational problem it is helpful to be able to set a default pseudocount to start with for estimating transition and emission probability see p in Durbin et al for more discussion on this By default all transition have a pseudocount of Raises KeyError if the transition is not allowed"
    },
    {
        "input_text": "summarize: def set_emission_score(self, seq_state, emission_state, probability):                if (seq_state, emission_state) in self.emission_prob:            self.emission_prob[(seq_state, emission_state)] = probability        else:            raise KeyError(                f\"Emission of {emission_state} from {seq_state} is not allowed.\"            )",
        "labels_text": "Set the probability of a emission from a particular state Raises KeyError if the emission from the given state is not allowed"
    },
    {
        "input_text": "summarize: def set_emission_pseudocount(self, seq_state, emission_state, count):                if (seq_state, emission_state) in self.emission_pseudo:            self.emission_pseudo[(seq_state, emission_state)] = count        else:            raise KeyError(                f\"Emission of {emission_state} from {seq_state} is not allowed.\"            )",
        "labels_text": "Set the default pseudocount for an emission To avoid computational problem it is helpful to be able to set a default pseudocount to start with for estimating transition and emission probability see p in Durbin et al for more discussion on this By default all emission have a pseudocount of Raises KeyError if the emission from the given state is not allowed"
    },
    {
        "input_text": "summarize: def get_blank_transitions(self):                return self._transition_pseudo",
        "labels_text": "Get the default transition for the model Returns a dictionary of all of the default transition between any two letter in the sequence alphabet The dictionary is structured with key a letter letter and value a the starting number of transition"
    },
    {
        "input_text": "summarize: def get_blank_emissions(self):                return self._emission_pseudo",
        "labels_text": "Get the starting default emission for each sequence This return a dictionary of the default emission for each letter The dictionary is structured with key a seqletter emissionletter and value a the starting number of emission"
    },
    {
        "input_text": "summarize: def transitions_from(self, state_letter):                if state_letter in self._transitions_from:            return self._transitions_from[state_letter]        else:            return []",
        "labels_text": "Get all destination state which can transition from source stateletter This return all letter which the given stateletter can transition to ie all the destination state reachable from stateletter An empty list is returned if stateletter ha no outgoing transition"
    },
    {
        "input_text": "summarize: def transitions_to(self, state_letter):                if state_letter in self._transitions_to:            return self._transitions_to[state_letter]        else:            return []",
        "labels_text": "Get all source state which can transition to destination stateletter This return all letter which the given stateletter is reachable from ie all the source state which can reach statelater An empty list is returned if stateletter is unreachable"
    },
    {
        "input_text": "summarize: def _log_transform(self, probability):                log_prob = copy.copy(probability)        for key in log_prob:            prob = log_prob[key]            if prob > 0:                log_prob[key] = math.log(log_prob[key])            else:                log_prob[key] = -math.inf        return log_prob",
        "labels_text": "Return log transform of the given probability dictionary PRIVATE When calculating the Viterbi equation add log of probability rather than multiplying probability to avoid underflow error This method return a new dictionary with the same key a the given dictionary and logtransformed value"
    },
    {
        "input_text": "summarize: def __init__(self, emissions, state_path):                if len(state_path) > 0 and len(emissions) != len(state_path):            raise ValueError(\"State path does not match associated emissions.\")        self.emissions = emissions        self.states = state_path",
        "labels_text": "Initialize a training sequence Arguments emission An iterable eg a tuple list or Seq object containing the sequence of emission in the training sequence statepath An iterable eg a tuple or list containing the sequence of state If there is no known state path then the sequence of state should be an empty iterable"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model):                self._markov_model = markov_model",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def log_likelihood(self, probabilities):                total_likelihood = 0        for probability in probabilities:            total_likelihood += math.log(probability)        return total_likelihood",
        "labels_text": "Calculate the log likelihood of the training seqs Arguments probability A list of the probability of each training sequence under the current parameter calculated using the forward algorithm"
    },
    {
        "input_text": "summarize: def estimate_params(self, transition_counts, emission_counts):                # now calculate the information        ml_transitions = self.ml_estimator(transition_counts)        ml_emissions = self.ml_estimator(emission_counts)        return ml_transitions, ml_emissions",
        "labels_text": "Get a maximum likelihood estimation of transition and emission Arguments transitioncounts A dictionary with the total number of count of transition between two state emissionscounts A dictionary with the total number of count of emission of a particular emission letter by a state letter This then return the maximum likelihood estimator for the transition and emission estimated by formula in Durbin et al akl Akl sumAkl ekb Ekb sumEkb Returns Transition and emission dictionary containing the maximum likelihood estimator"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model):                AbstractTrainer.__init__(self, markov_model)",
        "labels_text": "Initialize the trainer Arguments markovmodel The model we are going to estimate parameter for This should have the parameter with some initial estimate that we can build from"
    },
    {
        "input_text": "summarize: def __init__(self, markov_model):                AbstractTrainer.__init__(self, markov_model)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _count_emissions(self, training_seq, emission_counts):                for index in range(len(training_seq.emissions)):            cur_state = training_seq.states[index]            cur_emission = training_seq.emissions[index]            try:                emission_counts[(cur_state, cur_emission)] += 1            except KeyError:                raise KeyError(f\"Unexpected emission ({cur_state}, {cur_emission})\")        return emission_counts",
        "labels_text": "Add emission from the training sequence to the current count PRIVATE Arguments trainingseq A TrainingSequence with state and emission to get the count from emissioncounts The current emission count to add to"
    },
    {
        "input_text": "summarize: def _count_transitions(self, state_seq, transition_counts):                for cur_pos in range(len(state_seq) - 1):            cur_state = state_seq[cur_pos]            next_state = state_seq[cur_pos + 1]            try:                transition_counts[(cur_state, next_state)] += 1            except KeyError:                raise KeyError(f\"Unexpected transition ({cur_state}, {next_state})\")        return transition_counts",
        "labels_text": "Add transition from the training sequence to the current count PRIVATE Arguments stateseq A Seq object with the state of the current training sequence transitioncounts The current transition count to add to"
    },
    {
        "input_text": "summarize: def _default_wrap(indent):        return [indent, \"\", (\" \", \"\", 1, 0)]",
        "labels_text": "Return default wrap rule for wrapkegg PRIVATE A wrap rule is a list with the following element indent connect splitstr connect splitafter keep"
    },
    {
        "input_text": "summarize: def _struct_wrap(indent):        return [indent, \"\", (\"  \", \"\", 1, 1)]",
        "labels_text": "Return wrap rule for KEGG STRUCTURE PRIVATE"
    },
    {
        "input_text": "summarize: def _write_kegg(item, info, indent=KEGG_ITEM_LENGTH):        s = \"\"    for line in info:        partial_lines = line.splitlines()        for partial in partial_lines:            s += item.ljust(indent) + partial + \"\\n\"            if item:  # ensure item is only written on first line                item = \"\"    return s",
        "labels_text": "Write a indented KEGG record item PRIVATE Arguments item The name of the item to be written info The wrapped information to write indent Width of item field"
    },
    {
        "input_text": "summarize: def __init__(self):                self.entry = \"\"        self.name = []        self.formula = \"\"        self.mass = \"\"        self.pathway = []        self.enzyme = []        self.structures = []        self.dblinks = []",
        "labels_text": "Initialize a new record"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            self._entry()            + self._name()            + self._formula()            + self._mass()            + self._pathway()            + self._enzyme()            + self._structures()            + self._dblinks()            + \"///\"        )",
        "labels_text": "Return a string representation of this Record"
    },
    {
        "input_text": "summarize: def __init__(self):                self.entry = \"\"        self.name = []        self.classname = []        self.sysname = []        self.reaction = []        self.substrate = []        self.product = []        self.inhibitor = []        self.cofactor = []        self.effector = []        self.comment = []        self.pathway = []        self.genes = []        self.disease = []        self.structures = []        self.dblinks = []",
        "labels_text": "Initialize a new Record"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            self._entry()            + self._name()            + self._classname()            + self._sysname()            + self._reaction()            + self._substrate()            + self._product()            + self._inhibitor()            + self._cofactor()            + self._effector()            + self._comment()            + self._pathway()            + self._genes()            + self._disease()            + self._structures()            + self._dblinks()            + \"///\"        )",
        "labels_text": "Return a string representation of this Record"
    },
    {
        "input_text": "summarize: def read(handle):        records = parse(handle)    try:        record = next(records)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(records)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    return record",
        "labels_text": "Parse a KEGG Enzyme file with exactly one entry If the handle contains no record or more than one record an exception is raised For example with openKEGGenzymenew a handle record readhandle print s recordentry recordname benzoateCoA ligase"
    },
    {
        "input_text": "summarize: def __init__(self):                self.entry = \"\"        self.name = []        self.definition = \"\"        self.orthology = []        self.organism = \"\"        self.position = \"\"        self.motif = []        self.dblinks = []",
        "labels_text": "Initialize new record"
    },
    {
        "input_text": "summarize: def __str__(self):                return self._entry() + self._name() + self._dblinks() + \"///\"",
        "labels_text": "Return a string representation of this Record"
    },
    {
        "input_text": "summarize: def read(handle):        pathways = parse(handle)    try:        pathway = next(pathways)    except StopIteration:        raise ValueError(\"No pathways found in handle\") from None    try:        next(pathways)        raise ValueError(\"More than one pathway found in handle\")    except StopIteration:        pass    return pathway",
        "labels_text": "Parse a single KEGG Pathway from given file handle Returns a single Pathway object There should be one and only one pathway in each file but there may well be pathological example out there"
    },
    {
        "input_text": "summarize: def __init__(self, elem):                self.entry = elem",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self._name = \"\"        self.org = \"\"        self._number = None        self.title = \"\"        self.image = \"\"        self.link = \"\"        self.entries = {}        self._reactions = {}        self._relations = set()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def add_entry(self, entry):                # We insist that the node ID is an integer        if not isinstance(entry.id, int):            raise TypeError(                f\"Node ID must be an integer, got {type(entry.id)} ({entry.id})\"            )        entry._pathway = self  # Let the entry know about the pathway        self.entries[entry.id] = entry",
        "labels_text": "Add an Entry element to the pathway"
    },
    {
        "input_text": "summarize: def remove_entry(self, entry):                if not isinstance(entry.id, int):            raise TypeError(                f\"Node ID must be an integer, got {type(entry.id)} ({entry.id})\"            )        # We need to remove the entry from any other elements that may        # contain it, which means removing those elements        # TODO        del self.entries[entry.id]",
        "labels_text": "Remove an Entry element from the pathway"
    },
    {
        "input_text": "summarize: def remove_reaction(self, reaction):                if not isinstance(reaction.id, int):            raise TypeError(                f\"Node ID must be an integer, got {type(reaction.id)} ({reaction.id})\"            )        # We need to remove the reaction from any other elements that may        # contain it, which means removing those elements        # TODO        del self._reactions[reaction.id]",
        "labels_text": "Remove a Reaction element from the pathway"
    },
    {
        "input_text": "summarize: def add_relation(self, relation):                relation._pathway = self  # Let the reaction know about the pathway        self._relations.add(relation)",
        "labels_text": "Add a Relation element to the pathway"
    },
    {
        "input_text": "summarize: def remove_relation(self, relation):                self._relations.remove(relation)",
        "labels_text": "Remove a Relation element from the pathway"
    },
    {
        "input_text": "summarize: def compounds(self):                return [e for e in self.entries.values() if e.type == \"compound\"]",
        "labels_text": "Get a list of entry of type compound"
    },
    {
        "input_text": "summarize: def maps(self):                return [e for e in self.entries.values() if e.type == \"map\"]",
        "labels_text": "Get a list of entry of type map"
    },
    {
        "input_text": "summarize: def orthologs(self):                return [e for e in self.entries.values() if e.type == \"ortholog\"]",
        "labels_text": "Get a list of entry of type ortholog"
    },
    {
        "input_text": "summarize: def genes(self):                return [e for e in self.entries.values() if e.type == \"gene\"]",
        "labels_text": "Get a list of entry of type gene"
    },
    {
        "input_text": "summarize: def reactions(self):                return self._reactions.values()",
        "labels_text": "Get a list of reaction in the pathway"
    },
    {
        "input_text": "summarize: def reaction_entries(self):                return [self.entries[i] for i in self._reactions]",
        "labels_text": "List of entry corresponding to each reaction in the pathway"
    },
    {
        "input_text": "summarize: def relations(self):                return list(self._relations)",
        "labels_text": "Get a list of relation in the pathway"
    },
    {
        "input_text": "summarize: def bounds(self):                xlist, ylist = [], []        for b in [g.bounds for g in self.entries.values()]:            xlist.extend([b[0][0], b[1][0]])            ylist.extend([b[0][1], b[1][1]])        return [(min(xlist), min(ylist)), (max(xlist), max(ylist))]",
        "labels_text": "Coordinate bound for all Graphics element in the Pathway Returns the xmin ymin xmax ymax coordinate for all Graphics element in the Pathway"
    },
    {
        "input_text": "summarize: def __init__(self):                self._id = None        self._names = []        self.type = \"\"        self.image = \"\"        self.link = \"\"        self.graphics = []        self.components = set()        self.alt = []        self._pathway = None        self._reactions = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [            \"Entry node ID: %d\" % self.id,            f\"Names: {self.name}\",            f\"Type: {self.type}\",            f\"Components: {self.components}\",            f\"Reactions: {self.reaction}\",            \"Graphics elements: %d %s\" % (len(self.graphics), self.graphics),        ]        return \"\\n\".join(outstr) + \"\\n\"",
        "labels_text": "Return readable descriptive string"
    },
    {
        "input_text": "summarize: def add_component(self, element):                if self._pathway is not None:            if element.id not in self._pathway.entries:                raise ValueError(                    f\"Component {element.id} is not an entry in the pathway\"                )        self.components.add(element)",
        "labels_text": "Add an element to the entry If the Entry is already part of a pathway make sure the component already exists"
    },
    {
        "input_text": "summarize: def remove_component(self, value):                self.components.remove(value)",
        "labels_text": "Remove the entry with the passed ID from the group"
    },
    {
        "input_text": "summarize: def add_graphics(self, entry):                self.graphics.append(entry)",
        "labels_text": "Add the Graphics entry"
    },
    {
        "input_text": "summarize: def remove_graphics(self, entry):                self.graphics.remove(entry)",
        "labels_text": "Remove the Graphics entry with the passed ID from the group"
    },
    {
        "input_text": "summarize: def bounds(self):                xlist, ylist = [], []        for b in [g.bounds for g in self.graphics]:            xlist.extend([b[0][0], b[1][0]])            ylist.extend([b[0][1], b[1][1]])        return [(min(xlist), min(ylist)), (max(xlist), max(ylist))]",
        "labels_text": "Coordinate bound for all Graphics element in the Entry Return the xmin ymin xmax ymax coordinate for the Entry Graphics element"
    },
    {
        "input_text": "summarize: def is_reactant(self):                for rxn in self._pathway.reactions:            if self._id in rxn.reactant_ids:                return True        return False",
        "labels_text": "Return true if this Entry participates in any reaction in it parent pathway"
    },
    {
        "input_text": "summarize: def __init__(self, parent):                self._id = None        self._parent = parent",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def element(self):                # The root is this Component element        component = ET.Element(\"component\")        component.attrib = {\"id\": str(self._id)}        return component",
        "labels_text": "Return the Component a a valid KGML element"
    },
    {
        "input_text": "summarize: def __init__(self, parent):                self.name = \"\"        self._x = None        self._y = None        self._coords = None        self.type = \"\"        self._width = None        self._height = None        self.fgcolor = \"\"        self.bgcolor = \"\"        self._parent = parent",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def centre(self):                return (            0.5 * (self.bounds[0][0] + self.bounds[1][0]),            0.5 * (self.bounds[0][1] + self.bounds[1][1]),        )",
        "labels_text": "Return the centre of the Graphics object a an x y tuple"
    },
    {
        "input_text": "summarize: def __init__(self):                self._id = None        self._names = []        self.type = \"\"        self._substrates = set()        self._products = set()        self._pathway = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [            f\"Reaction node ID: {self.id}\",            f\"Reaction KEGG IDs: {self.name}\",            f\"Type: {self.type}\",            f\"Substrates: {','.join([s.name for s in self.substrates])}\",            f\"Products: {','.join([s.name for s in self.products])}\",        ]        return \"\\n\".join(outstr) + \"\\n\"",
        "labels_text": "Return an informative humanreadable string"
    },
    {
        "input_text": "summarize: def add_substrate(self, substrate_id):                if self._pathway is not None:            if int(substrate_id) not in self._pathway.entries:                raise ValueError(                    \"Couldn't add substrate, no node ID %d in Pathway\"                    % int(substrate_id)                )        self._substrates.add(substrate_id)",
        "labels_text": "Add a substrate identified by it node ID to the reaction"
    },
    {
        "input_text": "summarize: def add_product(self, product_id):                if self._pathway is not None:            if int(product_id) not in self._pathway.entries:                raise ValueError(                    \"Couldn't add product, no node ID %d in Pathway\" % product_id                )        self._products.add(int(product_id))",
        "labels_text": "Add a product identified by it node ID to the reaction"
    },
    {
        "input_text": "summarize: def substrates(self):                return [self._pathway.entries[sid] for sid in self._substrates]",
        "labels_text": "Return list of substrate Entry element"
    },
    {
        "input_text": "summarize: def products(self):                return [self._pathway.entries[pid] for pid in self._products]",
        "labels_text": "Return list of product Entry element"
    },
    {
        "input_text": "summarize: def entry(self):                return self._pathway.entries[self._id]",
        "labels_text": "Return the Entry corresponding to this reaction"
    },
    {
        "input_text": "summarize: def reactant_ids(self):                return self._products.union(self._substrates)",
        "labels_text": "Return a list of substrate and product reactant IDs"
    },
    {
        "input_text": "summarize: def __init__(self):                self._entry1 = None        self._entry2 = None        self.type = \"\"        self.subtypes = []        self._pathway = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                outstr = [            \"Relation (subtypes: %d):\" % len(self.subtypes),            \"Entry1:\",            str(self.entry1),            \"Entry2:\",            str(self.entry2),        ]        for s in self.subtypes:            outstr.extend([f\"Subtype: {s[0]}\", str(s[1])])        return \"\\n\".join(outstr)",
        "labels_text": "Return a useful humanreadable string"
    },
    {
        "input_text": "summarize: def element(self):                # The root is this Relation element        relation = ET.Element(\"relation\")        relation.attrib = {            \"entry1\": str(self._entry1),            \"entry2\": str(self._entry2),            \"type\": self.type,        }        for name, value in self.subtypes:            subtype = ET.Element(\"subtype\")            subtype.attrib = {\"name\": name, \"value\": str(value)}            relation.append(subtype)        return relation",
        "labels_text": "Return KGML element describing the Relation"
    },
    {
        "input_text": "summarize: def read(handle):        records = parse(handle)    return next(records)",
        "labels_text": "Read a single Medline record from the handle The handle is either is a Medline file a filelike object or a list of line describing a Medline record Typical usage from Bio import Medline with openMedlinepubmedresulttxt a handle record Medlinereadhandle printrecordTI The Bio toolkitsa brief overview"
    },
    {
        "input_text": "summarize: def __init__(self):                self.parameters = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"\\n\".join(str(motif) for motif in self)",
        "labels_text": "Return a string representation of the motif in the Record object"
    },
    {
        "input_text": "summarize: def __init__(self):                self.sequences = []        self.version = \"\"        self.database = \"\"        self.diagrams = {}        self.alphabet = None        self.strand_handling = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                if isinstance(key, str):            for motif in self:                if motif.name == key:                    return motif        else:            return list.__getitem__(self, key)",
        "labels_text": "Return the motif of index key"
    },
    {
        "input_text": "summarize: def read(handle):        record = Record()    try:        xml_tree = ET.parse(handle)    except ET.ParseError:        raise ValueError(            \"Improper MAST XML input file. XML root tag should start with <mast version= ...\"        )    __read_metadata(record, xml_tree)    __read_sequences(record, xml_tree)    return record",
        "labels_text": "Parse a MAST XML format handle a a Record object"
    },
    {
        "input_text": "summarize: def __read_sequences(record, xml_tree):        for sequence_tree in xml_tree.find(\"sequences\").findall(\"sequence\"):        sequence_name = sequence_tree.get(\"name\")        record.sequences.append(sequence_name)        diagram_str = __make_diagram(record, sequence_tree)        record.diagrams[sequence_name] = diagram_str",
        "labels_text": "Read sequence from XML ElementTree object"
    },
    {
        "input_text": "summarize: def __init__(self, alphabet, values):                self.length = None        for letter in alphabet:            if self.length is None:                self.length = len(values[letter])            elif self.length != len(values[letter]):                raise Exception(\"data has inconsistent lengths\")            # Cast any numpy floats into Python floats:            self[letter] = [float(_) for _ in values[letter]]        self.alphabet = alphabet",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                words = [\"%6d\" % i for i in range(self.length)]        line = \"   \" + \" \".join(words)        lines = [line]        for letter in self.alphabet:            words = [\"%6.2f\" % value for value in self[letter]]            line = \"%c: \" % letter + \" \".join(words)            lines.append(line)        text = \"\\n\".join(lines) + \"\\n\"        return text",
        "labels_text": "Return a string containing nucleotide and count of the alphabet in the Matrix"
    },
    {
        "input_text": "summarize: def consensus(self):                sequence = \"\"        for i in range(self.length):            maximum = -math.inf            for letter in self.alphabet:                count = self[letter][i]                if count > maximum:                    maximum = count                    sequence_letter = letter            sequence += sequence_letter        return Seq(sequence)",
        "labels_text": "Return the consensus sequence"
    },
    {
        "input_text": "summarize: def anticonsensus(self):                sequence = \"\"        for i in range(self.length):            minimum = math.inf            for letter in self.alphabet:                count = self[letter][i]                if count < minimum:                    minimum = count                    sequence_letter = letter            sequence += sequence_letter        return Seq(sequence)",
        "labels_text": "Return the anticonsensus sequence"
    },
    {
        "input_text": "summarize: def gc_content(self):                alphabet = self.alphabet        gc_total = 0.0        total = 0.0        for i in range(self.length):            for letter in alphabet:                if letter in \"CG\":                    gc_total += self[letter][i]                total += self[letter][i]        return gc_total / total",
        "labels_text": "Compute the fraction GC content"
    },
    {
        "input_text": "summarize: def __init__(self, alphabet, counts):                GenericPositionMatrix.__init__(self, alphabet, counts)        for i in range(self.length):            total = sum(self[letter][i] for letter in alphabet)            for letter in alphabet:                self[letter][i] /= total        for letter in alphabet:            self[letter] = tuple(self[letter])",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def max(self):                score = 0.0        letters = self.alphabet        for position in range(self.length):            score += max(self[letter][position] for letter in letters)        return score",
        "labels_text": "Maximal possible score for this motif return the score computed for the consensus sequence"
    },
    {
        "input_text": "summarize: def min(self):                score = 0.0        letters = self.alphabet        for position in range(self.length):            score += min(self[letter][position] for letter in letters)        return score",
        "labels_text": "Minimal possible score for this motif return the score computed for the anticonsensus sequence"
    },
    {
        "input_text": "summarize: def gc_content(self):                raise Exception(\"Cannot compute the %GC composition of a PSSM\")",
        "labels_text": "Compute the GCratio"
    },
    {
        "input_text": "summarize: def distribution(self, background=None, precision=10**3):                from .thresholds import ScoreDistribution        if background is None:            background = dict.fromkeys(self.alphabet, 1.0)        else:            background = dict(background)        total = sum(background.values())        for letter in self.alphabet:            background[letter] /= total        return ScoreDistribution(precision=precision, pssm=self, background=background)",
        "labels_text": "Calculate the distribution of the score at the given precision"
    },
    {
        "input_text": "summarize: def __init__(self, alphabet=None, alignment=None):                motifs.Motif.__init__(self, alphabet, alignment)        self.evalue = 0.0        self.num_occurrences = 0        self.name = None        self.id = None        self.alt_id = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version = \"\"        self.datafile = \"\"        self.command = \"\"        self.alphabet = \"\"        self.sequences = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                if isinstance(key, str):            for motif in self:                if motif.name == key:                    return motif        else:            return list.__getitem__(self, key)",
        "labels_text": "Return the motif of index key"
    },
    {
        "input_text": "summarize: def __convert_strand(strand):        if strand == \"minus\":        return \"-\"    if strand == \"plus\" or strand == \"none\":        return \"+\"",
        "labels_text": "Convert strand from XML if present Default"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version = \"\"        self.datafile = \"\"        self.command = \"\"        self.alphabet = None        self.background = {}        self.sequences = []",
        "labels_text": "Initialize record class value"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                if isinstance(key, str):            for motif in self:                if motif.name == key:                    return motif        else:            return list.__getitem__(self, key)",
        "labels_text": "Return the motif of index key"
    },
    {
        "input_text": "summarize: def _read_version(record, handle):        for line in handle:        if line.startswith(\"MEME version\"):            break    else:        raise ValueError(            \"Improper input file. File should contain a line starting MEME version.\"        )    line = line.strip()    ls = line.split()    record.version = ls[2]",
        "labels_text": "Read MEME version PRIVATE"
    },
    {
        "input_text": "summarize: def _read_motif_name(handle):        for line in handle:        if \"sorted by position p-value\" in line:            break    else:        raise ValueError(\"Unexpected end of stream: Failed to find motif name\")    line = line.strip()    words = line.split()    name = \" \".join(words[0:2])    return name",
        "labels_text": "Read motif name PRIVATE"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"\\n\".join(str(motif) for motif in self)",
        "labels_text": "Return a string representation of the motif in the Record object"
    },
    {
        "input_text": "summarize: def threshold_fpr(self, fpr):                i = self.n_points        prob = 0.0        while prob < fpr:            i -= 1            prob += self.bg_density[i]        return self.min_score + i * self.step",
        "labels_text": "Approximate the logodds threshold which make the type I error false positive rate"
    },
    {
        "input_text": "summarize: def threshold_fnr(self, fnr):                i = -1        prob = 0.0        while prob < fnr:            i += 1            prob += self.mo_density[i]        return self.min_score + i * self.step",
        "labels_text": "Approximate the logodds threshold which make the type II error false negative rate"
    },
    {
        "input_text": "summarize: def threshold_patser(self):                return self.threshold_fpr(fpr=2**-self.ic)",
        "labels_text": "Threshold selection mimicking the behaviour of patser Hertz Stormo software It selects such a threshold that the logfpricM note the actual patser software us natural logarithm instead of log so the number are not directly comparable"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return write(self)",
        "labels_text": "Turn the TRANSFAC matrix into a string"
    },
    {
        "input_text": "summarize: def __init__(self, doc):                self.record = Record()        for child in doc.getElementsByTagName(\"motif\"):            if child.nodeType == Node.ELEMENT_NODE:                self.handle_motif(child)",
        "labels_text": "Generate motif Record from xms document an XMLlike motif pfm file"
    },
    {
        "input_text": "summarize: def get_text(self, nodelist):                retlist = []        for node in nodelist:            if node.nodeType == Node.TEXT_NODE:                retlist.append(node.wholeText)            elif node.hasChildNodes:                retlist.append(self.get_text(node.childNodes))        return re.sub(r\"\\s+\", \" \", \"\".join(retlist))",
        "labels_text": "Return a string representation of the motif property listed on nodelist"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"\\n\".join(str(motif) for motif in self)",
        "labels_text": "Return a string representation of the motif in the Record object"
    },
    {
        "input_text": "summarize: def read(handle):        xms_doc = minidom.parse(handle)    record = XMSScanner(xms_doc).record    return record",
        "labels_text": "Read motif in XMS matrix format from a file handle XMS is an XML format for describing regulatory motif and PSSMs This format wa defined by Thomas Down and used in the NestedMICA and MotifExplorer program"
    },
    {
        "input_text": "summarize: def create(instances, alphabet=\"ACGT\"):        alignment = Alignment(instances)    return Motif(alignment=alignment, alphabet=alphabet)",
        "labels_text": "Create a Motif object"
    },
    {
        "input_text": "summarize: def read(handle, fmt, strict=True):        fmt = fmt.lower()    motifs = parse(handle, fmt, strict)    if len(motifs) == 0:        raise ValueError(\"No motifs found in handle\")    if len(motifs) > 1:        raise ValueError(\"More than one motif found in handle\")    motif = motifs[0]    return motif",
        "labels_text": "Read a motif from a handle using the specified fileformat This support the same format a Biomotifsparse but only for file containing exactly one motif For example reading a JASPARstyle pfm file from Bio import motif with openmotifsSRFpfm a handle m motifsreadhandle pfm mconsensus SeqGCCCATATATGG Or a singlemotif MEME file from Bio import motif with openmotifsmemepsptestclassiczoopsxml a handle m motifsreadhandle meme mconsensus SeqGCTTATGTAA If the handle contains no record or more than one record an exception is raised from Bio import motif with openmotifsalignaceout a handle motif motifsreadhandle AlignAce Traceback most recent call last ValueError More than one motif found in handle If however you want the first motif from a file containing multiple motif this function would raise an exception a shown in the example above Instead use from Bio import motif with openmotifsalignaceout a handle record motifsparsehandle alignace motif record motifconsensus SeqTCTACGATTGAG Use the Biomotifsparsehandle fmt function if you want to read multiple record from the handle If strict is True default the parser will raise a ValueError if the file content doe not strictly comply with the specified file format"
    },
    {
        "input_text": "summarize: def __str__(self):                text = \"\"        for instance in self:            text += str(instance) + \"\\n\"        return text",
        "labels_text": "Return a string containing the sequence of the motif"
    },
    {
        "input_text": "summarize: def count(self):                counts = {}        for letter in self.alphabet:            counts[letter] = [0] * self.length        for instance in self:            for position, letter in enumerate(instance):                counts[letter][position] += 1        return counts",
        "labels_text": "Count nucleotide in a position"
    },
    {
        "input_text": "summarize: def search(self, sequence):                warnings.warn(            ,            BiopythonDeprecationWarning,        )        for pos in range(len(sequence) - self.length + 1):            for instance in self:                if instance == sequence[pos : pos + self.length]:                    yield (pos, instance)                    break",
        "labels_text": "Find position of motif in a given sequence This is a generator function returning found position of motif instance in a given sequence"
    },
    {
        "input_text": "summarize: def pwm(self):                return self.counts.normalize(self._pseudocounts)",
        "labels_text": "Calculate and return the position weight matrix for this motif"
    },
    {
        "input_text": "summarize: def pssm(self):                return self.pwm.log_odds(self._background)",
        "labels_text": "Calculate and return the position specific scoring matrix for this motif"
    },
    {
        "input_text": "summarize: def instances(self):                warnings.warn(            ,            BiopythonDeprecationWarning,        )        if self.alignment is None:            return None        return self.alignment.sequences",
        "labels_text": "Return the sequence from which the motif wa built"
    },
    {
        "input_text": "summarize: def __str__(self, masked=False):                text = \"\"        if self.alignment is not None:            text += \"\\n\".join(self.alignment)        if masked:            for i in range(self.length):                if self.__mask[i]:                    text += \"*\"                else:                    text += \" \"            text += \"\\n\"        return text",
        "labels_text": "Return string representation of a motif"
    },
    {
        "input_text": "summarize: def __len__(self):                if self.length is None:            return 0        else:            return self.length",
        "labels_text": "Return the length of a motif Please use this method ie invoke lenm instead of referring to mlength directly"
    },
    {
        "input_text": "summarize: def consensus(self):                return self.counts.consensus",
        "labels_text": "Return the consensus sequence"
    },
    {
        "input_text": "summarize: def anticonsensus(self):                return self.counts.anticonsensus",
        "labels_text": "Return the least probable pattern to be generated from this motif"
    },
    {
        "input_text": "summarize: def degenerate_consensus(self):                return self.counts.degenerate_consensus",
        "labels_text": "Return the degenerate consensus sequence Following the rule adapted from D R Cavener Comparison of the consensus sequence flanking translational start site in Drosophila and vertebrate Nucleic Acids Research The same rule are used by TRANSFAC"
    },
    {
        "input_text": "summarize: def format(self, format_spec):                return self.__format__(format_spec)",
        "labels_text": "Return a string representation of the Motif in the given format Currently supported format clusterbuster Cluster Buster position frequency matrix format pfm JASPAR single Position Frequency Matrix jaspar JASPAR multiple Position Frequency Matrix transfac TRANSFAC like file"
    },
    {
        "input_text": "summarize: def __init__(self, host=None, name=None, user=None, password=None):                self.name = name        self.host = host        self.user = user        self.password = password        self.dbh = mdb.connect(host, user, password, name)",
        "labels_text": "Construct a JASPAR instance and connect to specified DB Arguments host host name of the the JASPAR DB server name name of the JASPAR database user user name to connect to the JASPAR DB password JASPAR DB password"
    },
    {
        "input_text": "summarize: def __str__(self):                return rf\"{self.user}\\@{self.host}:{self.name}\"",
        "labels_text": "Return a string representation of the JASPAR DB connection"
    },
    {
        "input_text": "summarize: def fetch_motifs_by_name(self, name):                return self.fetch_motifs(collection=None, tf_name=name)",
        "labels_text": "Fetch a list of JASPAR motif from a JASPAR DB by the given TF name Arguments name a single name or list of name Returns A list of BiomotifsjasparMotif object Notes Names are not guaranteed to be unique There may be more than one motif with the same name Therefore even if name specifies a single name a list of motif is returned This just call selffetchmotifscollection None tfname name This behaviour is different from the TFBS perl module getMatrixbyname method which always return a single matrix issuing a warning message and returning the first matrix retrieved in the case where multiple matrix have the same name"
    },
    {
        "input_text": "summarize: def base_id(self):                (base_id, __) = split_jaspar_id(self.matrix_id)        return base_id",
        "labels_text": "Return the JASPAR base matrix ID"
    },
    {
        "input_text": "summarize: def version(self):                (__, version) = split_jaspar_id(self.matrix_id)        return version",
        "labels_text": "Return the JASPAR matrix version"
    },
    {
        "input_text": "summarize: def __hash__(self):                return self.matrix_id.__hash__()",
        "labels_text": "Return the hash key corresponding to the JASPAR profile note We assume the unicity of matrix IDs"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                return self.matrix_id == other.matrix_id",
        "labels_text": "Return True if matrix IDs are the same"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"\\n\".join(str(the_motif) for the_motif in self)",
        "labels_text": "Return a string of all motif in the Record"
    },
    {
        "input_text": "summarize: def to_dict(self):                dic = {}        for motif in self:            dic[motif.matrix_id] = motif        return dic",
        "labels_text": "Return the list of matrix a a dictionary of matrix"
    },
    {
        "input_text": "summarize: def read(handle, format):        format = format.lower()    if format == \"pfm\":        record = _read_pfm(handle)        return record    elif format == \"sites\":        record = _read_sites(handle)        return record    elif format == \"jaspar\":        record = _read_jaspar(handle)        return record    else:        raise ValueError(\"Unknown JASPAR format %s\" % format)",
        "labels_text": "Read motif from a file in one of several different JASPAR format Return the record of PFMs Call the appropriate routine based on the format passed"
    },
    {
        "input_text": "summarize: def split_jaspar_id(id):        id_split = id.split(\".\")    base_id = None    version = None    if len(id_split) == 2:        base_id = id_split[0]        version = id_split[1]    else:        base_id = id    return (base_id, version)",
        "labels_text": "Split a JASPAR matrix ID into it component Components are base ID and version number eg MA is returned a MA"
    },
    {
        "input_text": "summarize: def __init__(self, string):                if string:            self.buffer = list(string)        else:            self.buffer = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def peek(self):                if self.buffer:            return self.buffer[0]        else:            return None",
        "labels_text": "Return the first character from the buffer"
    },
    {
        "input_text": "summarize: def peek_nonwhitespace(self):                b = \"\".join(self.buffer).strip()        if b:            return b[0]        else:            return None",
        "labels_text": "Return the first character from the buffer do not include space"
    },
    {
        "input_text": "summarize: def __next__(self):                if self.buffer:            return self.buffer.pop(0)        else:            return None",
        "labels_text": "Iterate over NEXUS character in the file"
    },
    {
        "input_text": "summarize: def next_nonwhitespace(self):                while True:            p = next(self)            if p is None:                break            if p not in WHITESPACE:                return p        return None",
        "labels_text": "Check for next non whitespace character in NEXUS file"
    },
    {
        "input_text": "summarize: def skip_whitespace(self):                while self.buffer[0] in WHITESPACE:            self.buffer = self.buffer[1:]",
        "labels_text": "Skip whitespace character in NEXUS file"
    },
    {
        "input_text": "summarize: def next_until(self, target):                for t in target:            try:                pos = self.buffer.index(t)            except ValueError:                pass            else:                found = \"\".join(self.buffer[:pos])                self.buffer = self.buffer[pos:]                return found        else:            return None",
        "labels_text": "Iterate over the NEXUS file until a target character is reached"
    },
    {
        "input_text": "summarize: def peek_word(self, word):                return \"\".join(self.buffer[: len(word)]) == word",
        "labels_text": "Return a word stored in the buffer"
    },
    {
        "input_text": "summarize: def rest(self):                return \"\".join(self.buffer)",
        "labels_text": "Return the rest of the string without parsing"
    },
    {
        "input_text": "summarize: def __init__(self, symbols, gap):                self.data = {}        self.symbols = sorted(symbols)        if gap:            self.symbols.append(gap)        for x in self.symbols:            for y in [s for s in self.symbols if s != x]:                self.set(x, y, 0)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def set(self, x, y, value):                if x > y:            x, y = y, x        self.data[x + y] = value",
        "labels_text": "Set a given value in the matrix position"
    },
    {
        "input_text": "summarize: def add(self, x, y, value):                if x > y:            x, y = y, x        self.data[x + y] += value",
        "labels_text": "Add the given value to existing in matrix position"
    },
    {
        "input_text": "summarize: def sum(self):                return reduce(lambda x, y: x + y, self.data.values())",
        "labels_text": "Calculate the association make matrix of association"
    },
    {
        "input_text": "summarize: def transformation(self):                total = self.sum()        if total != 0:            for k in self.data:                self.data[k] /= total        return self",
        "labels_text": "Calculate the transformation matrix Normalizes the column of the matrix of association"
    },
    {
        "input_text": "summarize: def weighting(self):                for k in self.data:            if self.data[k] != 0:                self.data[k] = -math.log(self.data[k])        return self",
        "labels_text": "Calculate the Phylogenetic weight matrix Constructed from the logarithmic transformation of the transformation matrix"
    },
    {
        "input_text": "summarize: def safename(name, mrbayes=False):        if mrbayes:        safe = name.replace(\" \", \"_\")        safe = \"\".join(c for c in safe if c in MRBAYESSAFE)    else:        safe = name.replace(\"'\", \"''\")        if set(safe).intersection(set(WHITESPACE + PUNCTUATION)):            safe = \"'\" + safe + \"'\"    return safe",
        "labels_text": "Return a taxon identifier according to NEXUS standard Wrap quote around name with punctuation or whitespace and double single quote mrbayesTrue write name without quote whitespace or punctuation for the mrbayes software package"
    },
    {
        "input_text": "summarize: def quotestrip(word):        if not word:        return None    while (word.startswith(\"'\") and word.endswith(\"'\")) or (        word.startswith('\"') and word.endswith('\"')    ):        word = word[1:-1]    return word",
        "labels_text": "Remove quote andor double quote around identifier"
    },
    {
        "input_text": "summarize: def get_start_end(sequence, skiplist=(\"-\", \"?\")):        length = len(sequence)    if length == 0:        return None, None    end = length - 1    while end >= 0 and (sequence[end] in skiplist):        end -= 1    start = 0    while start < length and (sequence[start] in skiplist):        start += 1    if start == length and end == -1:  # empty sequence        return -1, -1    else:        return start, end",
        "labels_text": "Return position of first and last character which is not in skiplist Skiplist default to"
    },
    {
        "input_text": "summarize: def _sort_keys_by_values(p):        return sorted((pn for pn in p if p[pn]), key=lambda pn: p[pn])",
        "labels_text": "Return a sorted list of key of p sorted by value of p PRIVATE"
    },
    {
        "input_text": "summarize: def _make_unique(values):        return sorted(set(values))",
        "labels_text": "Check all value in list are unique and return a pruned and sorted list PRIVATE"
    },
    {
        "input_text": "summarize: def _seqmatrix2strmatrix(matrix):        return {t: str(matrix[t]) for t in matrix}",
        "labels_text": "Convert a Seqobject matrix to a plain sequencestring matrix PRIVATE"
    },
    {
        "input_text": "summarize: def _adjust_lines(lines):        formatted_lines = []    for line in lines:        # Convert line endings        line = line.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()        if line.lower().startswith(\"matrix\"):            formatted_lines.append(line)        else:            line = line.replace(\"\\n\", \" \")            if line:                formatted_lines.append(line)    return formatted_lines",
        "labels_text": "Adjust linebreaks to match strip leadingtrailing whitespace PRIVATE listofcommandlinesadjustlinesinputtext Lines are adjusted so that no linebreaks occur within a commandline except matrix command line"
    },
    {
        "input_text": "summarize: def __init__(self, title=None):                self.title = title        self.commandlines = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _taxlabels(self, options):",
        "labels_text": "Get taxon label PRIVATE As the taxon name are already in the matrix this is superfluous except for transpose matrix which are currently unsupported anyway Thus we ignore the taxlabels command to make handling of duplicate taxon name easier"
    },
    {
        "input_text": "summarize: def _check_taxlabels(self, taxon):                # According to NEXUS standard, underscores shall be treated as spaces...,        # so checking for identity is more difficult        nextaxa = {t.replace(\" \", \"_\"): t for t in self.taxlabels}        nexid = taxon.replace(\" \", \"_\")        return nextaxa.get(nexid)",
        "labels_text": "Check for presence of taxon in selftaxlabels PRIVATE"
    },
    {
        "input_text": "summarize: def _utree(self, options):                self._tree(options)",
        "labels_text": "Use utree to denote an unrooted tree ex clustalx PRIVATE"
    },
    {
        "input_text": "summarize: def _apply_block_structure(self, title, lines):                block = Block(\"\")        block.title = title        for line in lines:            block.commandlines.append(Commandline(line, title))        self.structured.append(block)",
        "labels_text": "Apply Block structure to the NEXUS file PRIVATE"
    },
    {
        "input_text": "summarize: def _taxset(self, options):                name, taxa = self._get_indices(options, set_type=TAXSET)        self.taxsets[name] = _make_unique(taxa)",
        "labels_text": "Create unique taxset PRIVATE"
    },
    {
        "input_text": "summarize: def _charset(self, options):                name, sites = self._get_indices(options, set_type=CHARSET)        self.charsets[name] = _make_unique(sites)",
        "labels_text": "Create unique character set PRIVATE"
    },
    {
        "input_text": "summarize: def _codonposset(self, options):                prev_partitions = list(self.charpartitions.keys())        self._charpartition(options)        # mcclade calls it CodonPositions, but you never know...        codonname = [n for n in self.charpartitions if n not in prev_partitions]        if codonname == [] or len(codonname) > 1:            raise NexusError(f\"Formatting Error in codonposset: {options} \")        else:            self.codonposset = codonname[0]",
        "labels_text": "Read codon position from a codon block a written from McClade PRIVATE Here codonposset is just a fancy name for a character partition with the name CodonPositions and the partition N"
    },
    {
        "input_text": "summarize: def _get_indices(self, options, set_type=CHARSET, separator=\"=\"):        r        opts = CharBuffer(options)        name = self._name_n_vector(opts, separator=separator)        indices = self._parse_list(opts, set_type=set_type)        if indices is None:            raise NexusError(f\"Formatting error in line: {options} \")        return name, indices",
        "labels_text": "Parse the taxsetcharset specification PRIVATE eg dog cat dogcat"
    },
    {
        "input_text": "summarize: def invert(self, charlist):                return [c for c in range(self.nchar) if c not in charlist]",
        "labels_text": "Return all character index that are not in charlist"
    },
    {
        "input_text": "summarize: def gaponly(self, include_missing=False):                gap = set(self.gap)        if include_missing:            gap.add(self.missing)        sitesm = zip(*(str(self.matrix[t]) for t in self.taxlabels))        return [i for i, site in enumerate(sitesm) if set(site).issubset(gap)]",
        "labels_text": "Return gaponly site"
    },
    {
        "input_text": "summarize: def __init__(self) -> None:                self.chain: dict[int, Node] = {}        self.id = -1",
        "labels_text": "Initialize a node chain"
    },
    {
        "input_text": "summarize: def _get_id(self) -> int:                self.id += 1        return self.id",
        "labels_text": "Get a new id for a node in the chain PRIVATE"
    },
    {
        "input_text": "summarize: def all_ids(self) -> list[int]:                return list(self.chain.keys())",
        "labels_text": "Return a list of all node id"
    },
    {
        "input_text": "summarize: def add(self, node: \"Node\", prev: Optional[int] = None) -> int:                if prev is not None and prev not in self.chain:            raise ChainException(\"Unknown predecessor: \" + str(prev))        else:            id = self._get_id()            node.set_id(id)            node.set_prev(prev)            if prev is not None:                self.chain[prev].add_succ(id)            self.chain[id] = node        return id",
        "labels_text": "Attach node to another"
    },
    {
        "input_text": "summarize: def kill(self, id):                if id not in self.chain:            raise ChainException(\"Unknown ID: \" + str(id))        else:            del self.chain[id]",
        "labels_text": "Kill a node from chain without caring to what it is connected"
    },
    {
        "input_text": "summarize: def unlink(self, id):                if id not in self.chain:            raise ChainException(\"Unknown ID: \" + str(id))        else:            prev_id = self.chain[id].prev            if prev_id is not None:                self.chain[prev_id].succ.pop(self.chain[prev_id].succ.index(id))            self.chain[id].prev = None            return prev_id",
        "labels_text": "Disconnect node from his predecessor"
    },
    {
        "input_text": "summarize: def link(self, parent, child):                if child not in self.chain:            raise ChainException(\"Unknown ID: \" + str(child))        elif parent not in self.chain:            raise ChainException(\"Unknown ID: \" + str(parent))        else:            self.unlink(child)            self.chain[parent].succ.append(child)            self.chain[child].set_prev(parent)",
        "labels_text": "Connect son to parent"
    },
    {
        "input_text": "summarize: def is_parent_of(self, parent, grandchild):                if grandchild == parent or grandchild in self.chain[parent].get_succ():            return True        else:            for sn in self.chain[parent].get_succ():                if self.is_parent_of(sn, grandchild):                    return True            else:                return False",
        "labels_text": "Check if grandchild is a subnode of parent"
    },
    {
        "input_text": "summarize: def trace(self, start, finish):                if start not in self.chain or finish not in self.chain:            raise NodeException(\"Unknown node.\")        if not self.is_parent_of(start, finish) or start == finish:            return []        for sn in self.chain[start].get_succ():            if self.is_parent_of(sn, finish):                return [sn] + self.trace(sn, finish)",
        "labels_text": "Return a list of all nodeids between two node excluding start including end"
    },
    {
        "input_text": "summarize: def __init__(self, data=None):                self.id = None        self.data = data        self.prev = None        self.succ = []",
        "labels_text": "Represent a node with one predecessor and multiple successor"
    },
    {
        "input_text": "summarize: def set_id(self, id):                if self.id is not None:            raise NodeException(\"Node id cannot be changed.\")        self.id = id",
        "labels_text": "Set the id of a node if not set yet"
    },
    {
        "input_text": "summarize: def get_id(self):                return self.id",
        "labels_text": "Return the node id"
    },
    {
        "input_text": "summarize: def get_succ(self):                return self.succ",
        "labels_text": "Return a list of the node successor"
    },
    {
        "input_text": "summarize: def get_prev(self):                return self.prev",
        "labels_text": "Return the id of the node predecessor"
    },
    {
        "input_text": "summarize: def add_succ(self, id):                if isinstance(id, type([])):            self.succ.extend(id)        else:            self.succ.append(id)",
        "labels_text": "Add a node id to the node successor"
    },
    {
        "input_text": "summarize: def remove_succ(self, id):                self.succ.remove(id)",
        "labels_text": "Remove a node id from the node successor"
    },
    {
        "input_text": "summarize: def set_succ(self, new_succ):                if not isinstance(new_succ, type([])):            raise NodeException(\"Node successor must be of list type.\")        self.succ = new_succ",
        "labels_text": "Set the node successor"
    },
    {
        "input_text": "summarize: def set_prev(self, id):                self.prev = id",
        "labels_text": "Set the node predecessor"
    },
    {
        "input_text": "summarize: def get_data(self):                return self.data",
        "labels_text": "Return a node data"
    },
    {
        "input_text": "summarize: def set_data(self, data):                self.data = data",
        "labels_text": "Set a node data"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._data)",
        "labels_text": "Return the length of the coding use lenmycoding"
    },
    {
        "input_text": "summarize: def __getitem__(self, arg):                return self._data[arg]",
        "labels_text": "Pull out child by index"
    },
    {
        "input_text": "summarize: def __iter__(self):                return self",
        "labels_text": "Iterate over the item"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return_coding = self._data[self._current_pos]        except IndexError:            self._current_pos = 0            raise StopIteration from None        else:            self._current_pos += 1            return return_coding",
        "labels_text": "Return next item"
    },
    {
        "input_text": "summarize: def raw(self):                return self._data",
        "labels_text": "Return the full coding a a python list"
    },
    {
        "input_text": "summarize: def __str__(self):                str_return = \"\"        for coding in self._data:            if coding[\"t\"] == \"multi\":                str_return += \"(\" + \"\".join(coding[\"d\"]) + \")\"            elif coding[\"t\"] == \"uncer\":                str_return += \"{\" + \"\".join(coding[\"d\"]) + \"}\"            else:                str_return += coding[\"d\"][0]        return str_return",
        "labels_text": "Return the full coding a a python string use strmycoding"
    },
    {
        "input_text": "summarize: def __init__(self, taxon=None, branchlength=0.0, support=None, comment=None):                self.taxon = taxon        self.branchlength = branchlength        self.support = support        self.comment = comment",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _walk(self, node=None):                if node is None:            node = self.root        for n in self.node(node).succ:            yield n            yield from self._walk(n)",
        "labels_text": "Return all nodeids downwards from a node PRIVATE"
    },
    {
        "input_text": "summarize: def node(self, node_id):                if node_id not in self.chain:            raise TreeError(\"Unknown node_id: %d\" % node_id)        return self.chain[node_id]",
        "labels_text": "Return the instance of nodeid node nodeselfnodeid"
    },
    {
        "input_text": "summarize: def search_taxon(self, taxon):                for id, node in self.chain.items():            if node.data.taxon == taxon:                return id        return None",
        "labels_text": "Return the first matching taxon in selfdatataxon Not restricted to terminal node nodeid searchtaxonselftaxon"
    },
    {
        "input_text": "summarize: def get_terminals(self):                return [i for i in self.all_ids() if self.node(i).succ == []]",
        "labels_text": "Return a list of all terminal node"
    },
    {
        "input_text": "summarize: def is_terminal(self, node):                return self.node(node).succ == []",
        "labels_text": "Return True if node is a terminal node"
    },
    {
        "input_text": "summarize: def is_internal(self, node):                return len(self.node(node).succ) > 0",
        "labels_text": "Return True if node is an internal node"
    },
    {
        "input_text": "summarize: def is_preterminal(self, node):                if self.is_terminal(node):            return False not in [self.is_terminal(n) for n in self.node(node).succ]        else:            return False",
        "labels_text": "Return True if all successor of a node are terminal one"
    },
    {
        "input_text": "summarize: def count_terminals(self, node=None):                if node is None:            node = self.root        return len([n for n in self._walk(node) if self.is_terminal(n)])",
        "labels_text": "Count the number of terminal node that are attached to a node"
    },
    {
        "input_text": "summarize: def sum_branchlength(self, root=None, node=None):                if root is None:            root = self.root        if node is None:            raise TreeError(\"Missing node id.\")        blen = 0.0        while node is not None and node is not root:            blen += self.node(node).data.branchlength            node = self.node(node).prev        return blen",
        "labels_text": "Add up the branchlengths from root default selfroot to node sum sumbranchlengthselfrootNonenodeNone"
    },
    {
        "input_text": "summarize: def is_identical(self, tree2):                return self.set_subtree(self.root) == tree2.set_subtree(tree2.root)",
        "labels_text": "Compare tree and tree for identity result isidenticalselftree"
    },
    {
        "input_text": "summarize: def common_ancestor(self, node1, node2):                l1 = [self.root] + self.trace(self.root, node1)        l2 = [self.root] + self.trace(self.root, node2)        return [n for n in l1 if n in l2][-1]",
        "labels_text": "Return the common ancestor that connects two node nodeid commonancestorselfnodenode"
    },
    {
        "input_text": "summarize: def distance(self, node1, node2):                ca = self.common_ancestor(node1, node2)        return self.sum_branchlength(ca, node1) + self.sum_branchlength(ca, node2)",
        "labels_text": "Add and return the sum of the branchlengths between two node dist distanceselfnodenode"
    },
    {
        "input_text": "summarize: def branchlength2support(self):                for n in self.chain:            self.node(n).data.support = self.node(n).data.branchlength            self.node(n).data.branchlength = 0.0",
        "labels_text": "Move value stored in databranchlength to datasupport and set branchlength to This is necessary when support ha been stored a branchlength eg paup and ha thus been read in a branchlength"
    },
    {
        "input_text": "summarize: def convert_absolute_support(self, nrep):                for n in self._walk():            if self.node(n).data.support:                self.node(n).data.support /= nrep",
        "labels_text": "Convert absolute support cladecount to rel frequency Some software eg PHYLIP consense just calculate how often clade appear instead of calculating relative frequency"
    },
    {
        "input_text": "summarize: def has_support(self, node=None):                for n in self._walk(node):            if self.node(n).data.support:                return True        else:            return False",
        "labels_text": "Return True if any of the node ha datasupport None"
    },
    {
        "input_text": "summarize: def __str__(self):                return self.to_string(plain=True)",
        "labels_text": "Short version of tostring give plain tree"
    },
    {
        "input_text": "summarize: def __init__(self, entry, headline):                # Holds all fields from input line in a dictionary        # keys are data labels from the .xpk header        datlist = entry.split()        headlist = headline.split()        self.fields = dict(zip(headlist, datlist[1:]))        try:            self.fields[\"entrynum\"] = datlist[0]        except IndexError:            pass",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def replace_entry(line, fieldn, newentry):        # This method depends on xpktools._find_start_entry    start = _find_start_entry(line, fieldn)    leng = len(line[start:].split()[0])    newline = line[:start] + str(newentry) + line[(start + leng) :]    return newline",
        "labels_text": "Replace an entry in a string by the field number No padding is implemented currently Spacing will change if the original field entry and the new field entry are of different length"
    },
    {
        "input_text": "summarize: def _read_dicts(fn_list, keyatom):        dict_list = []    datalabel_list = []    for fn in fn_list:        peaklist = Peaklist(fn)        dictionary = peaklist.residue_dict(keyatom)        dict_list.append(dictionary)        datalabel_list.append(peaklist.datalabels)    return [dict_list, datalabel_list]",
        "labels_text": "Read multiple file into a list of residue dictionary PRIVATE"
    },
    {
        "input_text": "summarize: def __eq__(self, r):                return (            isinstance(r, Reaction)            and self.reactants == r.reactants            and self.catalysts == r.catalysts            and self.data == r.data            and self.reversible == r.reversible        )",
        "labels_text": "Return true iff self is equal to r"
    },
    {
        "input_text": "summarize: def __hash__(self):                t = tuple(self.species())        return hash(t)",
        "labels_text": "Return a hashcode for self"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"Reaction(%r, %r, %r, %r)\" % (            self.reactants,            self.catalysts,            self.reversible,            self.data,        )",
        "labels_text": "Return a debugging string representation of self"
    },
    {
        "input_text": "summarize: def reverse(self):                reactants = {}        for r in self.reactants:            reactants[r] = -self.reactants[r]        return Reaction(reactants, self.catalysts, self.reversible, self.data)",
        "labels_text": "Return a new Reaction that is the reverse of self"
    },
    {
        "input_text": "summarize: def species(self):                return list(self.reactants)",
        "labels_text": "Return a list of all Species involved in self"
    },
    {
        "input_text": "summarize: def __init__(self, reactions=()):                self.__reactions = set(reactions)",
        "labels_text": "Initialize a new System object"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"System(\" + \",\".join(map(repr, self.__reactions)) + \")\"",
        "labels_text": "Return a debugging string representation of self"
    },
    {
        "input_text": "summarize: def __str__(self):                return (            \"System of \"            + str(len(self.__reactions))            + \" reactions involving \"            + str(len(self.species()))            + \" species\"        )",
        "labels_text": "Return a string representation of self"
    },
    {
        "input_text": "summarize: def add_reaction(self, reaction):                self.__reactions.add(reaction)",
        "labels_text": "Add reaction to self"
    },
    {
        "input_text": "summarize: def remove_reaction(self, reaction):                self.__reactions.remove(reaction)",
        "labels_text": "Remove reaction from self"
    },
    {
        "input_text": "summarize: def reactions(self):                # TODO - Define __lt__ so that Reactions can be sorted on Python?        return list(self.__reactions)",
        "labels_text": "Return a list of the reaction in this system Note the order is arbitrary"
    },
    {
        "input_text": "summarize: def species(self):                return sorted(            set(reduce(lambda s, x: s + x, [x.species() for x in self.reactions()], []))        )",
        "labels_text": "Return a list of the specie in this system"
    },
    {
        "input_text": "summarize: def __hash__(self):                return hash(self.data)",
        "labels_text": "Return a hashcode for self"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"Interaction(\" + repr(self.data) + \")\"",
        "labels_text": "Return a debugging string representation of self"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"<\" + str(self.data) + \">\"",
        "labels_text": "Return a string representation of self"
    },
    {
        "input_text": "summarize: def __init__(self, species=()):                self.__graph = MultiGraph(species)",
        "labels_text": "Initialize a new Network object"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"<Network: __graph: \" + repr(self.__graph) + \">\"",
        "labels_text": "Return a debugging string representation of this network"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"Network of %i species and %i interactions.\" % (            len(self.species()),            len(self.interactions()),        )",
        "labels_text": "Return a string representation of this network"
    },
    {
        "input_text": "summarize: def add_species(self, species):                self.__graph.add_node(species)",
        "labels_text": "Add specie to this network"
    },
    {
        "input_text": "summarize: def add_interaction(self, source, sink, interaction):                self.__graph.add_edge(source, sink, interaction)",
        "labels_text": "Add interaction to this network"
    },
    {
        "input_text": "summarize: def source(self, species):                return self.__graph.parents(species)",
        "labels_text": "Return list of unique source for specie"
    },
    {
        "input_text": "summarize: def source_interactions(self, species):                return self.__graph.parent_edges(species)",
        "labels_text": "Return list of source interaction pair for specie"
    },
    {
        "input_text": "summarize: def sink(self, species):                return self.__graph.children(species)",
        "labels_text": "Return list of unique sink for specie"
    },
    {
        "input_text": "summarize: def sink_interactions(self, species):                return self.__graph.child_edges(species)",
        "labels_text": "Return list of sink interaction pair for specie"
    },
    {
        "input_text": "summarize: def species(self):                return self.__graph.nodes()",
        "labels_text": "Return list of the specie in this network"
    },
    {
        "input_text": "summarize: def interactions(self):                return self.__graph.labels()",
        "labels_text": "Return list of the unique interaction in this network"
    },
    {
        "input_text": "summarize: def __init__(self, nodes=()):                self._adjacency_list = {}  # maps parent -> set of child objects        for n in nodes:            self._adjacency_list[n] = set()        self._label_map = {}  # maps label -> set of (parent, child) pairs        self._edge_map = {}",
        "labels_text": "Initialize a new Graph object"
    },
    {
        "input_text": "summarize: def __eq__(self, g):                return (            isinstance(g, Graph)            and self._adjacency_list == g._adjacency_list            and self._label_map == g._label_map            and self._edge_map == g._edge_map        )",
        "labels_text": "Return true if g is equal to this graph"
    },
    {
        "input_text": "summarize: def __repr__(self):                s = \"<Graph: \"        for key in sorted(self._adjacency_list):            values = sorted(                (x, self._edge_map[(key, x)]) for x in list(self._adjacency_list[key])            )            s += f\"({key!r}: {','.join(repr(v) for v in values)})\"        return s + \">\"",
        "labels_text": "Return a unique string representation of this graph"
    },
    {
        "input_text": "summarize: def add_node(self, node):                if node not in self._adjacency_list:            self._adjacency_list[node] = set()",
        "labels_text": "Add a node to this graph"
    },
    {
        "input_text": "summarize: def child_edges(self, parent):                if parent not in self._adjacency_list:            raise ValueError(\"Unknown <parent> node: \" + str(parent))        return [            (x, self._edge_map[(parent, x)])            for x in sorted(self._adjacency_list[parent])        ]",
        "labels_text": "Return a list of child label pair for parent"
    },
    {
        "input_text": "summarize: def children(self, parent):                return sorted(self._adjacency_list[parent])",
        "labels_text": "Return a list of unique child for parent"
    },
    {
        "input_text": "summarize: def edges(self, label):                if label not in self._label_map:            raise ValueError(\"Unknown label: \" + str(label))        return sorted(self._label_map[label])",
        "labels_text": "Return a list of all the edge with this label"
    },
    {
        "input_text": "summarize: def labels(self):                return sorted(self._label_map.keys())",
        "labels_text": "Return a list of all the edge label in this graph"
    },
    {
        "input_text": "summarize: def nodes(self):                return list(self._adjacency_list.keys())",
        "labels_text": "Return a list of the node in this graph"
    },
    {
        "input_text": "summarize: def parent_edges(self, child):                if child not in self._adjacency_list:            raise ValueError(\"Unknown <child> node: \" + str(child))        parents = []        for parent, children in self._adjacency_list.items():            for x in children:                if x == child:                    parents.append((parent, self._edge_map[(parent, child)]))        return sorted(parents)",
        "labels_text": "Return a list of parent label pair for child"
    },
    {
        "input_text": "summarize: def parents(self, child):                return sorted({x[0] for x in self.parent_edges(child)})",
        "labels_text": "Return a list of unique parent for child"
    },
    {
        "input_text": "summarize: def remove_edge(self, parent, child, label):                # hm , this is a multigraph - how should this be implemented?        raise NotImplementedError(\"remove_edge is not yet implemented\")",
        "labels_text": "Remove edge NOT IMPLEMENTED"
    },
    {
        "input_text": "summarize: def __init__(self, nodes=()):                self._adjacency_list = {}  # maps parent -> set of (child, label) pairs        for n in nodes:            self._adjacency_list[n] = set()        self._label_map = {}",
        "labels_text": "Initialize a new MultiGraph object"
    },
    {
        "input_text": "summarize: def __eq__(self, g):                return (            isinstance(g, MultiGraph)            and self._adjacency_list == g._adjacency_list            and self._label_map == g._label_map        )",
        "labels_text": "Return true if g is equal to this graph"
    },
    {
        "input_text": "summarize: def __repr__(self):                s = \"<MultiGraph: \"        for key in sorted(self._adjacency_list):            values = sorted(self._adjacency_list[key])            s += f\"({key!r}: {','.join(repr(v) for v in values)})\"        return s + \">\"",
        "labels_text": "Return a unique string representation of this graph"
    },
    {
        "input_text": "summarize: def add_node(self, node):                if node not in self._adjacency_list:            self._adjacency_list[node] = set()",
        "labels_text": "Add a node to this graph"
    },
    {
        "input_text": "summarize: def child_edges(self, parent):                if parent not in self._adjacency_list:            raise ValueError(\"Unknown <parent> node: \" + str(parent))        return sorted(self._adjacency_list[parent])",
        "labels_text": "Return a list of child label pair for parent"
    },
    {
        "input_text": "summarize: def children(self, parent):                return sorted({x[0] for x in self.child_edges(parent)})",
        "labels_text": "Return a list of unique child for parent"
    },
    {
        "input_text": "summarize: def edges(self, label):                if label not in self._label_map:            raise ValueError(\"Unknown label: \" + str(label))        return sorted(self._label_map[label])",
        "labels_text": "Return a list of all the edge with this label"
    },
    {
        "input_text": "summarize: def labels(self):                return sorted(self._label_map.keys())",
        "labels_text": "Return a list of all the edge label in this graph"
    },
    {
        "input_text": "summarize: def nodes(self):                return list(self._adjacency_list.keys())",
        "labels_text": "Return a list of the node in this graph"
    },
    {
        "input_text": "summarize: def parent_edges(self, child):                if child not in self._adjacency_list:            raise ValueError(\"Unknown <child> node: \" + str(child))        parents = []        for parent, children in self._adjacency_list.items():            for x in children:                if x[0] == child:                    parents.append((parent, x[1]))        return sorted(parents)",
        "labels_text": "Return a list of parent label pair for child"
    },
    {
        "input_text": "summarize: def parents(self, child):                return sorted({x[0] for x in self.parent_edges(child)})",
        "labels_text": "Return a list of unique parent for child"
    },
    {
        "input_text": "summarize: def remove_edge(self, parent, child, label):                # hm , this is a multigraph - how should this be implemented?        raise NotImplementedError(\"remove_edge is not yet implemented\")",
        "labels_text": "Remove edge NOT IMPLEMENTED"
    },
    {
        "input_text": "summarize: def __init__(self, property_dict, property_keys, property_list):                self.property_dict = property_dict        self.property_keys = property_keys        self.property_list = property_list",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _translate_id(self, entity_id):                return entity_id",
        "labels_text": "Return entity identifier PRIVATE"
    },
    {
        "input_text": "summarize: def __contains__(self, id):                translated_id = self._translate_id(id)        return translated_id in self.property_dict",
        "labels_text": "Check if the mapping ha a property for this residue param chainid chain id type chainid char param resid residue id type resid char Examples This is an incomplete but illustrative example if chainid resid in apmap re prop apmapchainid resid"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                translated_id = self._translate_id(key)        return self.property_dict[translated_id]",
        "labels_text": "Return property for a residue param chainid chain id type chainid char param resid residue id type resid int or char int char return some residue property rtype anything can be a tuple"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.property_dict)",
        "labels_text": "Return number of residue for which the property is available return number of residue rtype int"
    },
    {
        "input_text": "summarize: def keys(self):                return self.property_keys",
        "labels_text": "Return the list of residue return list of residue for which the property wa calculated rtype chainid resid chainid resid"
    },
    {
        "input_text": "summarize: def __iter__(self):                for i in range(len(self.property_list)):            yield self.property_list[i]",
        "labels_text": "Iterate over the entity property list Handy alternative to the dictionarylike access return iterator Examples entitypropertylist entity property entity property map AbstractPropertyMap entitypropertylist for re property in itermap printres property entity property entity property"
    },
    {
        "input_text": "summarize: def __init__(self, property_dict, property_keys, property_list):                AbstractPropertyMap.__init__(self, property_dict, property_keys, property_list)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _translate_id(self, ent_id):                chain_id, res_id = ent_id        if isinstance(res_id, int):            ent_id = (chain_id, (\" \", res_id, \" \"))        return ent_id",
        "labels_text": "Return entity identifier on residue PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, property_dict, property_keys, property_list):                AbstractPropertyMap.__init__(self, property_dict, property_keys, property_list)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _translate_id(self, ent_id):                if len(ent_id) == 4:            chain_id, res_id, atom_name, icode = ent_id        else:            chain_id, res_id, atom_name = ent_id            icode = None        if isinstance(res_id, int):            ent_id = (chain_id, (\" \", res_id, \" \"), atom_name, icode)        return ent_id",
        "labels_text": "Return entity identifier on atom PRIVATE"
    },
    {
        "input_text": "summarize: def get_predictions(qualifier: str) -> Iterator[dict]:        url = f\"https://alphafold.com/api/prediction/{qualifier}\"    # Retrieve the AlphaFold predictions with urllib    with urlopen(url) as response:        yield from json.loads(response.read().decode())",
        "labels_text": "Get all AlphaFold prediction for a UniProt accession param qualifier A UniProt accession eg P type qualifier str return The AlphaFold prediction rtype Iteratordict"
    },
    {
        "input_text": "summarize: def _get_mmcif_file_path_for(    prediction: dict, directory: Optional[Union[str, bytes, PathLike]] = None) -> str:        if directory is None:        directory = os.getcwd()    cif_url = prediction[\"cifUrl\"]    # Get the file name from the URL    file_name = cif_url.split(\"/\")[-1]    return str(os.path.join(directory, file_name))",
        "labels_text": "Get the path to the mmCIF file for an AlphaFold prediction param prediction An AlphaFold prediction type prediction dict param directory The directory that store the mmCIF file default to the current working directory type directory Unionint str byte PathLike optional return The path to the mmCIF file rtype str"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if isinstance(other, Atom):            return self.full_id[1:] == other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test equality"
    },
    {
        "input_text": "summarize: def __ne__(self, other):                if isinstance(other, Atom):            return self.full_id[1:] != other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test inequality"
    },
    {
        "input_text": "summarize: def __hash__(self):                return hash(self.get_full_id())",
        "labels_text": "Return atom full identifier"
    },
    {
        "input_text": "summarize: def _assign_atom_mass(self):                try:            return IUPACData.atom_weights[self.element.capitalize()]        except (AttributeError, KeyError):            return float(\"NaN\")",
        "labels_text": "Return atom weight PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"<Atom {self.get_id()}>\"",
        "labels_text": "Print Atom object a Atom atomname"
    },
    {
        "input_text": "summarize: def __sub__(self, other):                diff = self.coord - other.coord        return np.sqrt(np.dot(diff, diff))",
        "labels_text": "Calculate distance between two atom param other the other atom type other LAtom Examples This is an incomplete but illustrative example distance atom atom"
    },
    {
        "input_text": "summarize: def set_serial_number(self, n):                self.serial_number = n",
        "labels_text": "Set serial number"
    },
    {
        "input_text": "summarize: def set_bfactor(self, bfactor):                self.bfactor = bfactor",
        "labels_text": "Set isotroptic B factor"
    },
    {
        "input_text": "summarize: def set_coord(self, coord):                self.coord = coord",
        "labels_text": "Set coordinate"
    },
    {
        "input_text": "summarize: def set_altloc(self, altloc):                self.altloc = altloc",
        "labels_text": "Set alternative location specifier"
    },
    {
        "input_text": "summarize: def set_occupancy(self, occupancy):                self.occupancy = occupancy",
        "labels_text": "Set occupancy"
    },
    {
        "input_text": "summarize: def set_sigatm(self, sigatm_array):                self.sigatm_array = sigatm_array",
        "labels_text": "Set standard deviation of atomic parameter The standard deviation of atomic parameter consists of positional B factor and occupancy standard deviation param sigatmarray standard deviation of atomic parameter type sigatmarray NumPy array length"
    },
    {
        "input_text": "summarize: def set_siguij(self, siguij_array):                self.siguij_array = siguij_array",
        "labels_text": "Set standard deviation of anisotropic temperature factor param siguijarray standard deviation of anisotropic temperature factor type siguijarray NumPy array length"
    },
    {
        "input_text": "summarize: def set_anisou(self, anisou_array):                self.anisou_array = anisou_array",
        "labels_text": "Set anisotropic B factor param anisouarray anisotropic B factor type anisouarray NumPy array length"
    },
    {
        "input_text": "summarize: def set_charge(self, pqr_charge):                self.pqr_charge = pqr_charge",
        "labels_text": "Set charge"
    },
    {
        "input_text": "summarize: def set_radius(self, radius):                self.radius = radius",
        "labels_text": "Set radius"
    },
    {
        "input_text": "summarize: def flag_disorder(self):                self.disordered_flag = 1",
        "labels_text": "Set the disordered flag to The disordered flag indicates whether the atom is disordered or not"
    },
    {
        "input_text": "summarize: def is_disordered(self):                return self.disordered_flag",
        "labels_text": "Return the disordered flag if disordered otherwise"
    },
    {
        "input_text": "summarize: def set_parent(self, parent):                self.parent = parent        self.full_id = self.get_full_id()",
        "labels_text": "Set the parent residue Arguments parent Residue object"
    },
    {
        "input_text": "summarize: def detach_parent(self):                self.parent = None",
        "labels_text": "Remove reference to parent"
    },
    {
        "input_text": "summarize: def get_sigatm(self):                return self.sigatm_array",
        "labels_text": "Return standard deviation of atomic parameter"
    },
    {
        "input_text": "summarize: def get_siguij(self):                return self.siguij_array",
        "labels_text": "Return standard deviation of anisotropic temperature factor"
    },
    {
        "input_text": "summarize: def get_anisou(self):                return self.anisou_array",
        "labels_text": "Return anisotropic B factor"
    },
    {
        "input_text": "summarize: def get_parent(self):                return self.parent",
        "labels_text": "Return parent residue"
    },
    {
        "input_text": "summarize: def get_serial_number(self):                return self.serial_number",
        "labels_text": "Return the serial number"
    },
    {
        "input_text": "summarize: def get_name(self):                return self.name",
        "labels_text": "Return atom name"
    },
    {
        "input_text": "summarize: def get_id(self):                return self.id",
        "labels_text": "Return the id of the atom which is it atom name"
    },
    {
        "input_text": "summarize: def get_full_id(self):                try:            return self.parent.get_full_id() + ((self.name, self.altloc),)        except AttributeError:            return (None, None, None, None, self.name, self.altloc)",
        "labels_text": "Return the full id of the atom The full id of an atom is a tuple used to uniquely identify the atom and consists of the following element structure id model id chain id residue id atom name altloc"
    },
    {
        "input_text": "summarize: def get_coord(self):                return self.coord",
        "labels_text": "Return atomic coordinate"
    },
    {
        "input_text": "summarize: def get_bfactor(self):                return self.bfactor",
        "labels_text": "Return B factor"
    },
    {
        "input_text": "summarize: def get_occupancy(self):                return self.occupancy",
        "labels_text": "Return occupancy"
    },
    {
        "input_text": "summarize: def get_fullname(self):                return self.fullname",
        "labels_text": "Return the atom name including leading and trailing space"
    },
    {
        "input_text": "summarize: def get_altloc(self):                return self.altloc",
        "labels_text": "Return alternative location specifier"
    },
    {
        "input_text": "summarize: def get_level(self):                return self.level",
        "labels_text": "Return level"
    },
    {
        "input_text": "summarize: def get_charge(self):                return self.pqr_charge",
        "labels_text": "Return charge"
    },
    {
        "input_text": "summarize: def get_radius(self):                return self.radius",
        "labels_text": "Return radius"
    },
    {
        "input_text": "summarize: def transform(self, rot, tran):                self.coord = np.dot(self.coord, rot) + tran",
        "labels_text": "Apply rotation and translation to the atomic coordinate param rot A right multiplying rotation matrix type rot x NumPy array param tran the translation vector type tran size NumPy array Examples This is an incomplete but illustrative example from numpy import pi array from BioPDBvectors import Vector rotmat rotation rotmatpi Vector translation array f atomtransformrotation translation"
    },
    {
        "input_text": "summarize: def get_vector(self):                x, y, z = self.coord        return Vector(x, y, z)",
        "labels_text": "Return coordinate a Vector return coordinate a D vector rtype BioPDBVector class"
    },
    {
        "input_text": "summarize: def copy(self):                # Do a shallow copy then explicitly copy what needs to be deeper.        shallow = copy.copy(self)        shallow.detach_parent()        shallow.set_coord(copy.copy(self.get_coord()))        shallow.xtra = self.xtra.copy()        return shallow",
        "labels_text": "Create a copy of the Atom Parent information is lost"
    },
    {
        "input_text": "summarize: def __init__(self, id):                # TODO - make this a private attribute?        self.last_occupancy = -sys.maxsize        DisorderedEntityWrapper.__init__(self, id)",
        "labels_text": "Create DisorderedAtom Arguments id string atom name"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self.disordered_get_list()",
        "labels_text": "Iterate through disordered atom"
    },
    {
        "input_text": "summarize: def __repr__(self):                if self.child_dict:            return f\"<DisorderedAtom {self.get_id()}>\"        else:            return f\"<Empty DisorderedAtom {self.get_id()}>\"",
        "labels_text": "Return disordered atom identifier"
    },
    {
        "input_text": "summarize: def center_of_mass(self):                children = self.disordered_get_list()        if not children:            raise ValueError(f\"{self} does not have children\")        coords = np.asarray([a.coord for a in children], dtype=np.float32)        return np.average(coords, axis=0, weights=None)",
        "labels_text": "Return the center of mass of the DisorderedAtom a a numpy array Assumes all child atom have the same mass same element"
    },
    {
        "input_text": "summarize: def disordered_get_list(self):                return sorted(self.child_dict.values(), key=lambda a: ord(a.altloc))",
        "labels_text": "Return list of atom instance Sorts child by altloc empty then alphabetical"
    },
    {
        "input_text": "summarize: def transform(self, rot, tran):                for child in self:            child.coord = np.dot(child.coord, rot) + tran",
        "labels_text": "Apply rotation and translation to all child See the documentation of Atomtransform for detail"
    },
    {
        "input_text": "summarize: def __init__(self):                self._structure_builder = StructureBuilder()",
        "labels_text": "Initialize a BinaryCIF parser"
    },
    {
        "input_text": "summarize: def __init__(self, window_size=8, max_gap=30):                assert window_size > 0, \"window_size must be greater than 0\"        assert max_gap >= 0, \"max_gap must be positive (or zero)\"        self.window_size = window_size        self.max_gap = max_gap        self.rms = None        self._rigid_motion = None        self.refcoord = None        self._coord = None        self._superimposer = QCPSuperimposer()",
        "labels_text": "Superimpose one set of atom onto another using structural data Structures are superimposed using guide atom CA and C for protein and nucleic acid molecule respectively Parameters windowsize float optional CE algorithm parameter Used to define path when building the CE similarity matrix Default is maxgap float optional CE algorithm parameter Maximum gap size Default is"
    },
    {
        "input_text": "summarize: def set_reference(self, structure):                self.refcoord = self.get_guide_coord_from_structure(structure)        if len(self.refcoord) < self.window_size * 2:            n_atoms = len(self.refcoord)            msg = (                f\"Too few atoms in the reference structure ({n_atoms}). \"                \"Try reducing the window_size parameter.\"            )            raise PDBException(msg)",
        "labels_text": "Define a reference structure onto which all others will be aligned"
    },
    {
        "input_text": "summarize: def __init__(self, id):                self.level = \"C\"        self.internal_coord = None        Entity.__init__(self, id)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __gt__(self, other):                if isinstance(other, Chain):            if self.id == \" \" and other.id != \" \":                return 0            elif self.id != \" \" and other.id == \" \":                return 1            else:                return self.id > other.id        else:            return NotImplemented",
        "labels_text": "Validate if id is greater than otherid"
    },
    {
        "input_text": "summarize: def __ge__(self, other):                if isinstance(other, Chain):            if self.id == \" \" and other.id != \" \":                return 0            elif self.id != \" \" and other.id == \" \":                return 1            else:                return self.id >= other.id        else:            return NotImplemented",
        "labels_text": "Validate if id is greater or equal than otherid"
    },
    {
        "input_text": "summarize: def __lt__(self, other):                if isinstance(other, Chain):            if self.id == \" \" and other.id != \" \":                return 0            elif self.id != \" \" and other.id == \" \":                return 1            else:                return self.id < other.id        else:            return NotImplemented",
        "labels_text": "Validate if id is less than otherid"
    },
    {
        "input_text": "summarize: def __le__(self, other):                if isinstance(other, Chain):            if self.id == \" \" and other.id != \" \":                return 0            elif self.id != \" \" and other.id == \" \":                return 1            else:                return self.id <= other.id        else:            return NotImplemented",
        "labels_text": "Validate if id is less or equal than other id"
    },
    {
        "input_text": "summarize: def _translate_id(self, id):                if isinstance(id, int):            id = (\" \", id, \" \")        return id",
        "labels_text": "Translate sequence identifier to tuple form PRIVATE A residue id is normally a tuple hetero flag sequence identifier insertion code Since for most residue the hetero flag and the insertion code are blank ie you can just use the sequence identifier to index a residue in a chain The translateid method translates the sequence identifier to the sequence identifier tuple Arguments id int residue resseq"
    },
    {
        "input_text": "summarize: def __getitem__(self, id):                id = self._translate_id(id)        return Entity.__getitem__(self, id)",
        "labels_text": "Return the residue with given id The id of a residue is hetero flag sequence identifier insertion code If id is an int it is translated to id by the translateid method Arguments id string int string or int"
    },
    {
        "input_text": "summarize: def __contains__(self, id):                id = self._translate_id(id)        return Entity.__contains__(self, id)",
        "labels_text": "Check if a residue with given id is present in this chain Arguments id string int string or int"
    },
    {
        "input_text": "summarize: def __delitem__(self, id):                id = self._translate_id(id)        return Entity.__delitem__(self, id)",
        "labels_text": "Delete item Arguments id string int string or int"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"<Chain id={self.get_id()}>\"",
        "labels_text": "Return the chain identifier"
    },
    {
        "input_text": "summarize: def get_unpacked_list(self):                unpacked_list = []        for residue in self.get_list():            if residue.is_disordered() == 2:                for dresidue in residue.disordered_get_list():                    unpacked_list.append(dresidue)            else:                unpacked_list.append(residue)        return unpacked_list",
        "labels_text": "Return a list of undisordered residue Some Residue object hide several disordered residue DisorderedResidue object This method unpacks them ie it return a list of simple Residue object"
    },
    {
        "input_text": "summarize: def has_id(self, id):                id = self._translate_id(id)        return Entity.has_id(self, id)",
        "labels_text": "Return if a residue with given id is present The id of a residue is hetero flag sequence identifier insertion code If id is an int it is translated to id by the translateid method Arguments id string int string or int"
    },
    {
        "input_text": "summarize: def get_residues(self):                yield from self",
        "labels_text": "Return residue"
    },
    {
        "input_text": "summarize: def get_atoms(self):                for r in self.get_residues():            yield from r",
        "labels_text": "Return atom from residue"
    },
    {
        "input_text": "summarize: def atom_to_internal_coordinates(self, verbose: bool = False) -> None:                if not self.internal_coord:            self.internal_coord = IC_Chain(self, verbose)        self.internal_coord.atom_to_internal_coordinates(verbose=verbose)",
        "labels_text": "Createupdate internal coordinate from Atom XYZ coordinate Internal coordinate are bond length angle and dihedral angle param verbose bool default False describe runtime problem"
    },
    {
        "input_text": "summarize: def __init__(self, chain_id, start, end, model_id=0):                self.chain_id = chain_id        self.start = start        self.end = end        self.model_id = model_id",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def accept_model(self, model):                # model - only keep model 0        if model.get_id() == self.model_id:            return 1        return 0",
        "labels_text": "Verify if model match the model identifier"
    },
    {
        "input_text": "summarize: def accept_chain(self, chain):                if chain.get_id() == self.chain_id:            return 1        return 0",
        "labels_text": "Verify if chain match chain identifier"
    },
    {
        "input_text": "summarize: def accept_residue(self, residue):                # residue - between start and end        hetatm_flag, resseq, icode = residue.get_id()        if hetatm_flag != \" \":            # skip HETATMS            return 0        if icode != \" \":            warnings.warn(                f\"WARNING: Icode {icode} at position {resseq}\", BiopythonWarning            )        if self.start <= resseq <= self.end:            return 1        return 0",
        "labels_text": "Verify if a residue sequence is between the start and end sequence"
    },
    {
        "input_text": "summarize: def accept_atom(self, atom):                # atoms - get rid of hydrogens        name = atom.get_id()        if _hydrogen.match(name):            return 0        else:            return 1",
        "labels_text": "Verify if atom are not Hydrogen"
    },
    {
        "input_text": "summarize: def extract(structure, chain_id, start, end, filename):        sel = ChainSelector(chain_id, start, end)    io = PDBIO()    io.set_structure(structure)    io.save(filename, sel)",
        "labels_text": "Write out selected portion to filename"
    },
    {
        "input_text": "summarize: def version(version_string):        return tuple(map(int, (version_string.split(\".\"))))",
        "labels_text": "Parse semantic version scheme for easy comparison"
    },
    {
        "input_text": "summarize: def ss_to_index(ss):        if ss == \"H\":        return 0    if ss == \"E\":        return 1    if ss == \"C\":        return 2    assert 0",
        "labels_text": "Secondary structure symbol to index H E C"
    },
    {
        "input_text": "summarize: def make_dssp_dict(filename):        with open(filename) as handle:        return _make_dssp_dict(handle)",
        "labels_text": "DSSP dictionary mapping identifier to property Return a DSSP dictionary that map chainid resid to aa s and accessibility from a DSSP file Parameters filename string the DSSP output file"
    },
    {
        "input_text": "summarize: def __init__(self, id):                self._id = id        self.full_id = None        self.parent = None        self.child_list = []        self.child_dict = {}        # Dictionary that keeps additional properties        self.xtra = {}",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.child_list)",
        "labels_text": "Return the number of child"
    },
    {
        "input_text": "summarize: def __getitem__(self, id):                return self.child_dict[id]",
        "labels_text": "Return the child with given id"
    },
    {
        "input_text": "summarize: def __delitem__(self, id):                return self.detach_child(id)",
        "labels_text": "Remove a child"
    },
    {
        "input_text": "summarize: def __contains__(self, id):                return id in self.child_dict",
        "labels_text": "Check if there is a child element with the given id"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self.child_list",
        "labels_text": "Iterate over child"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id == other.id            else:                return self.full_id[1:] == other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test for equality This compare fullid including the IDs of all parent"
    },
    {
        "input_text": "summarize: def __ne__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id != other.id            else:                return self.full_id[1:] != other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test for inequality"
    },
    {
        "input_text": "summarize: def __gt__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id > other.id            else:                return self.full_id[1:] > other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test greater than"
    },
    {
        "input_text": "summarize: def __ge__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id >= other.id            else:                return self.full_id[1:] >= other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test greater or equal"
    },
    {
        "input_text": "summarize: def __lt__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id < other.id            else:                return self.full_id[1:] < other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test less than"
    },
    {
        "input_text": "summarize: def __le__(self, other):                if isinstance(other, type(self)):            if self.parent is None:                return self.id <= other.id            else:                return self.full_id[1:] <= other.full_id[1:]        else:            return NotImplemented",
        "labels_text": "Test less or equal"
    },
    {
        "input_text": "summarize: def __hash__(self):                return hash(self.full_id)",
        "labels_text": "Hash method to allow uniqueness set"
    },
    {
        "input_text": "summarize: def _reset_full_id(self):                for child in self:            try:                child._reset_full_id()            except AttributeError:                pass  # Atoms do not cache their full ids.        self.full_id = self._generate_full_id()",
        "labels_text": "Reset the fullid PRIVATE Resets the fullid of this entity and recursively of all it child based on their ID"
    },
    {
        "input_text": "summarize: def _generate_full_id(self):                entity_id = self.get_id()        parts = [entity_id]        parent = self.get_parent()        while parent is not None:            entity_id = parent.get_id()            parts.append(entity_id)            parent = parent.get_parent()        parts.reverse()        return tuple(parts)",
        "labels_text": "Generate fullid PRIVATE Generate the fullid of the Entity based on it Id and the IDs of the parent"
    },
    {
        "input_text": "summarize: def id(self):                return self._id",
        "labels_text": "Return identifier"
    },
    {
        "input_text": "summarize: def get_level(self):                return self.level",
        "labels_text": "Return level in hierarchy A atom R residue C chain M model S structure"
    },
    {
        "input_text": "summarize: def set_parent(self, entity: _Parent):                self.parent = entity        self._reset_full_id()",
        "labels_text": "Set the parent Entity object"
    },
    {
        "input_text": "summarize: def detach_parent(self):                self.parent = None",
        "labels_text": "Detach the parent"
    },
    {
        "input_text": "summarize: def detach_child(self, id):                child = self.child_dict[id]        child.detach_parent()        del self.child_dict[id]        self.child_list.remove(child)",
        "labels_text": "Remove a child"
    },
    {
        "input_text": "summarize: def add(self, entity: _Child):                entity_id = entity.get_id()        if self.has_id(entity_id):            raise PDBConstructionException(f\"{entity_id} defined twice\")        entity.set_parent(self)        self.child_list.append(entity)        self.child_dict[entity_id] = entity",
        "labels_text": "Add a child to the Entity"
    },
    {
        "input_text": "summarize: def insert(self, pos: int, entity: _Child):                entity_id = entity.get_id()        if self.has_id(entity_id):            raise PDBConstructionException(f\"{entity_id} defined twice\")        entity.set_parent(self)        self.child_list[pos:pos] = [entity]        self.child_dict[entity_id] = entity",
        "labels_text": "Add a child to the Entity at a specified position"
    },
    {
        "input_text": "summarize: def get_iterator(self):                yield from self.child_list",
        "labels_text": "Return iterator over child"
    },
    {
        "input_text": "summarize: def get_list(self):                return copy(self.child_list)",
        "labels_text": "Return a copy of the list of child"
    },
    {
        "input_text": "summarize: def has_id(self, id):                return id in self.child_dict",
        "labels_text": "Check if a child with given id exists"
    },
    {
        "input_text": "summarize: def get_parent(self):                return self.parent",
        "labels_text": "Return the parent Entity object"
    },
    {
        "input_text": "summarize: def get_id(self):                return self.id",
        "labels_text": "Return the id"
    },
    {
        "input_text": "summarize: def get_full_id(self):                if self.full_id is None:            self.full_id = self._generate_full_id()        return self.full_id",
        "labels_text": "Return the full id The full id is a tuple containing all id starting from the top object Structure down to the current object A full id for a Residue object eg is something like abc A A This corresponds to Structure with id abc Model with id Chain with id A Residue with id A The Residue id indicates that the residue is not a heteroresidue or a water because it ha a blank hetero field that it sequence identifier is and it insertion code A"
    },
    {
        "input_text": "summarize: def transform(self, rot, tran):                for o in self.get_list():            o.transform(rot, tran)",
        "labels_text": "Apply rotation and translation to the atomic coordinate param rot A right multiplying rotation matrix type rot x NumPy array param tran the translation vector type tran size NumPy array Examples This is an incomplete but illustrative example from numpy import pi array from BioPDBvectors import Vector rotmat rotation rotmatpi Vector translation array f entitytransformrotation translation"
    },
    {
        "input_text": "summarize: def copy(self):                shallow = copy(self)        shallow.child_list = []        shallow.child_dict = {}        shallow.xtra = copy(self.xtra)        shallow.detach_parent()        for child in self.child_list:            shallow.add(child.copy())        return shallow",
        "labels_text": "Copy entity recursively"
    },
    {
        "input_text": "summarize: def __init__(self, id):                self.id = id        self.child_dict = {}        self.selected_child = None        self.parent = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __getattr__(self, method):                if method == \"__setstate__\":            # Avoid issues with recursion when attempting deepcopy            raise AttributeError        if not hasattr(self, \"selected_child\"):            # Avoid problems with pickling            # Unpickling goes into infinite loop!            raise AttributeError        return getattr(self.selected_child, method)",
        "labels_text": "Forward the method call to the selected child"
    },
    {
        "input_text": "summarize: def __getitem__(self, id):                return self.selected_child[id]",
        "labels_text": "Return the child with the given id"
    },
    {
        "input_text": "summarize: def __setitem__(self, id, child):                self.child_dict[id] = child",
        "labels_text": "Add a child associated with a certain id"
    },
    {
        "input_text": "summarize: def __contains__(self, id):                return id in self.selected_child",
        "labels_text": "Check if the child ha the given id"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.selected_child)",
        "labels_text": "Return the number of child"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.selected_child)",
        "labels_text": "Return the number of child"
    },
    {
        "input_text": "summarize: def __sub__(self, other):                return self.selected_child - other",
        "labels_text": "Subtraction with another object"
    },
    {
        "input_text": "summarize: def __gt__(self, other):                return self.selected_child > other",
        "labels_text": "Return if child is greater than other"
    },
    {
        "input_text": "summarize: def __ge__(self, other):                return self.selected_child >= other",
        "labels_text": "Return if child is greater or equal than other"
    },
    {
        "input_text": "summarize: def __lt__(self, other):                return self.selected_child < other",
        "labels_text": "Return if child is less than other"
    },
    {
        "input_text": "summarize: def __le__(self, other):                return self.selected_child <= other",
        "labels_text": "Return if child is less or equal than other"
    },
    {
        "input_text": "summarize: def copy(self):                shallow = copy(self)        shallow.child_dict = {}        shallow.detach_parent()        for child in self.disordered_get_list():            shallow.disordered_add(child.copy())        return shallow",
        "labels_text": "Copy disorderd entity recursively"
    },
    {
        "input_text": "summarize: def get_id(self):                return self.id",
        "labels_text": "Return the id"
    },
    {
        "input_text": "summarize: def disordered_has_id(self, id):                return id in self.child_dict",
        "labels_text": "Check if there is an object present associated with this id"
    },
    {
        "input_text": "summarize: def detach_parent(self):                self.parent = None        for child in self.disordered_get_list():            child.detach_parent()",
        "labels_text": "Detach the parent"
    },
    {
        "input_text": "summarize: def get_parent(self):                return self.parent",
        "labels_text": "Return parent"
    },
    {
        "input_text": "summarize: def set_parent(self, parent):                self.parent = parent        for child in self.disordered_get_list():            child.set_parent(parent)",
        "labels_text": "Set the parent for the object and it child"
    },
    {
        "input_text": "summarize: def disordered_select(self, id):                self.selected_child = self.child_dict[id]",
        "labels_text": "Select the object with given id a the currently active object Uncaught method call are forwarded to the selected child object"
    },
    {
        "input_text": "summarize: def disordered_add(self, child):                raise NotImplementedError",
        "labels_text": "Add disordered entry This is implemented by DisorderedAtom and DisorderedResidue"
    },
    {
        "input_text": "summarize: def disordered_remove(self, child):                raise NotImplementedError",
        "labels_text": "Remove disordered entry This is implemented by DisorderedAtom and DisorderedResidue"
    },
    {
        "input_text": "summarize: def is_disordered(self):                return 2",
        "labels_text": "Return indicating that this Entity is a collection of Entities"
    },
    {
        "input_text": "summarize: def disordered_get_id_list(self):                # sort id list alphabetically        return sorted(self.child_dict)",
        "labels_text": "Return a list of id"
    },
    {
        "input_text": "summarize: def disordered_get(self, id=None):                if id is None:            return self.selected_child        return self.child_dict[id]",
        "labels_text": "Get the child object associated with id If id is None the currently selected child is returned"
    },
    {
        "input_text": "summarize: def disordered_get_list(self):                return list(self.child_dict.values())",
        "labels_text": "Return list of child"
    },
    {
        "input_text": "summarize: def __init__(self, length, fid):                # nr of residues in fragment        self.length = length        # nr of residues added        self.counter = 0        self.resname_list = []        # CA coordinate matrix        self.coords_ca = np.zeros((length, 3), \"d\")        self.fid = fid",
        "labels_text": "Initialize fragment object param length length of the fragment type length int param fid id for the fragment type fid int"
    },
    {
        "input_text": "summarize: def get_resname_list(self):                return self.resname_list",
        "labels_text": "Get residue list return the residue name rtype string string"
    },
    {
        "input_text": "summarize: def get_id(self):                return self.fid",
        "labels_text": "Get identifier for the fragment return id for the fragment rtype int"
    },
    {
        "input_text": "summarize: def get_coords(self):                return self.coords_ca",
        "labels_text": "Get the CA coordinate in the fragment return the CA coords in the fragment rtype NumPy Nx array"
    },
    {
        "input_text": "summarize: def add_residue(self, resname, ca_coord):                if self.counter >= self.length:            raise PDBException(\"Fragment boundary exceeded.\")        self.resname_list.append(resname)        self.coords_ca[self.counter] = ca_coord        self.counter = self.counter + 1",
        "labels_text": "Add a residue param resname residue name eg GLY type resname string param cacoord the calpha coordinate of the residue type cacoord NumPy array with length"
    },
    {
        "input_text": "summarize: def __len__(self):                return self.length",
        "labels_text": "Return length of the fragment"
    },
    {
        "input_text": "summarize: def __sub__(self, other):                sup = SVDSuperimposer()        sup.set(self.coords_ca, other.coords_ca)        sup.run()        return sup.get_rms()",
        "labels_text": "Return rmsd between two fragment return rmsd between fragment rtype float Examples This is an incomplete but illustrative example rmsd fragment fragment"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"<Fragment length=%i id=%i>\" % (self.length, self.fid)",
        "labels_text": "Represent the fragment object a a string Returns Fragment lengthL idID where Llength of fragment and ID the identifier rank in the library"
    },
    {
        "input_text": "summarize: def _map_fragment_list(flist, reflist):        mapped = []    for f in flist:        rank = []        for i in range(len(reflist)):            rf = reflist[i]            rms = f - rf            rank.append((rms, rf))        rank.sort()        fragment = rank[0][1]        mapped.append(fragment)    return mapped",
        "labels_text": "Map flist fragment to closest entry in reflist PRIVATE Map all frgaments in flist to the closest in RMSD fragment in reflist Returns a list of reflist index param flist list of protein fragment type flist LFragment LFragment param reflist list of reference ie library fragment type reflist LFragment LFragment"
    },
    {
        "input_text": "summarize: def __init__(self, model, lsize=20, flength=5, fdir=\".\"):                if flength == 5:            self.edge = 2        elif flength == 7:            self.edge = 3        else:            raise PDBException(\"Fragment length should be 5 or 7.\")        self.flength = flength        self.lsize = lsize        self.reflist = _read_fragments(lsize, flength, fdir)        self.model = model        self.fd = self._map(self.model)",
        "labels_text": "Create instance of FragmentMapper param model the model that will be mapped type model LModel param lsize number of fragment in the library type lsize int param flength length of fragment in the library type flength int param fdir directory where the definition file are found default type fdir string"
    },
    {
        "input_text": "summarize: def __contains__(self, res):                return res in self.fd",
        "labels_text": "Check if the given residue is in any of the mapped fragment type re LResidue"
    },
    {
        "input_text": "summarize: def __getitem__(self, res):                return self.fd[res]",
        "labels_text": "Get an entry type re LResidue return fragment classification rtype LFragment"
    },
    {
        "input_text": "summarize: def __init__(self, model, radius=12, offset=0):                _AbstractHSExposure.__init__(            self,            model,            radius,            offset,            \"EXP_HSE_A_U\",            \"EXP_HSE_A_D\",            \"EXP_CB_PCB_ANGLE\",        )",
        "labels_text": "Initialize class param model the model that contains the residue type model LModel param radius radius of the sphere centred at the CA atom type radius float param offset number of flanking residue that are ignored in the calculation of the number of neighbor type offset int"
    },
    {
        "input_text": "summarize: def __init__(self, model, radius=12, offset=0):                _AbstractHSExposure.__init__(            self, model, radius, offset, \"EXP_HSE_B_U\", \"EXP_HSE_B_D\"        )",
        "labels_text": "Initialize class param model the model that contains the residue type model LModel param radius radius of the sphere centred at the CA atom type radius float param offset number of flanking residue that are ignored in the calculation of the number of neighbor type offset int"
    },
    {
        "input_text": "summarize: def _get_cb(self, r1, r2, r3):                if r2.get_resname() == \"GLY\":            return self._get_gly_cb_vector(r2), 0.0        else:            if r2.has_id(\"CB\") and r2.has_id(\"CA\"):                vcb = r2[\"CB\"].get_vector()                vca = r2[\"CA\"].get_vector()                return (vcb - vca), 0.0        return None",
        "labels_text": "Calculate CBCA vector PRIVATE param r r r three consecutive residue only r is used type r r r LResidue"
    },
    {
        "input_text": "summarize: def clear_ic(self):                for res in self.chain.get_residues():            res.internal_coord = None",
        "labels_text": "Clear residue internalcoord setting for this chain"
    },
    {
        "input_text": "summarize: def update_dCoordSpace(self, workSelector: Optional[np.ndarray] = None) -> None:                if workSelector is None:            self.assemble_residues()  # update atoms, fast if nothing to do            workSelector = np.logical_not(self.dcsValid)        workSet = self.dSet[workSelector]        self.dCoordSpace[:, workSelector] = multi_coord_space(            workSet, np.sum(workSelector), True        )        self.dcsValid[workSelector] = True",
        "labels_text": "Computeupdate coordinate space transforms for chain dihedra Requires all atom updated so call methassembleresidues return immediately if all atom already assembled param bool workSelector Optional mask to select dihedra for update"
    },
    {
        "input_text": "summarize: def distance_plot(        self, filter: Optional[Union[np.ndarray, None]] = None    ) -> np.ndarray:                if filter is None:            atomSet = self.atomArray        else:            atomSet = self.atomArray[filter]        # create distance matrix without scipy        # see https://jbencook.com/pairwise-distance-in-numpy/        return np.linalg.norm(atomSet[:, None, :] - atomSet[None, :, :], axis=-1)",
        "labels_text": "Generate D distance plot from atomArray Default is to calculate distance for all atom To generate the classic Calpha distance plot pas a boolean mask array like atmNameNdx internalcoordsAtomKeyfieldsatm CaSelect atomArrayIndexgetk for k in atomArrayIndexkeys if kaklatmNameNdx CA plot cicdistanceplotCaSelect Alternatively this will select all backbone atom backboneSelect atomArrayIndexgetk for k in atomArrayIndexkeys if kisbackbone param bool filter restrict atom for calculation seealso methdistancetointernalcoordinates which requires the default all atom distance plot"
    },
    {
        "input_text": "summarize: def dihedral_signs(self) -> np.ndarray:                return np.sign(self.dihedraAngle)",
        "labels_text": "Get sign array for each element of chain dihedraAngle array Required for methdistancetointernalcoordinates"
    },
    {
        "input_text": "summarize: def copy_initNCaCs(self, other: \"IC_Chain\") -> None:                ndx = [self.atomArrayIndex[ak] for iNCaC in other.initNCaCs for ak in iNCaC]        self.atomArray[ndx] = other.atomArray[ndx]        self.atomArrayValid[ndx] = True",
        "labels_text": "Copy atom coordinate for initNCaC atom from other ICChain Copies the coordinate and set atomArrayValid flag True for initial NCaC and after any chain break Needed for methdistancetointernalcoordinates if target ha chain break otherwise each fragment will start at origin Also useful if copying internal coordinate from another chain NB methICResiduesetangle and methICResiduesetlength invalidate their relevant atom so apply them before calling this function"
    },
    {
        "input_text": "summarize: def make_extended(self):                for ric in self.ordered_aa_ic_list:            ric.set_angle(\"psi\", 123)            ric.set_angle(\"phi\", -104)",
        "labels_text": "Set all psi and phi angle to extended conformation"
    },
    {
        "input_text": "summarize: def __contains__(self, ak: \"AtomKey\") -> bool:                if ak in self.ak_set:            akl = ak.akl            if (                int(akl[0]) == self.rbase[0]                and akl[1] == self.rbase[1]                and akl[2] == self.rbase[2]            ):                return True        return False",
        "labels_text": "Return True if atomkey is in this residue"
    },
    {
        "input_text": "summarize: def rak(self, atm: Union[str, Atom]) -> \"AtomKey\":                try:            ak = self.akc[atm]        except KeyError:            ak = self.akc[atm] = AtomKey(self, atm)            if isinstance(atm, str):                ak.missing = True        return ak",
        "labels_text": "Cache call to AtomKey for this residue"
    },
    {
        "input_text": "summarize: def _build_rak_cache(self) -> None:                for ak in sorted(self.ak_set):            atmName = ak.akl[3]            if self.akc.get(atmName) is None:                self.akc[atmName] = ak",
        "labels_text": "Create explicit entry for for atom so dont miss altlocs This ensures that selfakc atom key cache ha an entry for selected atom name eg CA amongst any that have altlocs Without this rak on the other altloc atom first may result in the main atom being missed"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return str(self.residue.full_id)",
        "labels_text": "Print string is parent Residue ID"
    },
    {
        "input_text": "summarize: def pretty_str(self) -> str:                id = self.residue.id        return f\"{self.residue.resname} {id[0]}{id[1]!s}{id[2]}\"",
        "labels_text": "Nice string for residue ID"
    },
    {
        "input_text": "summarize: def set_hbond(self) -> None:                for h in self.hedra.values():            if h.e_class == \"HNCA\":                h.hbond_1 = True            elif h.e_class == \"CACO\":                h.hbond_2 = True",
        "labels_text": "For OpenSCAD mark HN and CO bond to be hbonds magnet See funcSCADIOwriteSCAD"
    },
    {
        "input_text": "summarize: def clear_transforms(self):                for d in self.dihedra.values():            self.cic.dcsValid[d.ndx] = False",
        "labels_text": "Invalidate dihedra coordinate space attribute before assemble Coordinate space attribute are Dihedroncst and rcst and dataICChaindCoordSpace"
    },
    {
        "input_text": "summarize: def _residue_string(res: \"Residue\") -> str:                segid = res.get_segid()        if segid.isspace() or \"\" == segid:            segid = \"\"        else:            segid = \" [\" + segid + \"]\"        return str(res.get_full_id()) + \" \" + res.resname + segid + \"\\n\"",
        "labels_text": "Generate PIC Residue string Enough to create Biopython Residue object without actual Atoms param Residue re Biopython Residue object reference"
    },
    {
        "input_text": "summarize: def get_angle(self, angle_key: Union[EKT, str]) -> Optional[float]:                edron = self.pick_angle(angle_key)        if edron:            return float(edron.angle)        return None",
        "labels_text": "Get dihedron or hedron angle for specified key See methpickangle for key specification"
    },
    {
        "input_text": "summarize: def set_angle(self, angle_key: Union[EKT, str], v: float, overlap=True):                edron = self.pick_angle(angle_key)        if edron is None:            return        elif isinstance(edron, Hedron) or not overlap:            edron.angle = v        else:  # Dihedron, do overlap angles            delta = Dihedron.angle_dif(edron.angle, v)            self._do_bond_rotate(edron, delta)",
        "labels_text": "Set dihedron or hedron angle for specified key If angle is a Dihedron and overlap is True default overlapping dihedra are also changed a appropriate The overlap is a result of protein chain definition in modicdata and methcreateedra eg psi overlap NCACO The default overlapTrue is probably what you want for setanglechi val The default is probably NOT what you want when processing all dihedrals in a chain or residue such a copying from another structure a the overlapping dihedra will likely be in the set a well NB setting eg PRO chi is permitted without error or warning See methpickangle for anglekey specification See methbondrotate to change a dihedral by a number of degree param anglekey angle identifier param float v new angle in degree result adjusted to param bool overlap default True Modify overlapping dihedra a needed"
    },
    {
        "input_text": "summarize: def bond_rotate(self, angle_key: Union[EKT, str], delta: float):                base = self.pick_angle(angle_key)        if base is not None:            self._do_bond_rotate(base, delta)",
        "labels_text": "Rotate set of overlapping dihedrals by delta degree Changes a dihedral angle by a given delta ie newangle currentangle delta Values are adjusted so newangle will be within Changes overlapping dihedra a in methsetangle See methpickangle for key specification"
    },
    {
        "input_text": "summarize: def bond_set(self, angle_key: Union[EKT, str], val: float):                base = self.pick_angle(angle_key)        if base is not None:            delta = Dihedron.angle_dif(base.angle, val)            self._do_bond_rotate(base, delta)",
        "labels_text": "Set dihedron to val update overlapping dihedra by same amount Redundant to methsetangle retained for compatibility Unlike methsetangle this is for dihedra only and no option to not update overlapping dihedra See methpickangle for key specification"
    },
    {
        "input_text": "summarize: def get_length(self, ak_spec: Union[str, BKT]) -> Optional[float]:                hed_lst, ak_spec2 = self.pick_length(ak_spec)        if hed_lst is None or ak_spec2 is None:            return None        for hed in hed_lst:            val = hed.get_length(ak_spec2)            if val is not None:                return val        return None",
        "labels_text": "Get bond length for specified atom pair See methpicklength for akspec and detail"
    },
    {
        "input_text": "summarize: def set_length(self, ak_spec: Union[str, BKT], val: float) -> None:                hed_lst, ak_spec2 = self.pick_length(ak_spec)        if hed_lst is not None and ak_spec2 is not None:            for hed in hed_lst:                hed.set_length(ak_spec2, val)",
        "labels_text": "Set bond length for specified atom pair See methpicklength for akspec"
    },
    {
        "input_text": "summarize: def gen_key(lst: list[\"AtomKey\"]) -> str:                if 4 == len(lst):            return f\"{lst[0].id}:{lst[1].id}:{lst[2].id}:{lst[3].id}\"        else:            return f\"{lst[0].id}:{lst[1].id}:{lst[2].id}\"",
        "labels_text": "Generate string of joined AtomKey string from input Generate ACPNPCA from AC PN PCA param list lst list of AtomKey object"
    },
    {
        "input_text": "summarize: def gen_tuple(akstr: str) -> tuple:                return tuple([AtomKey(i) for i in akstr.split(\":\")])",
        "labels_text": "Generate AtomKey tuple for joined AtomKey string Generate AC PN PCA from ACPNPCA param str akstr string of separated AtomKey string"
    },
    {
        "input_text": "summarize: def __deepcopy__(self, memo):                existing = memo.get(id(self), False)        if existing:            return existing        dup = type(self).__new__(self.__class__)        memo[id(self)] = dup        dup.__dict__.update(self.__dict__)  # mostly static attribs        dup.cic = memo[id(self.cic)]        dup.atomkeys = copy.deepcopy(self.atomkeys, memo)        return dup",
        "labels_text": "Deep copy implementation for Edron"
    },
    {
        "input_text": "summarize: def __contains__(self, ak: \"AtomKey\") -> bool:                return ak in self.atomkeys",
        "labels_text": "Return True if atomkey is in this edron"
    },
    {
        "input_text": "summarize: def is_backbone(self) -> bool:                return all(ak.is_backbone() for ak in self.atomkeys)",
        "labels_text": "Report True for contains only N C CA O H atom"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return str(self.atomkeys)",
        "labels_text": "Tuple of AtomKeys is default repr string"
    },
    {
        "input_text": "summarize: def __hash__(self) -> int:                return self._hash",
        "labels_text": "Hash calculated at init from atomkeys tuple"
    },
    {
        "input_text": "summarize: def _cmp(self, other: \"Edron\") -> Union[tuple[\"AtomKey\", \"AtomKey\"], bool]:                for ak_s, ak_o in zip(self.atomkeys, other.atomkeys):            if ak_s != ak_o:                return ak_s, ak_o        return False",
        "labels_text": "Comparison function ranking self v other False on equal Priority is lowest value for sort psi chi"
    },
    {
        "input_text": "summarize: def __eq__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        return self.id == other.id",
        "labels_text": "Test for equality"
    },
    {
        "input_text": "summarize: def __ne__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        return self.id != other.id",
        "labels_text": "Test for inequality"
    },
    {
        "input_text": "summarize: def __gt__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        rslt = self._cmp(other)        if rslt:            rslt = cast(tuple[AtomKey, AtomKey], rslt)            return rslt[0] > rslt[1]        return False",
        "labels_text": "Test greater than"
    },
    {
        "input_text": "summarize: def __ge__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        rslt = self._cmp(other)        if rslt:            rslt = cast(tuple[AtomKey, AtomKey], rslt)            return rslt[0] >= rslt[1]        return True",
        "labels_text": "Test greater or equal"
    },
    {
        "input_text": "summarize: def __lt__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        rslt = self._cmp(other)        if rslt:            rslt = cast(tuple[AtomKey, AtomKey], rslt)            return rslt[0] < rslt[1]        return False",
        "labels_text": "Test less than"
    },
    {
        "input_text": "summarize: def __le__(self, other: object) -> bool:                if not isinstance(other, type(self)):            return NotImplemented        rslt = self._cmp(other)        if rslt:            rslt = cast(tuple[AtomKey, AtomKey], rslt)            return rslt[0] <= rslt[1]        return True",
        "labels_text": "Test less or equal"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return (            f\"3-{self.id} {self.re_class} {self.len12!s} \"            f\"{self.angle!s} {self.len23!s}\"        )",
        "labels_text": "Print string for Hedron object"
    },
    {
        "input_text": "summarize: def angle(self) -> float:                try:            return self.cic.hedraAngle[self.ndx]        except AttributeError:            return 0.0",
        "labels_text": "Get this hedron angle"
    },
    {
        "input_text": "summarize: def angle(self, angle_deg) -> None:                self.cic.hedraAngle[self.ndx] = angle_deg        self.cic.hAtoms_needs_update[self.ndx] = True        self.cic.atomArrayValid[self.cic.atomArrayIndex[self.atomkeys[2]]] = False",
        "labels_text": "Set this hedron angle set needsupdate"
    },
    {
        "input_text": "summarize: def len12(self):                try:            return self.cic.hedraL12[self.ndx]        except AttributeError:            return 0.0",
        "labels_text": "Get first length for Hedron"
    },
    {
        "input_text": "summarize: def len12(self, len):                self.cic.hedraL12[self.ndx] = len        self.cic.hAtoms_needs_update[self.ndx] = True        self.cic.atomArrayValid[self.cic.atomArrayIndex[self.atomkeys[1]]] = False        self.cic.atomArrayValid[self.cic.atomArrayIndex[self.atomkeys[2]]] = False",
        "labels_text": "Set first length for Hedron set needsupdate"
    },
    {
        "input_text": "summarize: def len23(self) -> float:                try:            return self.cic.hedraL23[self.ndx]        except AttributeError:            return 0.0",
        "labels_text": "Get second length for Hedron"
    },
    {
        "input_text": "summarize: def len23(self, len):                self.cic.hedraL23[self.ndx] = len        self.cic.hAtoms_needs_update[self.ndx] = True        self.cic.atomArrayValid[self.cic.atomArrayIndex[self.atomkeys[2]]] = False",
        "labels_text": "Set second length for Hedron set needsupdate"
    },
    {
        "input_text": "summarize: def get_length(self, ak_tpl: BKT) -> Optional[float]:                if 2 > len(ak_tpl):            return None        if all(ak in self.atomkeys[:2] for ak in ak_tpl):            return self.cic.hedraL12[self.ndx]        if all(ak in self.atomkeys[1:] for ak in ak_tpl):            return self.cic.hedraL23[self.ndx]        return None",
        "labels_text": "Get bond length for specified atom pair param tuple aktpl tuple of AtomKeys Pair of atom in this Hedron"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return f\"4-{self.id!s} {self.re_class} {self.angle!s} {self.ric!s}\"",
        "labels_text": "Print string for Dihedron object"
    },
    {
        "input_text": "summarize: def angle(self) -> float:                try:            return self.cic.dihedraAngle[self.ndx]        except AttributeError:            try:                return self._dihedral            except AttributeError:                return 360.0",
        "labels_text": "Get dihedral angle"
    },
    {
        "input_text": "summarize: def angle_dif(a1: Union[float, np.ndarray], a2: Union[float, np.ndarray]):                return 180.0 - ((180.0 - a2) + a1) % 360.0",
        "labels_text": "Get angle difference between two angle httpsstackoverflowcoma"
    },
    {
        "input_text": "summarize: def angle_avg(alst: list, in_rads: bool = False, out_rads: bool = False):                walst = alst if in_rads else np.deg2rad(alst)        ravg = np.arctan2(np.sum(np.sin(walst)), np.sum(np.cos(walst)))        return ravg if out_rads else np.rad2deg(ravg)",
        "labels_text": "Get average of list of angle param List alst list of angle to average param bool inrads input value are in radian param bool outrads report result in radian"
    },
    {
        "input_text": "summarize: def angle_pop_sd(alst: list, avg: float):                return np.sqrt(np.sum(np.square(Dihedron.angle_dif(alst, avg))) / len(alst))",
        "labels_text": "Get population standard deviation for list of angle should be sample std dev but avoid lenalst div by"
    },
    {
        "input_text": "summarize: def difference(self, other: \"Dihedron\") -> float:                return Dihedron.angle_dif(self.angle, other.angle)",
        "labels_text": "Get angle difference between this and other angle"
    },
    {
        "input_text": "summarize: def __deepcopy__(self, memo):                # will fail if .ric not in memo        existing = memo.get(id(self), False)        if existing:            return existing        dup = type(self).__new__(self.__class__)        memo[id(self)] = dup        dup.__dict__.update(self.__dict__)  # all static attribs except .ric        if self.ric is not None:            dup.ric = memo[id(self.ric)]        # deepcopy complete        return dup",
        "labels_text": "Deep copy implementation for AtomKey"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return self.id",
        "labels_text": "Repr string from id"
    },
    {
        "input_text": "summarize: def __hash__(self) -> int:                return self._hash",
        "labels_text": "Hash calculated at init from akl tuple"
    },
    {
        "input_text": "summarize: def altloc_match(self, other: \"AtomKey\") -> bool:                if isinstance(other, type(self)):            return self.akl[:4] == other.akl[:4]        else:            return NotImplemented",
        "labels_text": "Test AtomKey match to other discounting occupancy and altloc"
    },
    {
        "input_text": "summarize: def is_backbone(self) -> bool:                return self.akl[self.fields.atm] in (\"N\", \"C\", \"CA\", \"O\", \"H\")",
        "labels_text": "Return True if is N C CA O or H"
    },
    {
        "input_text": "summarize: def atm(self) -> str:                return self.akl[self.fields.atm]",
        "labels_text": "Return atom name N CA CB O etc"
    },
    {
        "input_text": "summarize: def __ne__(self, other: object) -> bool:                if isinstance(other, type(self)):            return self.akl != other.akl        else:            return NotImplemented",
        "labels_text": "Test for inequality"
    },
    {
        "input_text": "summarize: def __eq__(self, other: object) -> bool:  # type: ignore                if isinstance(other, type(self)):            return self.akl == other.akl        else:            return NotImplemented",
        "labels_text": "Test for equality"
    },
    {
        "input_text": "summarize: def __gt__(self, other: object) -> bool:                if isinstance(other, type(self)):            rslt = self._cmp(other)            return rslt[0] > rslt[1]        else:            return NotImplemented",
        "labels_text": "Test greater than"
    },
    {
        "input_text": "summarize: def __ge__(self, other: object) -> bool:                if isinstance(other, type(self)):            rslt = self._cmp(other)            return rslt[0] >= rslt[1]        else:            return NotImplemented",
        "labels_text": "Test greater or equal"
    },
    {
        "input_text": "summarize: def __lt__(self, other: object) -> bool:                if isinstance(other, type(self)):            rslt = self._cmp(other)            return rslt[0] < rslt[1]        else:            return NotImplemented",
        "labels_text": "Test less than"
    },
    {
        "input_text": "summarize: def __le__(self, other: object) -> bool:                if isinstance(other, type(self)):            rslt = self._cmp(other)            return rslt[0] <= rslt[1]        else:            return NotImplemented",
        "labels_text": "Test less or equal"
    },
    {
        "input_text": "summarize: def set_accuracy_95(num: float) -> float:        # return round(num, 5)  # much slower    return float(f\"{num:9.5f}\")",
        "labels_text": "Reduce floating point accuracy to xxxxxxxxx Used by classICResidue class writing PIC and SCAD file param float num input number return float with specified accuracy"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Initialise"
    },
    {
        "input_text": "summarize: def set_dict(self, dic):                self.dic = dic        # Remove self.structure if it has been set        if hasattr(self, \"structure\"):            delattr(self, \"structure\")",
        "labels_text": "Set the mmCIF dictionary to be written out"
    },
    {
        "input_text": "summarize: def get_structure(self, structure_id, filename):                with warnings.catch_warnings():            if self.QUIET:                warnings.filterwarnings(\"ignore\", category=PDBConstructionWarning)            self._mmcif_dict = MMCIF2Dict(filename)            self._build_structure(structure_id)            self._structure_builder.set_header(self._get_header())        return self._structure_builder.get_structure()",
        "labels_text": "Return the structure Arguments structureid string the id that will be used for the structure filename name of mmCIF file OR an open text mode file handle"
    },
    {
        "input_text": "summarize: def __init__(        self, structure_builder=None, auth_chains=True, auth_residues=True, QUIET=False    ):                if structure_builder is not None:            self._structure_builder = structure_builder        else:            self._structure_builder = StructureBuilder()        self.line_counter = 0        self.build_structure = None        self.auth_chains = bool(auth_chains)        self.auth_residues = bool(auth_residues)        self.QUIET = bool(QUIET)",
        "labels_text": "Create a FastMMCIFParser object The mmCIF parser call a number of standard method in an aggregated StructureBuilder object Normally this object is instantiated by the parser object itself but if the user provides hisher own StructureBuilder object the latter is used instead The main difference between this class and the regular MMCIFParser is that only ATOM and HETATM line are parsed here Use if you are interested only in coordinate information Arguments structurebuilder an optional user implemented StructureBuilder class authchains True by default If true use the author chain IDs If false use the reassigned mmCIF chain IDs authresidues True by default If true use the author residue numbering If false use the mmCIF label residue numbering which ha no insertion code and strictly increment residue number NOTE Nonpolymers such a water dont have a label residue number and will be skipped QUIET Evaluated a a Boolean If true warning issued in constructing the SMCRA data will be suppressed If false DEFAULT they will be shown These warning might be indicative of problem in the mmCIF file"
    },
    {
        "input_text": "summarize: def get_structure(self, structure_id, filename):                with warnings.catch_warnings():            if self.QUIET:                warnings.filterwarnings(\"ignore\", category=PDBConstructionWarning)            with as_handle(filename) as handle:                self._build_structure(structure_id, handle)        return self._structure_builder.get_structure()",
        "labels_text": "Return the structure Arguments structureid string the id that will be used for the structure filename name of the mmCIF file OR an open filehandle"
    },
    {
        "input_text": "summarize: def __init__(self, id, serial_num=None):                self.level = \"M\"        if serial_num is None:            self.serial_num = id        else:            self.serial_num = serial_num        Entity.__init__(self, id)",
        "labels_text": "Initialize Arguments id int serialnum int"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"<Model id={self.get_id()}>\"",
        "labels_text": "Return model identifier"
    },
    {
        "input_text": "summarize: def get_chains(self):                yield from self",
        "labels_text": "Return chain"
    },
    {
        "input_text": "summarize: def get_residues(self):                for c in self.get_chains():            yield from c",
        "labels_text": "Return residue"
    },
    {
        "input_text": "summarize: def get_atoms(self):                for r in self.get_residues():            yield from r",
        "labels_text": "Return atom"
    },
    {
        "input_text": "summarize: def atom_to_internal_coordinates(self, verbose: bool = False) -> None:                for chn in self.get_chains():            chn.atom_to_internal_coordinates(verbose)",
        "labels_text": "Createupdate internal coordinate from Atom XYZ coordinate Internal coordinate are bond length angle and dihedral angle param verbose bool default False describe runtime problem"
    },
    {
        "input_text": "summarize: def internal_to_atom_coordinates(self, verbose: bool = False) -> None:                for chn in self.get_chains():            chn.internal_to_atom_coordinates(verbose)",
        "labels_text": "Createupdate atom coordinate from internal coordinate param verbose bool default False describe runtime problem raise Exception if any chain doe not have pic attribute"
    },
    {
        "input_text": "summarize: def _chop_end_codes(line):        return re.sub(r\"\\s\\s\\s\\s+[\\w]{4}.\\s+\\d*\\Z\", \"\", line)",
        "labels_text": "Chops line ending with CSA and the like PRIVATE"
    },
    {
        "input_text": "summarize: def _chop_end_misc(line):        return re.sub(r\"\\s+\\d\\d-\\w\\w\\w-\\d\\d\\s+[1-9][0-9A-Z]{3}\\s*\\Z\", \"\", line)",
        "labels_text": "Chops line ending with JUL CSA and the like PRIVATE"
    },
    {
        "input_text": "summarize: def _nice_case(line):        line_lower = line.lower()    s = \"\"    i = 0    nextCap = 1    while i < len(line_lower):        c = line_lower[i]        if c >= \"a\" and c <= \"z\" and nextCap:            c = c.upper()            nextCap = 0        elif c in \" .,;:\\t-_\":            nextCap = 1        s += c        i += 1    return s",
        "labels_text": "Make A Lowercase String With Capitals PRIVATE"
    },
    {
        "input_text": "summarize: def parse_pdb_header(infile):        header = []    with File.as_handle(infile) as f:        for line in f:            record_type = line[0:6]            if record_type in (\"ATOM  \", \"HETATM\", \"MODEL \"):                break            header.append(line)    return _parse_pdb_header_list(header)",
        "labels_text": "Return the header line of a pdb file a a dictionary Dictionary key are head depositiondate releasedate structuremethod resolution structurereference journalreference author and compound"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"<Select all>\"",
        "labels_text": "Represent the output a a string for debugging"
    },
    {
        "input_text": "summarize: def accept_model(self, model):                return 1",
        "labels_text": "Overload this to reject model for output"
    },
    {
        "input_text": "summarize: def accept_chain(self, chain):                return 1",
        "labels_text": "Overload this to reject chain for output"
    },
    {
        "input_text": "summarize: def accept_residue(self, residue):                return 1",
        "labels_text": "Overload this to reject residue for output"
    },
    {
        "input_text": "summarize: def accept_atom(self, atom):                return 1",
        "labels_text": "Overload this to reject atom for output"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Initialise"
    },
    {
        "input_text": "summarize: def __init__(self, use_model_flag=0, is_pqr=False):                self.use_model_flag = use_model_flag        self.is_pqr = is_pqr",
        "labels_text": "Create the PDBIO object param usemodelflag if force use of the MODEL record in output type usemodelflag int param ispqr if True build PQR file Otherwise build PDB file type ispqr Boolean"
    },
    {
        "input_text": "summarize: def _print_default_format_warning(file_format):                if file_format is None:            sys.stderr.write(                \"WARNING: The default download format has changed from PDB to PDBx/mmCif\\n\"            )            return \"mmCif\"        return file_format",
        "labels_text": "Print a warning to stdout PRIVATE Temporary warning similar to a deprecation warning that file are being downloaded in mmCIF"
    },
    {
        "input_text": "summarize: def get_status_list(url):                with contextlib.closing(urlopen(url)) as handle:            answer = []            for line in handle:                pdb = line.strip()                assert len(pdb) == 4                answer.append(pdb.decode())        return answer",
        "labels_text": "Retrieve a list of pdb code in the weekly pdb status file from given URL Used by getrecentchanges Typical content of the list file parsed by this method is now very simply one PDB name per line"
    },
    {
        "input_text": "summarize: def get_recent_changes(self):                path = self.pdb_server + \"/pub/pdb/data/status/latest/\"        # Retrieve the lists        added = self.get_status_list(path + \"added.pdb\")        modified = self.get_status_list(path + \"modified.pdb\")        obsolete = self.get_status_list(path + \"obsolete.pdb\")        return [added, modified, obsolete]",
        "labels_text": "Return three list of the newest weekly file addedmodobsolete Reads the directory with changed entry from the PDB server and return a tuple of three URLs to the file of new modified and obsolete entry from the most recent list The directory with the largest numerical name is used Returns None if something go wrong Contents of the datastatus dir would be used drwxrwxrx sysadmin Oct drwxrwxrx sysadmin Oct rwrr sysadmin Mar README"
    },
    {
        "input_text": "summarize: def get_all_entries(self):                url = self.pdb_server + \"/pub/pdb/derived_data/index/entries.idx\"        if self._verbose:            print(\"Retrieving index file. Takes about 27 MB.\")        with contextlib.closing(urlopen(url)) as handle:            all_entries = [                line[:4].decode() for line in handle.readlines()[2:] if len(line) > 4            ]        return all_entries",
        "labels_text": "Retrieve the big file containing all the PDB entry and some annotation Returns a list of PDB code in the index file"
    },
    {
        "input_text": "summarize: def get_seqres_file(self, savefile=\"pdb_seqres.txt\"):                if self._verbose:            print(\"Retrieving sequence file (takes over 110 MB).\")        url = self.pdb_server + \"/pub/pdb/derived_data/pdb_seqres.txt\"        urlretrieve(url, savefile)",
        "labels_text": "Retrieve and save a big file containing all the sequence of PDB entry"
    },
    {
        "input_text": "summarize: def __init__(self):                self.structure_builder = StructureBuilder()",
        "labels_text": "Initialize a PDBML parser"
    },
    {
        "input_text": "summarize: def get_header(self):                return self.header",
        "labels_text": "Return the header"
    },
    {
        "input_text": "summarize: def get_trailer(self):                return self.trailer",
        "labels_text": "Return the trailer"
    },
    {
        "input_text": "summarize: def _parse(self, header_coords_trailer):                # Extract the header; return the rest of the file        self.header, coords_trailer = self._get_header(header_coords_trailer)        # Parse the atomic data; return the PDB file trailer        self.trailer = self._parse_coordinates(coords_trailer)",
        "labels_text": "Parse the PDB file PRIVATE"
    },
    {
        "input_text": "summarize: def enumerate_atoms(entity):        while entity.get_parent():        entity = entity.get_parent()  # get to top level    if \"S\" == entity.level:        for mdl in entity:  # each model starts with 1            _enumerate_entity_atoms(mdl)    else:  # only Chain or Residue, start with 1        _enumerate_entity_atoms(entity)",
        "labels_text": "Ensure all atom in entity have serialnumber set"
    },
    {
        "input_text": "summarize: def index_to_one(index):        return dindex_to_1[index]",
        "labels_text": "Index to corresponding one letter amino acid name indextoone A indextoone Y"
    },
    {
        "input_text": "summarize: def one_to_index(s):        return d1_to_index[s]",
        "labels_text": "One letter code to index onetoindexA onetoindexY"
    },
    {
        "input_text": "summarize: def index_to_three(i):        return dindex_to_3[i]",
        "labels_text": "Index to corresponding three letter amino acid name indextothree ALA indextothree TYR"
    },
    {
        "input_text": "summarize: def three_to_index(s):        return d3_to_index[s]",
        "labels_text": "Three letter code to index threetoindexALA threetoindexTYR"
    },
    {
        "input_text": "summarize: def is_aa(residue, standard=False):        if not isinstance(residue, str):        residue = f\"{residue.get_resname():<3s}\"    residue = residue.upper()    if standard:        return residue in protein_letters_3to1    else:        return residue in protein_letters_3to1_extended",
        "labels_text": "Return True if residue objectstring is an amino acid param residue a LResidue object OR a three letter amino acid code type residue LResidue or string param standard flag to check for the AA default false type standard boolean isaaALA True Known three letter code for modified amino acid are supported isaaFME True isaaFME standardTrue False"
    },
    {
        "input_text": "summarize: def is_nucleic(residue, standard=False):        if not isinstance(residue, str):        residue = f\"{residue.get_resname():<3s}\"    residue = residue.upper()    if standard:        return residue in nucleic_letters_3to1    else:        return residue in nucleic_letters_3to1_extended",
        "labels_text": "Return True if residue objectstring is a nucleic acid param residue a LResidue object OR a three letter code type residue LResidue or string param standard flag to check for the DNA RNA canonical base Default is False type standard boolean isnucleicDA True isnucleicA True Known three letter code for modified nucleotide are supported isnucleicAL True isnucleicAL standardTrue False"
    },
    {
        "input_text": "summarize: def get_ca_list(self):                ca_list = []        for res in self:            ca = res[\"CA\"]            ca_list.append(ca)        return ca_list",
        "labels_text": "Get list of Calpha atom in the polypeptide return the list of Calpha atom rtype LAtom LAtom"
    },
    {
        "input_text": "summarize: def get_sequence(self):                s = \"\".join(            protein_letters_3to1_extended.get(res.get_resname(), \"X\") for res in self        )        return Seq(s)",
        "labels_text": "Return the AA sequence a a Seq object return polypeptide sequence rtype LSeq"
    },
    {
        "input_text": "summarize: def __repr__(self):                start = self[0].get_id()[1]        end = self[-1].get_id()[1]        return f\"<Polypeptide start={start} end={end}>\"",
        "labels_text": "Return string representation of the polypeptide Return Polypeptide startSTART endEND where START and END are sequence identifier of the outer residue"
    },
    {
        "input_text": "summarize: def __init__(self, radius):                self.radius = radius",
        "labels_text": "Initialize the base class param radius distance type radius float"
    },
    {
        "input_text": "summarize: def __init__(self, radius=4.3):                _PPBuilder.__init__(self, radius)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, radius=1.8):                _PPBuilder.__init__(self, radius)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _test_dist(self, c, n):                if (c - n) < self.radius:            return 1        else:            return 0",
        "labels_text": "Return if distance between atomsradius PRIVATE"
    },
    {
        "input_text": "summarize: def psea(pname):        fname = run_psea(pname)    start = 0    ss = \"\"    with open(fname) as fp:        for line in fp:            if line[0:6] == \">p-sea\":                start = 1                continue            if not start:                continue            if line[0] == \"\\n\":                break            ss = ss + line[0:-1]    return ss",
        "labels_text": "Parse PSEA output file"
    },
    {
        "input_text": "summarize: def psea2HEC(pseq):        seq = []    for ss in pseq:        if ss == \"a\":            n = \"H\"        elif ss == \"b\":            n = \"E\"        elif ss == \"c\":            n = \"C\"        seq.append(n)    return seq",
        "labels_text": "Translate PSEA secondary structure string into HEC"
    },
    {
        "input_text": "summarize: def __init__(self, model, filename):                ss_seq = psea(filename)        ss_seq = psea2HEC(ss_seq)        annotate(model, ss_seq)        self.ss_seq = ss_seq",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def get_seq(self):                return self.ss_seq",
        "labels_text": "Return secondary structure string"
    },
    {
        "input_text": "summarize: def __init__(self):                self._reset_properties()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _reset_properties(self):                self.reference_coords = None        self.coords = None        self.transformed_coords = None        self.rot = None        self.tran = None        self.rms = None        self.init_rms = None",
        "labels_text": "Reset all relevant property to None to avoid conflict between run"
    },
    {
        "input_text": "summarize: def apply(self, atom_list):                if self.rotran is None:            raise PDBException(\"No transformation has been calculated yet\")        rot, tran = self.rotran        for atom in atom_list:            atom.transform(rot, tran)",
        "labels_text": "Apply the QCP rotation matrixtranslation vector to a set of atom"
    },
    {
        "input_text": "summarize: def set(self, reference_coords, coords):                self._reset_properties()        # store coordinates        self.reference_coords = reference_coords        self.coords = coords        self._natoms, n_dim = coords.shape        if reference_coords.shape != coords.shape:            raise PDBException(\"Coordinates must have the same dimensions.\")        if n_dim != 3:            raise PDBException(\"Coordinates must be Nx3 arrays.\")",
        "labels_text": "Set the coordinate to be superimposed coords will be put on top of referencecoords referencecoords an NxDIM array coords an NxDIM array DIM is the dimension of the point N is the number of point to be superimposed"
    },
    {
        "input_text": "summarize: def get_transformed(self):                if self.coords is None or self.reference_coords is None:            raise PDBException(\"No coordinates set.\")        if self.rot is None:            raise PDBException(\"Nothing is superimposed yet.\")        self.transformed_coords = np.dot(self.coords, self.rot) + self.tran        return self.transformed_coords",
        "labels_text": "Get the transformed coordinate set"
    },
    {
        "input_text": "summarize: def get_rotran(self):                if self.rot is None:            raise PDBException(\"Nothing is superimposed yet.\")        return self.rot, self.tran",
        "labels_text": "Return right multiplying rotation matrix and translation vector"
    },
    {
        "input_text": "summarize: def get_init_rms(self):                if self.coords is None:            raise PDBException(\"No coordinates set yet.\")        if self.init_rms is None:            diff = self.coords - self.reference_coords            self.init_rms = np.sqrt(np.sum(np.sum(diff * diff, axis=1) / self._natoms))        return self.init_rms",
        "labels_text": "Return the root mean square deviation of untransformed coordinate"
    },
    {
        "input_text": "summarize: def get_rms(self):                if self.rms is None:            raise PDBException(\"Nothing superimposed yet.\")        return self.rms",
        "labels_text": "Root mean square deviation of superimposed coordinate"
    },
    {
        "input_text": "summarize: def __init__(self, id, resname, segid):                self.level = \"R\"        self.disordered = 0        self.resname = resname        self.segid = segid        self.internal_coord = None        Entity.__init__(self, id)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                resname = self.get_resname()        hetflag, resseq, icode = self.get_id()        full_id = (resname, hetflag, resseq, icode)        return \"<Residue %s het=%s resseq=%s icode=%s>\" % full_id",
        "labels_text": "Return the residue full id"
    },
    {
        "input_text": "summarize: def strictly_equals(        self: _ResidueT, other: _ResidueT, compare_coordinates: bool = False    ) -> bool:                if not isinstance(other, type(self)):            return False        return (            self.resname == other.resname            and self.id == other.id            and Entity.strictly_equals(self, other, compare_coordinates)        )",
        "labels_text": "Compare this residue to the other residue using a strict definition of equality The residue are equal if they have the same name identifier and their constituent atom are strictly equal param other The residue to compare this residue to type other Residue param comparecoordinates Whether to compare the coordinate of the atom type comparecoordinates bool return Whether the residue are strictly equal rtype bool"
    },
    {
        "input_text": "summarize: def add(self, atom):                atom_id = atom.get_id()        if self.has_id(atom_id):            raise PDBConstructionException(                f\"Atom {atom_id} defined twice in residue {self}\"            )        Entity.add(self, atom)",
        "labels_text": "Add an Atom object Checks for adding duplicate atom and raise a PDBConstructionException if so"
    },
    {
        "input_text": "summarize: def flag_disordered(self):                self.disordered = 1",
        "labels_text": "Set the disordered flag"
    },
    {
        "input_text": "summarize: def is_disordered(self):                return self.disordered",
        "labels_text": "Return if the residue contains disordered atom"
    },
    {
        "input_text": "summarize: def get_resname(self):                return self.resname",
        "labels_text": "Return the residue name"
    },
    {
        "input_text": "summarize: def get_unpacked_list(self):                atom_list = self.get_list()        undisordered_atom_list = []        for atom in atom_list:            if atom.is_disordered():                undisordered_atom_list += atom.disordered_get_list()            else:                undisordered_atom_list.append(atom)        return undisordered_atom_list",
        "labels_text": "Return the list of all atom unpack DisorderedAtoms"
    },
    {
        "input_text": "summarize: def get_segid(self):                return self.segid",
        "labels_text": "Return the segment identifier"
    },
    {
        "input_text": "summarize: def get_atoms(self):                yield from self",
        "labels_text": "Return atom"
    },
    {
        "input_text": "summarize: def __init__(self, id):                DisorderedEntityWrapper.__init__(self, id)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                if self.child_dict:            resname = self.get_resname()            hetflag, resseq, icode = self.get_id()            full_id = (resname, hetflag, resseq, icode)            return \"<DisorderedResidue %s het=%s resseq=%i icode=%s>\" % full_id        else:            return \"<Empty DisorderedResidue>\"",
        "labels_text": "Return disordered residue full identifier"
    },
    {
        "input_text": "summarize: def sort(self):                for residue in self.disordered_get_list():            residue.sort()",
        "labels_text": "Sort the atom in the child Residue object"
    },
    {
        "input_text": "summarize: def disordered_add(self, residue):                resname = residue.get_resname()        # add chain parent to residue        chain = self.get_parent()        residue.set_parent(chain)        assert not self.disordered_has_id(resname)        self[resname] = residue        self.disordered_select(resname)",
        "labels_text": "Add a residue object and use it resname a key Arguments residue Residue object"
    },
    {
        "input_text": "summarize: def _read_vertex_array(filename):        with open(filename) as fp:        vertex_list = []        for line in fp:            sl = line.split()            if len(sl) != 9:                # skip header                continue            vl = [float(x) for x in sl[0:3]]            vertex_list.append(vl)    return np.array(vertex_list)",
        "labels_text": "Read the vertex list into a NumPy array PRIVATE"
    },
    {
        "input_text": "summarize: def min_dist(coord, surface):        d = surface - coord    d2 = np.sum(d * d, 1)    return np.sqrt(min(d2))",
        "labels_text": "Return minimum distance between coord and surface"
    },
    {
        "input_text": "summarize: def residue_depth(residue, surface):        atom_list = residue.get_unpacked_list()    length = len(atom_list)    d = 0    for atom in atom_list:        coord = atom.get_coord()        d = d + min_dist(coord, surface)    return d / length",
        "labels_text": "Residue depth a average depth of all it atom Return average distance to surface for all atom in a residue ie the residue depth"
    },
    {
        "input_text": "summarize: def ca_depth(residue, surface):        if not residue.has_id(\"CA\"):        return None    ca = residue[\"CA\"]    coord = ca.get_coord()    return min_dist(coord, surface)",
        "labels_text": "Return CA depth"
    },
    {
        "input_text": "summarize: def uniqueify(items):        return list(set(items))",
        "labels_text": "Return a list of the unique item in the given iterable Order is NOT preserved"
    },
    {
        "input_text": "summarize: def get_unique_parents(entity_list):        unique_parents = {entity.get_parent() for entity in entity_list}    return list(unique_parents)",
        "labels_text": "Translate a list of entity to a list of their unique parent"
    },
    {
        "input_text": "summarize: def __init__(self, id):                self.level = \"S\"        Entity.__init__(self, id)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"<Structure id={self.get_id()}>\"",
        "labels_text": "Return the structure identifier"
    },
    {
        "input_text": "summarize: def get_models(self):                yield from self",
        "labels_text": "Return model"
    },
    {
        "input_text": "summarize: def get_chains(self):                for m in self.get_models():            yield from m",
        "labels_text": "Return chain from model"
    },
    {
        "input_text": "summarize: def get_residues(self):                for c in self.get_chains():            yield from c",
        "labels_text": "Return residue from chain"
    },
    {
        "input_text": "summarize: def get_atoms(self):                for r in self.get_residues():            yield from r",
        "labels_text": "Return atom from residue"
    },
    {
        "input_text": "summarize: def atom_to_internal_coordinates(self, verbose: bool = False) -> None:                for chn in self.get_chains():            chn.atom_to_internal_coordinates(verbose)",
        "labels_text": "Createupdate internal coordinate from Atom XYZ coordinate Internal coordinate are bond length angle and dihedral angle param verbose bool default False describe runtime problem"
    },
    {
        "input_text": "summarize: def internal_to_atom_coordinates(self, verbose: bool = False) -> None:                for chn in self.get_chains():            chn.internal_to_atom_coordinates(verbose)",
        "labels_text": "Createupdate atom coordinate from internal coordinate param verbose bool default False describe runtime problem raise Exception if any chain doe not have internalcoord attribute"
    },
    {
        "input_text": "summarize: def _test_equivalence(self, r1, aa1):                resname = r1.get_resname()        resname = PDBData.protein_letters_3to1_extended[resname]        assert aa1 == resname",
        "labels_text": "Test if aa in sequence fit aa in structure PRIVATE"
    },
    {
        "input_text": "summarize: def get_maps(self):                return self.map12, self.map21",
        "labels_text": "Map residue between the structure Return two dictionary that map a residue in one structure to the equivealent residue in the other structure"
    },
    {
        "input_text": "summarize: def get_iterator(self):                for i in range(len(self.duos)):            yield self.duos[i]",
        "labels_text": "Create an iterator over all residue pair"
    },
    {
        "input_text": "summarize: def _is_completely_disordered(residue: Residue) -> bool:        atom_list = residue.get_unpacked_list()    for atom in atom_list:        altloc = atom.get_altloc()        if altloc == \" \":            return False    return True",
        "labels_text": "Return whether all atom in the residue have a nonblank altloc PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self):                self.atom = None        self.chain = None        self.header = {}        self.line_counter = 0        self.model = None        self.residue = None        self.segid = None        self.structure = None",
        "labels_text": "Initialize this instance"
    },
    {
        "input_text": "summarize: def set_header(self, header):                self.header = header",
        "labels_text": "Set header"
    },
    {
        "input_text": "summarize: def set_line_counter(self, line_counter: int):                self.line_counter = line_counter",
        "labels_text": "Tracks line in the PDB file that is being parsed Arguments linecounter int"
    },
    {
        "input_text": "summarize: def init_structure(self, structure_id: str):                self.structure = Structure(structure_id)",
        "labels_text": "Initialize a new Structure object with given id Arguments structureid string"
    },
    {
        "input_text": "summarize: def init_model(self, model_id: int, serial_num: Optional[int] = None):                self.model = Model(model_id, serial_num)        self.structure.add(self.model)",
        "labels_text": "Create a new Model object with given id Arguments id int serialnum int"
    },
    {
        "input_text": "summarize: def init_chain(self, chain_id: str):                if self.model.has_id(chain_id):            self.chain = self.model[chain_id]            warnings.warn(                \"WARNING: Chain %s is discontinuous at line %i.\"                % (chain_id, self.line_counter),                PDBConstructionWarning,            )        else:            self.chain = Chain(chain_id)            self.model.add(self.chain)",
        "labels_text": "Create a new Chain object with given id Arguments chainid string"
    },
    {
        "input_text": "summarize: def init_seg(self, segid: str):                self.segid = segid",
        "labels_text": "Flag a change in segid Arguments segid string"
    },
    {
        "input_text": "summarize: def set_anisou(self, anisou_array):                self.atom.set_anisou(anisou_array)",
        "labels_text": "Set anisotropic B factor of current Atom"
    },
    {
        "input_text": "summarize: def set_siguij(self, siguij_array):                self.atom.set_siguij(siguij_array)",
        "labels_text": "Set standard deviation of anisotropic B factor of current Atom"
    },
    {
        "input_text": "summarize: def set_sigatm(self, sigatm_array):                self.atom.set_sigatm(sigatm_array)",
        "labels_text": "Set standard deviation of atom position of current Atom"
    },
    {
        "input_text": "summarize: def get_structure(self):                # first sort everything        # self.structure.sort()        # Add the header dict        self.structure.header = self.header        return self.structure",
        "labels_text": "Return the structure"
    },
    {
        "input_text": "summarize: def set_symmetry(self, spacegroup, cell):",
        "labels_text": "Set symmetry"
    },
    {
        "input_text": "summarize: def __init__(self):                self.rotran = None        self.rms = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def apply(self, atom_list):                if self.rotran is None:            raise PDBException(\"No transformation has been calculated yet\")        rot, tran = self.rotran        rot = rot.astype(\"f\")        tran = tran.astype(\"f\")        for atom in atom_list:            atom.transform(rot, tran)",
        "labels_text": "Rotatetranslate a list of atom"
    },
    {
        "input_text": "summarize: def vector_to_axis(line, point):        line = line.normalized()    np = point.norm()    angle = line.angle(point)    return point - line ** (np * np.cos(angle))",
        "labels_text": "Vector to axis method Return the vector between a point and the closest point on a line ie the perpendicular projection of the point on the line type line LVector param line vector defining a line type point LVector param point vector defining the point"
    },
    {
        "input_text": "summarize: def refmat(p, q):        p = p.normalized()    q = q.normalized()    if (p - q).norm() < 1e-5:        return np.identity(3)    pq = p - q    pq.normalize()    b = pq.get_array()    b.shape = (3, 1)    i = np.identity(3)    ref = i - 2 * np.dot(b, np.transpose(b))    return ref",
        "labels_text": "Return a left multiplying matrix that mirror p onto q type pq LVector return The mirror operation a x NumPy array Examples from BioPDBvectors import refmat p q Vector Vector mirror refmatp q qq pleftmultiplymirror printq Vector printqq Vector"
    },
    {
        "input_text": "summarize: def rotmat(p, q):        rot = np.dot(refmat(q, -p), refmat(p, -p))    return rot",
        "labels_text": "Return a left multiplying matrix that rotates p onto q param p moving vector type p LVector param q fixed vector type q LVector return rotation matrix that rotates p onto q rtype x NumPy array Examples from BioPDBvectors import rotmat p q Vector Vector r rotmatp q printq Vector printp Vector pleftmultiplyr Vector"
    },
    {
        "input_text": "summarize: def calc_angle(v1, v2, v3):        v1 = v1 - v2    v3 = v3 - v2    return v1.angle(v3)",
        "labels_text": "Calculate angle method Calculate the angle between vector representing connected point param v v v the tree point that define the angle type v v v LVector return angle rtype float"
    },
    {
        "input_text": "summarize: def calc_dihedral(v1, v2, v3, v4):        ab = v1 - v2    cb = v3 - v2    db = v4 - v3    u = ab**cb    v = db**cb    w = u**v    angle = u.angle(v)    # Determine sign of angle    try:        if cb.angle(w) > 0.001:            angle = -angle    except ZeroDivisionError:        # dihedral=pi        pass    return angle",
        "labels_text": "Calculate dihedral angle method Calculate the dihedral angle between vector representing connected point The angle is in pi pi param v v v v the four point that define the dihedral angle type v v v v LVector"
    },
    {
        "input_text": "summarize: def __init__(self, x, y=None, z=None):                if y is None and z is None:            # Array, list, tuple...            if len(x) != 3:                raise ValueError(\"Vector: x is not a list/tuple/array of 3 numbers\")            self._ar = np.array(x, \"d\")        else:            # Three numbers            self._ar = np.array((x, y, z), \"d\")",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                x, y, z = self._ar        return f\"<Vector {x:.2f}, {y:.2f}, {z:.2f}>\"",
        "labels_text": "Return vector D coordinate"
    },
    {
        "input_text": "summarize: def __neg__(self):                a = -self._ar        return Vector(a)",
        "labels_text": "Return Vectorx y z"
    },
    {
        "input_text": "summarize: def __add__(self, other):                if isinstance(other, Vector):            a = self._ar + other._ar        else:            a = self._ar + np.array(other)        return Vector(a)",
        "labels_text": "Return Vectorother Vector or scalar"
    },
    {
        "input_text": "summarize: def __sub__(self, other):                if isinstance(other, Vector):            a = self._ar - other._ar        else:            a = self._ar - np.array(other)        return Vector(a)",
        "labels_text": "Return Vectorother Vector or scalar"
    },
    {
        "input_text": "summarize: def __mul__(self, other):                return sum(self._ar * other._ar)",
        "labels_text": "Return VectorVector dot product"
    },
    {
        "input_text": "summarize: def __truediv__(self, x):                a = self._ar / np.array(x)        return Vector(a)",
        "labels_text": "Return Vectorcoordsa"
    },
    {
        "input_text": "summarize: def __getitem__(self, i):                return self._ar[i]",
        "labels_text": "Return value of array index i"
    },
    {
        "input_text": "summarize: def __setitem__(self, i, value):                self._ar[i] = value",
        "labels_text": "Assign value to array index i"
    },
    {
        "input_text": "summarize: def __contains__(self, i):                return i in self._ar",
        "labels_text": "Validate if i is in array"
    },
    {
        "input_text": "summarize: def norm(self):                return np.sqrt(sum(self._ar * self._ar))",
        "labels_text": "Return vector norm"
    },
    {
        "input_text": "summarize: def normsq(self):                return abs(sum(self._ar * self._ar))",
        "labels_text": "Return square of vector norm"
    },
    {
        "input_text": "summarize: def normalize(self):                if self.norm():            self._ar = self._ar / self.norm()",
        "labels_text": "Normalize the Vector object Changes the state of self and doesnt return a value If you need to chain function call or create a new object use the normalized method"
    },
    {
        "input_text": "summarize: def normalized(self):                v = self.copy()        v.normalize()        return v",
        "labels_text": "Return a normalized copy of the Vector To avoid allocating new object use the normalize method"
    },
    {
        "input_text": "summarize: def angle(self, other):                n1 = self.norm()        n2 = other.norm()        c = (self * other) / (n1 * n2)        # Take care of roundoff errors        c = min(c, 1)        c = max(-1, c)        return np.arccos(c)",
        "labels_text": "Return angle between two vector"
    },
    {
        "input_text": "summarize: def get_array(self):                return np.array(self._ar)",
        "labels_text": "Return a copy of the array of coordinate"
    },
    {
        "input_text": "summarize: def left_multiply(self, matrix):                a = np.dot(matrix, self._ar)        return Vector(a)",
        "labels_text": "Return VectorMatrix x Vector"
    },
    {
        "input_text": "summarize: def right_multiply(self, matrix):                a = np.dot(self._ar, matrix)        return Vector(a)",
        "labels_text": "Return VectorVector x Matrix"
    },
    {
        "input_text": "summarize: def copy(self):                return Vector(self._ar)",
        "labels_text": "Return a deep copy of the Vector"
    },
    {
        "input_text": "summarize: def set_Z_homog_rot_mtx(angle_rads: float, mtx: np.ndarray):        cosang = np.cos(angle_rads)    sinang = np.sin(angle_rads)    mtx[0][0] = mtx[1][1] = cosang    mtx[1][0] = sinang    mtx[0][1] = -sinang",
        "labels_text": "Update existing Z rotation matrix to new angle"
    },
    {
        "input_text": "summarize: def set_Y_homog_rot_mtx(angle_rads: float, mtx: np.ndarray):        cosang = np.cos(angle_rads)    sinang = np.sin(angle_rads)    mtx[0][0] = mtx[2][2] = cosang    mtx[0][2] = sinang    mtx[2][0] = -sinang",
        "labels_text": "Update existing Y rotation matrix to new angle"
    },
    {
        "input_text": "summarize: def set_X_homog_rot_mtx(angle_rads: float, mtx: np.ndarray):        cosang = np.cos(angle_rads)    sinang = np.sin(angle_rads)    mtx[1][1] = mtx[2][2] = cosang    mtx[2][1] = sinang    mtx[1][2] = -sinang",
        "labels_text": "Update existing X rotation matrix to new angle"
    },
    {
        "input_text": "summarize: def homog_trans_mtx(x: float, y: float, z: float) -> np.ndarray:        return np.array(        ((1, 0, 0, x), (0, 1, 0, y), (0, 0, 1, z), (0, 0, 0, 1)),        dtype=np.float64,    )",
        "labels_text": "Generate a x NumPy translation matrix param x y z translation in each axis"
    },
    {
        "input_text": "summarize: def set_homog_trans_mtx(x: float, y: float, z: float, mtx: np.ndarray):        mtx[0][3] = x    mtx[1][3] = y    mtx[2][3] = z",
        "labels_text": "Update existing translation matrix to new value"
    },
    {
        "input_text": "summarize: def homog_scale_mtx(scale: float) -> np.ndarray:        return np.array(        [[scale, 0, 0, 0], [0, scale, 0, 0], [0, 0, scale, 0], [0, 0, 0, 1]],        dtype=np.float64,    )",
        "labels_text": "Generate a x NumPy scaling matrix param float scale scale multiplier"
    },
    {
        "input_text": "summarize: def get_spherical_coordinates(xyz: np.ndarray) -> tuple[float, float, float]:        r = float(np.linalg.norm(xyz))    if 0 == r:        return (0, 0, 0)    azimuth = _get_azimuth(xyz[0], xyz[1])    polar_angle: float = np.arccos(xyz[2] / r)    return (r, azimuth, polar_angle)",
        "labels_text": "Compute spherical coordinate r azimuth polarangle for XYZ point param array xyz column vector row x column NumPy array return tuple of r azimuth polarangle for input coordinate"
    },
    {
        "input_text": "summarize: def multi_rot_Y(angle_rads: np.ndarray) -> np.ndarray:        ry = np.empty((angle_rads.shape[0], 4, 4))    ry[...] = np.identity(4)    ry[:, 0, 0] = ry[:, 2, 2] = np.cos(angle_rads)    ry[:, 0, 2] = np.sin(angle_rads)    ry[:, 2, 0] = -ry[:, 0, 2]    return ry",
        "labels_text": "Create entry NumPy Y rotation matrix for entry angle param entry int number of matrix generated param anglerads NumPy array of angle return entry x x homogeneous rotation matrix"
    },
    {
        "input_text": "summarize: def __init__(self):                self.this_type = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def set_entity_info(self, chain_indices, sequence, description, entity_type):                for chain_ind in chain_indices:            self.chain_index_to_type_map[chain_ind] = entity_type            self.chain_index_to_seq_map[chain_ind] = sequence            self.chain_index_to_description_map[chain_ind] = description",
        "labels_text": "Set the entity level information for the structure param chainindices the index of the chain for this entity param sequence the one letter code sequence for this entity param description the description for this entity param entitytype the entity type polymernonpolymerwater"
    },
    {
        "input_text": "summarize: def set_model_info(self, model_id, chain_count):                self.structure_builder.init_model(model_id)",
        "labels_text": "Set the information for a model param modelid the index for the model param chaincount the number of chain in the model"
    },
    {
        "input_text": "summarize: def set_xtal_info(self, space_group, unit_cell):                self.structure_builder.set_symmetry(space_group, unit_cell)",
        "labels_text": "Set the crystallographic information for the structure param spacegroup the space group name eg P param unitcell an array of length with the unit cell parameter in order a b c alpha beta gamma"
    },
    {
        "input_text": "summarize: def set_header_info(        self,        r_free,        r_work,        resolution,        title,        deposition_date,        release_date,        experimnetal_methods,    ):",
        "labels_text": "Set the header information param rfree the measured RFree for the structure param rwork the measure RWork for the structure param resolution the resolution of the structure param title the title of the structure param depositiondate the deposition date of the structure param releasedate the release date of the structure param experimnetalmethods the list of experimental method in the structure"
    },
    {
        "input_text": "summarize: def set_bio_assembly_trans(        self, bio_assembly_index, input_chain_indices, input_transform    ):",
        "labels_text": "Set the Bioassembly transformation information A single bioassembly can have multiple transforms param bioassemblyindex the integer index of the bioassembly param inputchainindices the list of integer index for the chain of this bioassembly param inputtransform the list of double for the transform of this bioassmbly transform"
    },
    {
        "input_text": "summarize: def finalize_structure(self):",
        "labels_text": "Any function needed to cleanup the structure"
    },
    {
        "input_text": "summarize: def set_group_bond(self, atom_index_one, atom_index_two, bond_order):",
        "labels_text": "Add bond within a group param atomindexone the integer atom index in the group of the first partner in the bond param atomindextwo the integer atom index in the group of the second partner in the bond param bondorder the integer bond order"
    },
    {
        "input_text": "summarize: def set_inter_group_bond(self, atom_index_one, atom_index_two, bond_order):",
        "labels_text": "Add bond between group param atomindexone the integer atom index in the structure of the first partner in the bond param atomindextwo the integer atom index in the structure of the second partner in the bond param bondorder the bond order"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Initialise"
    },
    {
        "input_text": "summarize: def save(self, filepath, select=_select):                # Similar to the PDBIO save method, we check if the filepath is a        # string for a filepath or an open file handle        if not isinstance(filepath, str):            raise ValueError(                \"Writing to a file handle is not supported for MMTF, filepath must be a string\"            )        if hasattr(self, \"structure\"):            self._save_structure(filepath, select)        else:            raise ValueError(\"Use set_structure to set a structure to write out\")",
        "labels_text": "Save the structure to a file param filepath output file type filepath string param select selects which entity will be written type select object Typically select is a subclass of LSelect it should have the following method acceptmodelmodel acceptchainchain acceptresidueresidue acceptatomatom These method should return if the entity is to be written out otherwise"
    },
    {
        "input_text": "summarize: def _chain_id_iterator(self):                for size in itertools.count(1):            for s in itertools.product(ascii_uppercase, repeat=size):                yield \"\".join(s)",
        "labels_text": "Label chain sequentially A B Z AA AB etc"
    },
    {
        "input_text": "summarize: def get_from_decoded(decoder):        structure_decoder = StructureDecoder()    decoder.pass_data_on(structure_decoder)    return structure_decoder.structure_builder.get_structure()",
        "labels_text": "Return structure from decoder"
    },
    {
        "input_text": "summarize: def get_structure_from_url(pdb_id):                decoder = fetch(pdb_id)        return get_from_decoded(decoder)",
        "labels_text": "Get a structure from a URL given a PDB id param pdbid the input PDB id return the structure"
    },
    {
        "input_text": "summarize: def get_structure(file_path):                decoder = parse(file_path)        return get_from_decoded(decoder)",
        "labels_text": "Get a structure from a file given a file path param filepath the input file path return the structure"
    },
    {
        "input_text": "summarize: def _update(self):                self._rows = sorted({x[0] for x in self._wells})        self._columns = sorted({x[1:] for x in self._wells})",
        "labels_text": "Update the row and column string identifier PRIVATE"
    },
    {
        "input_text": "summarize: def _is_well(self, obj):                # Value should be of WellRecord type        if not isinstance(obj, WellRecord):            raise ValueError(                f\"A WellRecord type object is needed as value (got {type(obj)})\"            )",
        "labels_text": "Check if the given object is a WellRecord object PRIVATE Used both for the class constructor and the setitem method"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._wells)",
        "labels_text": "Return the number of well in this plate"
    },
    {
        "input_text": "summarize: def __add__(self, plate):                if not isinstance(plate, PlateRecord):            raise TypeError(\"Expecting a PlateRecord object\")        if {x.id for x in self} != {x.id for x in plate}:            raise ValueError(\"The two plates have different wells\")        wells = []        for w in self:            wells.append(w + plate[w.id])        newp = PlateRecord(self.id, wells=wells)        return newp",
        "labels_text": "Add another PlateRecord object The well in both plate must be the same A new PlateRecord object is returned having the same id a the left operand"
    },
    {
        "input_text": "summarize: def __sub__(self, plate):                if not isinstance(plate, PlateRecord):            raise TypeError(\"Expecting a PlateRecord object\")        if {x.id for x in self} != {x.id for x in plate}:            raise ValueError(\"The two plates have different wells\")        wells = []        for w in self:            wells.append(w - plate[w.id])        newp = PlateRecord(self.id, wells=wells)        return newp",
        "labels_text": "Subtract another PlateRecord object The well in both plate must be the same A new PlateRecord object is returned having the same id a the left operand"
    },
    {
        "input_text": "summarize: def get_row(self, row):                # Key is casted to str implicitly        try:            row = str(row)        except Exception:            # Is it even possible to get an exception here?            raise ValueError(\"Row identifier should be string-like\")        if len(row) > 1:            raise ValueError(\"Row identifier must be of maximum one letter\")        for w in sorted(filter(lambda x: x.startswith(row), self._wells)):            yield self._wells[w]",
        "labels_text": "Get all the well of a given row A row is identified with a letter eg A"
    },
    {
        "input_text": "summarize: def get_column(self, column):                # Column is casted to int implicitly        try:            column = int(column)        except Exception:            raise ValueError(\"Column identifier should be a number\")        # A 96-well plate has well numbers in two digits        for w in sorted(filter(lambda x: x.endswith(\"%02d\" % column), self._wells)):            yield self._wells[w]",
        "labels_text": "Get all the well of a given column A column is identified with a number eg"
    },
    {
        "input_text": "summarize: def _interpolate(self, time):                times = sorted(self._signals.keys())        return np.interp(            time, times, [self._signals[x] for x in times], left=np.nan, right=np.nan        )",
        "labels_text": "Linear interpolation of the signal at certain time point PRIVATE"
    },
    {
        "input_text": "summarize: def __setitem__(self, time, signal):                try:            time = float(time)        except ValueError:            raise ValueError(\"Time point should be a number\")        try:            signal = float(signal)        except ValueError:            raise ValueError(\"Signal should be a number\")        self._signals[time] = signal",
        "labels_text": "Assign a signal at a certain time point"
    },
    {
        "input_text": "summarize: def __add__(self, well):                if not isinstance(well, WellRecord):            raise TypeError(\"Expecting a WellRecord object\")        signals = {}        times = set(self._signals.keys()).union(set(well._signals.keys()))        for t in sorted(times):            signals[t] = self[t] + well[t]        neww = WellRecord(self.id, signals=signals)        return neww",
        "labels_text": "Add another WellRecord object A new WellRecord object is returned having the same id a the left operand"
    },
    {
        "input_text": "summarize: def __sub__(self, well):                if not isinstance(well, WellRecord):            raise TypeError(\"Expecting a WellRecord object\")        signals = {}        times = set(self._signals.keys()).union(set(well._signals.keys()))        for t in sorted(times):            signals[t] = self[t] - well[t]        neww = WellRecord(self.id, signals=signals)        return neww",
        "labels_text": "Subtract another WellRecord object A new WellRecord object is returned having the same id a the left operand"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._signals)",
        "labels_text": "Return the number of time point sampled"
    },
    {
        "input_text": "summarize: def get_raw(self):                return [(t, self._signals[t]) for t in sorted(self._signals.keys())]",
        "labels_text": "Get a list of timesignal pair"
    },
    {
        "input_text": "summarize: def get_times(self):                return sorted(self._signals.keys())",
        "labels_text": "Get a list of the recorded time point"
    },
    {
        "input_text": "summarize: def get_signals(self):                return [self._signals[t] for t in sorted(self._signals.keys())]",
        "labels_text": "Get a list of the recorded signal ordered by collection time"
    },
    {
        "input_text": "summarize: def __init__(self, plates):                self.plates = plates",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def write(self, handle):                out = []        for plate in self.plates:            try:                out.append(_toOPM(plate))            except ValueError:                raise ValueError(\"Could not export plate(s) in JSON format\")        handle.write(json.dumps(out) + \"\\n\")        return len(out)",
        "labels_text": "Write this instance plate to a file handle"
    },
    {
        "input_text": "summarize: def logistic(x, A, u, d, v, y0):        y = (A / (1 + np.exp((((4 * u) / A) * (d - x)) + 2))) + y0    return y",
        "labels_text": "Logistic growth model Proposed in Zwietering et al PMID"
    },
    {
        "input_text": "summarize: def gompertz(x, A, u, d, v, y0):        y = (A * np.exp(-np.exp((((u * np.e) / A) * (d - x)) + 1))) + y0    return y",
        "labels_text": "Gompertz growth model Proposed in Zwietering et al PMID"
    },
    {
        "input_text": "summarize: def richards(x, A, u, d, v, y0):        y = (        A        * pow(            1            + (                v                + (np.exp(1 + v) * np.exp((u / A) * (1 + v) * (1 + (1 / v)) * (d - x)))            ),            -(1 / v),        )    ) + y0    return y",
        "labels_text": "Richards growth model equivalent to Stannard Proposed in Zwietering et al PMID"
    },
    {
        "input_text": "summarize: def fit(function, x, y):        # Compute guesses for the parameters    # This is necessary to get significant fits    p0 = [guess_plateau(x, y), 4.0, guess_lag(x, y), 0.1, min(y)]    params, pcov = curve_fit(function, x, y, p0=p0)    return params, pcov",
        "labels_text": "Fit the provided function to the x and y value The function parameter and the parameter covariance"
    },
    {
        "input_text": "summarize: def get_area(y, x):        return trapezoid(y=y, x=x)",
        "labels_text": "Get the area under the curve"
    },
    {
        "input_text": "summarize: def read(handle, format):        iterator = parse(handle, format)    try:        first = next(iterator)    except StopIteration:        first = None    if first is None:        raise ValueError(\"No records found in handle\")    try:        second = next(iterator)    except StopIteration:        second = None    if second is not None:        raise ValueError(\"More than one record found in handle\")    return first",
        "labels_text": "Turn a phenotype file into a single PlateRecord handle handle to the file or the filename a a string note older version of Biopython only took a handle format string describing the file format This function is for use parsing phenotype file containing exactly one record For example reading a PM JSON file from Bio import phenotype record phenotypereadphenotypePlatejson pmjson printID s recordid ID PM printNumber of well i lenrecord Number of well If the handle contains no record or more than one record an exception is raised For example from Bio import phenotype record phenotypereadplatescsv pmcsv Traceback most recent call last ValueError More than one record found in handle If however you want the first record from a file containing multiple record this function would raise an exception a shown in the example above Instead use from Bio import phenotype record nextphenotypeparsephenotypePlatescsv pmcsv printFirst record ID s recordid First record ID PM Use the Biophenotypeparsehandle format function if you want to read multiple record from the handle"
    },
    {
        "input_text": "summarize: def _level_traverse(root, get_children):        Q = collections.deque([root])    while Q:        v = Q.popleft()        yield v        Q.extend(get_children(v))",
        "labels_text": "Traverse a tree in breadthfirst level order PRIVATE"
    },
    {
        "input_text": "summarize: def _preorder_traverse(root, get_children):        def dfs(elem):        yield elem        for v in get_children(elem):            yield from dfs(v)    yield from dfs(root)",
        "labels_text": "Traverse a tree in depthfirst preorder parent before child PRIVATE"
    },
    {
        "input_text": "summarize: def _postorder_traverse(root, get_children):        def dfs(elem):        for v in get_children(elem):            yield from dfs(v)        yield elem    yield from dfs(root)",
        "labels_text": "Traverse a tree in depthfirst postorder child before parent PRIVATE"
    },
    {
        "input_text": "summarize: def _sorted_attrs(elem):        singles = []    lists = []    # Sort attributes for consistent results    for attrname, child in sorted(elem.__dict__.items(), key=lambda kv: kv[0]):        if child is None:            continue        if isinstance(child, list):            lists.extend(child)        else:            singles.append(child)    return (x for x in singles + lists if isinstance(x, TreeElement))",
        "labels_text": "Get a flat list of elems attribute sorted for consistency PRIVATE"
    },
    {
        "input_text": "summarize: def _identity_matcher(target):        def match(node):        return node is target    return match",
        "labels_text": "Match a node to the target object by identity PRIVATE"
    },
    {
        "input_text": "summarize: def _class_matcher(target_cls):        def match(node):        return isinstance(node, target_cls)    return match",
        "labels_text": "Match a node if it an instance of the given class PRIVATE"
    },
    {
        "input_text": "summarize: def _function_matcher(matcher_func):        def match(node):        try:            return matcher_func(node)        except (LookupError, AttributeError, ValueError, TypeError):            return False    return match",
        "labels_text": "Safer attribute lookup return False instead of raising an error PRIVATE"
    },
    {
        "input_text": "summarize: def _object_matcher(obj):        if isinstance(obj, TreeElement):        return _identity_matcher(obj)    if isinstance(obj, type):        return _class_matcher(obj)    if isinstance(obj, str):        return _string_matcher(obj)    if isinstance(obj, dict):        return _attribute_matcher(obj)    if callable(obj):        return _function_matcher(obj)    raise ValueError(f\"{obj} (type {type(obj)}) is not a valid type for comparison.\")",
        "labels_text": "Retrieve a matcher function by passing an arbitrary object PRIVATE Passing a TreeElement such a a Clade or Tree instance return an identity matcher passing a type such a the PhyloXMLTaxonomy class return a class matcher and passing a dictionary return an attribute matcher The resulting match function return True when given an object matching the specification identity type or attribute value otherwise False This is useful for writing function that search the tree and probably shouldnt be used directly by the end user"
    },
    {
        "input_text": "summarize: def _combine_matchers(target, kwargs, require_spec):        if not target:        if not kwargs:            if require_spec:                raise ValueError(                    \"you must specify a target object or keyword arguments.\"                )            return lambda x: True        return _attribute_matcher(kwargs)    match_obj = _object_matcher(target)    if not kwargs:        return match_obj    match_kwargs = _attribute_matcher(kwargs)    return lambda x: match_obj(x) and match_kwargs(x)",
        "labels_text": "Merge target specification with keyword argument PRIVATE Dispatch the component to the various matcher function then merge into a single boolean function"
    },
    {
        "input_text": "summarize: def find_any(self, *args, **kwargs):                hits = self.find_elements(*args, **kwargs)        try:            return next(hits)        except StopIteration:            return None",
        "labels_text": "Return the first element found by findelements or None This is also useful for checking whether any matching element exists in the tree and can be used in a conditional expression"
    },
    {
        "input_text": "summarize: def find_elements(self, target=None, terminal=None, order=\"preorder\", **kwargs):                if terminal is not None:            kwargs[\"terminal\"] = terminal        is_matching_elem = _combine_matchers(target, kwargs, False)        return self._filter_search(is_matching_elem, order, True)",
        "labels_text": "Find all tree element matching the given attribute The arbitrary keyword argument indicate the attribute name of the subelement and the value to match string integer or boolean Strings are evaluated a regular expression match integer are compared directly for equality and booleans evaluate the attribute truth value True or False before comparing To handle nonzero float search with a boolean argument then filter the result manually If no keyword argument are given then just the class type is used for matching The result is an iterable through all matching object by depthfirst search Not necessarily the same order a the element appear in the source file Parameters target TreeElement instance type dict or callable Specifies the characteristic to search for The default TreeElement match any standard BioPhylo type terminal bool A boolean value to select for or against terminal node aka leaf node True search for only terminal node False excludes terminal node and the default None search both terminal and nonterminal node a well a any tree element lacking the isterminal method order preorder postorder level Tree traversal order preorder default is depthfirst search postorder is DFS with child node preceding parent and level is breadthfirst search Examples from Bio import Phylo phx PhyloPhyloXMLIOreadPhyloXMLphyloxmlexamplesxml match phxphylogeniesfindelementscodeOCTVU nextmatches TaxonomycodeOCTVU scientificnameOctopus vulgaris"
    },
    {
        "input_text": "summarize: def get_nonterminals(self, order=\"preorder\"):                return list(self.find_clades(terminal=False, order=order))",
        "labels_text": "Get a list of all of this tree nonterminal internal node"
    },
    {
        "input_text": "summarize: def get_terminals(self, order=\"preorder\"):                return list(self.find_clades(terminal=True, order=order))",
        "labels_text": "Get a list of all of this tree terminal leaf node"
    },
    {
        "input_text": "summarize: def trace(self, start, finish):                mrca = self.common_ancestor(start, finish)        fromstart = mrca.get_path(start)[-2::-1]        to = mrca.get_path(finish)        return fromstart + [mrca] + to",
        "labels_text": "List of all clade object between two target in this tree Excluding start including finish"
    },
    {
        "input_text": "summarize: def count_terminals(self):                return sum(1 for clade in self.find_clades(terminal=True))",
        "labels_text": "Count the number of terminal leaf node within this tree"
    },
    {
        "input_text": "summarize: def distance(self, target1, target2=None):                if target2 is None:            return sum(                n.branch_length                for n in self.get_path(target1)                if n.branch_length is not None            )        mrca = self.common_ancestor(target1, target2)        return mrca.distance(target1) + mrca.distance(target2)",
        "labels_text": "Calculate the sum of the branch length between two target If only one target is specified the other is the root of this tree"
    },
    {
        "input_text": "summarize: def is_parent_of(self, target=None, **kwargs):                return self.get_path(target, **kwargs) is not None",
        "labels_text": "Check if target is a descendent of this tree Not required to be a direct descendent To check only direct descendent of a clade simply use list membership testing if subclade in clade"
    },
    {
        "input_text": "summarize: def is_preterminal(self):                if self.root.is_terminal():            return False        for clade in self.root.clades:            if not clade.is_terminal():                return False        return True",
        "labels_text": "Check if all direct descendent are terminal"
    },
    {
        "input_text": "summarize: def total_branch_length(self):                return sum(node.branch_length for node in self.find_clades(branch_length=True))",
        "labels_text": "Calculate the sum of all the branch length in this tree"
    },
    {
        "input_text": "summarize: def collapse_all(self, target=None, **kwargs):                # Read the iterable into a list to protect against in-place changes        matches = list(self.find_clades(target, False, \"level\", **kwargs))        if not matches:            # No matching nodes to collapse            return        # Skip the root node -- it can't be collapsed        if matches[0] == self.root:            matches.pop(0)        for clade in matches:            self.collapse(clade)",
        "labels_text": "Collapse all the descendent of this tree leaving only terminal Total branch length are preserved ie the distance to each terminal stay the same For example this will safely collapse node with poor bootstrap support from Bio import Phylo tree PhyloreadPhyloXMLapafxml phyloxml printTotal branch length f treetotalbranchlength Total branch length treecollapsealllambda c cconfidence is not None and cconfidence printTotal branch length f treetotalbranchlength Total branch length This implementation avoids strange sideeffects by using levelorder traversal and testing all clade property versus the target specification up front In particular if a clade meet the target specification in the original tree it will be collapsed For example if the condition is from Bio import Phylo tree PhyloreadPhyloXMLapafxml phyloxml printTotal branch length f treetotalbranchlength Total branch length treecollapsealllambda c cbranchlength printTotal branch length f treetotalbranchlength Total branch length Collapsing a clade parent node add the parent branch length to the child so during the execution of collapseall a clade branchlength may increase In this implementation clade are collapsed according to their property in the original tree not the property when tree traversal reach the clade Its easier to debug If you want the other behavior incremental testing modifying the source code of this function is straightforward"
    },
    {
        "input_text": "summarize: def ladderize(self, reverse=False):                self.root.clades.sort(key=lambda c: c.count_terminals(), reverse=reverse)        for subclade in self.root.clades:            subclade.ladderize(reverse=reverse)",
        "labels_text": "Sort clade inplace according to the number of terminal node Deepest clade are last by default Use reverseTrue to sort clade deepesttoshallowest"
    },
    {
        "input_text": "summarize: def split(self, n=2, branch_length=1.0):                clade_cls = type(self.root)        base_name = self.root.name or \"n\"        for i in range(n):            clade = clade_cls(name=base_name + str(i), branch_length=branch_length)            self.root.clades.append(clade)",
        "labels_text": "Generate n default new descendant In a specie tree this is a speciation event New clade have the given branchlength and the same name a this clade root plus an integer suffix counting from For example splitting a clade named A produce subclades named A and A If the clade ha no name the prefix n is used for child node eg n and n"
    },
    {
        "input_text": "summarize: def __init__(self, root=None, rooted=True, id=None, name=None):                self.root = root or Clade()        self.rooted = rooted        self.id = id        self.name = name",
        "labels_text": "Initialize parameter for phylogenetic tree"
    },
    {
        "input_text": "summarize: def from_clade(cls, clade, **kwargs):                root = copy.deepcopy(clade)        return cls(root, **kwargs)",
        "labels_text": "Create a new Tree object given a clade Keyword argument are the usual Tree constructor parameter"
    },
    {
        "input_text": "summarize: def clade(self):                return self.root",
        "labels_text": "Return first clade in this tree not itself"
    },
    {
        "input_text": "summarize: def as_phyloxml(self, **kwargs):                from Bio.Phylo.PhyloXML import Phylogeny        return Phylogeny.from_tree(self, **kwargs)",
        "labels_text": "Convert this tree to a PhyloXMLcompatible Phylogeny This let you use the additional annotation type PhyloXML defines and save this information when you write this tree a phyloxml"
    },
    {
        "input_text": "summarize: def is_terminal(self):                return not self.root.clades",
        "labels_text": "Check if the root of this tree is terminal"
    },
    {
        "input_text": "summarize: def __format__(self, format_spec):                if format_spec:            from io import StringIO            from Bio.Phylo import _io            handle = StringIO()            _io.write([self], handle, format_spec)            return handle.getvalue()        else:            # Follow python convention and default to using __str__            return str(self)",
        "labels_text": "Serialize the tree a a string in the specified file format This method support Pythons format builtin function param formatspec a lowercase string supported by BioPhylowrite a an output file format"
    },
    {
        "input_text": "summarize: def format(self, fmt=None):                return self.__format__(fmt)",
        "labels_text": "Serialize the tree a a string in the specified file format param fmt a lowercase string supported by BioPhylowrite a an output file format"
    },
    {
        "input_text": "summarize: def __init__(        self,        branch_length=None,        name=None,        clades=None,        confidence=None,        color=None,        width=None,    ):                self.branch_length = branch_length        self.name = name        self.clades = clades or []        self.confidence = confidence        self.color = color        self.width = width",
        "labels_text": "Define parameter for the Clade tree"
    },
    {
        "input_text": "summarize: def root(self):                return self",
        "labels_text": "Allow TreeMixin method to traverse clade properly"
    },
    {
        "input_text": "summarize: def is_terminal(self):                return not self.clades",
        "labels_text": "Check if this is a terminal leaf node"
    },
    {
        "input_text": "summarize: def __getitem__(self, index):                if isinstance(index, (int, slice)):            return self.clades[index]        ref = self        for idx in index:            ref = ref[idx]        return ref",
        "labels_text": "Get clade by index integer or slice"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.clades)",
        "labels_text": "Iterate through this tree direct descendent clade subtrees"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.clades)",
        "labels_text": "Return the number of clade directly under the root"
    },
    {
        "input_text": "summarize: def __bool__(self):                return True",
        "labels_text": "Boolean value of an instance of this class True NB If this method is not defined but len is then the object is considered true if the result of len is nonzero We want Clade instance to always be considered True"
    },
    {
        "input_text": "summarize: def __str__(self) -> str:                if self.name:            return self.name[:37] + \"...\" if len(self.name) > 40 else self.name        return self.__class__.__name__",
        "labels_text": "Return name of the class instance"
    },
    {
        "input_text": "summarize: def __init__(self, red, green, blue):                for color in (red, green, blue):            assert (                isinstance(color, int) and 0 <= color <= 255            ), \"Color values must be integers between 0 and 255.\"        self.red = red        self.green = green        self.blue = blue",
        "labels_text": "Initialize BranchColor for a tree"
    },
    {
        "input_text": "summarize: def from_hex(cls, hexstr):                assert (            isinstance(hexstr, str) and hexstr.startswith(\"#\") and len(hexstr) == 7        ), \"need a 24-bit hexadecimal string, e.g. #000000\"        RGB = hexstr[1:3], hexstr[3:5], hexstr[5:]        return cls(*(int(\"0x\" + cc, base=16) for cc in RGB))",
        "labels_text": "Construct a BranchColor object from a hexadecimal string The string format is the same style used in HTML and CSS such a FF for an RGB value of"
    },
    {
        "input_text": "summarize: def from_name(cls, colorname):                return cls(*cls.color_names[colorname])",
        "labels_text": "Construct a BranchColor object by the color name"
    },
    {
        "input_text": "summarize: def to_hex(self):                return f\"#{self.red:02x}{self.green:02x}{self.blue:02x}\"",
        "labels_text": "Return a bit hexadecimal RGB representation of this color The returned string is suitable for use in HTMLCSS a a color parameter in matplotlib and perhaps other situation Examples bc BranchColor bctohex cc"
    },
    {
        "input_text": "summarize: def to_rgb(self):                return (self.red, self.green, self.blue)",
        "labels_text": "Return a tuple of RGB value to representing this color Examples bc BranchColor bctorgb"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                return \"%s(red=%d, green=%d, blue=%d)\" % (            self.__class__.__name__,            self.red,            self.green,            self.blue,        )",
        "labels_text": "Preserve the standard RGB order when representing this object"
    },
    {
        "input_text": "summarize: def __str__(self) -> str:                return \"(%d, %d, %d)\" % (self.red, self.green, self.blue)",
        "labels_text": "Show the color RGB value"
    },
    {
        "input_text": "summarize: def __init__(self, root=None, rooted=False, id=None, name=None, weight=1.0):                BaseTree.Tree.__init__(            self, root=root or Clade(), rooted=rooted, id=id, name=name        )        self.weight = weight        # a list of (predicate, object) pairs, containing additional triples        # using this tree as subject        self.attributes = []",
        "labels_text": "Initialize value of for the CDAO tree object"
    },
    {
        "input_text": "summarize: def qUri(x):        return resolve_uri(x, namespaces=RDF_NAMESPACES)",
        "labels_text": "Resolve URI for librdf"
    },
    {
        "input_text": "summarize: def format_label(x):        return x.replace(\"_\", \" \")",
        "labels_text": "Format label for librdf"
    },
    {
        "input_text": "summarize: def parse(handle, **kwargs):        return Parser(handle).parse(**kwargs)",
        "labels_text": "Iterate over the tree in a CDAO file handle return generator of BioPhyloCDAOTree object"
    },
    {
        "input_text": "summarize: def write(trees, handle, plain=False, **kwargs):        return Writer(trees).write(handle, plain=plain, **kwargs)",
        "labels_text": "Write a tree in CDAO format to the given file handle return number of tree written"
    },
    {
        "input_text": "summarize: def __init__(self, handle=None):                self.handle = handle        self.graph = None        self.node_info = None        self.children = {}        self.rooted = False",
        "labels_text": "Initialize CDAO tree parser"
    },
    {
        "input_text": "summarize: def from_string(cls, treetext):                handle = StringIO(treetext)        return cls(handle)",
        "labels_text": "Instantiate the class from the given string"
    },
    {
        "input_text": "summarize: def parse(self, **kwargs):                self.parse_handle_to_graph(**kwargs)        return self.parse_graph()",
        "labels_text": "Parse the text stream this object wa initialized with"
    },
    {
        "input_text": "summarize: def parse_graph(self, graph=None, context=None):                if graph is None:            graph = self.graph        # look up branch lengths/TUs for all nodes        self.get_node_info(graph, context=context)        for root_node in self.tree_roots:            clade = self.parse_children(root_node)            yield CDAO.Tree(root=clade, rooted=self.rooted)",
        "labels_text": "Iterate over RDF model yielding CDAOTree instance"
    },
    {
        "input_text": "summarize: def new_clade(self, node):                result = self.node_info[node]        kwargs = {}        if \"branch_length\" in result:            kwargs[\"branch_length\"] = result[\"branch_length\"]        if \"label\" in result:            kwargs[\"name\"] = result[\"label\"].replace(\"_\", \" \")        if \"confidence\" in result:            kwargs[\"confidence\"] = result[\"confidence\"]        clade = CDAO.Clade(**kwargs)        return clade",
        "labels_text": "Return a CDAOClade object for a given named node"
    },
    {
        "input_text": "summarize: def parse_children(self, node):                clade = self.new_clade(node)        children = self.children[node] if node in self.children else []        clade.clades = [self.parse_children(child_node) for child_node in children]        return clade",
        "labels_text": "Traverse the tree to create a nested clade structure Return a CDAOClade and call itself recursively for each child traversing the entire tree and creating a nested structure of CDAOClade object"
    },
    {
        "input_text": "summarize: def __init__(self, trees):                self.trees = trees        self.node_counter = 0        self.edge_counter = 0        self.tu_counter = 0        self.tree_counter = 0",
        "labels_text": "Initialize parameter for writing a CDAO tree"
    },
    {
        "input_text": "summarize: def __new__(cls, strdata):                if isinstance(strdata, str) and len(strdata) == strdata.count(            \"0\"        ) + strdata.count(\"1\"):            return str.__new__(cls, strdata)        else:            raise TypeError(                \"The input should be a binary string composed of '0' and '1'\"            )",
        "labels_text": "Init from a binary string data"
    },
    {
        "input_text": "summarize: def index_one(self):                return [i for i, n in enumerate(self) if n == \"1\"]",
        "labels_text": "Return a list of position where the element is"
    },
    {
        "input_text": "summarize: def index_zero(self):                return [i for i, n in enumerate(self) if n == \"0\"]",
        "labels_text": "Return a list of position where the element is"
    },
    {
        "input_text": "summarize: def contains(self, other):                xorbit = self ^ other        return xorbit.count(\"1\") == self.count(\"1\") - other.count(\"1\")",
        "labels_text": "Check if current bitstr contains another one bitstr That is to say the bitstrindexone is a subset of bitstrindexone Examples contains Be careful also contains Actually all BitString object contain allzero BitString of the same length"
    },
    {
        "input_text": "summarize: def independent(self, other):                xorbit = self ^ other        return xorbit.count(\"1\") == self.count(\"1\") + other.count(\"1\")",
        "labels_text": "Check if current bitstr is independent of another one bitstr That is to say the bitstrindexone and bitstrindexone have no intersection Be careful all BitString object are independent of allzero BitString of the same length"
    },
    {
        "input_text": "summarize: def iscompatible(self, other):                return self.contains(other) or other.contains(self) or self.independent(other)",
        "labels_text": "Check if current bitstr is compatible with another bitstr Two condition are considered a compatible bitstrcontainbitstr or vice versa bitstrindependentbitstr"
    },
    {
        "input_text": "summarize: def adam_consensus(trees):        clades = [tree.root for tree in trees]    return BaseTree.Tree(root=_part(clades), rooted=True)",
        "labels_text": "Search Adam Consensus tree from multiple tree Parameters tree list list of tree to produce consensus tree"
    },
    {
        "input_text": "summarize: def bootstrap(msa, times):        length = len(msa[0])    i = 0    while i < times:        i += 1        item = None        for j in range(length):            col = random.randint(0, length - 1)            if not item:                item = msa[:, col : col + 1]            else:                item += msa[:, col : col + 1]        yield item",
        "labels_text": "Generate bootstrap replicates from a multiple sequence alignment OBSOLETE Parameters msa MultipleSeqAlignment multiple sequence alignment to generate replicates time int number of bootstrap time"
    },
    {
        "input_text": "summarize: def bootstrap_consensus(alignment, times, tree_constructor, consensus):        trees = bootstrap_trees(alignment, times, tree_constructor)    tree = consensus(trees)    return tree",
        "labels_text": "Consensus tree of a series of bootstrap tree for a multiple sequence alignment Parameters alignment Alignment or MultipleSeqAlignment object Multiple sequence alignment to generate replicates time int Number of bootstrap time treeconstructor TreeConstructor Tree constructor to be used to build tree consensus function Consensus method in this module strictconsensus majorityconsensus adamconsensus"
    },
    {
        "input_text": "summarize: def _clade_to_bitstr(clade, tree_term_names):        clade_term_names = {term.name for term in clade.find_clades(terminal=True)}    return _BitString.from_bool((name in clade_term_names) for name in tree_term_names)",
        "labels_text": "Create a BitString representing a clade given ordered tree taxon name PRIVATE"
    },
    {
        "input_text": "summarize: def _tree_to_bitstrs(tree):        clades_bitstrs = {}    term_names = [term.name for term in tree.find_clades(terminal=True)]    for clade in tree.find_clades(terminal=False):        bitstr = _clade_to_bitstr(clade, term_names)        clades_bitstrs[clade] = bitstr    return clades_bitstrs",
        "labels_text": "Create a dict of a tree clade to corresponding BitStrings PRIVATE"
    },
    {
        "input_text": "summarize: def _bitstring_topology(tree):        bitstrs = {}    for clade, bitstr in _tree_to_bitstrs(tree).items():        bitstrs[bitstr] = round(clade.branch_length or 0.0, 5)    return bitstrs",
        "labels_text": "Generate a branch length dict for a tree keyed by BitStrings PRIVATE Create a dict of all clade BitStrings to the corresponding branch length rounded to decimal place"
    },
    {
        "input_text": "summarize: def _equal_topology(tree1, tree2):        term_names1 = {term.name for term in tree1.find_clades(terminal=True)}    term_names2 = {term.name for term in tree2.find_clades(terminal=True)}    return (term_names1 == term_names2) and (        _bitstring_topology(tree1) == _bitstring_topology(tree2)    )",
        "labels_text": "Are two tree are equal in term of topology and branch length PRIVATE Branch length checked to decimal place"
    },
    {
        "input_text": "summarize: def __init__(self, root=None, rooted=False, id=None, name=None, weight=1.0):                BaseTree.Tree.__init__(            self, root=root or Clade(), rooted=rooted, id=id, name=name        )        self.weight = weight",
        "labels_text": "Initialize parameter for a Newick tree object"
    },
    {
        "input_text": "summarize: def __init__(        self, branch_length=None, name=None, clades=None, confidence=None, comment=None    ):                BaseTree.Clade.__init__(            self,            branch_length=branch_length,            name=name,            clades=clades,            confidence=confidence,        )        self.comment = comment",
        "labels_text": "Initialize parameter for a Newick Clade object"
    },
    {
        "input_text": "summarize: def parse(handle, **kwargs):        return Parser(handle).parse(**kwargs)",
        "labels_text": "Iterate over the tree in a Newick file handle return generator of BioPhyloNewickTree object"
    },
    {
        "input_text": "summarize: def write(trees, handle, plain=False, **kwargs):        return Writer(trees).write(handle, plain=plain, **kwargs)",
        "labels_text": "Write a tree in Newick format to the given file handle return number of tree written"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                if handle.read(0) != \"\":            raise ValueError(\"Newick files must be opened in text mode\") from None        self.handle = handle",
        "labels_text": "Initialize file handle for the Newick Tree"
    },
    {
        "input_text": "summarize: def from_string(cls, treetext):                handle = StringIO(treetext)        return cls(handle)",
        "labels_text": "Instantiate the Newick Tree class from the given string"
    },
    {
        "input_text": "summarize: def new_clade(self, parent=None):                clade = Newick.Clade()        if parent:            clade.parent = parent        return clade",
        "labels_text": "Return new NewickClade optionally with temporary reference to parent"
    },
    {
        "input_text": "summarize: def __init__(self, trees):                self.trees = trees",
        "labels_text": "Initialize parameter for Tree Writer object"
    },
    {
        "input_text": "summarize: def write(self, handle, **kwargs):                count = 0        for treestr in self.to_strings(**kwargs):            handle.write(treestr + \"\\n\")            count += 1        return count",
        "labels_text": "Write this instance tree to a file handle"
    },
    {
        "input_text": "summarize: def __init__(self, root=None, rooted=False, id=None, name=None, weight=1.0):                BaseTree.Tree.__init__(            self, root=root or Clade(), rooted=rooted, id=id, name=name        )        self.weight = weight",
        "labels_text": "Instantiate a NeXML tree object with the given parameter"
    },
    {
        "input_text": "summarize: def __init__(        self,        branch_length=1.0,        name=None,        clades=None,        confidence=None,        comment=None,        **kwargs,    ):                BaseTree.Clade.__init__(            self,            branch_length=branch_length,            name=name,            clades=clades,            confidence=confidence,        )        self.comment = comment        for key, value in kwargs.items():            setattr(self, key, value)",
        "labels_text": "Initialize parameter for NeXML Clade object"
    },
    {
        "input_text": "summarize: def qUri(s):        return resolve_uri(s, namespaces=NAMESPACES, xml_style=True)",
        "labels_text": "Given a prefixed URI return the full URI"
    },
    {
        "input_text": "summarize: def cdao_to_obo(s):        return f\"obo:{cdao_elements[s[len('cdao:'):]]}\"",
        "labels_text": "Optionally convert a CDAOprefixed URI into an OBOprefixed URI"
    },
    {
        "input_text": "summarize: def matches(s):        if s.startswith(\"cdao:\"):        return (s, cdao_to_obo(s))    else:        return (s,)",
        "labels_text": "Check for match in both CDAO and OBO namespaces"
    },
    {
        "input_text": "summarize: def parse(handle, **kwargs):        return Parser(handle).parse(**kwargs)",
        "labels_text": "Iterate over the tree in a NeXML file handle return generator of BioPhyloNeXMLTree object"
    },
    {
        "input_text": "summarize: def write(trees, handle, plain=False, **kwargs):        return Writer(trees).write(handle, plain=plain, **kwargs)",
        "labels_text": "Write a tree in NeXML format to the given file handle return number of tree written"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle",
        "labels_text": "Initialize parameter for NeXML file parser"
    },
    {
        "input_text": "summarize: def from_string(cls, treetext):                handle = StringIO(treetext)        return cls(handle)",
        "labels_text": "Convert file handle to StringIO object"
    },
    {
        "input_text": "summarize: def add_annotation(self, node_dict, meta_node):                if \"property\" in meta_node.attrib:            prop = meta_node.attrib[\"property\"]        else:            prop = \"meta\"        if prop in matches(\"cdao:has_Support_Value\"):            node_dict[\"confidence\"] = float(meta_node.text)        else:            node_dict[prop] = meta_node.text",
        "labels_text": "Add annotation for the NeXML parser"
    },
    {
        "input_text": "summarize: def _make_tree(cls, node, node_dict, children):                this_node = node_dict[node]        clade = NeXML.Clade(**this_node)        if node in children:            clade.clades = [                cls._make_tree(child, node_dict, children) for child in children[node]            ]        return clade",
        "labels_text": "Traverse the tree creating a nested clade structure PRIVATE Return a NeXMLClade and call itself recursively for each child traversing the entire tree and creating a nested structure of NeXMLClade object"
    },
    {
        "input_text": "summarize: def __init__(self, trees):                self.trees = trees        self.node_counter = 0        self.edge_counter = 0        self.tree_counter = 0",
        "labels_text": "Initialize parameter for NeXML writer"
    },
    {
        "input_text": "summarize: def new_label(self, obj_type):                counter = f\"{obj_type}_counter\"        setattr(self, counter, getattr(self, counter) + 1)        return f\"{obj_type}{getattr(self, counter)}\"",
        "labels_text": "Create new label for the NeXML writer"
    },
    {
        "input_text": "summarize: def _check_str(text, testfunc):        if text is not None and not testfunc(text):        warnings.warn(            f\"String {text} doesn't match the given regexp\",            PhyloXMLWarning,            stacklevel=2,        )",
        "labels_text": "Check a string using testfunc and warn if there no match PRIVATE"
    },
    {
        "input_text": "summarize: def __getitem__(self, index):                if isinstance(index, (int, slice)):            return self.phylogenies[index]        if not isinstance(index, str):            raise KeyError(f\"can't use {type(index)} as an index\")        for tree in self.phylogenies:            if tree.name == index:                return tree        else:            raise KeyError(f\"no phylogeny found with name {index!r}\")",
        "labels_text": "Get a phylogeny by index or name"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.phylogenies)",
        "labels_text": "Iterate through the phylogenetic tree in this object"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.phylogenies)",
        "labels_text": "Return the number of phylogenetic tree in this object"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"%s([%s])\" % (            self.__class__.__name__,            \",\\n\".join(map(str, self.phylogenies)),        )",
        "labels_text": "Return name of phylogeny in the object"
    },
    {
        "input_text": "summarize: def __init__(self, tag, namespace=None, attributes=None, value=None, children=None):                self.tag = tag        self.namespace = namespace        self.attributes = attributes or {}        self.value = value        self.children = children or []",
        "labels_text": "Initialize value for nonphyloXML element"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.children)",
        "labels_text": "Iterate through the child of this object if any"
    },
    {
        "input_text": "summarize: def from_tree(cls, tree, **kwargs):                phy = cls(            root=Clade.from_clade(tree.root),            rooted=tree.rooted,            name=tree.name,            id=(tree.id is not None) and Id(str(tree.id)) or None,        )        phy.__dict__.update(kwargs)        return phy",
        "labels_text": "Create a new Phylogeny given a Tree from NewickNexus or BaseTree Keyword argument are the usual Phylogeny constructor parameter"
    },
    {
        "input_text": "summarize: def from_clade(cls, clade, **kwargs):                return Clade.from_clade(clade).to_phylogeny(**kwargs)",
        "labels_text": "Create a new Phylogeny given a Newick or BaseTree Clade object Keyword argument are the usual PhyloXMLClade constructor parameter"
    },
    {
        "input_text": "summarize: def as_phyloxml(self):                return self",
        "labels_text": "Return this tree a PhyloXMLcompatible Phylogeny object Overrides the BaseTree method"
    },
    {
        "input_text": "summarize: def to_phyloxml_container(self, **kwargs):                return Phyloxml(kwargs, phylogenies=[self])",
        "labels_text": "Create a new Phyloxml object containing just this phylogeny"
    },
    {
        "input_text": "summarize: def to_alignment(self):                def is_aligned_seq(elem):            if isinstance(elem, Sequence) and elem.mol_seq.is_aligned:                return True            return False        seqs = self._filter_search(is_aligned_seq, \"preorder\", True)        records = (seq.to_seqrecord() for seq in seqs)        return MultipleSeqAlignment(records)",
        "labels_text": "Construct a MultipleSeqAlignment from the aligned sequence in this tree"
    },
    {
        "input_text": "summarize: def _get_confidence(self):                if len(self.confidences) == 0:            return None        if len(self.confidences) > 1:            raise AttributeError(                \"more than 1 confidence value available; use Phylogeny.confidences\"            )        return self.confidences[0]",
        "labels_text": "Equivalent to selfconfidences if there is only value PRIVATE See Also Cladeconfidence Cladetaxonomy"
    },
    {
        "input_text": "summarize: def to_phylogeny(self, **kwargs):                phy = Phylogeny(root=self, date=self.date)        phy.__dict__.update(kwargs)        return phy",
        "labels_text": "Create a new phylogeny containing just this clade"
    },
    {
        "input_text": "summarize: def _get_confidence(self):                if len(self.confidences) == 0:            return None        if len(self.confidences) > 1:            raise AttributeError(                \"more than 1 confidence value available; use Clade.confidences\"            )        return self.confidences[0]",
        "labels_text": "Return confidence value PRIVATE"
    },
    {
        "input_text": "summarize: def _del_confidence(self):                self.confidences = []",
        "labels_text": "Delete confidence value PRIVATE"
    },
    {
        "input_text": "summarize: def _get_taxonomy(self):                if len(self.taxonomies) == 0:            return None        if len(self.taxonomies) > 1:            raise AttributeError(                \"more than 1 taxonomy value available; use Clade.taxonomies\"            )        return self.taxonomies[0]",
        "labels_text": "Get taxonomy list for the clade PRIVATE"
    },
    {
        "input_text": "summarize: def _set_taxonomy(self, value):                if not isinstance(value, Taxonomy):            raise ValueError(\"assigned value must be a Taxonomy instance\")        if len(self.taxonomies) == 0:            self.taxonomies.append(value)        elif len(self.taxonomies) == 1:            self.taxonomies[0] = value        else:            raise ValueError(                \"multiple taxonomy values already exist; \"                \"use Phylogeny.taxonomies instead\"            )",
        "labels_text": "Set a taxonomy for the clade PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, *args, **kwargs):                BaseTree.BranchColor.__init__(self, *args, **kwargs)",
        "labels_text": "Initialize parameter for the BranchColor object"
    },
    {
        "input_text": "summarize: def __init__(self, value, source):                self.value = value        self.source = source",
        "labels_text": "Initialize value for Accession object"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"{self.source}:{self.value}\"",
        "labels_text": "Show the class name and an identifying attribute"
    },
    {
        "input_text": "summarize: def __init__(self, type, id_ref_0, id_ref_1, distance=None, confidence=None):                self.distance = distance        self.type = type        self.id_ref_0 = id_ref_0        self.id_ref_1 = id_ref_1        self.confidence = confidence",
        "labels_text": "Initialize value for the CladeRelation object"
    },
    {
        "input_text": "summarize: def __new__(cls, value, type=\"unknown\"):                obj = super().__new__(cls, value)        obj.type = type        return obj",
        "labels_text": "Create and return a Confidence object with the specified value and type"
    },
    {
        "input_text": "summarize: def value(self):                return float(self)",
        "labels_text": "Return the float value of the Confidence object"
    },
    {
        "input_text": "summarize: def __init__(self, value=None, unit=None, desc=None, minimum=None, maximum=None):                self.value = value        self.unit = unit        self.desc = desc        self.minimum = minimum        self.maximum = maximum",
        "labels_text": "Initialize value of the Date object"
    },
    {
        "input_text": "summarize: def __str__(self):                if self.unit and self.value is not None:            return f\"{self.value} {self.unit}\"        if self.desc is not None:            return self.desc        return self.__class__.__name__",
        "labels_text": "Show the class name and the humanreadable date"
    },
    {
        "input_text": "summarize: def __init__(self, desc=None, points=None, polygons=None):                self.desc = desc        self.points = points or []        self.polygons = polygons or []",
        "labels_text": "Initialize value of Distribution object"
    },
    {
        "input_text": "summarize: def __init__(self, length=None, domains=None):                self.length = length        self.domains = domains",
        "labels_text": "Initialize value of the DomainArchitecture object"
    },
    {
        "input_text": "summarize: def __init__(        self,        type=None,        duplications=None,        speciations=None,        losses=None,        confidence=None,    ):                _check_str(type, self.ok_type.__contains__)        self.type = type        self.duplications = duplications        self.speciations = speciations        self.losses = losses        self.confidence = confidence",
        "labels_text": "Initialize value of the Events object"
    },
    {
        "input_text": "summarize: def items(self):                return [(k, v) for k, v in self.__dict__.items() if v is not None]",
        "labels_text": "Return Events item"
    },
    {
        "input_text": "summarize: def keys(self):                return [k for k, v in self.__dict__.items() if v is not None]",
        "labels_text": "Return Events key"
    },
    {
        "input_text": "summarize: def values(self):                return [v for v in self.__dict__.values() if v is not None]",
        "labels_text": "Return value from a keyvalue pair in an Events dict"
    },
    {
        "input_text": "summarize: def __len__(self):                # TODO - Better way to do this?        return len(self.values())",
        "labels_text": "Return number of Events"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                try:            val = getattr(self, key)        except AttributeError:            raise KeyError(key) from None        if val is None:            raise KeyError(f\"{key!r} has not been set in this object\")        return val",
        "labels_text": "Get value of Event with the given key"
    },
    {
        "input_text": "summarize: def __setitem__(self, key, val):                setattr(self, key, val)",
        "labels_text": "Add item to Event dict"
    },
    {
        "input_text": "summarize: def __delitem__(self, key):                setattr(self, key, None)",
        "labels_text": "Delete Event with given key"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.keys())",
        "labels_text": "Iterate over the key present in a Events dict"
    },
    {
        "input_text": "summarize: def __contains__(self, key):                try:            return getattr(self, key) is not None        except AttributeError:            return False",
        "labels_text": "Return True if Event dict contains key"
    },
    {
        "input_text": "summarize: def __init__(self, value, provider=None):                self.value = value        self.provider = provider",
        "labels_text": "Initialize value for the identifier object"
    },
    {
        "input_text": "summarize: def __str__(self):                if self.provider is not None:            return f\"{self.provider}:{self.value}\"        return self.value",
        "labels_text": "Return identifier a a string"
    },
    {
        "input_text": "summarize: def __init__(self, value, is_aligned=None):                _check_str(value, self.re_value.match)        self.value = value        self.is_aligned = is_aligned",
        "labels_text": "Initialize parameter for the MolSeq object"
    },
    {
        "input_text": "summarize: def __str__(self):                return self.value",
        "labels_text": "Return the value of the Molecular Sequence object"
    },
    {
        "input_text": "summarize: def __init__(self, geodetic_datum, lat, long, alt=None, alt_unit=None):                self.geodetic_datum = geodetic_datum        self.lat = lat        self.long = long        self.alt = alt        self.alt_unit = alt_unit",
        "labels_text": "Initialize value for the Point object"
    },
    {
        "input_text": "summarize: def __init__(self, points=None):                self.points = points or []",
        "labels_text": "Initialize value for the Polygon object"
    },
    {
        "input_text": "summarize: def __str__(self):                return \"%s([%s])\" % (self.__class__.__name__, \",\\n\".join(map(str, self.points)))",
        "labels_text": "Return list of point a a string"
    },
    {
        "input_text": "summarize: def __init__(self, value, start, end, confidence=None, id=None):                self.value = value        self.start = start        self.end = end        self.confidence = confidence        self.id = id",
        "labels_text": "Initialize value for a ProteinDomain object"
    },
    {
        "input_text": "summarize: def from_seqfeature(cls, feat):                return ProteinDomain(            feat.id,            feat.location.start,            feat.location.end,            confidence=feat.qualifiers.get(\"confidence\"),        )",
        "labels_text": "Create ProteinDomain object from SeqFeature"
    },
    {
        "input_text": "summarize: def to_seqfeature(self):                feat = SeqFeature(location=SimpleLocation(self.start, self.end), id=self.value)        try:            confidence = self.confidence        except AttributeError:            pass        else:            feat.qualifiers[\"confidence\"] = confidence        return feat",
        "labels_text": "Create a SeqFeature from the ProteinDomain Object"
    },
    {
        "input_text": "summarize: def __init__(self, doi=None, desc=None):                _check_str(doi, self.re_doi.match)        self.doi = doi        self.desc = desc",
        "labels_text": "Initialize element of the Reference class object"
    },
    {
        "input_text": "summarize: def __init__(self, type, id_ref_0, id_ref_1, distance=None, confidence=None):                _check_str(type, self.ok_type.__contains__)        self.distance = distance        self.type = type        self.id_ref_0 = id_ref_0        self.id_ref_1 = id_ref_1        self.confidence = confidence",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                if self.code is not None:            return self.code        if self.scientific_name is not None:            return self.scientific_name        if self.rank is not None:            return self.rank        if self.id is not None:            return str(self.id)        return self.__class__.__name__",
        "labels_text": "Show the class name and an identifying attribute"
    },
    {
        "input_text": "summarize: def __init__(self, value, desc=None, type=None):                self.value = value        self.desc = desc        self.type = type",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                if self.value:            return self.value        return repr(self)",
        "labels_text": "Return string representation of Uri"
    },
    {
        "input_text": "summarize: def read(file):        return Parser(file).read()",
        "labels_text": "Parse a phyloXML file or stream and build a tree of Biopython object The child of the root node are phylogeny and possibly other arbitrary nonphyloXML object return a single BioPhyloPhyloXMLPhyloxml object"
    },
    {
        "input_text": "summarize: def parse(file):        return Parser(file).parse()",
        "labels_text": "Iterate over the phylogenetic tree in a phyloXML file This ignores any additional data stored at the top level but may be more memoryefficient than the read function return a generator of BioPhyloPhyloXMLPhylogeny object"
    },
    {
        "input_text": "summarize: def _local(tag):        if tag[0] == \"{\":        return tag[tag.index(\"}\") + 1 :]    return tag",
        "labels_text": "Extract the local tag from a namespaced tag name PRIVATE"
    },
    {
        "input_text": "summarize: def _split_namespace(tag):        try:        return tag[1:].split(\"}\", 1)    except ValueError:        return (\"\", tag)",
        "labels_text": "Split a tag into namespace and local tag string PRIVATE"
    },
    {
        "input_text": "summarize: def _ns(tag, namespace=NAMESPACES[\"phy\"]):        return f\"{{{namespace}}}{tag}\"",
        "labels_text": "Format an XML tag with the given namespace PRIVATE"
    },
    {
        "input_text": "summarize: def _get_child_as(parent, tag, construct):        child = parent.find(_ns(tag))    if child is not None:        return construct(child)",
        "labels_text": "Find a child node by tag and pas it through a constructor PRIVATE Returns None if no matching child is found"
    },
    {
        "input_text": "summarize: def _get_child_text(parent, tag, construct=str):        child = parent.find(_ns(tag))    if child is not None and child.text:        return construct(child.text)",
        "labels_text": "Find a child node by tag pas it text through a constructor PRIVATE Returns None if no matching child is found"
    },
    {
        "input_text": "summarize: def _get_children_as(parent, tag, construct):        return [construct(child) for child in parent.findall(_ns(tag))]",
        "labels_text": "Find child node by tag pas each through a constructor PRIVATE Returns an empty list if no matching child is found"
    },
    {
        "input_text": "summarize: def _get_children_text(parent, tag, construct=str):        return [construct(child.text) for child in parent.findall(_ns(tag)) if child.text]",
        "labels_text": "Find child node by tag pas each node text through a constructor PRIVATE Returns an empty list if no matching child is found"
    },
    {
        "input_text": "summarize: def _str2bool(text):        if text == \"true\" or text == \"1\":        return True    if text == \"false\" or text == \"0\":        return False    raise ValueError(\"String could not be converted to boolean: \" + text)",
        "labels_text": "Convert string to boolean PRIVATE"
    },
    {
        "input_text": "summarize: def _dict_str2bool(dct, keys):        out = dct.copy()    for key in keys:        if key in out:            out[key] = _str2bool(out[key])    return out",
        "labels_text": "Return a new dictionary where string value are replaced with booleans PRIVATE"
    },
    {
        "input_text": "summarize: def _int(text):        if text is not None:        try:            return int(text)        except Exception:            return None",
        "labels_text": "Return text a an integer PRIVATE"
    },
    {
        "input_text": "summarize: def _float(text):        if text is not None:        try:            return float(text)        except Exception:            return None",
        "labels_text": "Return text a a float PRIVATE"
    },
    {
        "input_text": "summarize: def _collapse_wspace(text):        if text is not None:        return \" \".join(text.split())",
        "labels_text": "Replace all span of whitespace with a single space character PRIVATE Also remove leading and trailing whitespace See Collapse Whitespace Policy in the phyloXML spec glossary httpphyloxmlorgdocumentationversionphyloxmlxsdhtmlGlossary"
    },
    {
        "input_text": "summarize: def _replace_wspace(text):        for char in (\"\\t\", \"\\n\", \"\\r\"):        if char in text:            text = text.replace(char, \" \")    return text",
        "labels_text": "Replace tab LF and CR character with space but dont collapse PRIVATE See Replace Whitespace Policy in the phyloXML spec glossary httpphyloxmlorgdocumentationversionphyloxmlxsdhtmlGlossary"
    },
    {
        "input_text": "summarize: def __init__(self, file):                # Get an iterable context for XML parsing events        context = iter(ElementTree.iterparse(file, events=(\"start\", \"end\")))        event, root = next(context)        self.root = root        self.context = context",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def parse(self):                phytag = _ns(\"phylogeny\")        for event, elem in self.context:            if event == \"start\" and elem.tag == phytag:                yield self._parse_phylogeny(elem)",
        "labels_text": "Parse the phyloXML file incrementally and return each phylogeny"
    },
    {
        "input_text": "summarize: def other(self, elem, namespace, localtag):                return PX.Other(            localtag,            namespace,            elem.attrib,            value=elem.text and elem.text.strip() or None,            children=[                self.other(child, *_split_namespace(child.tag)) for child in elem            ],        )",
        "labels_text": "Create an Other object a nonphyloXML element"
    },
    {
        "input_text": "summarize: def accession(self, elem):                return PX.Accession(elem.text.strip(), elem.get(\"source\"))",
        "labels_text": "Create accession object"
    },
    {
        "input_text": "summarize: def annotation(self, elem):                return PX.Annotation(            desc=_collapse_wspace(_get_child_text(elem, \"desc\")),            confidence=_get_child_as(elem, \"confidence\", self.confidence),            properties=_get_children_as(elem, \"property\", self.property),            uri=_get_child_as(elem, \"uri\", self.uri),            **elem.attrib,        )",
        "labels_text": "Create annotation object"
    },
    {
        "input_text": "summarize: def clade_relation(self, elem):                return PX.CladeRelation(            elem.get(\"type\"),            elem.get(\"id_ref_0\"),            elem.get(\"id_ref_1\"),            distance=elem.get(\"distance\"),            confidence=_get_child_as(elem, \"confidence\", self.confidence),        )",
        "labels_text": "Create clade relationship object"
    },
    {
        "input_text": "summarize: def color(self, elem):                red, green, blue = (            _get_child_text(elem, color, int) for color in (\"red\", \"green\", \"blue\")        )        return PX.BranchColor(red, green, blue)",
        "labels_text": "Create branch color object"
    },
    {
        "input_text": "summarize: def confidence(self, elem):                return PX.Confidence(_float(elem.text), elem.get(\"type\"))",
        "labels_text": "Create confidence object"
    },
    {
        "input_text": "summarize: def date(self, elem):                return PX.Date(            unit=elem.get(\"unit\"),            desc=_collapse_wspace(_get_child_text(elem, \"desc\")),            value=_get_child_text(elem, \"value\", float),            minimum=_get_child_text(elem, \"minimum\", float),            maximum=_get_child_text(elem, \"maximum\", float),        )",
        "labels_text": "Create date object"
    },
    {
        "input_text": "summarize: def distribution(self, elem):                return PX.Distribution(            desc=_collapse_wspace(_get_child_text(elem, \"desc\")),            points=_get_children_as(elem, \"point\", self.point),            polygons=_get_children_as(elem, \"polygon\", self.polygon),        )",
        "labels_text": "Create geographic distribution object"
    },
    {
        "input_text": "summarize: def domain(self, elem):                return PX.ProteinDomain(            elem.text.strip(),            int(elem.get(\"from\")) - 1,            int(elem.get(\"to\")),            confidence=_float(elem.get(\"confidence\")),            id=elem.get(\"id\"),        )",
        "labels_text": "Create protein domain object"
    },
    {
        "input_text": "summarize: def domain_architecture(self, elem):                return PX.DomainArchitecture(            length=int(elem.get(\"length\")),            domains=_get_children_as(elem, \"domain\", self.domain),        )",
        "labels_text": "Create domain architecture object"
    },
    {
        "input_text": "summarize: def events(self, elem):                return PX.Events(            type=_get_child_text(elem, \"type\"),            duplications=_get_child_text(elem, \"duplications\", int),            speciations=_get_child_text(elem, \"speciations\", int),            losses=_get_child_text(elem, \"losses\", int),            confidence=_get_child_as(elem, \"confidence\", self.confidence),        )",
        "labels_text": "Create event object"
    },
    {
        "input_text": "summarize: def id(self, elem):                provider = elem.get(\"provider\") or elem.get(\"type\")        return PX.Id(elem.text.strip(), provider)",
        "labels_text": "Create identifier object"
    },
    {
        "input_text": "summarize: def mol_seq(self, elem):                is_aligned = elem.get(\"is_aligned\")        if is_aligned is not None:            is_aligned = _str2bool(is_aligned)        return PX.MolSeq(elem.text.strip(), is_aligned=is_aligned)",
        "labels_text": "Create molecular sequence object"
    },
    {
        "input_text": "summarize: def point(self, elem):                return PX.Point(            elem.get(\"geodetic_datum\"),            _get_child_text(elem, \"lat\", float),            _get_child_text(elem, \"long\", float),            alt=_get_child_text(elem, \"alt\", float),            alt_unit=elem.get(\"alt_unit\"),        )",
        "labels_text": "Create point object coordinate of a point"
    },
    {
        "input_text": "summarize: def polygon(self, elem):                return PX.Polygon(points=_get_children_as(elem, \"point\", self.point))",
        "labels_text": "Create polygon object list of point"
    },
    {
        "input_text": "summarize: def property(self, elem):                return PX.Property(            elem.text.strip(),            elem.get(\"ref\"),            elem.get(\"applies_to\"),            elem.get(\"datatype\"),            unit=elem.get(\"unit\"),            id_ref=elem.get(\"id_ref\"),        )",
        "labels_text": "Create property from external resource"
    },
    {
        "input_text": "summarize: def reference(self, elem):                return PX.Reference(doi=elem.get(\"doi\"), desc=_get_child_text(elem, \"desc\"))",
        "labels_text": "Create literature reference object"
    },
    {
        "input_text": "summarize: def sequence_relation(self, elem):                return PX.SequenceRelation(            elem.get(\"type\"),            elem.get(\"id_ref_0\"),            elem.get(\"id_ref_1\"),            distance=_float(elem.get(\"distance\")),            confidence=_get_child_as(elem, \"confidence\", self.confidence),        )",
        "labels_text": "Create sequence relationship object relationship between two sequence"
    },
    {
        "input_text": "summarize: def uri(self, elem):                return PX.Uri(            elem.text.strip(),            desc=_collapse_wspace(elem.get(\"desc\")),            type=elem.get(\"type\"),        )",
        "labels_text": "Create uri object expected to be a url"
    },
    {
        "input_text": "summarize: def _serialize(value):        if isinstance(value, float):        return str(value).upper()    elif isinstance(value, bool):        return str(value).lower()    return str(value)",
        "labels_text": "Convert a Python primitive to a phyloXMLcompatible string PRIVATE"
    },
    {
        "input_text": "summarize: def _clean_attrib(obj, attrs):        out = {}    for key in attrs:        val = getattr(obj, key)        if val is not None:            out[key] = _serialize(val)    return out",
        "labels_text": "Create a dictionary from an object specified nonNone attribute PRIVATE"
    },
    {
        "input_text": "summarize: def _handle_simple(tag):        def wrapped(self, obj):                elem = ElementTree.Element(tag)        elem.text = _serialize(obj)        return elem    wrapped.__doc__ = f\"Serialize a simple {tag} node.\"    return wrapped",
        "labels_text": "Handle to serialize simple node PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, phyloxml):                assert isinstance(phyloxml, PX.Phyloxml), \"Not a Phyloxml object\"        self._tree = ElementTree.ElementTree(self.phyloxml(phyloxml))",
        "labels_text": "Build an ElementTree from a PhyloXML object"
    },
    {
        "input_text": "summarize: def write(self, file, encoding=DEFAULT_ENCODING, indent=True):                if indent:            _indent(self._tree.getroot())        self._tree.write(file, encoding)        return len(self._tree.getroot())",
        "labels_text": "Write PhyloXML to a file"
    },
    {
        "input_text": "summarize: def phyloxml(self, obj):                elem = ElementTree.Element(\"phyloxml\", obj.attributes)  # Namespaces        for tree in obj.phylogenies:            elem.append(self.phylogeny(tree))        for otr in obj.other:            elem.append(self.other(otr))        return elem",
        "labels_text": "Convert phyloxml to Etree element"
    },
    {
        "input_text": "summarize: def other(self, obj):                elem = ElementTree.Element(_ns(obj.tag, obj.namespace), obj.attributes)        elem.text = obj.value        for child in obj.children:            elem.append(self.other(child))        return elem",
        "labels_text": "Convert other to Etree element"
    },
    {
        "input_text": "summarize: def domain(self, obj):                elem = ElementTree.Element(            \"domain\", {\"from\": str(obj.start + 1), \"to\": str(obj.end)}        )        if obj.confidence is not None:            elem.set(\"confidence\", _serialize(obj.confidence))        if obj.id is not None:            elem.set(\"id\", obj.id)        elem.text = _serialize(obj.value)        return elem",
        "labels_text": "Serialize a domain node"
    },
    {
        "input_text": "summarize: def __delitem__(self, item):                index = None        if isinstance(item, int):            index = item        elif isinstance(item, str):            index = self.names.index(item)        else:            raise TypeError(\"Invalid index type.\")        # remove distances related to index        for i in range(index + 1, len(self)):            del self.matrix[i][index]        del self.matrix[index]        # remove name        del self.names[index]",
        "labels_text": "Delete related distance by the index or name"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.names)",
        "labels_text": "Matrix length"
    },
    {
        "input_text": "summarize: def __repr__(self):                return self.__class__.__name__ + \"(names=%s, matrix=%s)\" % tuple(            map(repr, (self.names, self.matrix))        )",
        "labels_text": "Return Matrix a a string"
    },
    {
        "input_text": "summarize: def __str__(self):                matrix_string = \"\\n\".join(            [                self.names[i]                + \"\\t\"                + \"\\t\".join([format(n, \"f\") for n in self.matrix[i]])                for i in range(0, len(self))            ]        )        matrix_string = matrix_string + \"\\n\\t\" + \"\\t\".join(self.names)        return matrix_string.expandtabs(tabsize=4)",
        "labels_text": "Get a lower triangular matrix string"
    },
    {
        "input_text": "summarize: def __init__(self, names, matrix=None):                _Matrix.__init__(self, names, matrix)        self._set_zero_diagonal()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __setitem__(self, item, value):                _Matrix.__setitem__(self, item, value)        self._set_zero_diagonal()",
        "labels_text": "Set Matrixs item to value"
    },
    {
        "input_text": "summarize: def _set_zero_diagonal(self):                for i in range(0, len(self)):            self.matrix[i][i] = 0",
        "labels_text": "Set all diagonal element to zero PRIVATE"
    },
    {
        "input_text": "summarize: def build_tree(self, msa):                raise NotImplementedError(\"Method not implemented!\")",
        "labels_text": "Caller to build the tree from an Alignment or MultipleSeqAlignment object This should be implemented in subclass"
    },
    {
        "input_text": "summarize: def __init__(self, distance_calculator=None, method=\"nj\"):                if distance_calculator is None or isinstance(            distance_calculator, DistanceCalculator        ):            self.distance_calculator = distance_calculator        else:            raise TypeError(\"Must provide a DistanceCalculator object.\")        if method in self.methods:            self.method = method        else:            raise TypeError(                \"Bad method: \"                + method                + \". Available methods: \"                + \", \".join(self.methods)            )",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def build_tree(self, msa):                if self.distance_calculator:            dm = self.distance_calculator.get_distance(msa)            tree = None            if self.method == \"upgma\":                tree = self.upgma(dm)            else:                tree = self.nj(dm)            return tree        else:            raise TypeError(\"Must provide a DistanceCalculator object.\")",
        "labels_text": "Construct and return a Tree Neighbor Joining or UPGMA"
    },
    {
        "input_text": "summarize: def _height_of(self, clade):                height = 0        if clade.is_terminal():            height = clade.branch_length        else:            height = height + max(self._height_of(c) for c in clade.clades)        return height",
        "labels_text": "Calculate clade height the longest path to any terminal PRIVATE"
    },
    {
        "input_text": "summarize: def get_score(self, tree, alignment):                raise NotImplementedError(\"Method not implemented!\")",
        "labels_text": "Caller to get the score of a tree for the given alignment This should be implemented in subclass"
    },
    {
        "input_text": "summarize: def search(self, starting_tree, alignment):                raise NotImplementedError(\"Method not implemented!\")",
        "labels_text": "Caller to search the best tree with a starting tree This should be implemented in subclass"
    },
    {
        "input_text": "summarize: def __init__(self, scorer):                if isinstance(scorer, Scorer):            self.scorer = scorer        else:            raise TypeError(\"Must provide a Scorer object.\")",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def search(self, starting_tree, alignment):                return self._nni(starting_tree, alignment)",
        "labels_text": "Implement the TreeSearchersearch method Parameters startingtree Tree starting tree of NNI method alignment Alignment or MultipleSeqAlignment object multiple sequence alignment used to calculate parsimony score of different NNI tree"
    },
    {
        "input_text": "summarize: def __init__(self, matrix=None):                if not matrix or isinstance(matrix, _Matrix):            self.matrix = matrix        else:            raise TypeError(\"Must provide a _Matrix object.\")",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, searcher, starting_tree=None):                self.searcher = searcher        self.starting_tree = starting_tree",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def build_tree(self, alignment):                # if starting_tree is none,        # create a upgma tree with 'identity' scoring matrix        if self.starting_tree is None:            dtc = DistanceTreeConstructor(DistanceCalculator(\"identity\"), \"upgma\")            self.starting_tree = dtc.build_tree(alignment)        return self.searcher.search(self.starting_tree, alignment)",
        "labels_text": "Build the tree Parameters alignment MultipleSeqAlignment multiple sequence alignment to calculate parsimony tree"
    },
    {
        "input_text": "summarize: def parse(file, format, **kwargs):        with File.as_handle(file) as fp:        yield from getattr(supported_formats[format], \"parse\")(fp, **kwargs)",
        "labels_text": "Parse a file iteratively and yield each of the tree it contains If a file only contains one tree this still return an iterable object that contains one element Examples import BioPhylo tree BioPhyloparsePhyloXMLapafxml phyloxml for tree in tree printtreerooted True"
    },
    {
        "input_text": "summarize: def read(file, format, **kwargs):        try:        tree_gen = parse(file, format, **kwargs)        tree = next(tree_gen)    except StopIteration:        raise ValueError(\"There are no trees in this file.\") from None    try:        next(tree_gen)    except StopIteration:        return tree    else:        raise ValueError(\"There are multiple trees in this file; use parse() instead.\")",
        "labels_text": "Parse a file in the given format and return a single tree Raises a ValueError if there are zero or multiple tree if this occurs use parse instead to get the complete sequence of tree"
    },
    {
        "input_text": "summarize: def write(trees, file, format, **kwargs):        if isinstance(trees, (BaseTree.Tree, BaseTree.Clade)):        # Passed a single tree instead of an iterable -- that's OK        trees = [trees]    with File.as_handle(file, \"w+\") as fp:        n = getattr(supported_formats[format], \"write\")(trees, fp, **kwargs)    return n",
        "labels_text": "Write a sequence of tree to file in the given format"
    },
    {
        "input_text": "summarize: def convert(in_file, in_format, out_file, out_format, parse_args=None, **kwargs):        if parse_args is None:        parse_args = {}    trees = parse(in_file, in_format, **parse_args)    return write(trees, out_file, out_format, **kwargs)",
        "labels_text": "Convert between two tree file format"
    },
    {
        "input_text": "summarize: def _is_int(x):        return isinstance(x, int) or str(x).isdigit()",
        "labels_text": "Test whether the argument can be serialized a an integer PRIVATE"
    },
    {
        "input_text": "summarize: def _is_numeric(x):        try:        float(str(x))        return True    except ValueError:        return False",
        "labels_text": "Test whether the argument can be serialized a a number PRIVATE"
    },
    {
        "input_text": "summarize: def _set_rel_paths(self):                Paml._set_rel_paths(self)        if self.tree is not None:            self._rel_tree = os.path.relpath(self.tree, self.working_dir)",
        "labels_text": "Make filedirectory path relative to the PWD PRIVATE BASEML requires that all path specified in the control file be relative to the directory from which it is called rather than absolute path"
    },
    {
        "input_text": "summarize: def run(self, ctl_file=None, verbose=False, command=\"baseml\", parse=True):                if self.tree is None:            raise ValueError(\"Tree file not specified.\")        if not os.path.exists(self.tree):            raise FileNotFoundError(\"The specified tree file does not exist.\")        Paml.run(self, ctl_file, verbose, command)        if parse:            return read(self.out_file)        return None",
        "labels_text": "Run baseml using the current configuration Check that the tree attribute is specified and exists and then run baseml If parse is True then read and return the result otherwise return none The argument may be passed a either absolute or relative path despite the fact that BASEML requires relative path"
    },
    {
        "input_text": "summarize: def cdf_chi2(df, stat):        if df < 1:        raise ValueError(\"df must be at least 1\")    if stat < 0:        raise ValueError(\"The test statistic must be positive\")    x = 0.5 * stat    alpha = df / 2.0    prob = 1 - _incomplete_gamma(x, alpha)    return prob",
        "labels_text": "Compute pvalue from distribution function and test statistic"
    },
    {
        "input_text": "summarize: def print_options(self):                for option in self._options.items():            if option[0] == \"NSsites\" and option[1] is not None:                # NSsites is stored in Python as a list but in the                # control file it is specified as a series of numbers                # separated by spaces.                NSsites = \" \".join(str(site) for site in option[1])                print(f\"{option[0]} = {NSsites}\")            else:                print(f\"{option[0]} = {option[1]}\")",
        "labels_text": "Print out all of the option and their current setting"
    },
    {
        "input_text": "summarize: def _set_rel_paths(self):                Paml._set_rel_paths(self)        if self.tree is not None:            self._rel_tree = os.path.relpath(self.tree, self.working_dir)",
        "labels_text": "Make all filedirectory path relative to the PWD PRIVATE CODEML requires that all path specified in the control file be relative to the directory from which it is called rather than absolute path"
    },
    {
        "input_text": "summarize: def run(self, ctl_file=None, verbose=False, command=\"codeml\", parse=True):                if self.tree is None:            raise ValueError(\"Tree file not specified.\")        if not os.path.exists(self.tree):            raise FileNotFoundError(\"The specified tree file does not exist.\")        Paml.run(self, ctl_file, verbose, command)        if parse:            return read(self.out_file)        return None",
        "labels_text": "Run codeml using the current configuration If parse is True then read and return the result otherwise return None The argument may be passed a either absolute or relative path despite the fact that CODEML requires relative path"
    },
    {
        "input_text": "summarize: def __init__(self, alignment=None, working_dir=None, out_file=None):                Paml.__init__(self, alignment, working_dir, out_file)        self.ctl_file = \"yn00.ctl\"        self._options = {            \"verbose\": None,            \"icode\": None,            \"weighting\": None,            \"commonf3x4\": None,            \"ndata\": None,        }",
        "labels_text": "Initialize the Yn instance The user may optionally pas in string specifying the location of the input alignment the working directory and the final output file"
    },
    {
        "input_text": "summarize: def run(self, ctl_file=None, verbose=False, command=\"yn00\", parse=True):                Paml.run(self, ctl_file, verbose, command)        if parse:            return read(self.out_file)        return None",
        "labels_text": "Run yn using the current configuration If parse is True then read and return the result otherwise return None"
    },
    {
        "input_text": "summarize: def __init__(self, alignment=None, working_dir=None, out_file=None):                if working_dir is None:            self.working_dir = os.getcwd()        else:            self.working_dir = working_dir        if alignment is not None:            if not os.path.exists(alignment):                raise FileNotFoundError(\"The specified alignment file does not exist.\")        self.alignment = alignment        self.out_file = out_file        self._options = {}",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def write_ctl_file(self):",
        "labels_text": "Write control file"
    },
    {
        "input_text": "summarize: def read_ctl_file(self):",
        "labels_text": "Read control file"
    },
    {
        "input_text": "summarize: def print_options(self):                for option in self._options.items():            print(f\"{option[0]} = {option[1]}\")",
        "labels_text": "Print out all of the option and their current setting"
    },
    {
        "input_text": "summarize: def set_options(self, **kwargs):                for option, value in kwargs.items():            if option not in self._options:                raise KeyError(\"Invalid option: \" + option)            else:                self._options[option] = value",
        "labels_text": "Set the value of an option This function abstract the option dict to prevent the user from adding option that do not exist or misspelling option"
    },
    {
        "input_text": "summarize: def get_option(self, option):                if option not in self._options:            raise KeyError(\"Invalid option: \" + option)        else:            return self._options.get(option)",
        "labels_text": "Return the value of an option"
    },
    {
        "input_text": "summarize: def get_all_options(self):                return list(self._options.items())",
        "labels_text": "Return the value of all the option"
    },
    {
        "input_text": "summarize: def _set_rel_paths(self):                if self.working_dir is not None:            self._rel_working_dir = os.path.relpath(self.working_dir)        if self.alignment is not None:            self._rel_alignment = os.path.relpath(self.alignment, self.working_dir)        if self.out_file is not None:            self._rel_out_file = os.path.relpath(self.out_file, self.working_dir)",
        "labels_text": "Convert all filedirectory location to path relative to the current working directory PRIVATE paml requires that all path specified in the control file be relative to the directory from which it is called rather than absolute path"
    },
    {
        "input_text": "summarize: def parse_parameters(lines, results, num_params):        parameters = {}    parameters = parse_parameter_list(lines, parameters, num_params)    parameters = parse_kappas(lines, parameters)    parameters = parse_rates(lines, parameters)    parameters = parse_freqs(lines, parameters)    results[\"parameters\"] = parameters    return results",
        "labels_text": "Parse the various parameter from the file"
    },
    {
        "input_text": "summarize: def parse_siteclass_proportions(line_floats):        site_classes = {}    if line_floats:        for n in range(len(line_floats)):            site_classes[n] = {\"proportion\": line_floats[n]}    return site_classes",
        "labels_text": "Find proportion of alignment assigned to each class For model which have multiple site class find the proportion of the alignment assigned to each class"
    },
    {
        "input_text": "summarize: def parse_clademodelc(branch_type_no, line_floats, site_classes):        if not site_classes or len(line_floats) == 0:        return    for n in range(len(line_floats)):        if site_classes[n].get(\"branch types\") is None:            site_classes[n][\"branch types\"] = {}        site_classes[n][\"branch types\"][branch_type_no] = line_floats[n]    return site_classes",
        "labels_text": "Parse result specific to the clade model C"
    },
    {
        "input_text": "summarize: def _gp_float(tok):        try:        return float(tok)    except ValueError:        return str(tok)",
        "labels_text": "Get a float from a token if it fails return the string PRIVATE"
    },
    {
        "input_text": "summarize: def _gp_int(tok):        try:        return int(tok)    except ValueError:        return str(tok)",
        "labels_text": "Get a int from a token if it fails return the string PRIVATE"
    },
    {
        "input_text": "summarize: def set_menu(self, option_list):                self.set_parameter(            \"command\", \"MenuOptions=\" + \".\".join(str(x) for x in option_list)        )",
        "labels_text": "Set the menu option Example setmenu get all F statistic menu"
    },
    {
        "input_text": "summarize: def set_input(self, fname):                self.set_parameter(\"input\", \"InputFile=\" + fname)",
        "labels_text": "Set the input file name"
    },
    {
        "input_text": "summarize: def __init__(self, genepop_dir=None):                self.controller = _GenePopCommandline(genepop_dir)",
        "labels_text": "Initialize the controller genepopdir is the directory where GenePop is The binary should be called Genepop capital G"
    },
    {
        "input_text": "summarize: def test_pop_hz_deficiency(        self, fname, enum_test=True, dememorization=10000, batches=20, iterations=5000    ):                return self._test_pop_hz_both(            fname, 1, \".D\", enum_test, dememorization, batches, iterations        )",
        "labels_text": "Use HardyWeinberg test for heterozygote deficiency Returns a population iterator containing a dictionary where dictionarylocusPval SE FisWC FisRH step Some locus have a None if the info is not available SE might be none for enumeration"
    },
    {
        "input_text": "summarize: def test_pop_hz_excess(        self, fname, enum_test=True, dememorization=10000, batches=20, iterations=5000    ):                return self._test_pop_hz_both(            fname, 2, \".E\", enum_test, dememorization, batches, iterations        )",
        "labels_text": "Use HardyWeinberg test for heterozygote deficiency Returns a population iterator containing a dictionary where dictionarylocusPval SE FisWC FisRH step Some locus have a None if the info is not available SE might be none for enumeration"
    },
    {
        "input_text": "summarize: def test_global_hz_deficiency(        self, fname, enum_test=True, dememorization=10000, batches=20, iterations=5000    ):                return self._test_global_hz_both(            fname, 4, \".DG\", enum_test, dememorization, batches, iterations        )",
        "labels_text": "Use Global HardyWeinberg test for heterozygote deficiency Returns a triple with An list per population containing popname Pval SE switch Some pop have a None if the info is not available SE might be none for enumeration An list per locus containing locusname Pval SE switch Some locus have a None if the info is not available SE might be none for enumeration Overall result Pval SE switch"
    },
    {
        "input_text": "summarize: def test_global_hz_excess(        self, fname, enum_test=True, dememorization=10000, batches=20, iterations=5000    ):                return self._test_global_hz_both(            fname, 5, \".EG\", enum_test, dememorization, batches, iterations        )",
        "labels_text": "Use Global HardyWeinberg test for heterozygote excess Returns a triple with A list per population containing popname Pval SE switch Some pop have a None if the info is not available SE might be none for enumeration A list per locus containing locusname Pval SE switch Some locus have a None if the info is not available SE might be none for enumeration Overall result Pval SE switch"
    },
    {
        "input_text": "summarize: def create_contingency_tables(self, fname):                raise NotImplementedError",
        "labels_text": "Provision for creating Genotypic contingency table"
    },
    {
        "input_text": "summarize: def test_genic_diff_all(        self, fname, dememorization=10000, batches=20, iterations=5000    ):                raise NotImplementedError",
        "labels_text": "Provision for Genic differentiation for all population"
    },
    {
        "input_text": "summarize: def test_genic_diff_pair(        self, fname, dememorization=10000, batches=20, iterations=5000    ):                raise NotImplementedError",
        "labels_text": "Provision for Genic differentiation for all population pair"
    },
    {
        "input_text": "summarize: def test_genotypic_diff_all(        self, fname, dememorization=10000, batches=20, iterations=5000    ):                raise NotImplementedError",
        "labels_text": "Provision for Genotypic differentiation for all population"
    },
    {
        "input_text": "summarize: def test_genotypic_diff_pair(        self, fname, dememorization=10000, batches=20, iterations=5000    ):                raise NotImplementedError",
        "labels_text": "Provision for Genotypic differentiation for all population pair"
    },
    {
        "input_text": "summarize: def calc_diversities_fis_with_identity(self, fname):                return self._calc_diversities_fis(fname, \".DIV\")",
        "labels_text": "Compute identitybase Gene diversity and Fis"
    },
    {
        "input_text": "summarize: def calc_diversities_fis_with_size(self, fname):                raise NotImplementedError",
        "labels_text": "Provision to Computer Allele sizebased Gene diversity and Fis"
    },
    {
        "input_text": "summarize: def calc_rho_all(self, fname):                raise NotImplementedError",
        "labels_text": "Provision for estimating spatial structure from Allele size for all population"
    },
    {
        "input_text": "summarize: def calc_rho_pair(self, fname):                raise NotImplementedError",
        "labels_text": "Provision for estimating spatial structure from Allele size for all population pair"
    },
    {
        "input_text": "summarize: def calc_ibd_diplo(self, fname, stat=\"a\", scale=\"Log\", min_dist=0.00001):                return self._calc_ibd(fname, 5, stat, scale, min_dist)",
        "labels_text": "Calculate isolation by distance statistic for diploid data See calcibd for parameter detail Note that each pop can only have a single individual and the individual name ha to be the sample coordinate"
    },
    {
        "input_text": "summarize: def calc_ibd_haplo(self, fname, stat=\"a\", scale=\"Log\", min_dist=0.00001):                return self._calc_ibd(fname, 6, stat, scale, min_dist)",
        "labels_text": "Calculate isolation by distance statistic for haploid data See calcibd for parameter detail Note that each pop can only have a single individual and the individual name ha to be the sample coordinate"
    },
    {
        "input_text": "summarize: def __init__(self, fname, genepop_dir=None):                self._fname = fname        self._controller = GenePopController(genepop_dir)        self.__fst_pair_locus = {}  # More caches like this needed!        self.__allele_frequency = {}",
        "labels_text": "Initialize the controller genepopdir is the directory where GenePop is The binary should be called Genepop capital G"
    },
    {
        "input_text": "summarize: def get_basic_info(self):                with open(self._fname) as f:            rec = GenePop.read(f)        return rec.pop_list, rec.loci_list",
        "labels_text": "Obtain the population list and locus list from the file"
    },
    {
        "input_text": "summarize: def test_ld_all_pair(        self, locus1, locus2, dememorization=10000, batches=20, iterations=5000    ):                all_ld = self._controller.test_ld(            self._fname, dememorization, batches, iterations        )[1]        for ld_case in all_ld:            (l1, l2), result = ld_case            if (l1 == locus1 and l2 == locus2) or (l1 == locus2 and l2 == locus1):                return result",
        "labels_text": "Test for linkage disequilibrium for each pair of locus in each population"
    },
    {
        "input_text": "summarize: def estimate_nm(self):                return self._controller.estimate_nm(self._fname)",
        "labels_text": "Estimate Nm Just a simple bridge"
    },
    {
        "input_text": "summarize: def get_heterozygosity_info(self, pop_pos, locus_name):                geno_freqs = self._controller.calc_allele_genotype_freqs(self._fname)        pop_iter, loc_iter = geno_freqs        pops = list(pop_iter)        return pops[pop_pos][1][locus_name][1]",
        "labels_text": "Return the heterozygosity info for a certain locus on a population Returns Expected homozygote observed homozygote Expected heterozygote observed heterozygote"
    },
    {
        "input_text": "summarize: def get_genotype_count(self, pop_pos, locus_name):                geno_freqs = self._controller.calc_allele_genotype_freqs(self._fname)        pop_iter, loc_iter = geno_freqs        pop_iter = list(pop_iter)        return pop_iter[pop_pos][1][locus_name][0]",
        "labels_text": "Return the genotype count for a certain population and locus"
    },
    {
        "input_text": "summarize: def get_fis(self, pop_pos, locus_name):                geno_freqs = self._controller.calc_allele_genotype_freqs(self._fname)        pop_iter, loc_iter = geno_freqs        pops = list(pop_iter)        return pops[pop_pos][1][locus_name][2:]",
        "labels_text": "Return the Fis for a certain population and locus Below CW mean Cockerham and Weir and RH mean Robertson and Hill Returns a pair dictionary allele repetition count frequency Fis CW with information for each allele a triple with total number of allele Fis CW Fis RH"
    },
    {
        "input_text": "summarize: def get_alleles(self, pop_pos, locus_name):                geno_freqs = self._controller.calc_allele_genotype_freqs(self._fname)        pop_iter, loc_iter = geno_freqs        pop_iter = list(pop_iter)        return list(pop_iter[pop_pos][1][locus_name][2].keys())",
        "labels_text": "Return the allele for a certain population and locus"
    },
    {
        "input_text": "summarize: def get_alleles_all_pops(self, locus_name):                geno_freqs = self._controller.calc_allele_genotype_freqs(self._fname)        pop_iter, loc_iter = geno_freqs        for locus_info in loc_iter:            if locus_info[0] == locus_name:                return locus_info[1]",
        "labels_text": "Return the allele for a certain population and locus"
    },
    {
        "input_text": "summarize: def get_multilocus_f_stats(self):                return self._controller.calc_fst_all(self._fname)[0]",
        "labels_text": "Return the multilocus F stats Explain averaging Returns FisCW Fst Fit"
    },
    {
        "input_text": "summarize: def get_f_stats(self, locus_name):                loci_iter = self._controller.calc_fst_all(self._fname)[1]        for name, fis, fst, fit, qintra, qinter in loci_iter:            if name == locus_name:                return fis, fst, fit, qintra, qinter",
        "labels_text": "Return F stats for a locus Returns FisCW Fst Fit Qintra Qinter"
    },
    {
        "input_text": "summarize: def get_avg_fis(self):                return self._controller.calc_diversities_fis_with_identity(self._fname)[1]",
        "labels_text": "Calculate identitybase average Fis"
    },
    {
        "input_text": "summarize: def get_avg_fst_pair(self):                return self._controller.calc_fst_pair(self._fname)[1]",
        "labels_text": "Calculate Allele sizebase average Fis for all population pair"
    },
    {
        "input_text": "summarize: def get_avg_fst_pair_locus(self, locus):                if len(self.__fst_pair_locus) == 0:            iter = self._controller.calc_fst_pair(self._fname)[0]            for locus_info in iter:                self.__fst_pair_locus[locus_info[0]] = locus_info[1]        return self.__fst_pair_locus[locus]",
        "labels_text": "Calculate Allele sizebase average Fis for all population pair of the given locus"
    },
    {
        "input_text": "summarize: def calc_ibd(self, is_diplo=True, stat=\"a\", scale=\"Log\", min_dist=0.00001):                if is_diplo:            return self._controller.calc_ibd_diplo(self._fname, stat, scale, min_dist)        else:            return self._controller.calc_ibd_haplo(self._fname, stat, scale, min_dist)",
        "labels_text": "Calculate isolation by distance statistic for Diploid or Haploid"
    },
    {
        "input_text": "summarize: def read(fname):        record = FileRecord(fname)    return record",
        "labels_text": "Parse a file containing a GenePop file fname is a file name that contains a GenePop record"
    },
    {
        "input_text": "summarize: def __init__(self, fname):                self.comment_line = \"\"        self.loci_list = []        self.fname = fname        self.start_read()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def skip_header(self):                self.current_pop = 0        self.current_ind = 0        for line in self._handle:            if line.rstrip().upper() == \"POP\":                return",
        "labels_text": "Skip the Header To be done after a reopen"
    },
    {
        "input_text": "summarize: def seek_position(self, pop, indiv):                self._handle.seek(0)        self.skip_header()        while pop > 0:            self.skip_population()            pop -= 1        while indiv > 0:            self.get_individual()            indiv -= 1",
        "labels_text": "Seek a certain position in the file Arguments pop pop position is first indiv individual in pop"
    },
    {
        "input_text": "summarize: def skip_population(self):                for line in self._handle:            if line == \"\":                return False            line = line.rstrip()            if line.upper() == \"POP\":                self.current_pop += 1                self.current_ind = 0                return True",
        "labels_text": "Skip the current population Returns true if there is another pop"
    },
    {
        "input_text": "summarize: def get_individual(self):                for line in self._handle:            line = line.rstrip()            if line.upper() == \"POP\":                self.current_pop += 1                self.current_ind = 0                return True            else:                self.current_ind += 1                indiv_name, allele_list, ignore = get_indiv(line)                return indiv_name, allele_list        return False",
        "labels_text": "Get the next individual Returns individual information if there are more individual in the current population Returns True if there are no more individual in the current population but there are more population Next read will be of the following pop Returns False if at end of file"
    },
    {
        "input_text": "summarize: def remove_locus_by_name(self, name, fname):                for i, locus in enumerate(self.loci_list):            if locus == name:                self.remove_locus_by_position(i, fname)                return",
        "labels_text": "Remove a locus by name Arguments name name fname file to be created with locus removed"
    },
    {
        "input_text": "summarize: def remove_loci_by_name(self, names, fname):                positions = []        for i, locus in enumerate(self.loci_list):            if locus in names:                positions.append(i)        self.remove_loci_by_position(positions, fname)",
        "labels_text": "Remove a locus list by name Arguments name name fname file to be created with locus removed"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle        self.marker_len = 0        self.comment_line = \"\"        self.loci_list = []        self.populations = []        self.stack = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.marker_len = 0        self.comment_line = \"\"        self.loci_list = []        self.pop_list = []        self.populations = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def remove_population(self, pos):                del self.populations[pos]",
        "labels_text": "Remove a population by position"
    },
    {
        "input_text": "summarize: def remove_locus_by_position(self, pos):                del self.loci_list[pos]        for pop in self.populations:            for indiv in pop:                name, loci = indiv                del loci[pos]",
        "labels_text": "Remove a locus by position"
    },
    {
        "input_text": "summarize: def remove_locus_by_name(self, name):                for i, locus in enumerate(self.loci_list):            if locus == name:                self.remove_locus_by_position(i)                return",
        "labels_text": "Remove a locus by name"
    },
    {
        "input_text": "summarize: def print_as(self, what=\"list\"):                if what == \"map\":            self.make_format = self._make_map        elif what == \"number\":            self.make_format = self._make_number        else:            self.make_format = self._make_list",
        "labels_text": "Print the result a specified Valid format are list alphabetical order number number of site in the sequence map a map representation of the sequence with the site If you want more flexibility override the virtual method makeformat"
    },
    {
        "input_text": "summarize: def format_output(self, dct, title=\"\", s1=\"\"):                if not dct:            dct = self.results        ls, nc = [], []        for k, v in dct.items():            if v:                ls.append((k, v))            else:                nc.append(k)        return self.make_format(ls, title, nc, s1)",
        "labels_text": "Summarise result a a nicely formatted string Arguments dct is a dictionary a returned by a RestrictionBatchsearch title is the title of the map It must be a formatted string ie you must include the line break s is the title separating the list of enzyme that have site from those without site s must be a formatted string a well The format of printthat is a list"
    },
    {
        "input_text": "summarize: def print_that(self, dct, title=\"\", s1=\"\"):                print(self.format_output(dct, title, s1))",
        "labels_text": "Print the output of the formatoutput method OBSOLETE Arguments dct is a dictionary a returned by a RestrictionBatchsearch title is the title of the map It must be a formatted string ie you must include the line break s is the title separating the list of enzyme that have site from those without site s must be a formatted string a well This method print the output of Aformatoutput and it is here for backwards compatibility"
    },
    {
        "input_text": "summarize: def make_format(self, cut=(), title=\"\", nc=(), s1=\"\"):                return self._make_list(cut, title, nc, s1)",
        "labels_text": "Virtual method used for formatting result Virtual method Here to be pointed to one of the make method You can a well create a new method and point makeformat to it"
    },
    {
        "input_text": "summarize: def _make_list(self, ls, title, nc, s1):                return self._make_list_only(ls, title) + self._make_nocut_only(nc, s1)",
        "labels_text": "Summarise a list of position by enzyme PRIVATE Return a string of form title enzyme position position enzyme position position position Arguments l is a tuple or list of cutting enzyme title is the title nc is a tuple or list of non cutting enzyme s is the sentence before the non cutting enzyme"
    },
    {
        "input_text": "summarize: def _make_map(self, ls, title, nc, s1):                return self._make_map_only(ls, title) + self._make_nocut_only(nc, s1)",
        "labels_text": "Summarise mapping information a a string PRIVATE Return a string of form title enzyme position AAAAAAAAAAAAAAAAAAAAA TTTTTTTTTTTTTTTTTTTTT Arguments l is a list of cutting enzyme title is the title nc is a list of non cutting enzyme s is the sentence before the non cutting enzyme"
    },
    {
        "input_text": "summarize: def _make_number(self, ls, title, nc, s1):                return self._make_number_only(ls, title) + self._make_nocut_only(nc, s1)",
        "labels_text": "Format cutting position information a a string PRIVATE Returns a string in the form title enzyme which cut time enzyme position enzyme which cut time enzyme position position Arguments l is a list of cutting enzyme title is the title nc is a list of non cutting enzyme s is the sentence before the non cutting enzyme"
    },
    {
        "input_text": "summarize: def _make_nocut(self, ls, title, nc, s1):                return title + self._make_nocut_only(nc, s1)",
        "labels_text": "Summarise noncutting enzyme PRIVATE Return a formatted string of the non cutting enzyme l is a list of cutting enzyme will not be used Here for compatibility with makeformat Arguments title is the title nc is a list of non cutting enzyme s is the sentence before the non cutting enzyme"
    },
    {
        "input_text": "summarize: def _make_list_only(self, ls, title, nc=(), s1=\"\"):                if not ls:            return title        return self.__next_section(ls, title)",
        "labels_text": "Summarise list of position per enzyme PRIVATE Return a string of form title enzyme position position enzyme position position position Arguments l is a tuple or list of result title is a string Non cutting enzyme are not included"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.data) - 1",
        "labels_text": "Return length of FormattedSeq FormattedSeq ha a leading space thus subtract"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"FormattedSeq({self[1:]!r}, linear={self.linear!r})\"",
        "labels_text": "Represent FormattedSeq class a a string"
    },
    {
        "input_text": "summarize: def __eq__(self, other):                if isinstance(other, FormattedSeq):            if repr(self) == repr(other):                return True            else:                return False        return False",
        "labels_text": "Implement equality operator for FormattedSeq object"
    },
    {
        "input_text": "summarize: def circularise(self):                self.linear = False",
        "labels_text": "Circularise sequence in place"
    },
    {
        "input_text": "summarize: def linearise(self):                self.linear = True",
        "labels_text": "Linearise sequence in place"
    },
    {
        "input_text": "summarize: def to_linear(self):                new = self.__class__(self)        new.linear = True        return new",
        "labels_text": "Make a new instance of sequence a linear"
    },
    {
        "input_text": "summarize: def to_circular(self):                new = self.__class__(self)        new.linear = False        return new",
        "labels_text": "Make a new instance of sequence a circular"
    },
    {
        "input_text": "summarize: def is_linear(self):                return self.linear",
        "labels_text": "Return if sequence is linear True or circular False"
    },
    {
        "input_text": "summarize: def finditer(self, pattern, size):                if self.is_linear():            data = self.data        else:            data = self.data + self.data[1:size]        return [(i.start(), i.group) for i in re.finditer(pattern, data)]",
        "labels_text": "Return a list of a given pattern which occurs in the sequence The list is made of tuple location patterngroup The latter is used with non palindromic site Pattern is the regular expression pattern corresponding to the enzyme restriction site Size is the size of the restriction enzyme recognitionsite size"
    },
    {
        "input_text": "summarize: def __getitem__(self, i):                if self.lower:            return self.klass(self.data[i].lower())        return self.klass(self.data[i])",
        "labels_text": "Return substring of FormattedSeq The class of the returned object is the class of the respective sequence Note that due to the leading space indexing is based from BioSeq import Seq from BioRestrictionRestriction import FormattedSeq fseq FormattedSeqSeqATGCATGC fseq SeqA"
    },
    {
        "input_text": "summarize: def __add__(cls, other):                if isinstance(other, RestrictionType):            return RestrictionBatch([cls, other])        elif isinstance(other, RestrictionBatch):            return other.add_nocheck(cls)        else:            raise TypeError",
        "labels_text": "Add restriction enzyme to a RestrictionBatch If other is an enzyme return a batch of the two enzyme If other is already a RestrictionBatch add enzyme to it"
    },
    {
        "input_text": "summarize: def __truediv__(cls, other):                return cls.search(other)",
        "labels_text": "Override operator to use a search method from BioRestriction import EcoRI EcoRISeqGAATTC Returns REsearchother"
    },
    {
        "input_text": "summarize: def __rtruediv__(cls, other):                return cls.search(other)",
        "labels_text": "Override division with reversed operand to use a search method from BioRestriction import EcoRI SeqGAATTCEcoRI Returns REsearchother"
    },
    {
        "input_text": "summarize: def __floordiv__(cls, other):                return cls.catalyse(other)",
        "labels_text": "Override operator to use a catalyse method from BioRestriction import EcoRI EcoRISeqGAATTC SeqG SeqAATTC Returns REcatalyseother"
    },
    {
        "input_text": "summarize: def __rfloordiv__(cls, other):                return cls.catalyse(other)",
        "labels_text": "As floordiv with reversed operand from BioRestriction import EcoRI SeqGAATTCEcoRI SeqG SeqAATTC Returns REcatalyseother"
    },
    {
        "input_text": "summarize: def __str__(cls):                return cls.__name__",
        "labels_text": "Return the name of the enzyme a string"
    },
    {
        "input_text": "summarize: def __repr__(cls):                return f\"{cls.__name__}\"",
        "labels_text": "Implement repr method Used with eval or exec will instantiate the enzyme"
    },
    {
        "input_text": "summarize: def __len__(cls):                try:            return cls.size        except AttributeError:            # Happens if the instance was not initialised as expected.            # e.g. if instance created by a documentation framework            # like Sphinx trying to inspect the class automatically,            # Also seen within IPython.            return 0",
        "labels_text": "Return length of recognition site of enzyme a int"
    },
    {
        "input_text": "summarize: def __hash__(cls):                return id(cls)",
        "labels_text": "Implement hash method for RestrictionType Python default is to use id This is consistent with the eq implementation"
    },
    {
        "input_text": "summarize: def __eq__(cls, other):                # assert (id(cls)==id(other)) == (other is cls) == (cls is other)        return id(cls) == id(other)",
        "labels_text": "Override operator True if RE and other are the same enzyme Specifically this check they are the same Python object"
    },
    {
        "input_text": "summarize: def __ne__(cls, other):                if not isinstance(other, RestrictionType):            return True        elif cls.charac == other.charac:            return False        else:            return True",
        "labels_text": "Override operator Isoschizomer strict same recognition site same restriction False All the other True WARNING This is not the inverse of the eq method from BioRestriction import SacI SstI SacI SstI true isoschizomers False SacI SstI False"
    },
    {
        "input_text": "summarize: def __rshift__(cls, other):                if not isinstance(other, RestrictionType):            return False        elif cls.site == other.site and cls.charac != other.charac:            return True        else:            return False",
        "labels_text": "Override operator to test for neoschizomers neoschizomer same recognition site different restriction True all the others False from BioRestriction import SmaI XmaI SmaI XmaI True"
    },
    {
        "input_text": "summarize: def __mod__(cls, other):                if not isinstance(other, RestrictionType):            raise TypeError(f\"expected RestrictionType, got {type(other)} instead\")        return cls._mod1(other)",
        "labels_text": "Override operator to test for compatible overhang True if a and b have compatible overhang from BioRestriction import XhoI SalI XhoI SalI True"
    },
    {
        "input_text": "summarize: def __ge__(cls, other):                if not isinstance(other, RestrictionType):            raise NotImplementedError        if len(cls) > len(other):            return True        elif cls.size == len(other) and cls.__name__ >= other.__name__:            return True        else:            return False",
        "labels_text": "Compare length of recognition site of two enzyme Override a is greater or equal than b if the a site is longer than b site If their site have the same length sort by alphabetical order of their name from BioRestriction import EcoRI EcoRV EcoRIsize EcoRVsize EcoRI EcoRV False"
    },
    {
        "input_text": "summarize: def __gt__(cls, other):                if not isinstance(other, RestrictionType):            raise NotImplementedError        if len(cls) > len(other):            return True        elif cls.size == len(other) and cls.__name__ > other.__name__:            return True        else:            return False",
        "labels_text": "Compare length of recognition site of two enzyme Override Sorting order size of the recognition site if equal size alphabetical order of the name"
    },
    {
        "input_text": "summarize: def __le__(cls, other):                if not isinstance(other, RestrictionType):            raise NotImplementedError        elif len(cls) < len(other):            return True        elif len(cls) == len(other) and cls.__name__ <= other.__name__:            return True        else:            return False",
        "labels_text": "Compare length of recognition site of two enzyme Override Sorting order size of the recognition site if equal size alphabetical order of the name"
    },
    {
        "input_text": "summarize: def __lt__(cls, other):                if not isinstance(other, RestrictionType):            raise NotImplementedError        elif len(cls) < len(other):            return True        elif len(cls) == len(other) and cls.__name__ < other.__name__:            return True        else:            return False",
        "labels_text": "Compare length of recognition site of two enzyme Override Sorting order size of the recognition site if equal size alphabetical order of the name"
    },
    {
        "input_text": "summarize: def all_suppliers(cls):                supply = sorted(x[0] for x in suppliers_dict.values())        print(\",\\n\".join(supply))",
        "labels_text": "Print all the supplier of restriction enzyme"
    },
    {
        "input_text": "summarize: def is_equischizomer(cls, other):                return not cls != other",
        "labels_text": "Test for real isoschizomer True if other is an isoschizomer of RE but not an neoschizomer else False Equischizomer same site same position of restriction from BioRestriction import SacI SstI SmaI XmaI SacIisequischizomerSstI True SmaIisequischizomerXmaI False"
    },
    {
        "input_text": "summarize: def is_neoschizomer(cls, other):                return cls >> other",
        "labels_text": "Test for neoschizomer True if other is an isoschizomer of RE else False Neoschizomer same site different position of restriction"
    },
    {
        "input_text": "summarize: def is_isoschizomer(cls, other):                return (not cls != other) or cls >> other",
        "labels_text": "Test for same recognition site True if other ha the same recognition site else False Isoschizomer same site from BioRestriction import SacI SstI SmaI XmaI SacIisisoschizomerSstI True SmaIisisoschizomerXmaI True"
    },
    {
        "input_text": "summarize: def equischizomers(cls, batch=None):                if not batch:            batch = AllEnzymes        r = [x for x in batch if not cls != x]        i = r.index(cls)        del r[i]        r.sort()        return r",
        "labels_text": "List equischizomers of the enzyme Return a tuple of all the isoschizomers of RE If batch is supplied it is used instead of the default AllEnzymes Equischizomer same site same position of restriction"
    },
    {
        "input_text": "summarize: def neoschizomers(cls, batch=None):                if not batch:            batch = AllEnzymes        r = sorted(x for x in batch if cls >> x)        return r",
        "labels_text": "List neoschizomers of the enzyme Return a tuple of all the neoschizomers of RE If batch is supplied it is used instead of the default AllEnzymes Neoschizomer same site different position of restriction"
    },
    {
        "input_text": "summarize: def isoschizomers(cls, batch=None):                if not batch:            batch = AllEnzymes        r = [x for x in batch if (cls >> x) or (not cls != x)]        i = r.index(cls)        del r[i]        r.sort()        return r",
        "labels_text": "List all isoschizomers of the enzyme Return a tuple of all the equischizomers and neoschizomers of RE If batch is supplied it is used instead of the default AllEnzymes"
    },
    {
        "input_text": "summarize: def frequency(cls):                return cls.freq",
        "labels_text": "Return the theoretically cutting frequency of the enzyme Frequency of the site given a one cut per x base int"
    },
    {
        "input_text": "summarize: def cut_once(cls):                return False",
        "labels_text": "Return if the cutting pattern ha one cut True if the enzyme cut the sequence one time on each strand"
    },
    {
        "input_text": "summarize: def cut_twice(cls):                return False",
        "labels_text": "Return if the cutting pattern ha two cut True if the enzyme cut the sequence twice on each strand"
    },
    {
        "input_text": "summarize: def _modify(cls, location):                yield location",
        "labels_text": "Return a generator that move the cutting position by PRIVATE For internal use only location is an integer corresponding to the location of the match for the enzyme pattern in the sequence modify return the real place where the enzyme will cut Example EcoRI pattern GAATTC EcoRI will cut after the G so in the sequence GAATACACGGAATTCGA dnafinditerGAATTC will return a G is the th base EcoRI cut after the G so EcoRImodify If the enzyme cut twice modify will return two integer corresponding to each cutting site"
    },
    {
        "input_text": "summarize: def _rev_modify(cls, location):                yield location",
        "labels_text": "Return a generator that move the cutting position by PRIVATE For internal use only As modify for site situated on the antiparallel strand when the enzyme is not palindromic"
    },
    {
        "input_text": "summarize: def characteristic(cls):                return None, None, None, None, cls.site",
        "labels_text": "Return a list of the enzyme characteristic a tuple the tuple contains the attribute fst first cut current strand or None fst first cut complementary strand or None scd second cut current strand or None scd second cut complementary strand or None site recognition site"
    },
    {
        "input_text": "summarize: def cut_once(cls):                return True",
        "labels_text": "Return if the cutting pattern ha one cut True if the enzyme cut the sequence one time on each strand"
    },
    {
        "input_text": "summarize: def cut_twice(cls):                return False",
        "labels_text": "Return if the cutting pattern ha two cut True if the enzyme cut the sequence twice on each strand"
    },
    {
        "input_text": "summarize: def _modify(cls, location):                yield location + cls.fst5",
        "labels_text": "Return a generator that move the cutting position by PRIVATE For internal use only location is an integer corresponding to the location of the match for the enzyme pattern in the sequence modify return the real place where the enzyme will cut Example EcoRI pattern GAATTC EcoRI will cut after the G so in the sequence GAATACACGGAATTCGA dnafinditerGAATTC will return a G is the th base EcoRI cut after the G so EcoRImodify if the enzyme cut twice modify will return two integer corresponding to each cutting site"
    },
    {
        "input_text": "summarize: def _rev_modify(cls, location):                yield location - cls.fst3",
        "labels_text": "Return a generator that move the cutting position by PRIVATE For internal use only As modify for site situated on the antiparallel strand when the enzyme is not palindromic"
    },
    {
        "input_text": "summarize: def characteristic(cls):                return cls.fst5, cls.fst3, None, None, cls.site",
        "labels_text": "Return a list of the enzyme characteristic a tuple The tuple contains the attribute fst first cut current strand or None fst first cut complementary strand or None scd second cut current strand or None scd second cut complementary strand or None site recognition site"
    },
    {
        "input_text": "summarize: def cut_once(cls):                return False",
        "labels_text": "Return if the cutting pattern ha one cut True if the enzyme cut the sequence one time on each strand"
    },
    {
        "input_text": "summarize: def cut_twice(cls):                return True",
        "labels_text": "Return if the cutting pattern ha two cut True if the enzyme cut the sequence twice on each strand"
    },
    {
        "input_text": "summarize: def _modify(cls, location):                yield location + cls.fst5        yield location + cls.scd5",
        "labels_text": "Return a generator that move the cutting position by PRIVATE For internal use only location is an integer corresponding to the location of the match for the enzyme pattern in the sequence modify return the real place where the enzyme will cut example EcoRI pattern GAATTC EcoRI will cut after the G so in the sequence GAATACACGGAATTCGA dnafinditerGAATTC will return a G is the th base EcoRI cut after the G so EcoRImodify if the enzyme cut twice modify will return two integer corresponding to each cutting site"
    },
    {
        "input_text": "summarize: def _rev_modify(cls, location):                yield location - cls.fst3        yield location - cls.scd3",
        "labels_text": "Return a generator that move the cutting position by PRIVATE for internal use only a modify for site situated on the antiparallel strand when the enzyme is not palindromic"
    },
    {
        "input_text": "summarize: def characteristic(cls):                return cls.fst5, cls.fst3, cls.scd5, cls.scd3, cls.site",
        "labels_text": "Return a list of the enzyme characteristic a tuple the tuple contains the attribute fst first cut current strand or None fst first cut complementary strand or None scd second cut current strand or None scd second cut complementary strand or None site recognition site"
    },
    {
        "input_text": "summarize: def is_methylable(cls):                return True",
        "labels_text": "Return if recognition site can be methylated True if the recognition site is a methylable"
    },
    {
        "input_text": "summarize: def is_methylable(cls):                return False",
        "labels_text": "Return if recognition site can be methylated True if the recognition site is a methylable"
    },
    {
        "input_text": "summarize: def _search(cls):                siteloc = cls.dna.finditer(cls.compsite, cls.size)        cls.results = [r for s, g in siteloc for r in cls._modify(s)]        if cls.results:            cls._drop()        return cls.results",
        "labels_text": "Return a list of cutting site of the enzyme in the sequence PRIVATE For internal use only Implement the search method for palindromic enzyme"
    },
    {
        "input_text": "summarize: def is_palindromic(cls):                return True",
        "labels_text": "Return if the enzyme ha a palindromic recognition site"
    },
    {
        "input_text": "summarize: def is_palindromic(cls):                return False",
        "labels_text": "Return if the enzyme ha a palindromic recognition site"
    },
    {
        "input_text": "summarize: def catalyse(cls, dna, linear=True):                raise NotImplementedError(f\"{cls.__name__} restriction is unknown.\")",
        "labels_text": "List the sequence fragment after cutting dna with enzyme Return a tuple of dna a will be produced by using RE to restrict the dna dna must be a BioSeqSeq instance or a BioSeqMutableSeq instance If linear is False the sequence is considered to be circular and the output will be modified accordingly"
    },
    {
        "input_text": "summarize: def is_blunt(cls):                return False",
        "labels_text": "Return if the enzyme produce blunt end True if the enzyme produce blunt end Related method REisoverhang REisoverhang REisunknown"
    },
    {
        "input_text": "summarize: def is_5overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def is_3overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def overhang(cls):                return \"unknown\"",
        "labels_text": "Return the type of the enzyme overhang a string Can be overhang overhang blunt unknown"
    },
    {
        "input_text": "summarize: def compatible_end(cls):                return []",
        "labels_text": "List all enzyme that produce compatible end for the enzyme"
    },
    {
        "input_text": "summarize: def _mod1(cls, other):                return False",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def is_blunt(cls):                return True",
        "labels_text": "Return if the enzyme produce blunt end True if the enzyme produce blunt end Related method REisoverhang REisoverhang REisunknown"
    },
    {
        "input_text": "summarize: def is_5overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def is_3overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def overhang(cls):                return \"blunt\"",
        "labels_text": "Return the type of the enzyme overhang a string Can be overhang overhang blunt unknown"
    },
    {
        "input_text": "summarize: def compatible_end(cls, batch=None):                if not batch:            batch = AllEnzymes        r = sorted(x for x in iter(AllEnzymes) if x.is_blunt())        return r",
        "labels_text": "List all enzyme that produce compatible end for the enzyme"
    },
    {
        "input_text": "summarize: def _mod1(other):                return issubclass(other, Blunt)",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def is_blunt(cls):                return False",
        "labels_text": "Return if the enzyme produce blunt end True if the enzyme produce blunt end Related method REisoverhang REisoverhang REisunknown"
    },
    {
        "input_text": "summarize: def is_5overhang(cls):                return True",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def is_3overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def overhang(cls):                return \"5' overhang\"",
        "labels_text": "Return the type of the enzyme overhang a string Can be overhang overhang blunt unknown"
    },
    {
        "input_text": "summarize: def compatible_end(cls, batch=None):                if not batch:            batch = AllEnzymes        r = sorted(x for x in iter(AllEnzymes) if x.is_5overhang() and x % cls)        return r",
        "labels_text": "List all enzyme that produce compatible end for the enzyme"
    },
    {
        "input_text": "summarize: def _mod1(cls, other):                if issubclass(other, Ov5):            return cls._mod2(other)        else:            return False",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def is_blunt(cls):                return False",
        "labels_text": "Return if the enzyme produce blunt end True if the enzyme produce blunt end Related method REisoverhang REisoverhang REisunknown"
    },
    {
        "input_text": "summarize: def is_5overhang(cls):                return False",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def is_3overhang(cls):                return True",
        "labels_text": "Return if the enzyme produce overhanging end True if the enzyme produce overhang sticky end Related method REisoverhang REisblunt REisunknown"
    },
    {
        "input_text": "summarize: def overhang(cls):                return \"3' overhang\"",
        "labels_text": "Return the type of the enzyme overhang a string Can be overhang overhang blunt unknown"
    },
    {
        "input_text": "summarize: def compatible_end(cls, batch=None):                if not batch:            batch = AllEnzymes        r = sorted(x for x in iter(AllEnzymes) if x.is_3overhang() and x % cls)        return r",
        "labels_text": "List all enzyme that produce compatible end for the enzyme"
    },
    {
        "input_text": "summarize: def _mod1(cls, other):                #        #   called by RE._mod1(other) when the one of the enzyme is ambiguous        #        if issubclass(other, Ov3):            return cls._mod2(other)        else:            return False",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def is_defined(cls):                return True",
        "labels_text": "Return if recognition sequence and cut are defined True if the sequence recognised and cut is constant ie the recognition site is not degenerated AND the enzyme cut inside the site Related method REisambiguous REisunknown"
    },
    {
        "input_text": "summarize: def is_ambiguous(cls):                return False",
        "labels_text": "Return if recognition sequence and cut may be ambiguous True if the sequence recognised and cut is ambiguous ie the recognition site is degenerated ANDOR the enzyme cut outside the site Related method REisdefined REisunknown"
    },
    {
        "input_text": "summarize: def is_unknown(cls):                return False",
        "labels_text": "Return if recognition sequence is unknown True if the sequence is unknown ie the recognition site ha not been characterised yet Related method REisdefined REisambiguous"
    },
    {
        "input_text": "summarize: def _mod2(cls, other):                #        #   called by RE._mod1(other) when the one of the enzyme is ambiguous        #        if other.ovhgseq == cls.ovhgseq:            return True        elif issubclass(other, Ambiguous):            return other._mod2(cls)        else:            return False",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def is_defined(cls):                return False",
        "labels_text": "Return if recognition sequence and cut are defined True if the sequence recognised and cut is constant ie the recognition site is not degenerated AND the enzyme cut inside the site Related method REisambiguous REisunknown"
    },
    {
        "input_text": "summarize: def is_ambiguous(cls):                return True",
        "labels_text": "Return if recognition sequence and cut may be ambiguous True if the sequence recognised and cut is ambiguous ie the recognition site is degenerated ANDOR the enzyme cut outside the site Related method REisdefined REisunknown"
    },
    {
        "input_text": "summarize: def is_unknown(cls):                return False",
        "labels_text": "Return if recognition sequence is unknown True if the sequence is unknown ie the recognition site ha not been characterised yet Related method REisdefined REisambiguous"
    },
    {
        "input_text": "summarize: def _drop(cls):                if cls.dna.is_linear():            return        else:            super()._drop()",
        "labels_text": "Remove cut that are outsite of the sequence PRIVATE For internal use only Drop the site that are situated outside the sequence in linear sequence Modify the index for site in circular sequence"
    },
    {
        "input_text": "summarize: def is_defined(cls):                return False",
        "labels_text": "Return if recognition sequence and cut are defined True if the sequence recognised and cut is constant ie the recognition site is not degenerated AND the enzyme cut inside the site Related method REisambiguous REisunknown"
    },
    {
        "input_text": "summarize: def is_ambiguous(cls):                return False",
        "labels_text": "Return if recognition sequence and cut may be ambiguous True if the sequence recognised and cut is ambiguous ie the recognition site is degenerated ANDOR the enzyme cut outside the site Related method REisdefined REisunknown"
    },
    {
        "input_text": "summarize: def is_unknown(cls):                return True",
        "labels_text": "Return if recognition sequence is unknown True if the sequence is unknown ie the recognition site ha not been characterised yet Related method REisdefined REisambiguous"
    },
    {
        "input_text": "summarize: def _mod2(cls, other):                #        #   Normally we should not arrive here. But well better safe than        #   sorry.        #   the overhang is not defined we are compatible with nobody.        #   could raise an Error may be rather than return quietly.        #        # return False        raise ValueError(            \"%s.mod2(%s), %s : NotDefined. pas glop pas glop!\"            % (str(cls), str(other), str(cls))        )",
        "labels_text": "Test if other enzyme produce compatible end for enzyme PRIVATE For internal use only Test for the compatibility of restriction ending of RE and other"
    },
    {
        "input_text": "summarize: def elucidate(cls):                return f\"? {cls.site} ?\"",
        "labels_text": "Return a string representing the recognition site and cutting Return a representation of the site with the cut on the strand represented a and the cut on the strand a ie from BioRestriction import EcoRI KpnI EcoRV SnaI EcoRIelucidate overhang GAATTC KpnIelucidate overhang GGTACC EcoRVelucidate blunt GATATC SnaIelucidate NotDefined cut profile unknown GTATAC"
    },
    {
        "input_text": "summarize: def suppliers(cls):                for s in cls.suppl:            print(suppliers_dict[s][0] + \",\")",
        "labels_text": "Print a list of supplier of the enzyme"
    },
    {
        "input_text": "summarize: def supplier_list(cls):                return [v[0] for k, v in suppliers_dict.items() if k in cls.suppl]",
        "labels_text": "Return a list of supplier of the enzyme"
    },
    {
        "input_text": "summarize: def buffers(cls, supplier):",
        "labels_text": "Return the recommended buffer of the supplier for this enzyme Not implemented yet"
    },
    {
        "input_text": "summarize: def is_comm(cls):                return True",
        "labels_text": "Return if enzyme is commercially available True if RE ha supplier"
    },
    {
        "input_text": "summarize: def suppliers():                return None",
        "labels_text": "Print a list of supplier of the enzyme"
    },
    {
        "input_text": "summarize: def supplier_list(cls):                return []",
        "labels_text": "Return a list of supplier of the enzyme"
    },
    {
        "input_text": "summarize: def buffers(cls, supplier):                raise TypeError(\"Enzyme not commercially available.\")",
        "labels_text": "Return the recommended buffer of the supplier for this enzyme Not implemented yet"
    },
    {
        "input_text": "summarize: def is_comm(cls):                return False",
        "labels_text": "Return if enzyme is commercially available True if RE ha supplier"
    },
    {
        "input_text": "summarize: def __init__(self, first=(), suppliers=()):                first = [self.format(x) for x in first]        first += [eval(x) for n in suppliers for x in suppliers_dict[n][1]]        set.__init__(self, first)        self.mapping = dict.fromkeys(self)        self.already_mapped = None        self.suppliers = [x for x in suppliers if x in suppliers_dict]",
        "labels_text": "Initialize empty RB or prefill with enzyme from supplier"
    },
    {
        "input_text": "summarize: def __str__(self):                if len(self) < 5:            return \"+\".join(self.elements())        else:            return \"...\".join(                (\"+\".join(self.elements()[:2]), \"+\".join(self.elements()[-2:]))            )",
        "labels_text": "Return a readable representation of the RestrictionBatch"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"RestrictionBatch({self.elements()})\"",
        "labels_text": "Represent RestrictionBatch class a a string for debugging"
    },
    {
        "input_text": "summarize: def __contains__(self, other):                try:            other = self.format(other)        except ValueError:  # other is not a restriction enzyme            return False        return set.__contains__(self, other)",
        "labels_text": "Implement in for RestrictionBatch"
    },
    {
        "input_text": "summarize: def __div__(self, other):                return self.search(other)",
        "labels_text": "Override operator to use a search method"
    },
    {
        "input_text": "summarize: def __rdiv__(self, other):                return self.search(other)",
        "labels_text": "Override division with reversed operand to use a search method"
    },
    {
        "input_text": "summarize: def __truediv__(self, other):                return self.search(other)",
        "labels_text": "Override Python division operator to use a search method Like div"
    },
    {
        "input_text": "summarize: def __rtruediv__(self, other):                return self.search(other)",
        "labels_text": "As truediv with reversed operand Like rdiv"
    },
    {
        "input_text": "summarize: def get(self, enzyme, add=False):                e = self.format(enzyme)        if e in self:            return e        elif add:            self.add(e)            return e        else:            raise ValueError(f\"enzyme {e.__name__} is not in RestrictionBatch\")",
        "labels_text": "Check if enzyme is in batch and return it If add is True and enzyme is not in batch add enzyme to batch If add is False which is the default only return enzyme If enzyme is not a RestrictionType or can not be evaluated to a RestrictionType raise a ValueError"
    },
    {
        "input_text": "summarize: def lambdasplit(self, func):                d = list(filter(func, self))        new = RestrictionBatch()        new._data = dict(zip(d, [True] * len(d)))        return new",
        "labels_text": "Filter enzyme in batch with supplied function The new batch will contain only the enzyme for which func return True"
    },
    {
        "input_text": "summarize: def add_supplier(self, letter):                supplier = suppliers_dict[letter]        self.suppliers.append(letter)        for x in supplier[1]:            self.add_nocheck(eval(x))",
        "labels_text": "Add all enzyme from a given supplier to batch letter represents the supplier a defined in the dictionary RestrictionDictionarysuppliers Returns None Raise a KeyError if letter is not a supplier code"
    },
    {
        "input_text": "summarize: def current_suppliers(self):                suppl_list = sorted(suppliers_dict[x][0] for x in self.suppliers)        return suppl_list",
        "labels_text": "List the current supplier for the restriction batch Return a sorted list of the supplier which have been used to create the batch"
    },
    {
        "input_text": "summarize: def __iadd__(self, other):                self.add(other)        return self",
        "labels_text": "Override for use with set b other add other to b check the type of other"
    },
    {
        "input_text": "summarize: def __add__(self, other):                new = self.__class__(self)        new.add(other)        return new",
        "labels_text": "Override for use with set b other new RestrictionBatch"
    },
    {
        "input_text": "summarize: def remove(self, other):                return set.remove(self, self.format(other))",
        "labels_text": "Remove enzyme from restriction batch Safe setremove method Verify that other is a RestrictionType or can be evaluated to a RestrictionType Raise a ValueError if other can not be evaluated to a RestrictionType Raise a KeyError if other is not in B"
    },
    {
        "input_text": "summarize: def add(self, other):                return set.add(self, self.format(other))",
        "labels_text": "Add a restriction enzyme to the restriction batch Safe setadd method Verify that other is a RestrictionType or can be evaluated to a RestrictionType Raise a ValueError if other can not be evaluated to a RestrictionType"
    },
    {
        "input_text": "summarize: def add_nocheck(self, other):                return set.add(self, other)",
        "labels_text": "Add restriction enzyme to batch without checking it type"
    },
    {
        "input_text": "summarize: def format(self, y):                try:            if isinstance(y, RestrictionType):                return y            elif isinstance(eval(str(y)), RestrictionType):                return eval(y)        except (NameError, SyntaxError):            pass        raise ValueError(f\"{y.__class__} is not a RestrictionType\")",
        "labels_text": "Evaluate enzyme name and return it a RestrictionType If y is a RestrictionType return y If y can be evaluated to a RestrictionType return evaly Raise a ValueError in all other case"
    },
    {
        "input_text": "summarize: def is_restriction(self, y):                return isinstance(y, RestrictionType) or isinstance(            eval(str(y)), RestrictionType        )",
        "labels_text": "Return if enzyme name is a known enzyme True if y or evaly is a RestrictionType"
    },
    {
        "input_text": "summarize: def split(self, *classes, **bool):                def splittest(element):            for klass in classes:                b = bool.get(klass.__name__, True)                if issubclass(element, klass):                    if b:                        continue                    else:                        return False                elif b:                    return False                else:                    continue            return True        d = list(filter(splittest, self))        new = RestrictionBatch()        new._data = dict(zip(d, [True] * len(d)))        return new",
        "labels_text": "Extract enzyme of a certain class and put in new RestrictionBatch It work but it is slow so it ha really an interest when splitting over multiple condition"
    },
    {
        "input_text": "summarize: def elements(self):                return sorted(str(e) for e in self)",
        "labels_text": "List the enzyme of the RestrictionBatch a list of string Give all the name of the enzyme in B sorted alphabetically"
    },
    {
        "input_text": "summarize: def as_string(self):                return [str(e) for e in self]",
        "labels_text": "List the name of the enzyme of the RestrictionBatch Return a list of the name of the element of the batch"
    },
    {
        "input_text": "summarize: def suppl_codes(cls):                supply = {k: v[0] for k, v in suppliers_dict.items()}        return supply",
        "labels_text": "Return a dictionary with supplier code Letter code for the supplier"
    },
    {
        "input_text": "summarize: def show_codes(cls):                supply = [\" = \".join(i) for i in cls.suppl_codes().items()]        print(\"\\n\".join(supply))",
        "labels_text": "Print a list of supplier code"
    },
    {
        "input_text": "summarize: def __init__(        self, restrictionbatch=_restrictionbatch, sequence=_empty_DNA, linear=True    ):                RestrictionBatch.__init__(self, restrictionbatch)        self.rb = restrictionbatch        self.sequence = sequence        self.linear = linear        if self.sequence:            self.search(self.sequence, self.linear)",
        "labels_text": "Initialize an Analysis with RestrictionBatch and sequence For most of the method of this class if a dictionary is given it will be used a the base to calculate the result If no dictionary is given a new analysis using the RestrictionBatch which ha been given when the Analysis class ha been instantiated will be carried out and used"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"Analysis({self.rb!r},{self.sequence!r},{self.linear})\"",
        "labels_text": "Represent Analysis class a a string"
    },
    {
        "input_text": "summarize: def _sub_set(self, wanted):                # It seems that this method is not used in the whole class!        return {k: v for k, v in self.mapping.items() if k in wanted}",
        "labels_text": "Filter result for key which are in wanted PRIVATE Internal use only Returns a dict Screen the result through wanted set Keep only the result for which the enzyme is in wanted set"
    },
    {
        "input_text": "summarize: def _test_normal(self, start, end, site):                return start <= site < end",
        "labels_text": "Test if site is between start and end PRIVATE Internal use only"
    },
    {
        "input_text": "summarize: def _test_reverse(self, start, end, site):                return start <= site <= len(self.sequence) or 1 <= site < end",
        "labels_text": "Test if site is between end and start for circular sequence PRIVATE Internal use only"
    },
    {
        "input_text": "summarize: def format_output(self, dct=None, title=\"\", s1=\"\"):                if not dct:            dct = self.mapping        return PrintFormat.format_output(self, dct, title, s1)",
        "labels_text": "Collect data and pas to PrintFormat If dct is not given the full dictionary is used"
    },
    {
        "input_text": "summarize: def print_that(self, dct=None, title=\"\", s1=\"\"):                print(self.format_output(dct, title, s1))",
        "labels_text": "Print the output of the analysis If dct is not given the full dictionary is used s Title for noncutting enzyme This method print the output of Aformatoutput and it is here for backwards compatibility"
    },
    {
        "input_text": "summarize: def full(self, linear=True):                return self.mapping",
        "labels_text": "Perform analysis with all enzyme of batch and return all result Full Restriction Map of the sequence a a dictionary"
    },
    {
        "input_text": "summarize: def blunt(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if k.is_blunt()}",
        "labels_text": "Return only cut that have blunt end"
    },
    {
        "input_text": "summarize: def overhang5(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if k.is_5overhang()}",
        "labels_text": "Return only cut that have overhang"
    },
    {
        "input_text": "summarize: def overhang3(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if k.is_3overhang()}",
        "labels_text": "Return only cut that have overhang"
    },
    {
        "input_text": "summarize: def defined(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if k.is_defined()}",
        "labels_text": "Return only result from enzyme that produce defined overhang"
    },
    {
        "input_text": "summarize: def with_sites(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if v}",
        "labels_text": "Return only result from enzyme with at least one cut"
    },
    {
        "input_text": "summarize: def without_site(self, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if not v}",
        "labels_text": "Return only result from enzyme that dont cut the sequence"
    },
    {
        "input_text": "summarize: def with_N_sites(self, N, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if len(v) == N}",
        "labels_text": "Return only result from enzyme that cut the sequence N time"
    },
    {
        "input_text": "summarize: def with_number_list(self, list, dct=None):                if not dct:            dct = self.mapping        return {k: v for k, v in dct.items() if len(v) in list}",
        "labels_text": "Return only result from enzyme that cut xyz time"
    },
    {
        "input_text": "summarize: def with_name(self, names, dct=None):                for i, enzyme in enumerate(names):            if enzyme not in AllEnzymes:                warnings.warn(f\"no data for the enzyme: {enzyme}\", BiopythonWarning)                del names[i]        if not dct:            return RestrictionBatch(names).search(self.sequence, self.linear)        return {n: dct[n] for n in names if n in dct}",
        "labels_text": "Return only result from enzyme which name are listed"
    },
    {
        "input_text": "summarize: def with_site_size(self, site_size, dct=None):                sites = [name for name in self if name.size == site_size]        if not dct:            return RestrictionBatch(sites).search(self.sequence)        return {k: v for k, v in dct.items() if k in site_size}",
        "labels_text": "Return only result form enzyme with a given site size"
    },
    {
        "input_text": "summarize: def only_between(self, start, end, dct=None):                start, end, test = self._boundaries(start, end)        if not dct:            dct = self.mapping        d = dict(dct)        for key, sites in dct.items():            if not sites:                del d[key]                continue            for site in sites:                if test(start, end, site):                    continue                else:                    del d[key]                    break        return d",
        "labels_text": "Return only result from enzyme that only cut within start end"
    },
    {
        "input_text": "summarize: def between(self, start, end, dct=None):                start, end, test = self._boundaries(start, end)        d = {}        if not dct:            dct = self.mapping        for key, sites in dct.items():            for site in sites:                if test(start, end, site):                    d[key] = sites                    break                continue        return d",
        "labels_text": "Return only result from enzyme that cut at least within border Enzymes that cut the sequence at least in between start and end They may cut outside a well"
    },
    {
        "input_text": "summarize: def show_only_between(self, start, end, dct=None):                d = []        if start <= end:            d = [                (k, [vv for vv in v if start <= vv <= end])                for k, v in self.between(start, end, dct).items()            ]        else:            d = [                (k, [vv for vv in v if start <= vv or vv <= end])                for k, v in self.between(start, end, dct).items()            ]        return dict(d)",
        "labels_text": "Return only result from within start end Enzymes must cut inside startend and may also cut outside However only the cutting position within startend will be returned"
    },
    {
        "input_text": "summarize: def only_outside(self, start, end, dct=None):                start, end, test = self._boundaries(start, end)        if not dct:            dct = self.mapping        d = dict(dct)        for key, sites in dct.items():            if not sites:                del d[key]                continue            for site in sites:                if test(start, end, site):                    del d[key]                    break                else:                    continue        return d",
        "labels_text": "Return only result from enzyme that only cut outside start end Enzymes that cut the sequence outside of the region in between start and end but do not cut inside"
    },
    {
        "input_text": "summarize: def outside(self, start, end, dct=None):                start, end, test = self._boundaries(start, end)        if not dct:            dct = self.mapping        d = {}        for key, sites in dct.items():            for site in sites:                if test(start, end, site):                    continue                else:                    d[key] = sites                    break        return d",
        "labels_text": "Return only result from enzyme that at least cut outside border Enzymes that cut outside the region in between start and end They may cut inside a well"
    },
    {
        "input_text": "summarize: def do_not_cut(self, start, end, dct=None):                if not dct:            dct = self.mapping        d = self.without_site()        d.update(self.only_outside(start, end, dct))        return d",
        "labels_text": "Return only result from enzyme that dont cut between border"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.sid = \"\"        self.residues = None        self.sccs = \"\"        self.sunid = \"\"        self.hierarchy = {}        if line:            self._process(line)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(self.sid)        s += str(self.residues).split(\" \")        s.append(self.sccs)        s.append(self.sunid)        s.append(            \",\".join(                \"=\".join((key, str(value))) for key, value in self.hierarchy.items()            )        )        return \"\\t\".join(map(str, s)) + \"\\n\"",
        "labels_text": "Represent the SCOP classification record a a tabseparated string"
    },
    {
        "input_text": "summarize: def parse(handle):        for line in handle:        if line.startswith(\"#\"):            continue        yield Record(line)",
        "labels_text": "Iterate over a CLA file a Cla record for each line Arguments handle filelike object"
    },
    {
        "input_text": "summarize: def __init__(self, filename):                dict.__init__(self)        self.filename = filename        with open(self.filename) as f:            position = 0            while True:                line = f.readline()                if not line:                    break                if line.startswith(\"#\"):                    continue                record = Record(line)                key = record.sid                if key is not None:                    self[key] = position                position = f.tell()",
        "labels_text": "Create CLA index Arguments filename The file to index"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                position = dict.__getitem__(self, key)        with open(self.filename) as f:            f.seek(position)            line = f.readline()            record = Record(line)        return record",
        "labels_text": "Return an item from the indexed file"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.sunid = \"\"        self.nodetype = \"\"        self.sccs = \"\"        self.name = \"\"        self.description = \"\"        if line:            self._process(line)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(self.sunid)        s.append(self.nodetype)        s.append(self.sccs)        if self.name:            s.append(self.name)        else:            s.append(\"-\")        s.append(self.description)        return \"\\t\".join(map(str, s)) + \"\\n\"",
        "labels_text": "Represent the SCOP description record a a tabseparated string"
    },
    {
        "input_text": "summarize: def parse(handle):        for line in handle:        if line.startswith(\"#\"):            continue        yield Record(line)",
        "labels_text": "Iterate over a DES file a a Des record for each line Arguments handle filelike object"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.sid = \"\"        self.residues = []        self.hierarchy = \"\"        if line:            self._process(line)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(self.sid)        s.append(str(self.residues).replace(\" \", \"\\t\"))        s.append(self.hierarchy)        return \"\\t\".join(s) + \"\\n\"",
        "labels_text": "Represent the SCOP domain record a a tabseparated string"
    },
    {
        "input_text": "summarize: def parse(handle):        for line in handle:        if line.startswith(\"#\"):            continue        yield Record(line)",
        "labels_text": "Iterate over a DOM file a a Dom record for each line Arguments handle filelike object"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.sunid = \"\"        self.parent = \"\"        self.children = []        if line:            self._process(line)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(str(self.sunid))        if self.parent:            s.append(str(self.parent))        else:            if self.sunid != 0:                s.append(\"0\")            else:                s.append(\"-\")        if self.children:            s.append(\",\".join(str(x) for x in self.children))        else:            s.append(\"-\")        return \"\\t\".join(s) + \"\\n\"",
        "labels_text": "Represent the SCOP hierarchy record a a string"
    },
    {
        "input_text": "summarize: def parse(handle):        for line in handle:        if line.startswith(\"#\"):            continue        yield Record(line)",
        "labels_text": "Iterate over a HIE file a Hie record for each line Arguments handle filelike object"
    },
    {
        "input_text": "summarize: def normalize_letters(one_letter_code):        if one_letter_code == \".\":        return \"X\"    else:        return one_letter_code.upper()",
        "labels_text": "Convert RAF oneletter amino acid code into IUPAC standard code Letters are uppercased and Unknown is converted to X"
    },
    {
        "input_text": "summarize: def __init__(self, filename):                dict.__init__(self)        self.filename = filename        with open(self.filename) as f:            position = 0            while True:                line = f.readline()                if not line:                    break                key = line[0:5]                if key is not None:                    self[key] = position                position = f.tell()",
        "labels_text": "Initialize the RAF file index Arguments filename The file to index"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                position = dict.__getitem__(self, key)        with open(self.filename) as f:            f.seek(position)            line = f.readline()            record = SeqMap(line)        return record",
        "labels_text": "Return an item from the indexed file"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.pdbid = \"\"        self.pdb_datestamp = \"\"        self.version = \"\"        self.flags = \"\"        self.res = []        if line:            self._process(line)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def index(self, resid, chainid=\"_\"):                for i in range(len(self.res)):            if self.res[i].resid == resid and self.res[i].chainid == chainid:                return i        raise KeyError(\"No such residue \" + chainid + resid)",
        "labels_text": "Return the index of the SeqMap for the given resid and chainid"
    },
    {
        "input_text": "summarize: def __getitem__(self, index):                if not isinstance(index, slice):            raise NotImplementedError        s = copy(self)        s.res = s.res[index]        return s",
        "labels_text": "Extract a single Res object from the SeqMap"
    },
    {
        "input_text": "summarize: def append(self, res):                self.res.append(res)",
        "labels_text": "Append another Res object onto the list of residue mapping"
    },
    {
        "input_text": "summarize: def __iadd__(self, other):                self.extend(other)        return self",
        "labels_text": "In place addition of SeqMap object"
    },
    {
        "input_text": "summarize: def __add__(self, other):                s = copy(self)        s.extend(other)        return s",
        "labels_text": "Addition of SeqMap object"
    },
    {
        "input_text": "summarize: def __init__(self):                self.chainid = \"\"        self.resid = \"\"        self.atom = \"\"        self.seqres = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def parse(handle):        for line in handle:        yield SeqMap(line)",
        "labels_text": "Iterate over RAF file giving a SeqMap object for each line Arguments handle filelike object"
    },
    {
        "input_text": "summarize: def __init__(self, str=None):                self.pdbid = \"\"        self.fragments = ()        if str is not None:            self._parse(str)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __str__(self):                prefix = \"\"        if self.pdbid:            prefix = self.pdbid + \" \"        if not self.fragments:            return prefix + \"-\"        strs = []        for chain, start, end in self.fragments:            s = []            if chain:                s.append(f\"{chain}:\")            if start:                s.append(f\"{start}-{end}\")            strs.append(\"\".join(s))        return prefix + \",\".join(strs)",
        "labels_text": "Represent the SCOP residue record a a string"
    },
    {
        "input_text": "summarize: def getRoot(self):                return self.getNodeBySunid(0)",
        "labels_text": "Get root node"
    },
    {
        "input_text": "summarize: def getDomainBySid(self, sid):                if sid in self._sidDict:            return self._sidDict[sid]        if self.db_handle:            self.getDomainFromSQL(sid=sid)            if sid in self._sidDict:                return self._sidDict[sid]        else:            return None",
        "labels_text": "Return a domain from it sid"
    },
    {
        "input_text": "summarize: def getNodeBySunid(self, sunid):                if sunid in self._sunidDict:            return self._sunidDict[sunid]        if self.db_handle:            self.getDomainFromSQL(sunid=sunid)            if sunid in self._sunidDict:                return self._sunidDict[sunid]        else:            return None",
        "labels_text": "Return a node from it sunid"
    },
    {
        "input_text": "summarize: def getDomains(self):                if self.db_handle:            return self.getRoot().getDescendents(\"px\")        else:            return self._domains",
        "labels_text": "Return an ordered tuple of all SCOP Domains"
    },
    {
        "input_text": "summarize: def write_hie(self, handle):                # We order nodes to ease comparison with original file        for n in sorted(self._sunidDict.values(), key=lambda x: x.sunid):            handle.write(str(n.toHieRecord()))",
        "labels_text": "Build an HIE SCOP parsable file from this object"
    },
    {
        "input_text": "summarize: def write_des(self, handle):                # Original SCOP file is not ordered?        for n in sorted(self._sunidDict.values(), key=lambda x: x.sunid):            if n != self.root:                handle.write(str(n.toDesRecord()))",
        "labels_text": "Build a DES SCOP parsable file from this object"
    },
    {
        "input_text": "summarize: def write_cla(self, handle):                # We order nodes to ease comparison with original file        for n in sorted(self._sidDict.values(), key=lambda x: x.sunid):            handle.write(str(n.toClaRecord()))",
        "labels_text": "Build a CLA SCOP parsable file from this object"
    },
    {
        "input_text": "summarize: def getAscendentFromSQL(self, node, type):                if nodeCodeOrder.index(type) >= nodeCodeOrder.index(node.type):            return None        cur = self.db_handle.cursor()        cur.execute(            \"SELECT \" + type + \" from cla WHERE \" + node.type + \"=%s\", (node.sunid)        )        result = cur.fetchone()        if result is not None:            return self.getNodeBySunid(result[0])        else:            return None",
        "labels_text": "Get ascendent using SQL backend"
    },
    {
        "input_text": "summarize: def write_hie_sql(self, handle):                cur = handle.cursor()        cur.execute(\"DROP TABLE IF EXISTS hie\")        cur.execute(            \"CREATE TABLE hie (parent INT, child INT, PRIMARY KEY (child), \"            \"INDEX (parent) )\"        )        for p in self._sunidDict.values():            for c in p.children:                cur.execute(f\"INSERT INTO hie VALUES ({p.sunid},{c.sunid})\")",
        "labels_text": "Write HIE data to SQL database"
    },
    {
        "input_text": "summarize: def __init__(self, scop=None):                self.sunid = \"\"        self.parent = None        self.children = []        self.sccs = \"\"        self.type = \"\"        self.description = \"\"        self.scop = scop",
        "labels_text": "Initialize a Node in the scop hierarchy If a Scop instance is provided to the constructor this will be used to lookup related reference using the SQL method If no instance is provided it is assumed the whole tree exists and is connected"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(str(self.sunid))        s.append(self.sccs)        s.append(self.type)        s.append(self.description)        return \" \".join(s)",
        "labels_text": "Represent the node a a string"
    },
    {
        "input_text": "summarize: def toHieRecord(self):                rec = Hie.Record()        rec.sunid = str(self.sunid)        if self.getParent():  # Not root node            rec.parent = str(self.getParent().sunid)        else:            rec.parent = \"-\"        for c in self.getChildren():            rec.children.append(str(c.sunid))        return rec",
        "labels_text": "Return an HieRecord"
    },
    {
        "input_text": "summarize: def toDesRecord(self):                rec = Des.Record()        rec.sunid = str(self.sunid)        rec.nodetype = self.type        rec.sccs = self.sccs        rec.description = self.description        return rec",
        "labels_text": "Return a DesRecord"
    },
    {
        "input_text": "summarize: def getChildren(self):                if self.scop is None:            return self.children        else:            return [self.scop.getNodeBySunid(x) for x in self.children]",
        "labels_text": "Return a list of child of this Node"
    },
    {
        "input_text": "summarize: def getParent(self):                if self.scop is None:            return self.parent        else:            return self.scop.getNodeBySunid(self.parent)",
        "labels_text": "Return the parent of this Node"
    },
    {
        "input_text": "summarize: def __init__(self, scop=None):                Node.__init__(self, scop=scop)        self.sid = \"\"        self.residues = None",
        "labels_text": "Initialize a SCOP Domain object"
    },
    {
        "input_text": "summarize: def __str__(self):                s = []        s.append(self.sid)        s.append(self.sccs)        s.append(\"(\" + str(self.residues) + \")\")        if not self.getParent():            s.append(self.description)        else:            sp = self.getParent()            dm = sp.getParent()            s.append(dm.description)            s.append(\"{\" + sp.description + \"}\")        return \" \".join(s)",
        "labels_text": "Represent the SCOP Domain a a string"
    },
    {
        "input_text": "summarize: def toDesRecord(self):                rec = Node.toDesRecord(self)        rec.name = self.sid        return rec",
        "labels_text": "Return a DesRecord"
    },
    {
        "input_text": "summarize: def getAstralDomainsFromSQL(self, column):                cur = self.db_handle.cursor()        cur.execute(\"SELECT sid FROM astral WHERE \" + column + \"=1\")        data = cur.fetchall()        data = [self.scop.getDomainBySid(x[0]) for x in data]        return data",
        "labels_text": "Load ASTRAL domain from the MySQL database Load a set of astral domain from a column in the astral table of a MYSQL database which can be created with writeToSQL"
    },
    {
        "input_text": "summarize: def getSeqBySid(self, domain):                if self.db_handle is None:            return self.fasta_dict[domain].seq        else:            cur = self.db_handle.cursor()            cur.execute(\"SELECT seq FROM astral WHERE sid=%s\", domain)            return Seq(cur.fetchone()[0])",
        "labels_text": "Get the seq record of a given domain from it sid"
    },
    {
        "input_text": "summarize: def getSeq(self, domain):                return self.getSeqBySid(domain.sid)",
        "labels_text": "Return seq associated with domain"
    },
    {
        "input_text": "summarize: def hashedDomainsById(self, id):                if id not in self.IdDatahash:            self.IdDatahash[id] = {}            for d in self.domainsClusteredById(id):                self.IdDatahash[id][d] = 1        return self.IdDatahash[id]",
        "labels_text": "Get domain clustered by sequence identity in a dict"
    },
    {
        "input_text": "summarize: def hashedDomainsByEv(self, id):                if id not in self.EvDatahash:            self.EvDatahash[id] = {}            for d in self.domainsClusteredByEv(id):                self.EvDatahash[id][d] = 1        return self.EvDatahash[id]",
        "labels_text": "Get domain clustered by evalue in a dict"
    },
    {
        "input_text": "summarize: def isDomainInId(self, dom, id):                return dom in self.hashedDomainsById(id)",
        "labels_text": "Return true if the domain is in the astral cluster for percent ID"
    },
    {
        "input_text": "summarize: def isDomainInEv(self, dom, id):                return dom in self.hashedDomainsByEv(id)",
        "labels_text": "Return true if the domain is in the ASTRAL cluster for evalues"
    },
    {
        "input_text": "summarize: def _open(cgi, params=None, get=1):        # Open a handle to SCOP.    if params is None:        params = {}    options = urlencode(params)    if get:  # do a GET        if options:            cgi += \"?\" + options        handle = urlopen(cgi)    else:  # do a POST        handle = urlopen(cgi, data=options)    return handle",
        "labels_text": "Open a handle to SCOP and return it PRIVATE Open a handle to SCOP cgi is the URL for the cgi script to access params is a dictionary with the option to pas to it get is a boolean that describes whether a GET should be used"
    },
    {
        "input_text": "summarize: def _list_from_csv(csv_string, caster=None):        if caster is None:        return [x for x in csv_string.split(\",\") if x]    else:        return [caster(x) for x in csv_string.split(\",\") if x]",
        "labels_text": "Transform the given commaseparated string into a list PRIVATE param csvstring commaseparated input string type csvstring string param caster function used to cast each item in the input string to it intended type type caster callable accepts string return object"
    },
    {
        "input_text": "summarize: def _calc_score(psl, is_protein):        # adapted from http://genome.ucsc.edu/FAQ/FAQblat.html#blat4    size_mul = 3 if is_protein else 1    return (        size_mul * (psl[\"matches\"] + (psl[\"repmatches\"] >> 1))        - size_mul * psl[\"mismatches\"]        - psl[\"qnuminsert\"]        - psl[\"tnuminsert\"]    )",
        "labels_text": "Calculate score PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, handle, pslx=False):                self.handle = handle        self.line = self.handle.readline()        self.pslx = pslx",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                # break out if it's an empty file        if not self.line:            return        # read through header        # this assumes that the result row match the regex        while not re.search(_RE_ROW_CHECK, self.line.strip()):            self.line = self.handle.readline()            if not self.line:                return        # parse into query results        for qresult in self._parse_qresult():            qresult.program = \"blat\"            yield qresult",
        "labels_text": "Iterate over BlatPslParser yield query result"
    },
    {
        "input_text": "summarize: def _validate_cols(self, cols):                if not self.pslx:            if len(cols) != 21:                raise ValueError(                    \"Invalid PSL line: %r. Expected 21 tab-separated columns, found %i\"                    % (self.line, len(cols))                )        else:            if len(cols) != 23:                raise ValueError(                    \"Invalid PSLX line: %r. Expected 23 tab-separated columns, found %i\"                    % (self.line, len(cols))                )",
        "labels_text": "Validate column length of PSL or PSLX PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, filename, pslx=False):                SearchIndexer.__init__(self, filename, pslx=pslx)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, handle, header=False, pslx=False):                self.handle = handle        # flag for writing header or not        self.header = header        self.pslx = pslx",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, handle, __parse_hit_table=False):                self.handle = handle        self._preamble = self._parse_preamble()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                for qresult in self._parse_qresult():            # re-set desc, for hsp query description            qresult.description = qresult.description            yield qresult",
        "labels_text": "Iterate over FastaMParser object yield query result"
    },
    {
        "input_text": "summarize: def __parse_hit_table(self):                # parse hit table until we see an empty line        hit_rows = []        while True:            line = self.handle.readline()            if (not line) or line.strip():                break            hit_rows.append(\"\")        self.line = line        return hit_rows",
        "labels_text": "Parse hit table row"
    },
    {
        "input_text": "summarize: def __init__(self, filename):                SearchIndexer.__init__(self, filename)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, filename, **kwargs):                self._handle = _open_for_random_access(filename)        self._kwargs = kwargs",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _parse(self, handle):                return next(iter(self._parser(handle, **self._kwargs)))",
        "labels_text": "Pass handle and argument to the next iterable PRIVATE"
    },
    {
        "input_text": "summarize: def get(self, offset):                return self._parse(StringIO(self.get_raw(offset).decode()))",
        "labels_text": "Get offset and convert it from byte to string"
    },
    {
        "input_text": "summarize: def getattr_str(obj, attr, fmt=None, fallback=\"?\"):        try:        value = getattr(obj, attr)    except AttributeError:        return fallback    if fmt is None:        return str(value)    return fmt % value",
        "labels_text": "Return string of the given object attribute Defaults to the given fallback value if attribute is not present"
    },
    {
        "input_text": "summarize: def read_forward(handle):        while True:        line = handle.readline()        # if line is empty or line has characters and stripping does not remove        # them, return the line        if (not line) or (line and line.strip()):            return line",
        "labels_text": "Read through whitespaces return the first nonwhitespace line"
    },
    {
        "input_text": "summarize: def singleitem(attr=None, doc=\"\"):        def getter(self):        if len(self._items) > 1:            raise ValueError(\"More than one HSPFragment objects found in HSP\")        if attr is None:            return self._items[0]        return getattr(self._items[0], attr)    return property(fget=getter, doc=doc)",
        "labels_text": "Property for fetching attribute from first entry of container Returns a property that fetch the given attribute from the first item in a SearchIO container object"
    },
    {
        "input_text": "summarize: def allitems(attr=None, doc=\"\"):        def getter(self):        if attr is None:            return self._items        return [getattr(frag, attr) for frag in self._items]    return property(fget=getter, doc=doc)",
        "labels_text": "Property for fetching attribute from all entry of container Returns a property that fetch the given attribute from all item in a SearchIO container object"
    },
    {
        "input_text": "summarize: def fullcascade(attr, doc=\"\"):        def getter(self):        return getattr(self._items[0], attr)    def setter(self, value):        for item in self:            setattr(item, attr, value)    return property(fget=getter, fset=setter, doc=doc)",
        "labels_text": "Return a getter property with a cascading setter This is similar to optionalcascade but for SearchIO container that have at least one item HSP The getter always retrieves the attribute value from the first item If the item have more than one attribute value an error will be raised The setter behaves like partialcascade except that it only set attribute to item in the object not the object itself"
    },
    {
        "input_text": "summarize: def fragcascade(attr, seq_type, doc=\"\"):        assert seq_type in (\"hit\", \"query\")    attr_name = f\"_{seq_type}_{attr}\"    def getter(self):        return getattr(self, attr_name)    def setter(self, value):        setattr(self, attr_name, value)        seq = getattr(self, seq_type)        if seq is not None:            setattr(seq, attr, value)    return property(fget=getter, fset=setter, doc=doc)",
        "labels_text": "Return a getter property with cascading setter for HSPFragment object Similar to partialcascade but for HSPFragment object and act on query or hit property of the object if they are not None"
    },
    {
        "input_text": "summarize: def removesuffix(string, suffix):        # This method is a compatibility wrapper for Python 3.8. It should be    # removed when support for Python 3.8 is dropped. At the time of writing,    # 3.8 is already the oldest supported version.    major, minor, *_ = sys.version_info    if major == 3 and minor == 8:        if suffix and string.endswith(suffix):            return string[: -len(suffix)]        return string    return string.removesuffix(suffix)",
        "labels_text": "Remove the suffix from the string if it exists"
    },
    {
        "input_text": "summarize: def parse(handle, format=None, **kwargs):        # get the iterator object and do error checking    iterator = get_processor(format, _ITERATOR_MAP)    # HACK: force BLAST XML decoding to use utf-8    handle_kwargs = {}    if format == \"blast-xml\":        handle_kwargs[\"encoding\"] = \"utf-8\"    # and start iterating    with as_handle(handle, **handle_kwargs) as source_file:        generator = iterator(source_file, **kwargs)        yield from generator",
        "labels_text": "Iterate over search tool output file a QueryResult object Arguments handle Handle to the file or the filename a a string format Lower case string denoting one of the supported format kwargs Formatspecific keyword argument This function is used to iterate over each query in a given search output file from Bio import SearchIO qresults SearchIOparseBlastmirnaxml blastxml qresults generator object for qresult in qresults printSearch s ha i hit qresultid lenqresult Search ha hit Search ha hit Search ha hit Depending on the file format parse may also accept additional keyword argument that modifies the behavior of the format parser Here is a simple example where the keyword argument enables parsing of a commented BLAST tabular output file from Bio import SearchIO for qresult in SearchIOparseBlastmirnatab blasttab commentsTrue printSearch s ha i hit qresultid lenqresult Search ha hit Search ha hit Search ha hit"
    },
    {
        "input_text": "summarize: def read(handle, format=None, **kwargs):        query_results = parse(handle, format, **kwargs)    try:        query_result = next(query_results)    except StopIteration:        raise ValueError(\"No query results found in handle\") from None    try:        next(query_results)        raise ValueError(\"More than one query result found in handle\")    except StopIteration:        pass    return query_result",
        "labels_text": "Turn a search output file containing one query into a single QueryResult handle Handle to the file or the filename a a string format Lower case string denoting one of the supported format kwargs Formatspecific keyword argument read is used for parsing search output file containing exactly one query from Bio import SearchIO qresult SearchIOreadBlastxmlblastpxml blastxml print s qresultid qresultdescription gi pleckstrin Mus musculus If the given handle ha no result an exception will be raised from Bio import SearchIO qresult SearchIOreadBlasttabtblastntxt blasttab Traceback most recent call last ValueError No query result found in handle Similarly if the given handle ha more than one result an exception will be raised from Bio import SearchIO qresult SearchIOreadBlasttabtblastntxt blasttab Traceback most recent call last ValueError More than one query result found in handle Like parse read may also accept keyword argument depending on the search output file format"
    },
    {
        "input_text": "summarize: def to_dict(qresults, key_function=None):        def _default_key_function(rec):        return rec.id    if key_function is None:        key_function = _default_key_function    qdict = {}    for qresult in qresults:        key = key_function(qresult)        if key in qdict:            raise ValueError(\"Duplicate key %r\" % key)        qdict[key] = qresult    return qdict",
        "labels_text": "Turn a QueryResult iterator or list into a dictionary qresults Iterable returning QueryResult object keyfunction Optional callback function which when given a QueryResult object should return a unique key for the dictionary Defaults to using id of the result This function enables access of QueryResult object from a single search output file using it identifier from Bio import SearchIO qresults SearchIOparseBlastwntsxml blastxml searchdict SearchIOtodictqresults listsearchdict gi gi gi searchdictgi QueryResultidgi hit By default the dictionary key is the QueryResults string ID This may be changed by supplying a callback function that return the desired identifier Here is an example using a function that remove the gi part in the beginning of the QueryResult ID from Bio import SearchIO qresults SearchIOparseBlastwntsxml blastxml keyfunc lambda qresult qresultidsplit searchdict SearchIOtodictqresults keyfunc listsearchdict searchdict QueryResultidgi hit Note that the callback function doe not change the QueryResults ID value It only change the key value used to retrieve the associated QueryResult As this function load all QueryResult object into memory it may be unsuitable for dealing with file containing many query In that case it is recommended that you use either index or indexdb Since Python the default dict class maintains key order meaning this dictionary will reflect the order of record given to it For CPython and PyPy this wa already implemented for Python so effectively you can always assume the record order is preserved"
    },
    {
        "input_text": "summarize: def convert(in_file, in_format, out_file, out_format, in_kwargs=None, out_kwargs=None):        if in_kwargs is None:        in_kwargs = {}    if out_kwargs is None:        out_kwargs = {}    qresults = parse(in_file, in_format, **in_kwargs)    return write(qresults, out_file, out_format, **out_kwargs)",
        "labels_text": "Convert between two search output format return number of record infile Handle to the input file or the filename a string informat Lower case string denoting the format of the input file outfile Handle to the output file or the filename a string outformat Lower case string denoting the format of the output file inkwargs Dictionary of keyword argument for the input function outkwargs Dictionary of keyword argument for the output function The convert function is a shortcut function for parse and write It ha the same return type a write Formatspecific argument may be passed to the convert function but only a dictionary Here is an example of using convert to convert from a BLAST XML file into a tabular file with comment from Bio import SearchIO infile Blastmirnaxml infmt blastxml outfile resultstab outfmt blasttab outkwarg comment True SearchIOconvertinfile infmt outfile outfmt outkwargsoutkwarg stdout Given that different search output file provide different statistic and different level of detail the convert function is limited only to converting format that have the same statistic and for conversion to format with the same level of detail or less For example converting from a BLAST XML output to a HMMER table file is not possible a these are two search program with different kind of statistic In theory you may provide the necessary value required by the HMMER table file eg conditional evalues envelope coordinate etc However these value are likely to hold little meaning a they are not true HMMERcomputed value Another example is converting from BLAST XML to BLAST tabular file This is possible a BLAST XML provide all the value necessary to create a BLAST tabular file However the reverse conversion may not be possible There are more detail covered in the XML file that are not found in a tabular file eg the lambda and kappa value"
    },
    {
        "input_text": "summarize: def _compute_gapopen_num(hsp):        gapopen = 0    for seq_type in (\"query\", \"hit\"):        seq = str(getattr(hsp, seq_type).seq)        gapopen += len(re.findall(_RE_GAPOPEN, seq))    return gapopen",
        "labels_text": "Return the number of gap opening in the given HSP PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, handle, comments=False, fields=_DEFAULT_FIELDS):                self.handle = handle        self.has_comments = comments        self.fields = self._prep_fields(fields)        self.line = self.handle.readline().strip()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                # stop iteration if file has no lines        if not self.line:            return        # determine which iterator to use        elif self.has_comments:            iterfunc = self._parse_commented_qresult        else:            if self.line.startswith(\"#\"):                raise ValueError(                    \"Encountered unexpected character '#' at the beginning of a line. \"                    \"Set comments=True if the file is a commented file.\"                )            iterfunc = self._parse_qresult        yield from iterfunc()",
        "labels_text": "Iterate over BlastTabParser yield query result"
    },
    {
        "input_text": "summarize: def _parse_fields_line(self):                raw_field_str = self.line[len(\"# Fields: \") :]        long_fields = raw_field_str.split(\", \")        fields = [_LONG_SHORT_MAP[long_name] for long_name in long_fields]        return self._prep_fields(fields)",
        "labels_text": "Return column short name line from Fields comment line PRIVATE"
    },
    {
        "input_text": "summarize: def __iter__(self):                handle = self._handle        handle.seek(0)        if not self._kwargs[\"comments\"]:            iterfunc = self._qresult_index        else:            iterfunc = self._qresult_index_commented        for key, offset, length in iterfunc():            yield key.decode(), offset, length",
        "labels_text": "Iterate over the file handle yield key start offset and length"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                if self._kwargs[\"comments\"]:            getfunc = self._get_raw_qresult_commented        else:            getfunc = self._get_raw_qresult        return getfunc(offset)",
        "labels_text": "Return the raw byte string of a QueryResult object from the given offset"
    },
    {
        "input_text": "summarize: def __init__(self, handle, comments=False, fields=_DEFAULT_FIELDS):                self.handle = handle        self.has_comments = comments        self.fields = fields",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, handle, use_raw_query_ids=False, use_raw_hit_ids=False):                self.xml_iter = iter(ElementTree.iterparse(handle, events=(\"start\", \"end\")))        self._use_raw_query_ids = use_raw_query_ids        self._use_raw_hit_ids = use_raw_hit_ids        self._meta, self._fallback = self._parse_preamble()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self._parse_qresult()",
        "labels_text": "Iterate over BlastXmlParser object yield query result"
    },
    {
        "input_text": "summarize: def __init__(self, filename, **kwargs):                SearchIndexer.__init__(self, filename)        # TODO: better way to do this?        iter_obj = self._parser(self._handle, **kwargs)        self._meta, self._fallback = iter_obj._meta, iter_obj._fallback",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _parse(self, handle):                generator = self._parser(handle, **self._kwargs)        generator._meta = self._meta        generator._fallback = self._fallback        return next(iter(generator))",
        "labels_text": "Overwrite SearchIndexer parse PRIVATE As we need to set the meta and fallback dictionary to the parser"
    },
    {
        "input_text": "summarize: def __init__(self, out, encoding=\"utf-8\", indent=\" \", increment=2):                XMLGenerator.__init__(self, out, encoding)        # the indentation character        self._indent = indent        # nest level        self._level = 0        # how many indentation character should we increment per level        self._increment = increment        # container for names of tags with children        self._parent_stack = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def startDocument(self):                self._write(            '<?xml version=\"1.0\"?>\\n'            '<!DOCTYPE BlastOutput PUBLIC \"-//NCBI//NCBI BlastOutput/EN\" '            '\"http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd\">\\n'        )",
        "labels_text": "Start the XML document"
    },
    {
        "input_text": "summarize: def startElement(self, name, attrs=None, children=False):                if attrs is None:            attrs = {}        self.ignorableWhitespace(self._indent * self._level)        XMLGenerator.startElement(self, name, attrs)",
        "labels_text": "Start an XML element param name element name type name string param attrs element attribute type attrs dictionary string object param child whether the element ha child or not type child bool"
    },
    {
        "input_text": "summarize: def endElement(self, name):                XMLGenerator.endElement(self, name)        self._write(\"\\n\")",
        "labels_text": "End and XML element of the given name"
    },
    {
        "input_text": "summarize: def startParent(self, name, attrs=None):                if attrs is None:            attrs = {}        self.startElement(name, attrs, children=True)        self._level += self._increment        self._write(\"\\n\")        # append the element name, so we can end it later        self._parent_stack.append(name)",
        "labels_text": "Start an XML element which ha child param name element name type name string param attrs element attribute type attrs dictionary string object"
    },
    {
        "input_text": "summarize: def endParent(self):                # the element to end is the one on top of the stack        name = self._parent_stack.pop()        self._level -= self._increment        self.ignorableWhitespace(self._indent * self._level)        self.endElement(name)",
        "labels_text": "End an XML element with child"
    },
    {
        "input_text": "summarize: def startParents(self, *names):                for name in names:            self.startParent(name)",
        "labels_text": "Start XML element without child"
    },
    {
        "input_text": "summarize: def endParents(self, num):                for i in range(num):            self.endParent()",
        "labels_text": "End XML element according to the given number"
    },
    {
        "input_text": "summarize: def simpleElement(self, name, content=None):                self.startElement(name, attrs={})        if content:            self.characters(content)        self.endElement(name)",
        "labels_text": "Create an XML element without child with the given content"
    },
    {
        "input_text": "summarize: def characters(self, content):                content = escape(str(content))        for a, b in (('\"', \"&quot;\"), (\"'\", \"&apos;\")):            content = content.replace(a, b)        self._write(content)",
        "labels_text": "Replace quote and apostrophe"
    },
    {
        "input_text": "summarize: def __init__(self, handle, use_raw_query_ids=True, use_raw_hit_ids=True):                self.xml = _BlastXmlGenerator(handle, \"utf-8\")        self._use_raw_query_ids = use_raw_query_ids        self._use_raw_hit_ids = use_raw_hit_ids",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _write_param(self, qresult):                xml = self.xml        xml.startParent(\"Parameters\")        self._write_elem_block(\"Parameters_\", \"param\", qresult)        xml.endParent()",
        "labels_text": "Write the parameter block of the preamble PRIVATE"
    },
    {
        "input_text": "summarize: def get_qresult_id(self, pos):                handle = self._handle        handle.seek(pos)        # get line, check if it's a vulgar line, and get query ID        line = handle.readline()        assert line.startswith(self._query_mark), line        id = re.search(_RE_CIGAR, line.decode())        return id.group(1)",
        "labels_text": "Return the query ID of the nearest cigar line"
    },
    {
        "input_text": "summarize: def _flip_codons(codon_seq, target_seq):        a, b = \"\", \"\"    for char1, char2 in zip(codon_seq, target_seq):        # no need to do anything if the codon seq line has nothing        if char1 == \" \":            a += char1            b += char2        else:            a += char2            b += char1    return a, b",
        "labels_text": "Flips the codon character from one seq to another PRIVATE"
    },
    {
        "input_text": "summarize: def _get_block_coords(parsed_seq, row_dict, has_ner=False):        start = 0    coords = []    if not has_ner:        splitter = _RE_EXON    else:        splitter = _RE_NER    # use the query line for reference    seq = parsed_seq[row_dict[\"query\"]]    for block in re.split(splitter, seq):        start += seq[start:].find(block)        end = start + len(block)        coords.append((start, end))    return coords",
        "labels_text": "Return a list of start end coordinate for each given block in the sequence PRIVATE"
    },
    {
        "input_text": "summarize: def _clean_blocks(tmp_seq_blocks):        seq_blocks = []    for seq_block in tmp_seq_blocks:        for line_name in seq_block:            seq_block[line_name] = (                seq_block[line_name].replace(\"{\", \"\").replace(\"}\", \"\")            )        seq_blocks.append(seq_block)    return seq_blocks",
        "labels_text": "Remove curly brace split codon marker from the given sequence PRIVATE"
    },
    {
        "input_text": "summarize: def get_qresult_id(self, pos):                handle = self._handle        handle.seek(pos)        sentinel = b\"Query:\"        while True:            line = handle.readline().strip()            if line.startswith(sentinel):                break            if not line:                raise StopIteration        qid, desc = _parse_hit_or_query_line(line.decode())        return qid",
        "labels_text": "Return the query ID from the nearest Query line"
    },
    {
        "input_text": "summarize: def get_qresult_id(self, pos):                handle = self._handle        handle.seek(pos)        # get line, check if it's a vulgar line, and get query ID        line = handle.readline()        assert line.startswith(self._query_mark), line        id = re.search(_RE_VULGAR, line.decode())        return id.group(1)",
        "labels_text": "Return the query ID of the nearest vulgar line"
    },
    {
        "input_text": "summarize: def _set_frame(frag):        frag.hit_frame = (frag.hit_start % 3 + 1) * frag.hit_strand    frag.query_frame = (frag.query_start % 3 + 1) * frag.query_strand",
        "labels_text": "Set the HSPFragment frame PRIVATE"
    },
    {
        "input_text": "summarize: def _make_triplets(seq, phase=0):        pre = seq[:phase]    np_seq = seq[phase:]    non_triplets = len(np_seq) % 3    post = \"\" if not non_triplets else np_seq[-1 * non_triplets :]    intacts = [np_seq[3 * i : 3 * (i + 1)] for i in range(len(np_seq) // 3)]    return pre, intacts, post",
        "labels_text": "Select a valid amino acid sequence given a letter code input PRIVATE This function take a single threeletter amino acid sequence and the phase of the sequence to return the longest intact amino acid sequence possible Parts of the input sequence before and after the selected sequence are also returned This is an internal private function and is meant for parsing Exonerates threeletter amino acid output from BioSearchIOExonerateIObase import maketriplets maketripletsGlyThrSerAlaPro Gly Thr Ser Ala Pro maketripletsyThrSerAla phase y Thr Ser Ala maketripletsyThrSerAlaPr phase y Thr Ser Ala Pr"
    },
    {
        "input_text": "summarize: def _get_fragments_coord(frags):        if not frags:        return []    # first fragment always starts from position 0    init = [0]    return reduce(lambda acc, frag: acc + [acc[-1] + len(frag)], frags[:-1], init)",
        "labels_text": "Return the letter coordinate of the given list of fragment PRIVATE This function take a list of threeletter amino acid sequence and return a list of coordinate for each fragment had all the input sequence been flattened This is an internal private function and is meant for parsing Exonerates threeletter amino acid output from BioSearchIOExonerateIObase import getfragmentscoord getfragmentscoordThr Ser Ala getfragmentscoordThr SerAlaPro GlyLeu getfragmentscoordThr SerAlaPro GlyLeu Cys"
    },
    {
        "input_text": "summarize: def _get_fragments_phase(frags):        return [(3 - (x % 3)) % 3 for x in _get_fragments_coord(frags)]",
        "labels_text": "Return the phase of the given list of letter amino acid fragment PRIVATE This is an internal private function and is meant for parsing Exonerates threeletter amino acid output from BioSearchIOExonerateIObase import getfragmentsphase getfragmentsphaseThr Ser Ala getfragmentsphaseThrSe rAla getfragmentsphaseThrSe rAlaLeu ProCys getfragmentsphaseThrSe rAlaLeuP roCys getfragmentsphaseThrSe rAlaLeuPr oCys"
    },
    {
        "input_text": "summarize: def _parse_hit_or_query_line(line):        try:        mark, id, desc = line.split(\" \", 2)    except ValueError:  # no desc        mark, id = line.split(\" \", 1)        desc = \"\"    return id, desc",
        "labels_text": "Parse the Query line of exonerate alignment output PRIVATE"
    },
    {
        "input_text": "summarize: def _get_strand_from_desc(desc, is_protein, modify_desc=True):        if is_protein:        return \".\", desc    suffix = \"\"    if desc.endswith(\"[revcomp]\"):        suffix = \":[revcomp]\" if desc.endswith(\":[revcomp]\") else \"[revcomp]\"    if not suffix:        return \"+\", desc    if modify_desc:        return \"-\", desc[: -len(suffix)]    return \"-\", desc",
        "labels_text": "Determine the strand from the description PRIVATE Exonerate appends revcomp version or revcomp version to the query andor hit description string This function output if the description ha such modification or if not If the query andor hit is a protein sequence a is output instead Aside from the strand the input description value is also returned It is returned unmodified if modifydesc is False Otherwise the appended revcomp or revcomp is removed"
    },
    {
        "input_text": "summarize: def read_until(self, bool_func):                while True:            if not self.line or bool_func(self.line):                return            else:                self.line = self.handle.readline()",
        "labels_text": "Read the file handle until the given bool function return True"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle        self.line = read_forward(self.handle)        self.done = False        self.query_id = None        self.seq_len = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self._parse_qresult()",
        "labels_text": "Iterate over query result there will only ever be one"
    },
    {
        "input_text": "summarize: def _read_until(self, bool_func, stop_on_blank=True, max_read_until=MAX_READ_UNTIL):                count = 0        while True:            if stop_on_blank and not self.line:                return            if bool_func(self.line):                return            else:                self.line = read_forward(self.handle)            count += 1            if count >= max_read_until:                raise RuntimeError(\"Exceeded max_read_until in _read_until\")",
        "labels_text": "Read the file handle until the given function return True PRIVATE"
    },
    {
        "input_text": "summarize: def _parse_qresult(self):                hit_block_data = []        self._parse_preamble()        self._read_until(            lambda line: re.search(_RE_HIT_BLOCK_START, line), stop_on_blank=False        )        while not self.done:            hit_dict = self._parse_hit_block()            hit_block_data.append(hit_dict)        return self._create_qresult(hit_block_data)",
        "labels_text": "Parse HHSUITE output file PRIVATE"
    },
    {
        "input_text": "summarize: def _parse_preamble(self):                meta = {}        while self.line:            regx = re.search(_RE_QUERY, self.line)            if regx:                self.query_id = regx.group(1)            if self.line.startswith(\"Match_columns\"):                self.seq_len = int(self.line.strip().split()[1])            self.line = self.handle.readline().strip()        return meta",
        "labels_text": "Parse metadata about query PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle        self.buf = []        self._meta = self.parse_preamble()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                for qresult in self.parse_qresult():            qresult.program = self._meta.get(\"program\")            qresult.target = self._meta.get(\"target\")            qresult.version = self._meta.get(\"version\")            yield qresult",
        "labels_text": "Iterate over HmmerTextParser yield query result"
    },
    {
        "input_text": "summarize: def read_next(self, rstrip=True):                if len(self.buf) > 0:            return self.buf.pop()        self.line = self.handle.readline()        while self.line and rstrip and not self.line.strip():            self.line = self.handle.readline()        if self.line:            if rstrip:                self.line = self.line.rstrip()        return self.line",
        "labels_text": "Return the next nonempty line trailing whitespace removed"
    },
    {
        "input_text": "summarize: def push_back(self, line):                self.buf.append(line)",
        "labels_text": "Unread a line that should not be parsed yet"
    },
    {
        "input_text": "summarize: def parse_key_value(self):                key, value = self.line.split(\":\", 1)        return key.strip(), value.strip()",
        "labels_text": "Parse keyvalue pair separated by colon"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle        self.line = self.handle.readline()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                header_mark = \"#\"        # read through the header if it exists        while self.line.startswith(header_mark):            self.line = self.handle.readline()        # if we have result rows, parse it        if self.line:            yield from self._parse_qresult()",
        "labels_text": "Iterate over HmmerTabParser yield query result"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.handle = handle        self.line = read_forward(self.handle)        self._meta = self._parse_preamble()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self._parse_qresult()",
        "labels_text": "Iterate over query result"
    },
    {
        "input_text": "summarize: def _read_until(self, bool_func):                while True:            if not self.line or bool_func(self.line):                return            else:                self.line = read_forward(self.handle)",
        "labels_text": "Read the file handle until the given function return True PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self.xml_iter = iter(ElementTree.iterparse(handle, events=(\"start\", \"end\")))        self._meta = self._parse_header()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                yield from self._parse_qresult()",
        "labels_text": "Iterate qresults"
    },
    {
        "input_text": "summarize: def _parse_header(self):                event, elem = next(self.xml_iter)        meta = {}        meta[\"target\"] = \"InterPro\"        meta[\"program\"] = \"InterProScan\"        meta[\"version\"] = elem.attrib[\"interproscan-version\"]        # store the namespace value        self.NS = re.sub(\"protein-matches\", \"\", elem.tag)        return meta",
        "labels_text": "Parse the header for the InterProScan version PRIVATE"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"Hit(id={self.id!r}, query_id={self.query_id!r}, {len(self)!r} hsps)\"",
        "labels_text": "Return string representation of Hit object"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.hsps)",
        "labels_text": "Iterate over hsps"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.hsps)",
        "labels_text": "Return number of hsps"
    },
    {
        "input_text": "summarize: def __bool__(self):                return bool(self.hsps)",
        "labels_text": "Return True if there are hsps"
    },
    {
        "input_text": "summarize: def __contains__(self, hsp):                return hsp in self._items",
        "labels_text": "Return True if hsp in item"
    },
    {
        "input_text": "summarize: def __getitem__(self, idx):                # if key is slice, return a new Hit instance        if isinstance(idx, slice):            obj = self.__class__(self.hsps[idx])            self._transfer_attrs(obj)            return obj        return self._items[idx]",
        "labels_text": "Return the HSP object at the given index"
    },
    {
        "input_text": "summarize: def __setitem__(self, idx, hsps):                # handle case if hsps is a list of hsp        if isinstance(hsps, (list, tuple)):            for hsp in hsps:                self._validate_hsp(hsp)        else:            self._validate_hsp(hsps)        self._items[idx] = hsps",
        "labels_text": "Assign hsps to index idx"
    },
    {
        "input_text": "summarize: def __delitem__(self, idx):                del self._items[idx]",
        "labels_text": "Delete item of index idx"
    },
    {
        "input_text": "summarize: def id_all(self):                return [self.id] + self._id_alt",
        "labels_text": "Alternative IDs of the Hit"
    },
    {
        "input_text": "summarize: def description_all(self):                return [self.description] + self._description_alt",
        "labels_text": "Alternative description of the Hit"
    },
    {
        "input_text": "summarize: def fragments(self):                return list(chain(*self._items))",
        "labels_text": "Access the HSPFragment object contained in the Hit"
    },
    {
        "input_text": "summarize: def append(self, hsp):                self._validate_hsp(hsp)        self._items.append(hsp)",
        "labels_text": "Add a HSP object to the end of Hit Parameters hsp HSP object to append Any HSP object appended must have the same hitid property a the Hit object id property and the same queryid property a the Hit object queryid property"
    },
    {
        "input_text": "summarize: def filter(self, func=None):                hsps = list(filter(func, self.hsps))        if hsps:            obj = self.__class__(hsps)            self._transfer_attrs(obj)            return obj",
        "labels_text": "Create new Hit object whose HSP object pas the filter function param func function for filtering type func callable accepts HSP return bool filter is analogous to Pythons builtin filter function except that instead of returning a list it return a Hit object Here is an example of using filter to select for HSPs having bitscores bigger than from Bio import SearchIO qresult nextSearchIOparseBlastmirnaxml blastxml hit qresult evaluefilter lambda hsp hspbitscore filteredhit hitfilterevaluefilter lenhit lenfilteredhit printfilteredhit Query mir Hit girefNR Pan troglodyte microRNA mirc MIRC microRNA HSPs Evalue Bit score Span Query range Hit range e"
    },
    {
        "input_text": "summarize: def index(self, hsp):                return self._items.index(hsp)",
        "labels_text": "Return the index of a given HSP object zerobased param hsp object to look up type hsp HSP"
    },
    {
        "input_text": "summarize: def map(self, func=None):                if func is not None:            hsps = [func(x) for x in self.hsps[:]]  # this creates a shallow copy        else:            hsps = self.hsps[:]        if hsps:            obj = self.__class__(hsps)            self._transfer_attrs(obj)            return obj",
        "labels_text": "Create new Hit object mapping the given function to it HSPs param func function for mapping type func callable accepts HSP return HSP map is analogous to Pythons builtin map function It is applied to all HSPs contained in the Hit object and return a new Hit object"
    },
    {
        "input_text": "summarize: def pop(self, index=-1):                return self._items.pop(index)",
        "labels_text": "Remove and return the HSP object at the specified index param index index of HSP object to pop type index int"
    },
    {
        "input_text": "summarize: def sort(self, key=None, reverse=False, in_place=True):                if in_place:            self._items.sort(key=key, reverse=reverse)        else:            hsps = self.hsps[:]            hsps.sort(key=key, reverse=reverse)            obj = self.__class__(hsps)            self._transfer_attrs(obj)            return obj",
        "labels_text": "Sort the HSP object param key sorting function type key callable accepts HSP return key for sorting param reverse whether to reverse sorting result or no type reverse bool param inplace whether to do inplace sorting or no type inplace bool sort default to sorting inplace to mimic Pythons listsort method If you set the inplace argument to False it will treat return a new sorted Hit object and keep the initial one unsorted"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"%s(hit_id=%r, query_id=%r, %r fragments)\" % (            self.__class__.__name__,            self.hit_id,            self.query_id,            len(self),        )",
        "labels_text": "Return string representation of HSP object"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self._items)",
        "labels_text": "Iterate over HSP item"
    },
    {
        "input_text": "summarize: def __contains__(self, fragment):                return fragment in self._items",
        "labels_text": "Return True if HSPFragment is on HSP item"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._items)",
        "labels_text": "Return number of HSPs item"
    },
    {
        "input_text": "summarize: def __bool__(self):                return bool(self._items)",
        "labels_text": "Return True if it ha HSPs"
    },
    {
        "input_text": "summarize: def __getitem__(self, idx):                # if key is slice, return a new HSP instance        if isinstance(idx, slice):            obj = self.__class__(self._items[idx])            self._transfer_attrs(obj)            return obj        return self._items[idx]",
        "labels_text": "Return object of index idx"
    },
    {
        "input_text": "summarize: def __setitem__(self, idx, fragments):                # handle case if hsps is a list of hsp        if isinstance(fragments, (list, tuple)):            for fragment in fragments:                self._validate_fragment(fragment)        else:            self._validate_fragment(fragments)        self._items[idx] = fragments",
        "labels_text": "Set an item of index idx with the given fragment"
    },
    {
        "input_text": "summarize: def __delitem__(self, idx):                # note that this may result in an empty HSP object, which should be        # invalid        del self._items[idx]",
        "labels_text": "Delete item of index idx"
    },
    {
        "input_text": "summarize: def __repr__(self):                info = \"hit_id=%r, query_id=%r\" % (self.hit_id, self.query_id)        try:            info += \", %i columns\" % len(self)        except AttributeError:            pass        return \"%s(%s)\" % (self.__class__.__name__, info)",
        "labels_text": "Return HSPFragment info hit id query id number of column"
    },
    {
        "input_text": "summarize: def __len__(self):                return self.aln_span",
        "labels_text": "Return alignment span"
    },
    {
        "input_text": "summarize: def __str__(self):                return self._str_hsp_header() + \"\\n\" + self._str_aln()",
        "labels_text": "Return string of HSP header and alignment"
    },
    {
        "input_text": "summarize: def _query_frame_get(self):                return self._query_frame",
        "labels_text": "Get query sequence reading frame PRIVATE"
    },
    {
        "input_text": "summarize: def _query_frame_set(self, value):                self._query_frame = self._prep_frame(value)",
        "labels_text": "Set query sequence reading frame PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_start_get(self):                return self._hit_start",
        "labels_text": "Get the sequence hit start coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_start_set(self, value):                self._hit_start = self._prep_coord(value, \"hit_end\", le)",
        "labels_text": "Set the sequence hit start coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _query_start_get(self):                return self._query_start",
        "labels_text": "Get the query sequence start coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _query_start_set(self, value):                self._query_start = self._prep_coord(value, \"query_end\", le)",
        "labels_text": "Set the query sequence start coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_end_get(self):                return self._hit_end",
        "labels_text": "Get the hit sequence end coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_end_set(self, value):                self._hit_end = self._prep_coord(value, \"hit_start\", ge)",
        "labels_text": "Set the hit sequence end coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _query_end_get(self):                return self._query_end",
        "labels_text": "Get the query sequence end coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _query_end_set(self, value):                self._query_end = self._prep_coord(value, \"query_start\", ge)",
        "labels_text": "Set the query sequence end coordinate PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_span_get(self):                try:            return self.hit_end - self.hit_start        except TypeError:  # triggered if any of the coordinates are None            return None",
        "labels_text": "Return the number of residue covered by the hit sequence PRIVATE"
    },
    {
        "input_text": "summarize: def _query_span_get(self):                try:            return self.query_end - self.query_start        except TypeError:  # triggered if any of the coordinates are None            return None",
        "labels_text": "Return the number or residue covered by the query PRIVATE"
    },
    {
        "input_text": "summarize: def _hit_range_get(self):                return (self.hit_start, self.hit_end)",
        "labels_text": "Return the start and end of a hit PRIVATE"
    },
    {
        "input_text": "summarize: def _query_range_get(self):                return (self.query_start, self.query_end)",
        "labels_text": "Return the start and end of a query PRIVATE"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self.hits)",
        "labels_text": "Iterate over hit"
    },
    {
        "input_text": "summarize: def hits(self):                return list(self._items.values())",
        "labels_text": "Hit object contained in the QueryResult"
    },
    {
        "input_text": "summarize: def hit_keys(self):                return list(self._items.keys())",
        "labels_text": "Hit IDs of the Hit object contained in the QueryResult"
    },
    {
        "input_text": "summarize: def items(self):                return list(self._items.items())",
        "labels_text": "List of tuples of Hit IDs and Hit object"
    },
    {
        "input_text": "summarize: def iterhits(self):                yield from self._items.values()",
        "labels_text": "Return an iterator over the Hit object"
    },
    {
        "input_text": "summarize: def iterhit_keys(self):                yield from self._items",
        "labels_text": "Return an iterator over the ID of the Hit object"
    },
    {
        "input_text": "summarize: def iteritems(self):                yield from self._items.items()",
        "labels_text": "Return an iterator yielding tuples of Hit ID and Hit object"
    },
    {
        "input_text": "summarize: def __contains__(self, hit_key):                if isinstance(hit_key, Hit):            return self._hit_key_function(hit_key) in self._items        return hit_key in self._items or hit_key in self.__alt_hit_ids",
        "labels_text": "Return True if hit key in item or alternative hit identifier"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self._items)",
        "labels_text": "Return the number of item"
    },
    {
        "input_text": "summarize: def __bool__(self):                return bool(self._items)",
        "labels_text": "Return True if there are item"
    },
    {
        "input_text": "summarize: def __repr__(self):                return \"QueryResult(id=%r, %r hits)\" % (self.id, len(self))",
        "labels_text": "Return string representation of the QueryResult object"
    },
    {
        "input_text": "summarize: def hsps(self):                return sorted(            (hsp for hsp in chain(*self.hits)), key=lambda hsp: hsp.output_index        )",
        "labels_text": "Access the HSP object contained in the QueryResult"
    },
    {
        "input_text": "summarize: def fragments(self):                return list(chain(*self.hsps))",
        "labels_text": "Access the HSPFragment object contained in the QueryResult"
    },
    {
        "input_text": "summarize: def absorb(self, hit):                try:            self.append(hit)        except ValueError:            assert hit.id in self            for hsp in hit:                self[hit.id].append(hsp)",
        "labels_text": "Add a Hit object to the end of QueryResult If the QueryResult already ha a Hit with the same ID append the new Hits HSPs into the existing Hit param hit object to absorb type hit Hit This method is used for file format that may output the same Hit in separate place such a BLAT or Exonerate In both format Hit with different strand are put in different place However SearchIO considers them to be the same a a Hit object should be all database entry with the same ID regardless of strand orientation"
    },
    {
        "input_text": "summarize: def hit_filter(self, func=None):                hits = list(filter(func, self.hits))        obj = self.__class__(hits, self.id, self._hit_key_function)        self._transfer_attrs(obj)        return obj",
        "labels_text": "Create new QueryResult object whose Hit object pas the filter function param func filter function type func callable accepts Hit return bool Here is an example of using hitfilter to select Hits whose description begin with the string Homo sapiens case sensitive from Bio import SearchIO qresult nextSearchIOparseBlastmirnaxml blastxml def descfilterhit return hitdescriptionstartswithHomo sapiens lenqresult filtered qresulthitfilterdescfilter lenfiltered printfiltered Program blastn Query mir Target refseqrna Hits HSP ID description girefNR Homo sapiens microRNA girefNR Homo sapiens microRNA girefNR Homo sapiens microRNA girefNR Homo sapiens microRNA Note that instance attribute other than the hit from the unfiltered QueryResult are retained in the filtered object qresultprogram filteredprogram True qresulttarget filteredtarget True"
    },
    {
        "input_text": "summarize: def hit_map(self, func=None):                hits = [deepcopy(hit) for hit in self.hits]        if func is not None:            hits = [func(x) for x in hits]        obj = self.__class__(hits, self.id, self._hit_key_function)        self._transfer_attrs(obj)        return obj",
        "labels_text": "Create new QueryResult object mapping the given function to it Hits param func map function type func callable accepts Hit return Hit Here is an example of using hitmap with a function that discard all HSPs in a Hit except for the first one from Bio import SearchIO qresult nextSearchIOparseBlastmirnaxml blastxml printqresult Program blastn Query mir Target refseqrna Hits HSP ID description girefNR Homo sapiens microRNA girefNR Pan troglodyte microRNA girefNR Macaca mulatta microRNA girefNR Pan troglodyte microRNA girefNR Pan troglodyte microRNA girefNR Homo sapiens microRNA girefNR Homo sapiens microRNA girefNR Pan troglodyte microRNA tophsp lambda hit hit mappedqresult qresulthitmaptophsp printmappedqresult Program blastn Query mir Target refseqrna Hits HSP ID description girefNR Homo sapiens microRNA girefNR Pan troglodyte microRNA girefNR Macaca mulatta microRNA girefNR Pan troglodyte microRNA girefNR Pan troglodyte microRNA girefNR Homo sapiens microRNA girefNR Homo sapiens microRNA girefNR Pan troglodyte microRNA"
    },
    {
        "input_text": "summarize: def hsp_filter(self, func=None):                hits = [x for x in (hit.filter(func) for hit in self.hits) if x]        obj = self.__class__(hits, self.id, self._hit_key_function)        self._transfer_attrs(obj)        return obj",
        "labels_text": "Create new QueryResult object whose HSP object pas the filter function hspfilter is the same a hitfilter except that it filter directly on each HSP object in every Hit If the filtering remove all HSP object in a given Hit the entire Hit will be discarded This will result in the QueryResult having less Hit after filtering"
    },
    {
        "input_text": "summarize: def hsp_map(self, func=None):                hits = [x for x in (hit.map(func) for hit in list(self.hits)[:]) if x]        obj = self.__class__(hits, self.id, self._hit_key_function)        self._transfer_attrs(obj)        return obj",
        "labels_text": "Create new QueryResult object mapping the given function to it HSPs hspmap is the same a hitmap except that it applies the given function to all HSP object in every Hit instead of the Hit object"
    },
    {
        "input_text": "summarize: def index(self, hit_key):                if isinstance(hit_key, Hit):            return list(self.hit_keys).index(hit_key.id)        try:            return list(self.hit_keys).index(hit_key)        except ValueError:            if hit_key in self.__alt_hit_ids:                return self.index(self.__alt_hit_ids[hit_key])            raise",
        "labels_text": "Return the index of a given hit key zerobased param hitkey hit ID type hitkey string This method is useful for finding out the integer index usually correlated with search rank of a given hit key from Bio import SearchIO qresult nextSearchIOparseBlastmirnaxml blastxml qresultindexgirefNR"
    },
    {
        "input_text": "summarize: def _hit_key_func(hit):        return hit.id",
        "labels_text": "Map hit to it identifier PRIVATE Default hit key function for QueryResultinit use"
    },
    {
        "input_text": "summarize: def _transfer_attrs(self, obj):                # list of attribute names we don't want to transfer        for attr in self.__dict__:            if attr not in self._NON_STICKY_ATTRS:                setattr(obj, attr, self.__dict__[attr])",
        "labels_text": "Transfer instance attribute to the given object PRIVATE This method is used to transfer attribute set externally for example using setattr to a new object created from this one for example from slicing The reason this method is necessary is because different parser will set different attribute for each QueryResult Hit HSP or HSPFragment object depending on the attribute they found in the search output file Ideally we want these attribute to stick with any new instance object created from the original one"
    },
    {
        "input_text": "summarize: def _get_string_tag(opt_bytes_value, default=None):        if opt_bytes_value is None:        return default    try:        return opt_bytes_value.decode()    except UnicodeDecodeError:        return opt_bytes_value.decode(encoding=sys.getdefaultencoding())",
        "labels_text": "Return the string value of the given an optional raw byte tag value If the byte value is None return the given default value"
    },
    {
        "input_text": "summarize: def __init__(self, source, trim=False):                super().__init__(source, mode=\"b\", fmt=\"ABI\")        # check if input file is a valid Abi file        marker = self.stream.read(4)        if not marker:            # handle empty file gracefully            raise ValueError(\"Empty file.\")        if marker != b\"ABIF\":            raise ValueError(f\"File should start with ABIF, not {marker!r}\")        self.trim = trim",
        "labels_text": "Return an iterator for the Abi file format"
    },
    {
        "input_text": "summarize: def _AbiTrimIterator(stream):        return AbiIterator(stream, trim=True)",
        "labels_text": "Return an iterator for the Abi file format that yield trimmed SeqRecord object PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(        self,        source: _TextIOSource,    ) -> None:                super().__init__(source, mode=\"t\", fmt=\"ACE\")        self.ace_contigs = Ace.parse(self.stream)",
        "labels_text": "Iterate over SeqRecord object read from an ACE file Arguments source input stream opened in text mode or a path to a file This us the BioSequencingAce module to do the hard work Note that by iterating over the file in a single pas we are forced to ignore any WA CT RT or WR footer tag Ace file include the base quality for each position which are taken to be PHRED style score Just a if you had read in a FASTQ or QUAL file using PHRED score using BioSeqIO these are stored in the SeqRecords letterannotations dictionary under the phredquality key from Bio import SeqIO with openAceconsedsampleace a handle for record in SeqIOparsehandle ace print s i recordid recordseq lenrecord printmaxrecordletterannotationsphredquality Contig agccccgggc However ACE file do not include a base quality for any gap in the consensus sequence and these are represented in Biopython with quality of zero Using zero is perhaps misleading a there may be very strong evidence to support the gap in the consensus Previous version of Biopython therefore used None instead but this complicated usage and prevented output of the gapped sequence a FASTQ format from Bio import SeqIO with openAcecontigace a handle for record in SeqIOparsehandle ace print s recordid recordseq printrecordletterannotationsphredquality printmaxrecordletterannotationsphredquality Contig AGAGGATGC Contig GAATTACTAT"
    },
    {
        "input_text": "summarize: def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ) -> None:                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"Fasta\")        self._data = SimpleFastaParser(self.stream)",
        "labels_text": "Iterate over Fasta record a SeqRecord object Arguments source input stream opened in text mode or a path to a file alphabet optional alphabet not used Leave a None This parser expects a plain Fasta format without comment or header line By default this will act like calling BioSeqIOparsehandle fasta with no custom handling of the title line with openFastadupsfasta a handle for record in FastaIteratorhandle printrecordid alpha beta gamma alpha delta If you want to modify the record before writing for example to change the ID of each record you can use a generator function a follows def modifyrecordsrecords for record in record recordid recordidupper yield record with openFastadupsfasta a handle for record in modifyrecordsFastaIteratorhandle printrecordid ALPHA BETA GAMMA ALPHA DELTA"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"FASTA\")        self._data = FastaTwoLineParser(self.stream)",
        "labels_text": "Iterate over twoline Fasta record a SeqRecord object Arguments source input stream opened in text mode or a path to a file This us a strict interpretation of the FASTA a requiring exactly two line per record no line wrapping Only the default title to IDnamedescription parsing offered by the relaxed FASTA parser is offered"
    },
    {
        "input_text": "summarize: def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ) -> None:                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"Fasta\")        for line in self.stream:            if line.startswith(\">\"):                self._line = line                break        else:            self._line = None",
        "labels_text": "Iterate over Fasta record a SeqRecord object Arguments source input stream opened in text mode or a path to a file alphabet optional alphabet not used Leave a None This parser expects a Fasta format allowing for a header before the first sequence record and comment line starting with a in William Pearsons FASTA aligner software This iterator act a calling BioSeqIOparsehandle fastapearson with no custom handling of the title line with openFastadupsfasta a handle for record in FastaIteratorhandle printrecordid alpha beta gamma alpha delta If you want to modify the record before writing for example to change the ID of each record you can use a generator function a follows def modifyrecordsrecords for record in record recordid recordidupper yield record with openFastadupsfasta a handle for record in modifyrecordsFastaIteratorhandle printrecordid ALPHA BETA GAMMA ALPHA DELTA"
    },
    {
        "input_text": "summarize: def __init__(self, target, wrap=60, record2title=None):                super().__init__(target)        if wrap:            if wrap < 1:                raise ValueError        self.wrap = wrap        self.record2title = record2title",
        "labels_text": "Create a Fasta writer OBSOLETE Arguments target Output stream opened in text mode or a path to a file wrap Optional line length used to wrap sequence line Defaults to wrapping the sequence at character Use zero or None for no wrapping giving a single long line for the sequence recordtitle Optional function to return the text to be used for the title line of each record By default a combination of the recordid and recorddescription is used If the recorddescription start with the recordid then just the recorddescription is used You can either use handle openfilename w writer FastaWriterhandle writerwritefilemyRecords handleclose Or follow the sequential file writer system for example handle openfilename w writer FastaWriterhandle writerwriteheader doe nothing for Fasta file Multiple writerwriterecord andor writerwriterecords call writerwritefooter doe nothing for Fasta file handleclose"
    },
    {
        "input_text": "summarize: def __init__(self, handle, record2title=None):                super().__init__(handle, wrap=None, record2title=record2title)",
        "labels_text": "Create a line per record Fasta writer OBSOLETE Arguments handle Handle to an output file eg a returned by openfilename w recordtitle Optional function to return the text to be used for the title line of each record By default a combination of the recordid and recorddescription is used If the recorddescription start with the recordid then just the recorddescription is used You can either use handle openfilename w writer FastaWriterhandle writerwritefilemyRecords handleclose Or follow the sequential file writer system for example handle openfilename w writer FastaWriterhandle writerwriteheader doe nothing for Fasta file Multiple writerwriterecord andor writerwriterecords call writerwritefooter doe nothing for Fasta file handleclose"
    },
    {
        "input_text": "summarize: def _read(stream, length):        data = stream.read(length)    if len(data) < length:        raise ValueError(f\"Cannot read {length} bytes from stream\")    return data",
        "labels_text": "Read the specified number of byte from the given stream"
    },
    {
        "input_text": "summarize: def _read_packet(stream):        length = stream.read(4)    if len(length) == 0:        return    if len(length) < 4:        raise ValueError(\"Cannot read packet size from stream\")    length = unpack(\">I\", length)[0]    data = _read(stream, length)    return data",
        "labels_text": "Read a lengthprefixed packet Parts of a GCK file are made of packet comprising of byte giving the packet size followed by the packet data There is no type tag The type of a packet and thus the type of data it contains is solely indicated by the position of the packet within the GCK file"
    },
    {
        "input_text": "summarize: def _read_pstring(stream):        length = _read(stream, 1)    length = unpack(\">B\", length)[0]    data = _read(stream, length).decode(\"ASCII\")    return data",
        "labels_text": "Read a Pascal string A Pascal string is one byte for length followed by the actual string"
    },
    {
        "input_text": "summarize: def _read_p4string(stream):        length = _read(stream, 4)    length = unpack(\">I\", length)[0]    data = _read(stream, length).decode(\"ASCII\")    return data",
        "labels_text": "Read a bit Pascal string Similar to a Pascal string but length is encoded on byte"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"IntelliGenetics\")        for line in self.stream:            if not line.startswith(\";;\"):                break        else:            # Empty file, or header only            line = None        self._line = line",
        "labels_text": "Iterate over IntelliGenetics record a SeqRecord object source filelike object opened in text mode or a path to a file The optional free format file header line which start with two semicolon are ignored The free format commentary line at the start of each record which start with a semicolon are recorded a a single string with embedded new line character in the SeqRecords annotation dictionary under the key comment Examples with openIntelliGeneticsTATmasenuctxt a stream for record in IgIteratorstream printfrecordid length lenrecord AU length BHXBR length CUGA length DELI length FBZA length OANT length OMVP length CPZGAB length CPZANT length AROD length BEHOA length DMM length STMSTM length VERAGM length GRIAGM length SABSABC length SYKSYK length"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"GenBank\")        self.records = GenBankScanner(debug=0).parse_records(self.stream)",
        "labels_text": "Break up a Genbank file into SeqRecord object Argument source is a filelike object opened in text mode or a path to a file Every section from the LOCUS line to the terminating becomes a single SeqRecord with associated annotation and feature Note that for genome or chromosome there is typically only one record This get called internally by BioSeqIO for the GenBank file format from Bio import SeqIO for record in SeqIOparseGenBankcorgb gb printrecordid X X M AJ L AF Equivalently with openGenBankcorgb a handle for record in GenBankIteratorhandle printrecordid X X M AJ L AF"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return next(self.records)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next SeqRecord"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"EMBL\")        self.records = EmblScanner(debug=0).parse_records(self.stream)",
        "labels_text": "Break up an EMBL file into SeqRecord object Argument source is a filelike object opened in text mode or a path to a file Every section from the LOCUS line to the terminating becomes a single SeqRecord with associated annotation and feature Note that for genome or chromosome there is typically only one record This get called internally by BioSeqIO for the EMBL file format from Bio import SeqIO for record in SeqIOparseEMBLepoprtselectionembl embl printrecordid A A A A A A A A CQ Equivalently with openEMBLepoprtselectionembl a handle for record in EmblIteratorhandle printrecordid A A A A A A A A CQ"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return next(self.records)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next SeqRecord"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"IMGT\")        self.records = _ImgtScanner(debug=0).parse_records(self.stream)",
        "labels_text": "Break up an IMGT file into SeqRecord object Argument source is a filelike object opened in text mode or a path to a file Every section from the LOCUS line to the terminating becomes a single SeqRecord with associated annotation and feature Note that for genome or chromosome there is typically only one record"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return next(self.records)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next SeqRecord"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"GenBank\")        self.records = GenBankScanner(debug=0).parse_cds_features(self.stream)",
        "labels_text": "Break up a Genbank file into SeqRecord object for each CDS feature Argument source is a filelike object opened in text mode or a path to a file Every section from the LOCUS line to the terminating can contain many CDS feature These are returned a with the stated amino acid translation sequence if given"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return next(self.records)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next SeqRecord"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"EMBL\")        self.records = EmblScanner(debug=0).parse_cds_features(self.stream)",
        "labels_text": "Break up a EMBL file into SeqRecord object for each CDS feature Argument source is a filelike object opened in text mode or a path to a file Every section from the LOCUS line to the terminating can contain many CDS feature These are returned a with the stated amino acid translation sequence if given"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            return next(self.records)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next SeqRecord"
    },
    {
        "input_text": "summarize: def _get_annotation_str(record, key, default=\".\", just_first=False):                try:            answer = record.annotations[key]        except KeyError:            return default        if isinstance(answer, list):            if not just_first:                assert len(answer) == 1            return str(answer[0])        else:            return str(answer)",
        "labels_text": "Get an annotation dictionary entry a a string PRIVATE Some entry are list in which case if justfirstTrue the first entry is returned If justfirstFalse default this verifies there is only one entry before returning it"
    },
    {
        "input_text": "summarize: def _write_multi_line(self, tag, text):                # TODO - Do the line splitting while preserving white space?        max_len = self.MAX_WIDTH - self.HEADER_WIDTH        lines = self._split_multi_line(text, max_len)        self._write_single_line(tag, lines[0])        for line in lines[1:]:            self._write_single_line(\"\", line)",
        "labels_text": "Write multiple line in each GenBank record PRIVATE Used in the header of each GenBank record"
    },
    {
        "input_text": "summarize: def _get_topology(self, record):                max_topology_len = len(\"circular\")        topology = self._get_annotation_str(record, \"topology\", default=\"\")        if topology and len(topology) <= max_topology_len:            return topology.ljust(max_topology_len)        else:            return \" \" * max_topology_len",
        "labels_text": "Set the topology to circular linear if defined PRIVATE"
    },
    {
        "input_text": "summarize: def _genbank_convert_fasta(in_file, out_file):        # We don't need to parse the features...    records = GenBankScanner().parse_records(in_file, do_features=False)    return SeqIO.write(records, out_file, \"fasta\")",
        "labels_text": "Fast GenBank to FASTA PRIVATE"
    },
    {
        "input_text": "summarize: def _embl_convert_fasta(in_file, out_file):        # We don't need to parse the features...    records = EmblScanner().parse_records(in_file, do_features=False)    return SeqIO.write(records, out_file, \"fasta\")",
        "labels_text": "Fast EMBL to FASTA PRIVATE"
    },
    {
        "input_text": "summarize: def __next__(self):",
        "labels_text": "Return the next SeqRecord This method must be implemented by the subclass"
    },
    {
        "input_text": "summarize: def __iter__(self):                return self",
        "labels_text": "Iterate over the entry a a SeqRecord object Example usage for Fasta file with openexamplefastar a myFile myFastaReader FastaIteratormyFile for record in myFastaReader printrecordid printrecordseq This method SHOULD NOT be overridden by any subclass"
    },
    {
        "input_text": "summarize: def _get_seq_string(record: SeqRecord) -> str:        if not isinstance(record, SeqRecord):        raise TypeError(\"Expected a SeqRecord object\")    if record.seq is None:        raise TypeError(f\"SeqRecord (id={record.id}) has None for its sequence.\")    elif not isinstance(record.seq, (Seq, MutableSeq)):        raise TypeError(f\"SeqRecord (id={record.id}) has an invalid sequence.\")    return str(record.seq)",
        "labels_text": "Use this to catch error like the sequence being None PRIVATE"
    },
    {
        "input_text": "summarize: def _clean(text: str) -> str:        return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
        "labels_text": "Use this to avoid getting newlines in the output PRIVATE"
    },
    {
        "input_text": "summarize: def clean(self, text: str) -> str:                return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
        "labels_text": "Use this to avoid getting newlines in the output"
    },
    {
        "input_text": "summarize: def write_header(self):",
        "labels_text": "Write the file header to the output file"
    },
    {
        "input_text": "summarize: def write_footer(self):",
        "labels_text": "Write the file footer to the output file"
    },
    {
        "input_text": "summarize: def write_record(self, record):                raise NotImplementedError(\"This method should be implemented\")",
        "labels_text": "Write a single record to the output file record a SeqRecord object"
    },
    {
        "input_text": "summarize: def write_records(self, records, maxcount=None):                count = 0        if maxcount is None:            for record in records:                self.write_record(record)                count += 1        else:            for record in records:                if count == maxcount:                    if maxcount == 1:                        raise ValueError(\"More than one sequence found\")                    else:                        raise ValueError(                            \"Number of sequences is larger than %d\" % maxcount                        )                self.write_record(record)                count += 1        return count",
        "labels_text": "Write record to the output file and return the number of record record A list or iterator returning SeqRecord object maxcount The maximum number of record allowed by the file format or None if there is no maximum"
    },
    {
        "input_text": "summarize: def __init__(self, target):                super().__init__(target, mode=\"wb\")",
        "labels_text": "Initialize a Nib writer object Arguments target output stream opened in binary mode or a path to a file"
    },
    {
        "input_text": "summarize: def write_header(self):                super().write_header()        handle = self.handle        byteorder = sys.byteorder        if byteorder == \"little\":  # little-endian            signature = \"3a3de96b\"        elif byteorder == \"big\":  # big-endian            signature = \"6be93d3a\"        else:            raise RuntimeError(f\"unexpected system byte order {byteorder}\")        handle.write(bytes.fromhex(signature))",
        "labels_text": "Write the file header"
    },
    {
        "input_text": "summarize: def write_file(self, records):                count = super().write_file(records, mincount=1, maxcount=1)        return count",
        "labels_text": "Write the complete file with the record and return the number of record"
    },
    {
        "input_text": "summarize: def _res2aacode(residue, undef_code=\"X\"):        if isinstance(residue, str):        return _aa3to1_dict.get(residue, undef_code)    return _aa3to1_dict.get(residue.resname, undef_code)",
        "labels_text": "Return the oneletter amino acid code from the residue name Nonamino acid are returned a X"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"PDB\")        self.cache = None",
        "labels_text": "Return SeqRecord object for each chain in a PDB file Arguments source input stream opened in text mode or a path to a file The sequence are derived from the SEQRES line in the PDB file header not the atom of the D structure Specifically these PDB record are handled DBREF DBREF DBREF SEQADV SEQRES MODRES See httpwwwwwpdborgdocumentationformatsecthtml This get called internally via BioSeqIO for the SEQRES based interpretation of the PDB file format from Bio import SeqIO for record in SeqIOparsePDBAOpdb pdbseqres printRecord id s chain s recordid recordannotationschain printrecorddbxrefs Record id AOA chain A UNPP UNPPOLHVN Equivalently with openPDBAOpdb a handle for record in PdbSeqresIteratorhandle printRecord id s chain s recordid recordannotationschain printrecorddbxrefs Record id AOA chain A UNPP UNPPOLHVN Note the chain is recorded in the annotation dictionary and any PDB DBREF line are recorded in the database crossreferences list"
    },
    {
        "input_text": "summarize: def __init__(self, handle: _IOSource) -> None:                super().__init__(handle)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"Pir\")        # Skip any text before the first record (e.g. blank lines, comments)        for line in self.stream:            if line[0] == \">\":                self._line = line                break        else:            self._line = None",
        "labels_text": "Iterate over a PIR file and yield SeqRecord object source filelike object or a path to a file Examples with openNBRFDMBprotpir a handle for record in PirIteratorhandle print length i recordid lenrecord HLAHLA length HLAHLA length HLAHLA length HLAHLA length HLAHLA length HLAHLA length"
    },
    {
        "input_text": "summarize: def __init__(self, handle, wrap=60, record2title=None, code=None):                super().__init__(handle)        self.wrap = None        if wrap:            if wrap < 1:                raise ValueError(\"wrap should be None, 0, or a positive integer\")        self.wrap = wrap        self.record2title = record2title        self.code = code",
        "labels_text": "Create a PIR writer Arguments handle Handle to an output file eg a returned by openfilename w wrap Optional line length used to wrap sequence line Defaults to wrapping the sequence at character Use zero or None for no wrapping giving a single long line for the sequence recordtitle Optional function to return the text to be used for the title line of each record By default a combination of the recordid recordname and recorddescription is used code Optional sequence code must be one of P F D DL DC RL RC N and XX By default None is used which mean auto detection based on the molecule type in the record annotation You can either use handle openfilename w writer PirWriterhandle writerwritefilemyRecords handleclose Or follow the sequential file writer system for example handle openfilename w writer PirWriterhandle writerwriteheader doe nothing for PIR file Multiple writerwriterecord andor writerwriterecords call writerwritefooter doe nothing for PIR file handleclose"
    },
    {
        "input_text": "summarize: def phred_quality_from_solexa(solexa_quality: float) -> float:        if solexa_quality is None:        # Assume None is used as some kind of NULL or NA value; return None        return None    if solexa_quality < -5:        warnings.warn(            f\"Solexa quality less than -5 passed, {solexa_quality!r}\", BiopythonWarning        )    return 10 * log(10 ** (solexa_quality / 10.0) + 1, 10)",
        "labels_text": "Convert a Solexa quality which can be negative to a PHRED quality PHRED and Solexa quality score are both log transformation of a probality of error high score low probability of error This function take a Solexa score transforms it back to a probability of error and then reexpresses it a a PHRED score This assumes the error estimate are equivalent The underlying formula are given in the documentation for the sister function solexaqualityfromphred in this case the operation is phredquality logsolexaquality This will return a floating point number it is up to you to round this to the nearest integer if appropriate eg printf roundphredqualityfromsolexa printf roundphredqualityfromsolexa printf roundphredqualityfromsolexa printf roundphredqualityfromsolexa printf roundphredqualityfromsolexa Note that a solexaquality less then is not expected will trigger a warning but will still be converted a per the logarithmic mapping giving a number between and back As a special case where None is used for a missing value None is returned printphredqualityfromsolexaNone None"
    },
    {
        "input_text": "summarize: def _get_phred_quality(record: SeqRecord) -> Union[list[float], list[int]]:        try:        return record.letter_annotations[\"phred_quality\"]    except KeyError:        pass    try:        return [            phred_quality_from_solexa(q)            for q in record.letter_annotations[\"solexa_quality\"]        ]    except KeyError:        raise ValueError(            \"No suitable quality scores found in \"            \"letter_annotations of SeqRecord (id=%s).\" % record.id        ) from None",
        "labels_text": "Extract PHRED quality from a SeqRecords letterannotations PRIVATE If there are no PHRED quality but there are Solexa quality those are used instead after conversion"
    },
    {
        "input_text": "summarize: def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ):                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"Fastq\")        self._data = FastqGeneralIterator(self.stream)",
        "labels_text": "Iterate over FASTQ record a SeqRecord object Arguments source input stream opened in text mode or a path to a file alphabet optional alphabet no longer used Leave a None For each sequence in a Sanger style FASTQ file there is a matching string encoding the PHRED quality integer between and about using ASCII value with an offset of For example consider a file containing three short read EASR CCCTTCTTGTCTTCAGCGTTTCTCC EASR TTGGCAGGCCAAGGCCGATGGATCA EASR GTTGCTTCTGGCGTGGGTGGGGGGG For each sequence eg CCCTTCTTGTCTTCAGCGTTTCTCC there is a matching string encoding the PHRED quality using a ASCII value with an offset of eg Using this module directly you might run with openQualityexamplefastq a handle for record in FastqPhredIteratorhandle print s recordid recordseq EASR CCCTTCTTGTCTTCAGCGTTTCTCC EASR TTGGCAGGCCAAGGCCGATGGATCA EASR GTTGCTTCTGGCGTGGGTGGGGGGG Typically however you would call this via BioSeqIO instead with fastq or fastqsanger a the format from Bio import SeqIO with openQualityexamplefastq a handle for record in SeqIOparsehandle fastq print s recordid recordseq EASR CCCTTCTTGTCTTCAGCGTTTCTCC EASR TTGGCAGGCCAAGGCCGATGGATCA EASR GTTGCTTCTGGCGTGGGTGGGGGGG If you want to look at the quality they are record in each record perletterannotation dictionary a a simple list of integer printrecordletterannotationsphredquality To modify the record returned by the parser you can use a generator function For example to store the mean PHRED quality in the record description use from statistic import mean def modifyrecordsrecords for record in record recorddescription meanrecordletterannotationsphredquality yield record with openQualityexamplefastq a handle for record in modifyrecordsFastqPhredIteratorhandle printrecordid recorddescription EASR EASR EASR"
    },
    {
        "input_text": "summarize: def __init__(        self,        source: _TextIOSource,        alphabet: None = None,    ) -> None:                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        super().__init__(source, mode=\"t\", fmt=\"QUAL\")        # Skip any text before the first record (e.g. blank lines, comments)        for line in self.stream:            if line[0] == \">\":                break        else:            line = None        self._line = line",
        "labels_text": "For QUAL file which include PHRED quality score but no sequence For example consider this short QUAL file EASR EASR EASR Using this module directly you might run with openQualityexamplequal a handle for record in QualPhredIteratorhandle print read of length d recordid lenrecordseq EASR read of length EASR read of length EASR read of length Typically however you would call this via BioSeqIO instead with qual a the format from Bio import SeqIO with openQualityexamplequal a handle for record in SeqIOparsehandle qual print read of length d recordid lenrecordseq EASR read of length EASR read of length EASR read of length Only the sequence length is known a the QUAL file doe not contain the sequence string itself The quality score themselves are available a a list of integer in each record perletterannotation printrecordletterannotationsphredquality You can still slice one of these SeqRecord object subrecord record print s subrecordid subrecordletterannotationsphredquality EASR As of Biopython this parser will accept file with negative quality score but will replace them with the lowest possible PHRED score of zero This will trigger a warning previously it raised a ValueError exception"
    },
    {
        "input_text": "summarize: def __init__(        self,        handle: _TextIOSource,        wrap: int = 60,        record2title: Optional[Callable[[SeqRecord], str]] = None,    ) -> None:                super().__init__(handle)        # self.handle = handle        self.wrap: Optional[int] = None        if wrap:            if wrap < 1:                raise ValueError            self.wrap = wrap        self.record2title = record2title",
        "labels_text": "Create a QUAL writer Arguments handle Handle to an output file eg a returned by openfilename w wrap Optional line length used to wrap sequence line Defaults to wrapping the sequence at character Use zero or None for no wrapping giving a single long line for the sequence recordtitle Optional function to return the text to be used for the title line of each record By default a combination of the recordid and recorddescription is used If the recorddescription start with the recordid then just the recorddescription is used The recordtitle argument is present for consistency with the BioSeqIOFastaIO writer class"
    },
    {
        "input_text": "summarize: def _fastq_sanger_convert_fastq_sanger(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # Map unexpected chars to null    mapping = \"\".join(        [chr(0) for ascii in range(33)]        + [chr(ascii) for ascii in range(33, 127)]        + [chr(0) for ascii in range(127, 256)]    )    assert len(mapping) == 256    return _fastq_generic(in_file, out_file, mapping)",
        "labels_text": "Fast Sanger FASTQ to Sanger FASTQ conversion PRIVATE Useful for removing line wrapping and the redundant second identifier on the plus line Will check also check the quality string is valid Avoids creating SeqRecord and Seq object in order to speed up this conversion"
    },
    {
        "input_text": "summarize: def _fastq_solexa_convert_fastq_solexa(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # Map unexpected chars to null    mapping = \"\".join(        [chr(0) for ascii in range(59)]        + [chr(ascii) for ascii in range(59, 127)]        + [chr(0) for ascii in range(127, 256)]    )    assert len(mapping) == 256    return _fastq_generic(in_file, out_file, mapping)",
        "labels_text": "Fast Solexa FASTQ to Solexa FASTQ conversion PRIVATE Useful for removing line wrapping and the redundant second identifier on the plus line Will check also check the quality string is valid Avoids creating SeqRecord and Seq object in order to speed up this conversion"
    },
    {
        "input_text": "summarize: def _fastq_convert_tab(in_file: _TextIOSource, out_file: _TextIOSource) -> int:        # For real speed, don't even make SeqRecord and Seq objects!    count = 0    with as_handle(out_file, \"w\") as out_handle:        for title, seq, qual in FastqGeneralIterator(in_file):            count += 1            out_handle.write(f\"{title.split(None, 1)[0]}\\t{seq}\\n\")    return count",
        "labels_text": "Fast FASTQ to simple tabbed conversion PRIVATE Avoids dealing with the FASTQ quality encoding and creating SeqRecord and Seq object in order to speed up this conversion NOTE This doe NOT check the character used in the FASTQ quality string are valid"
    },
    {
        "input_text": "summarize: def _fastq_sanger_convert_qual(in_file: _TextIOSource, out_file: _TextIOSource) -> int:        mapping = {chr(q + 33): str(q) for q in range(93 + 1)}    return _fastq_convert_qual(in_file, out_file, mapping)",
        "labels_text": "Fast Sanger FASTQ to QUAL conversion PRIVATE"
    },
    {
        "input_text": "summarize: def _fastq_solexa_convert_qual(in_file: _TextIOSource, out_file: _TextIOSource) -> int:        mapping = {        chr(q + 64): str(int(round(phred_quality_from_solexa(q))))        for q in range(-5, 62 + 1)    }    return _fastq_convert_qual(in_file, out_file, mapping)",
        "labels_text": "Fast Solexa FASTQ to QUAL conversion PRIVATE"
    },
    {
        "input_text": "summarize: def _fastq_illumina_convert_qual(    in_file: _TextIOSource, out_file: _TextIOSource) -> int:        mapping = {chr(q + 64): str(q) for q in range(62 + 1)}    return _fastq_convert_qual(in_file, out_file, mapping)",
        "labels_text": "Fast Illumina FASTQ to QUAL conversion PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.source = None        self.sourceVersion = None        self.seqXMLversion = None        self.ncbiTaxID = None        self.speciesName = None        self.startElementNS = None        self.data = None        self.records = []",
        "labels_text": "Create a handler to handle XML event"
    },
    {
        "input_text": "summarize: def startDocument(self):                self.startElementNS = self.startSeqXMLElement",
        "labels_text": "Set XML handler when an XML declaration is found"
    },
    {
        "input_text": "summarize: def endSeqXMLElement(self, name, qname):                namespace, localname = name        if namespace is not None:            raise RuntimeError(f\"Unexpected namespace '{namespace}' for seqXML end\")        if qname is not None:            raise RuntimeError(f\"Unexpected qname '{qname}' for seqXML end\")        if localname != \"seqXML\":            raise RuntimeError(\"Failed to find end of seqXML element\")        self.startElementNS = None        self.endElementNS = None",
        "labels_text": "Handle end of the seqXML element"
    },
    {
        "input_text": "summarize: def endEntryElement(self, name, qname):                if name != (None, \"entry\"):            raise ValueError(\"Expected to find the end of an entry element\")        if qname is not None:            raise RuntimeError(\"Unexpected qname for entry element\")        if self.records[-1].seq is None:            raise ValueError(\"Failed to find a sequence for entry element\")        self.startElementNS = self.startEntryElement        self.endElementNS = self.endSeqXMLElement",
        "labels_text": "Handle end of an entry element"
    },
    {
        "input_text": "summarize: def endSpeciesElement(self, name, qname):                namespace, localname = name        if namespace is not None:            raise RuntimeError(f\"Unexpected namespace '{namespace}' for species end\")        if qname is not None:            raise RuntimeError(f\"Unexpected qname '{qname}' for species end\")        if localname != \"species\":            raise RuntimeError(\"Failed to find end of species element\")        self.endElementNS = self.endEntryElement",
        "labels_text": "Handle end of a specie element"
    },
    {
        "input_text": "summarize: def startDescriptionElement(self, attrs):                if attrs:            raise ValueError(\"Unexpected attributes found in description element\")        if self.data is not None:            raise RuntimeError(f\"Unexpected data found: '{self.data}'\")        self.data = \"\"        self.endElementNS = self.endDescriptionElement",
        "labels_text": "Parse the description"
    },
    {
        "input_text": "summarize: def startSequenceElement(self, attrs):                if attrs:            raise ValueError(\"Unexpected attributes found in sequence element\")        if self.data is not None:            raise RuntimeError(f\"Unexpected data found: '{self.data}'\")        self.data = \"\"        self.endElementNS = self.endSequenceElement",
        "labels_text": "Parse DNA RNA or protein sequence"
    },
    {
        "input_text": "summarize: def endPropertyElement(self, name, qname):                namespace, localname = name        if namespace is not None:            raise RuntimeError(                f\"Unexpected namespace '{namespace}' for property element\"            )        if qname is not None:            raise RuntimeError(f\"Unexpected qname '{qname}' for property element\")        if localname != \"property\":            raise RuntimeError(                f\"Unexpected localname '{localname}' for property element\"            )        self.endElementNS = self.endEntryElement",
        "labels_text": "Handle the end of a property element"
    },
    {
        "input_text": "summarize: def characters(self, data):                if self.data is not None:            self.data += data",
        "labels_text": "Handle character data"
    },
    {
        "input_text": "summarize: def __init__(        self, target, source=None, source_version=None, species=None, ncbiTaxId=None    ):                super().__init__(target, \"wb\")        handle = self.handle        self.xml_generator = XMLGenerator(handle, \"utf-8\")        self.xml_generator.startDocument()        self.source = source        self.source_version = source_version        self.species = species        self.ncbiTaxId = ncbiTaxId",
        "labels_text": "Create Object and start the xml generator Arguments target Output stream opened in binary mode or a path to a file source The source programdatabase of the file for example UniProt sourceversion The version or release number of the source program or database from which the data originated specie The scientific name of the specie of origin of all entry in the file ncbiTaxId The NCBI taxonomy identifier of the specie of origin"
    },
    {
        "input_text": "summarize: def write_footer(self):                self.xml_generator.endElement(\"seqXML\")        self.xml_generator.endDocument()",
        "labels_text": "Close the root node and finish the XML document"
    },
    {
        "input_text": "summarize: def _write_description(self, record):                if record.description:            if not isinstance(record.description, str):                raise TypeError(\"Description should be of type string\")            description = record.description            if description == \"<unknown description>\":                description = \"\"            if len(record.description) > 0:                self.xml_generator.startElement(\"description\", AttributesImpl({}))                self.xml_generator.characters(description)                self.xml_generator.endElement(\"description\")",
        "labels_text": "Write the description if given PRIVATE"
    },
    {
        "input_text": "summarize: def ReadRocheXmlManifest(handle):        (        number_of_reads,        header_length,        index_offset,        index_length,        xml_offset,        xml_size,        read_index_offset,        read_index_size,    ) = _sff_find_roche_index(handle)    if not xml_offset or not xml_size:        raise ValueError(\"No XML manifest found\")    handle.seek(xml_offset)    return handle.read(xml_size).decode()",
        "labels_text": "Read any Roche style XML manifest data in the SFF index The SFF file format allows for multiple different index block and Roche took advantage of this to define their own index block which also embeds an XML manifest string This is not a publicly documented extension to the SFF file format this wa reverse engineered The handle should be to an SFF file opened in binary mode This function will use the handle seektell function and leave the handle in an arbitrary location Any XML manifest found is returned a a Python string which you can then parse a appropriate or reuse when writing out SFF file with the SffWriter class Returns a string or raise a ValueError if an Roche manifest could not be found"
    },
    {
        "input_text": "summarize: def _get_read_xy(read_name):        number = _string_as_base_36(read_name[9:])    return divmod(number, 4096)",
        "labels_text": "Extract coordinate from last character of read name PRIVATE"
    },
    {
        "input_text": "summarize: def _get_read_time(read_name):        time_list = []    remainder = _string_as_base_36(read_name[:6])    for denominator in _time_denominators:        this_term, remainder = divmod(remainder, denominator)        time_list.append(this_term)    time_list.append(remainder)    time_list[0] += 2000    return time_list",
        "labels_text": "Extract time from first character of read name PRIVATE"
    },
    {
        "input_text": "summarize: def _get_read_region(read_name):        return int(read_name[8])",
        "labels_text": "Extract region from read name PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, target, index=True, xml=None):                super().__init__(target, \"wb\")        self._xml = xml        if index:            self._index = []        else:            self._index = None",
        "labels_text": "Initialize an SFF writer object Arguments target Output stream opened in binary mode or a path to a file index Boolean argument should we try and write an index xml Optional string argument xml manifest to be recorded in the index block see function ReadRocheXmlManifest for reading this data"
    },
    {
        "input_text": "summarize: def _parse_dna_packet(length, data, record):        if record.seq:        raise ValueError(\"The file contains more than one DNA packet\")    flags, sequence = unpack(\">B%ds\" % (length - 1), data)    record.seq = Seq(sequence.decode(\"ASCII\"))    record.annotations[\"molecule_type\"] = \"DNA\"    if flags & 0x01:        record.annotations[\"topology\"] = \"circular\"    else:        record.annotations[\"topology\"] = \"linear\"",
        "labels_text": "Parse a DNA sequence packet A DNA sequence packet contains a single byte flag followed by the sequence itself"
    },
    {
        "input_text": "summarize: def _parse_cookie_packet(length, data):        cookie, seq_type, exp_version, imp_version = unpack(\">8sHHH\", data)    if cookie.decode(\"ASCII\") != \"SnapGene\":        raise ValueError(\"The file is not a valid SnapGene file\")",
        "labels_text": "Parse a SnapGene cookie packet Every SnapGene file start with a packet of this type It act a a magic cookie identifying the file a a SnapGene file"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"b\", fmt=\"SnapGene\")        self.packets = _iterate(self.stream)        try:            packet_type, length, data = next(self.packets)        except StopIteration:            raise ValueError(\"Empty file.\") from None        if packet_type != 0x09:            raise ValueError(\"The file does not start with a SnapGene cookie packet\")        _parse_cookie_packet(length, data)",
        "labels_text": "Parse a SnapGene file and return a SeqRecord object Argument source is a filelike object or a path to a file Note that a SnapGene file can only contain one sequence so this iterator will always return a single record"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"t\", fmt=\"Tab-separated plain-text\")",
        "labels_text": "Iterate over tab separated line a SeqRecord object Each line of the file should contain one tab only dividing the line into an identifier and the full sequence Arguments source filelike object opened in text mode or a path to a file The first field is taken a the record id and name regardless of any space within the text and the second field is the sequence Any blank line are ignored Examples with openGenBankNCtsv a handle for record in TabIteratorhandle print length i recordid lenrecord girefNP length girefNP length girefNP length girefNP length girefNP length girefNP length girefNP length girefNP length girefNP length girefNP length"
    },
    {
        "input_text": "summarize: def write_record(self, record):                assert self._header_written        assert not self._footer_written        self._record_written = True        self.handle.write(as_tab(record))",
        "labels_text": "Write a single tab line to the file"
    },
    {
        "input_text": "summarize: def as_tab(record):        title = _clean(record.id)    seq = _get_seq_string(record)  # Catches sequence being None    assert \"\\t\" not in title    assert \"\\n\" not in title    assert \"\\r\" not in title    assert \"\\t\" not in seq    assert \"\\n\" not in seq    assert \"\\r\" not in seq    return f\"{title}\\t{seq}\\n\"",
        "labels_text": "Return record a tab separated idtabseq string"
    },
    {
        "input_text": "summarize: def __init__(self, stream, offset, length):                self.stream = stream        self.offset = offset        self.length = length        super().__init__()",
        "labels_text": "Initialize the file stream and file position of the sequence data"
    },
    {
        "input_text": "summarize: def __len__(self):                return self.length",
        "labels_text": "Get the sequence length"
    },
    {
        "input_text": "summarize: def upper(self):                data = _TwoBitSequenceData(self.stream, self.offset, self.length)        data.nBlocks = self.nBlocks[:, :]        data.maskBlocks = np.empty((0, 2), dtype=\"uint32\")        return data",
        "labels_text": "Remove the sequence mask"
    },
    {
        "input_text": "summarize: def lower(self):                data = _TwoBitSequenceData(self.stream, self.offset, self.length)        data.nBlocks = self.nBlocks[:, :]        data.maskBlocks = np.array([[0, self.length]], dtype=\"uint32\")        return data",
        "labels_text": "Extend the sequence mask to the full sequence"
    },
    {
        "input_text": "summarize: def __next__(self):                try:            name = next(self._names)            sequence = self.sequences[name]            return SeqRecord(sequence, id=name)        except Exception:            if self.should_close_stream:                self.stream.close()            raise",
        "labels_text": "Return the next entry"
    },
    {
        "input_text": "summarize: def __getitem__(self, name):                try:            sequence = self.sequences[name]        except ValueError:            raise KeyError(name) from None        return SeqRecord(sequence, id=name)",
        "labels_text": "Return sequence associated with given name a a SeqRecord object"
    },
    {
        "input_text": "summarize: def keys(self):                return self.sequences.keys()",
        "labels_text": "Return a list with the name of the sequence in the file"
    },
    {
        "input_text": "summarize: def __len__(self):                return len(self.sequences)",
        "labels_text": "Return number of sequence"
    },
    {
        "input_text": "summarize: def __init__(self, elem, alphabet=None, return_raw_comments=False):                if alphabet is not None:            raise ValueError(\"The alphabet argument is no longer supported\")        self.entry = elem        self.return_raw_comments = return_raw_comments",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _read(handle, length):        data = handle.read(length)    if len(data) < length:        raise ValueError(\"Cannot read %d bytes from handle\" % length)    return data",
        "labels_text": "Read the specified number of byte from the given handle"
    },
    {
        "input_text": "summarize: def _read_pstring(handle):        length = unpack(\">B\", _read(handle, 1))[0]    return unpack(\"%ds\" % length, _read(handle, length))[0].decode(\"ASCII\")",
        "labels_text": "Read a Pascal string A Pascal string comprises a single byte giving the length of the string followed by a many byte"
    },
    {
        "input_text": "summarize: def _read_overhang(handle):        length = _read_pstring_as_integer(handle)    if length != 0:        overhang = _read(handle, abs(length))        return (length, overhang)    else:        return (None, None)",
        "labels_text": "Read an overhang specification An overhang is represented in a XDNA file a a Pascal string containing the text representation of the overhang length which also indicates the nature of the overhang a length of zero mean no overhang a negative length mean a overhang a positive length mean a overhang the actual overhang sequence Examples x x no overhang a a Pstring x x x x AA overhang Pstring then AA x xD x x C overhang Pstring then C Returns a tuple length sequence"
    },
    {
        "input_text": "summarize: def __init__(self, source):                super().__init__(source, mode=\"b\", fmt=\"Xdna\")        header = self.stream.read(112)        if not header:            raise ValueError(\"Empty file.\")        if len(header) < 112:            raise ValueError(\"Improper header, cannot read 112 bytes from stream\")        self._header = header",
        "labels_text": "Parse a Xdna file and return a SeqRecord object Argument source is a filelike object in binary mode or a path to a file Note that this is an iterator in name only since an Xdna file always contain a single sequence"
    },
    {
        "input_text": "summarize: def __init__(self, target):                super().__init__(target, mode=\"wb\")",
        "labels_text": "Initialize an Xdna writer object Arguments target Output stream opened in binary mode or a path to a file"
    },
    {
        "input_text": "summarize: def _write_pstring(self, s):                if len(s) > 255:            self._has_truncated_strings = True            s = s[:255]        self.handle.write(pack(\">B\", len(s)))        self.handle.write(s.encode(\"ASCII\"))",
        "labels_text": "Write the given string a a Pascal string"
    },
    {
        "input_text": "summarize: def __init__(self, filename, format):                self._handle = _open_for_random_access(filename)        self._format = format        # Load the parser class/function once an avoid the dict lookup in each        # __getitem__ call:        self._iterator = SeqIO._FormatToIterator[format]",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def get(self, offset):                # Should be overridden for binary file formats etc:        return next(self._iterator(StringIO(self.get_raw(offset).decode())))",
        "labels_text": "Return SeqRecord"
    },
    {
        "input_text": "summarize: def get(self, offset):                handle = self._handle        handle.seek(offset)        self._offset = offset        self.trim = False        return SeqIO.SffIO.SffIterator._sff_read_seq_record(self, handle)",
        "labels_text": "Return the SeqRecord starting at the given offset"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                handle = self._handle        handle.seek(offset)        self.stream = handle        return SeqIO.SffIO._sff_read_raw_record(handle, self.number_of_flows_per_read)",
        "labels_text": "Return the raw record from the file a a byte string"
    },
    {
        "input_text": "summarize: def get(self, offset):                handle = self._handle        handle.seek(offset)        self._offset = offset        self.trim = True        return SeqIO.SffIO.SffIterator._sff_read_seq_record(self, handle)",
        "labels_text": "Return the SeqRecord starting at the given offset"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                # For non-trivial file formats this must be over-ridden in the subclass        handle = self._handle        marker_re = self._marker_re        handle.seek(offset)        lines = [handle.readline()]        while True:            line = handle.readline()            if marker_re.match(line) or not line:                # End of file, or start of next record => end of this record                break            lines.append(line)        return b\"\".join(lines)",
        "labels_text": "Return the raw record from the file a a byte string"
    },
    {
        "input_text": "summarize: def get(self, offset):                # TODO - Can we handle this directly in the parser?        # This is a hack - use get_raw for <entry>...</entry> and wrap it with        # the apparently required XML header and footer.        data = (            b            + self.get_raw(offset)            + b\"</uniprot>\"        )        return next(SeqIO.UniprotIO.UniprotIterator(BytesIO(data)))",
        "labels_text": "Return the SeqRecord starting at the given offset"
    },
    {
        "input_text": "summarize: def __init__(self, filename, format):                SeqFileRandomAccess.__init__(self, filename, format)        self._marker_re = re.compile(b\"^;\")",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                handle = self._handle        handle.seek(offset)        marker_re = self._marker_re        lines = []        line = handle.readline()        while line.startswith(b\";\"):            lines.append(line)            line = handle.readline()        while line and not line.startswith(b\";\"):            lines.append(line)            line = handle.readline()        return b\"\".join(lines)",
        "labels_text": "Return the raw record from the file a a byte string"
    },
    {
        "input_text": "summarize: def __iter__(self):                handle = self._handle        handle.seek(0)        tab_char = b\"\\t\"        while True:            start_offset = handle.tell()            line = handle.readline()            if not line:                break  # End of file            try:                key = line.split(tab_char)[0]            except ValueError:                if not line.strip():                    # Ignore blank lines                    continue                else:                    raise            else:                yield key.decode(), start_offset, len(line)",
        "labels_text": "Iterate over the sequence record in the file"
    },
    {
        "input_text": "summarize: def get_raw(self, offset):                handle = self._handle        handle.seek(offset)        return handle.readline()",
        "labels_text": "Return the raw record from the file a a byte string"
    },
    {
        "input_text": "summarize: def read(handle, format, alphabet=None):        iterator = parse(handle, format, alphabet)    try:        record = next(iterator)    except StopIteration:        raise ValueError(\"No records found in handle\") from None    try:        next(iterator)        raise ValueError(\"More than one record found in handle\")    except StopIteration:        pass    return record",
        "labels_text": "Turn a sequence file into a single SeqRecord Arguments handle handle to the file or the filename a a string note older version of Biopython only took a handle format string describing the file format alphabet no longer used should be None This function is for use parsing sequence file containing exactly one record For example reading a GenBank file from Bio import SeqIO record SeqIOreadGenBankarabgb genbank printID s recordid ID AC printSequence length i lenrecord Sequence length If the handle contains no record or more than one record an exception is raised For example from Bio import SeqIO record SeqIOreadGenBankcorgb genbank Traceback most recent call last ValueError More than one record found in handle If however you want the first record from a file containing multiple record this function would raise an exception a shown in the example above Instead use from Bio import SeqIO record nextSeqIOparseGenBankcorgb genbank printFirst record ID s recordid First record ID X Use the BioSeqIOparsehandle format function if you want to read multiple record from the handle"
    },
    {
        "input_text": "summarize: def to_dict(sequences, key_function=None):        # This is to avoid a lambda function:    def _default_key_function(rec):        return rec.id    if key_function is None:        key_function = _default_key_function    d = {}    for record in sequences:        key = key_function(record)        if key in d:            raise ValueError(f\"Duplicate key '{key}'\")        d[key] = record    return d",
        "labels_text": "Turn a sequence iterator or list into a dictionary Arguments sequence An iterator that return SeqRecord object or simply a list of SeqRecord object keyfunction Optional callback function which when given a SeqRecord should return a unique key for the dictionary eg keyfunction lambda rec recname or keyfunction lambda rec recdescriptionsplit If keyfunction is omitted then recordid is used on the assumption that the record object returned are SeqRecords with a unique id If there are duplicate key an error is raised Since Python the default dict class maintains key order meaning this dictionary will reflect the order of record given to it For CPython and PyPy this wa already implemented for Python so effectively you can always assume the record order is preserved Example usage defaulting to using the recordid a key from Bio import SeqIO filename GenBankcorgb format genbank iddict SeqIOtodictSeqIOparsefilename format printlistiddict X X M AJ L AF printiddictLdescription Brassica rapa clone bif kin mRNA complete cd A more complex example using the keyfunction argument in order to use a sequence checksum a the dictionary key from Bio import SeqIO from BioSeqUtilsCheckSum import seguid filename GenBankcorgb format genbank seguiddict SeqIOtodictSeqIOparsefilename format keyfunction lambda rec seguidrecseq for key record in sortedseguiddictitems print s key recordid wQvmrlQWcmllOefgVgg AJ BUgYxXSKWEcFFHLJzaLGhQs L SabZaAVeLEFmFnyYyJ X TtWsXoSZclIByXWJcCY M lgjJFEWSjJnASrUKWFA X uVEYeAQSVEDQOnFoeMmVeaOow AF This approach is not suitable for very large set of sequence a all the SeqRecord object are held in memory Instead consider using the BioSeqIOindex function if it support your particular file format This dictionary will reflect the order of record given to it"
    },
    {
        "input_text": "summarize: def __init__(self):                self.name = \"\"        self.padded_bases = None        self.info_items = None        self.read_tags = None        self.sequence = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.name = \"\"        self.coru = None        self.padded_start = None        if line:            header = line.split()            self.name = header[1]            self.coru = header[2]            self.padded_start = int(header[3])",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.name = \"\"        self.padded_start = None        self.padded_end = None        if line:            header = line.split()            self.padded_start = int(header[1])            self.padded_end = int(header[2])            self.name = header[3]",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.tag_type = \"\"        self.program = \"\"        self.date = \"\"        self.info = []        if line:            header = line.split()            self.tag_type = header[0]            self.program = header[1]            self.date = header[2]",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, line=None):                self.name = \"\"        self.aligned = \"\"        self.program = \"\"        self.date = []        if line:            header = line.split()            self.name = header[0]            self.aligned = header[1]            self.program = header[2]            self.date = header[3]",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.ncontigs = None        self.nreads = None        self.contigs = []        self.wa = None",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self):                self.file_name = \"\"        self.comments = {}        for kw in CKEYWORDS:            self.comments[kw.lower()] = None        self.sites = []        self.seq = \"\"        self.seq_trimmed = \"\"",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def read(source):        handle = _open(source)    try:        record = _read(handle)        try:            next(handle)        except StopIteration:            return record        else:            raise ValueError(\"More than one PHD record found\")    finally:        if handle is not source:            handle.close()",
        "labels_text": "Read one PHD record from the file and return it a a Record object Argument source is a filelike object opened in text mode or a path to a file This function read PHD file data line by line from the source and return a single Record object A ValueError is raised if more than one record is found in the file"
    },
    {
        "input_text": "summarize: def parse(source):        handle = _open(source)    try:        while True:            record = _read(handle)            if not record:                return            yield record    finally:        if handle is not source:            handle.close()",
        "labels_text": "Iterate over a file yielding multiple PHD record Argument source is a filelike object opened in text mode or a path to a file The data is read line by line from the source Typical usage record parsehandle for record in record do something with the record object"
    },
    {
        "input_text": "summarize: def __init__(self, cmd=\"samtools\", **kwargs):                self.program_name = cmd        self.parameters = [            _StaticArgument(\"index\"),            _Argument([\"input\", \"in_bam\", \"input_bam\"], \"BAM file to be indexed\"),        ]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, cmd=\"samtools\", **kwargs):                self.program_name = cmd        self.parameters = [            _StaticArgument(\"idxstats\"),            _Argument([\"input\", \"in_bam\", \"input_bam\"], \"BAM file to be indexed\"),        ]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, cmd=\"samtools\", **kwargs):                self.program_name = cmd        self.parameters = [            _StaticArgument(\"faidx\"),            _Argument(                [\"reference\", \"reference_fasta\", \"ref\"],                \"Reference FASTA to be indexed\",                filename=True,                is_required=True,            ),        ]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def crc32(seq):        try:        # Assume it's a Seq object        s = bytes(seq)    except TypeError:        # Assume it's a string        s = seq.encode()    return binascii.crc32(s)",
        "labels_text": "Return the crc checksum for a sequence string or Seq object Note that the case is important crcACGTACGTACGT crcacgtACGTacgt"
    },
    {
        "input_text": "summarize: def crc64(s):        crcl = 0    crch = 0    for c in s:        shr = (crch & 0xFF) << 24        temp1h = crch >> 8        temp1l = (crcl >> 8) | shr        idx = (crcl ^ ord(c)) & 0xFF        crch = temp1h ^ _table_h[idx]        crcl = temp1l    return f\"CRC-{crch:08X}{crcl:08X}\"",
        "labels_text": "Return the crc checksum for a sequence string or Seq object Note that the case is important crcACGTACGTACGT CRCCFBBCAEBD crcacgtACGTacgt CRCDADCAEBD"
    },
    {
        "input_text": "summarize: def gcg(seq):        index = checksum = 0    for char in seq:        index += 1        checksum += index * ord(char.upper())        if index == 57:            index = 0    return checksum % 10000",
        "labels_text": "Return the GCG checksum int for a sequence string or Seq object Given a nucleotide or aminoacid sequence or any string return the GCG checksum int Checksum used by GCG program seq type str Based on BioPerl GCGchecksum Adapted by Sebastian Bassi with the help of John Lenton Pablo Ziliani and Gabriel Genellina All sequence are converted to uppercase gcgACGTACGTACGT gcgacgtACGTacgt"
    },
    {
        "input_text": "summarize: def seguid(seq):        import base64    import hashlib    m = hashlib.sha1()    try:        # Assume it's a Seq object        seq = bytes(seq)    except TypeError:        # Assume it's a string        seq = seq.encode()    m.update(seq.upper())    tmp = base64.encodebytes(m.digest())    return tmp.decode().replace(\"\\n\", \"\").rstrip(\"=\")",
        "labels_text": "Return the SEGUID string for a sequence string or Seq object Given a nucleotide or aminoacid sequence or any string return the SEGUID string A SEquence Globally Unique IDentifier seq type str Note that the case is not important seguidACGTACGTACGT IfHIvcnRSQDVNiAoefAzySci seguidacgtACGTacgt IfHIvcnRSQDVNiAoefAzySci For more information about SEGUID see httpbioinformaticsanlgovseguid httpsdoiorgpmic"
    },
    {
        "input_text": "summarize: def __init__(self, protein_sequence, aa_content=None):                self.sequence = protein_sequence.upper()        if not aa_content:            from Bio.SeqUtils.ProtParam import ProteinAnalysis as _PA            aa_content = _PA(self.sequence).count_amino_acids()        self.charged_aas_content = self._select_charged(aa_content)        self.pos_pKs, self.neg_pKs = self._update_pKs_tables()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def pi(self, pH=7.775, min_=4.05, max_=12):        r        charge = self.charge_at_pH(pH)        if max_ - min_ > 0.0001:            if charge > 0.0:                min_ = pH            else:                max_ = pH            next_pH = (min_ + max_) / 2            return self.pi(next_pH, min_, max_)        return pH",
        "labels_text": "Calculate and return the isoelectric point a float This is a recursive function that us bisection method Wiki on bisection httpsenwikipediaorgwikiBisectionmethod Arguments pH the pH at which the current charge of the protein is computed This pH lie at the centre of the interval mean of min and max min the minimum of the interval Initial value default to which is below the theoretical minimum when the protein is composed exclusively of aspartate max the maximum of the the interval Initial value default to which is above the theoretical maximum when the protein is composed exclusively of arginine"
    },
    {
        "input_text": "summarize: def _key_error(neighbors, strict):        # We haven't found the key in the tables    if strict:        raise ValueError(f\"no thermodynamic data for neighbors {neighbors!r} available\")    else:        warnings.warn(            \"no themodynamic data for neighbors %r available. \"            \"Calculation will be wrong\" % neighbors,            BiopythonWarning,        )",
        "labels_text": "Throw an error or a warning if there is no data for the neighbor PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, prot_sequence, monoisotopic=False):                self.sequence = prot_sequence.upper()        self.amino_acids_content = None        self.length = len(self.sequence)        self.monoisotopic = monoisotopic",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def count_amino_acids(self):                if self.amino_acids_content is None:            prot_dic = {k: 0 for k in IUPACData.protein_letters}            for aa in prot_dic:                prot_dic[aa] = self.sequence.count(aa)            self.amino_acids_content = prot_dic        return self.amino_acids_content",
        "labels_text": "Count standard amino acid return a dict Counts the number time each amino acid is in the protein sequence Returns a dictionary AminoAcidNumber The return value is cached in selfaminoacidscontent It is not recalculated upon subsequent call"
    },
    {
        "input_text": "summarize: def get_amino_acids_percent(self):                warnings.warn(            \"The get_amino_acids_percent method has been deprecated \"            \"and will likely be removed from Biopython in the near \"            \"future. Please use the amino_acids_percent attribute instead.\",            BiopythonDeprecationWarning,        )        return {aa: percent / 100 for aa, percent in self.amino_acids_percent.items()}",
        "labels_text": "Included for backwards compatibility DEPRECATED"
    },
    {
        "input_text": "summarize: def amino_acids_percent(self):                aa_counts = self.count_amino_acids()        percentages = {            aa: (count * 100 / self.length) for aa, count in aa_counts.items()        }        return percentages",
        "labels_text": "Get the amino acid content in percentage The same a countaminoacids only return the Number in percentage of entire sequence Returns a dictionary of AminoAcidpercentage Unlike the deprecated getaminoacidspercent method this attribute return percentage in the range"
    },
    {
        "input_text": "summarize: def molecular_weight(self):                return molecular_weight(            self.sequence, seq_type=\"protein\", monoisotopic=self.monoisotopic        )",
        "labels_text": "Calculate MW from Protein sequence"
    },
    {
        "input_text": "summarize: def aromaticity(self):                aromatic_aas = \"YWF\"        aa_percentages = self.amino_acids_percent        aromaticity = sum(aa_percentages[aa] / 100 for aa in aromatic_aas)        return aromaticity",
        "labels_text": "Calculate the aromaticity according to Lobry Calculates the aromaticity value of a protein according to Lobry It is simply the relative frequency of PheTrpTyr"
    },
    {
        "input_text": "summarize: def instability_index(self):                index = ProtParamData.DIWV        score = 0.0        for i in range(self.length - 1):            this, next = self.sequence[i : i + 2]            dipeptide_value = index[this][next]            score += dipeptide_value        return (10.0 / self.length) * score",
        "labels_text": "Calculate the instability index according to Guruprasad et al Implementation of the method of Guruprasad et al to test a protein for stability Any value above mean the protein is unstable ha a short half life See Guruprasad K Reddy BVB Pandit MW Protein Engineering"
    },
    {
        "input_text": "summarize: def gravy(self, scale=\"KyteDoolitle\"):                selected_scale = ProtParamData.gravy_scales.get(scale, -1)        if selected_scale == -1:            raise ValueError(f\"scale: {scale} not known\")        total_gravy = sum(selected_scale[aa] for aa in self.sequence)        return total_gravy / self.length",
        "labels_text": "Calculate the GRAVY Grand Average of Hydropathy according to Kyte and Doolitle Utilizes the given Hydrophobicity scale by default us the original proposed by Kyte and Doolittle KyteDoolitle Other option are Aboderin AbrahamLeo Argos BlackMould BullBreese Casari Cid Cowan Cowan Eisenberg Engelman Fasman Fauchere GoldSack Guy Jones Juretic Kidera Miyazawa ParkerPonnuswamy Rose Roseman Sweet Tanford Wilson and Zimmerman New scale can be added in ProtParamData"
    },
    {
        "input_text": "summarize: def _weight_list(self, window, edge):                unit = 2 * (1.0 - edge) / (window - 1)        weights = [0.0] * (window // 2)        for i in range(window // 2):            weights[i] = edge + unit * i        return weights",
        "labels_text": "Make list of relative weight of window edge PRIVATE The relative weight of window edge are compared to the window center The weight are linear It actually generates half a list For a window of size and edge you get a list of"
    },
    {
        "input_text": "summarize: def isoelectric_point(self):                aa_content = self.count_amino_acids()        ie_point = IsoelectricPoint.IsoelectricPoint(self.sequence, aa_content)        return ie_point.pi()",
        "labels_text": "Calculate the isoelectric point Uses the module IsoelectricPoint to calculate the pI of a protein"
    },
    {
        "input_text": "summarize: def charge_at_pH(self, pH):                aa_content = self.count_amino_acids()        charge = IsoelectricPoint.IsoelectricPoint(self.sequence, aa_content)        return charge.charge_at_pH(pH)",
        "labels_text": "Calculate the charge of a protein at given pH"
    },
    {
        "input_text": "summarize: def secondary_structure_fraction(self):                aa_percentages = self.amino_acids_percent        helix = sum(aa_percentages[r] / 100 for r in \"EMALK\")        turn = sum(aa_percentages[r] / 100 for r in \"NPGSD\")        sheet = sum(aa_percentages[r] / 100 for r in \"VIYFWLT\")        return helix, turn, sheet",
        "labels_text": "Calculate fraction of helix turn and sheet Returns a list of the fraction of amino acid which tend to be in Helix Turn or Sheet according to Haimov and Srebnik Hutchinson and Thornton and Kim and Berg respectively Amino acid in helix E M A L K Amino acid in turn N P G S D Amino acid in sheet V I Y F W L T Note that prior to v this method wrongly returned Sheet Turn Helix while claiming to return Helix Turn Sheet Returns a tuple of three float Helix Turn Sheet"
    },
    {
        "input_text": "summarize: def molar_extinction_coefficient(self):                num_aa = self.count_amino_acids()        mec_reduced = num_aa[\"W\"] * 5500 + num_aa[\"Y\"] * 1490        mec_cystines = mec_reduced + (num_aa[\"C\"] // 2) * 125        return (mec_reduced, mec_cystines)",
        "labels_text": "Calculate the molar extinction coefficient Calculates the molar extinction coefficient assuming cysteine reduced and cystine residue CysCysbond"
    },
    {
        "input_text": "summarize: def __init__(self):                self._clear()",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def _rms(self, coords1, coords2):                diff = coords1 - coords2        return sqrt(sum(sum(diff * diff)) / coords1.shape[0])",
        "labels_text": "Return rms deviation between coords and coords PRIVATE"
    },
    {
        "input_text": "summarize: def set(self, reference_coords, coords):                # clear everything from previous runs        self._clear()        # store coordinates        self.reference_coords = reference_coords        self.coords = coords        n = reference_coords.shape        m = coords.shape        if n != m or not (n[1] == m[1] == 3):            raise Exception(\"Coordinate number/dimension mismatch.\")        self.n = n[0]",
        "labels_text": "Set the coordinate to be superimposed coords will be put on top of referencecoords referencecoords an NxDIM array coords an NxDIM array DIM is the dimension of the point N is the number of point to be superimposed"
    },
    {
        "input_text": "summarize: def get_transformed(self):                if self.coords is None or self.reference_coords is None:            raise Exception(\"No coordinates set.\")        if self.rot is None:            raise Exception(\"Nothing superimposed yet.\")        if self.transformed_coords is None:            self.transformed_coords = dot(self.coords, self.rot) + self.tran        return self.transformed_coords",
        "labels_text": "Get the transformed coordinate set"
    },
    {
        "input_text": "summarize: def get_rotran(self):                if self.rot is None:            raise Exception(\"Nothing superimposed yet.\")        return self.rot, self.tran",
        "labels_text": "Right multiplying rotation matrix and translation"
    },
    {
        "input_text": "summarize: def get_init_rms(self):                if self.coords is None:            raise Exception(\"No coordinates set yet.\")        if self.init_rms is None:            self.init_rms = self._rms(self.coords, self.reference_coords)        return self.init_rms",
        "labels_text": "Root mean square deviation of untransformed coordinate"
    },
    {
        "input_text": "summarize: def get_rms(self):                if self.rms is None:            transformed_coords = self.get_transformed()            self.rms = self._rms(transformed_coords, self.reference_coords)        return self.rms",
        "labels_text": "Root mean square deviation of superimposed coordinate"
    },
    {
        "input_text": "summarize: def __init__(self):                dict.__init__(self)        for keyword in (\"DE\", \"SY\", \"GO\", \"HI\", \"WW\"):            self[keyword] = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __init__(self, *args, line=None):                super().__init__(*args)        self.line = line",
        "labels_text": "Create a SwissProtParserError object with the offending line"
    },
    {
        "input_text": "summarize: def __init__(self):                self.number = None        self.positions = []        self.comments = []        self.references = []        self.authors = []        self.title = []        self.location = []",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def parse(source):        handle = _open(source)    try:        while True:            record = _read(handle)            if not record:                return            yield record    finally:        if handle is not source:            handle.close()",
        "labels_text": "Read multiple SwissProt record from file Argument source is a filelike object or a path to a file Returns a generator object which yield BioSwissProtRecord object"
    },
    {
        "input_text": "summarize: def read(source):        handle = _open(source)    try:        record = _read(handle)        if not record:            raise ValueError(\"No SwissProt record found\")        # We should have reached the end of the record by now.        # Try to read one more line to be sure:        try:            next(handle)        except StopIteration:            return record        raise ValueError(\"More than one SwissProt record found\")    finally:        if handle is not source:            handle.close()",
        "labels_text": "Read one SwissProt record from file Argument source is a filelike object or a path to a file Returns a Record object"
    },
    {
        "input_text": "summarize: def _get_fields(url):        handle = _open(url)    fields = handle.read().strip().split()    handle.close()    return fields",
        "labels_text": "Query a TogoWS URL for a plain text list of value PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, text=None):                self.acc = \"\"        self.nid = \"\"        self.lid = \"\"        self.pid = \"\"        self.clone = \"\"        self.image = \"\"        self.is_image = False        self.end = \"\"        self.mgc = \"\"        self.seqtype = \"\"        self.trace = \"\"        if text is not None:            self.text = text            self._init_from_text(text)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return self.text",
        "labels_text": "Return UniGene SequenceLine object a a string"
    },
    {
        "input_text": "summarize: def __init__(self, text=None):                self.org = \"\"        self.protgi = \"\"        self.protid = \"\"        self.pct = \"\"        self.aln = \"\"        if text is not None:            self.text = text            self._init_from_text(text)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return self.text",
        "labels_text": "Return UniGene ProtsimLine object a a string"
    },
    {
        "input_text": "summarize: def __init__(self, text=None):                self.acc = \"\"        self.unists = \"\"        if text is not None:            self.text = text            self._init_from_text(text)",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __repr__(self):                return self.text",
        "labels_text": "Return UniGene STSLine object a a string"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"<{self.__class__.__name__}> {self.ID} {self.symbol} {self.title}\"",
        "labels_text": "Represent the UniGene Record object a a string for debugging"
    },
    {
        "input_text": "summarize: def parse(handle):        while True:        record = _read(handle)        if not record:            return        yield record",
        "labels_text": "Read and load a UniGene record for file containing multiple record"
    },
    {
        "input_text": "summarize: def read(handle):        record = _read(handle)    if not record:        raise ValueError(\"No SwissProt record found\")    # We should have reached the end of the record by now    remainder = handle.read()    if remainder:        raise ValueError(\"More than one SwissProt record found\")    return record",
        "labels_text": "Read and load a UniGene record one record per file"
    },
    {
        "input_text": "summarize: def _gpi10iterator(handle):        for inline in handle:        if inline[0] == \"!\":            continue        inrec = inline.rstrip(\"\\n\").split(\"\\t\")        if len(inrec) == 1:            continue        inrec[5] = inrec[5].split(\"|\")  # DB_Object_Synonym(s)        inrec[8] = inrec[8].split(\"|\")  # Annotation_Target_Set        yield dict(zip(GPI10FIELDS, inrec))",
        "labels_text": "Read GPI format file PRIVATE This iterator is used to read a gpinformationgoauniprot file which is in the GPI format"
    },
    {
        "input_text": "summarize: def gpa_iterator(handle):        inline = handle.readline()    if inline.strip() == \"!gpa-version: 1.1\":        # sys.stderr.write(\"gpa 1.1\\n\")        return _gpa11iterator(handle)    elif inline.strip() == \"!gpa-version: 1.0\":        # sys.stderr.write(\"gpa 1.0\\n\")        return _gpa10iterator(handle)    else:        raise ValueError(f\"Unknown GPA version {inline}\\n\")",
        "labels_text": "Read GPA format file This function should be called to read a geneassociationgoauniprot file Reads the first record and return a gpa or a gpa iterator a needed"
    },
    {
        "input_text": "summarize: def writerec(outrec, handle, fields=GAF20FIELDS):        outstr = \"\"    for field in fields[:-1]:        if isinstance(outrec[field], list):            for subfield in outrec[field]:                outstr += subfield + \"|\"            outstr = outstr[:-1] + \"\\t\"        else:            outstr += outrec[field] + \"\\t\"    outstr += outrec[fields[-1]] + \"\\n\"    handle.write(outstr)",
        "labels_text": "Write a single UniProtGOA record to an output stream Caller should know the format version Default gaf If header ha a value then it is assumed this is the first record a header is written"
    },
    {
        "input_text": "summarize: def writebyproteinrec(outprotrec, handle, fields=GAF20FIELDS):        for outrec in outprotrec:        writerec(outrec, handle, fields=fields)",
        "labels_text": "Write a list of GAF record to an output stream Caller should know the format version Default gaf If header ha a value then it is assumed this is the first record a header is written Typically the list is the one read by fafbyproteinrec which contains all consecutive line with the same DBObjectID"
    },
    {
        "input_text": "summarize: def record_has(inrec, fieldvals):        retval = False    for field in fieldvals:        if isinstance(inrec[field], str):            set1 = {inrec[field]}        else:            set1 = set(inrec[field])        if set1 & fieldvals[field]:            retval = True            break    return retval",
        "labels_text": "Accept a record and a dictionary of field value The format is fieldname setval val If any field in the record ha a matching value the function return True Otherwise return False"
    },
    {
        "input_text": "summarize: def __len__(self) -> int:                return self.search_result_count",
        "labels_text": "Return the total number of search result regardless of the batch size"
    },
    {
        "input_text": "summarize: def _fetch_for(self, index: int) -> None:                assert index in range(len(self))        while index >= len(self.results_cache):            self._fetch_next_batch()",
        "labels_text": "Fetch batch until the given index is in the cache"
    },
    {
        "input_text": "summarize: def search(    query: str, fields: Optional[list[str]] = None, batch_size: int = 500) -> _UniProtSearchResults:        parameters = {        \"query\": query,        \"size\": batch_size,        \"format\": \"json\",    }    if fields:        parameters[\"fields\"] = \",\".join(fields)    url = f\"https://rest.uniprot.org/uniprotkb/search?{urllib.parse.urlencode(parameters)}\"    return _UniProtSearchResults(url)",
        "labels_text": "Search the UniProt database Consider using query syntax httpswwwuniprotorghelptextsearch and query field httpswwwuniprotorghelpqueryfields to refine your search See the API detail here httpswwwuniprotorghelpapiqueries from Bio import UniProt from itertools import islice Get the first result result UniProtsearchorganismid AND reviewedtrue param query The query string to search UniProt with type query str param field The column to retrieve in the result default to all field type field Liststr optional param batchsize The number of result to retrieve in each batch default to type batchsize int return An iterator over the search result rtype UniProtSearchResults"
    },
    {
        "input_text": "summarize: def __init__(self, primary_id, adaptor, start=0, length=0):                self.primary_id = primary_id        self.adaptor = adaptor        self._length = length        self.start = start        super().__init__()",
        "labels_text": "Create a new BioSQLSequenceData object referring to a BioSQL entry You wouldnt normally create a BioSQLSequenceData object yourself this is done for you when retrieving a DBSeqRecord object from the database which creates a Seq object using a BioSQLSequenceData instance a the data provider"
    },
    {
        "input_text": "summarize: def __len__(self):                return self._length",
        "labels_text": "Return the length of the sequence"
    },
    {
        "input_text": "summarize: def dbxrefs(self) -> list[str]:                if not hasattr(self, \"_dbxrefs\"):            self._dbxrefs = _retrieve_dbxrefs(self._adaptor, self._primary_id)        return self._dbxrefs",
        "labels_text": "Database cross reference"
    },
    {
        "input_text": "summarize: def annotations(self) -> SeqRecord._AnnotationsDict:                if not hasattr(self, \"_annotations\"):            self._annotations = _retrieve_annotations(                self._adaptor, self._primary_id, self._taxon_id            )            if self._identifier:                self._annotations[\"gi\"] = self._identifier            if self._division:                self._annotations[\"data_file_division\"] = self._division        return self._annotations",
        "labels_text": "Annotations"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"{self.__class__.__name__}({self.adaptor.conn!r})\"",
        "labels_text": "Return a short description of the class name and database connection"
    },
    {
        "input_text": "summarize: def __getitem__(self, name):                return BioSeqDatabase(self.adaptor, name)",
        "labels_text": "Return a BioSeqDatabase object Arguments name The name of the BioSeqDatabase"
    },
    {
        "input_text": "summarize: def __len__(self):                sql = \"SELECT COUNT(name) FROM biodatabase;\"        return int(self.adaptor.execute_and_fetch_col0(sql)[0])",
        "labels_text": "Return number of namespaces subdatabases in this database"
    },
    {
        "input_text": "summarize: def __contains__(self, value):                sql = \"SELECT COUNT(name) FROM biodatabase WHERE name=%s;\"        return bool(self.adaptor.execute_and_fetch_col0(sql, (value,))[0])",
        "labels_text": "Check if a namespace subdatabase in this database"
    },
    {
        "input_text": "summarize: def __iter__(self):                # TODO - Iterate over the cursor, much more efficient        return iter(self.adaptor.list_biodatabase_names())",
        "labels_text": "Iterate over namespaces subdatabases in the database"
    },
    {
        "input_text": "summarize: def keys(self):                return iter(self)",
        "labels_text": "Iterate over namespaces subdatabases in the database"
    },
    {
        "input_text": "summarize: def values(self):                for key in self:            yield self[key]",
        "labels_text": "Iterate over BioSeqDatabase object in the database"
    },
    {
        "input_text": "summarize: def items(self):                for key in self:            yield key, self[key]",
        "labels_text": "Iterate over namespace BioSeqDatabase in the database"
    },
    {
        "input_text": "summarize: def __delitem__(self, name):                if name not in self:            raise KeyError(name)        db_id = self.adaptor.fetch_dbid_by_dbname(name)        remover = Loader.DatabaseRemover(self.adaptor, db_id)        remover.remove()",
        "labels_text": "Remove a namespace and all it entry"
    },
    {
        "input_text": "summarize: def new_database(self, db_name, authority=None, description=None):                # make the database        sql = (            \"INSERT INTO biodatabase (name, authority, description)\"            \" VALUES (%s, %s, %s)\"        )        self.adaptor.execute(sql, (db_name, authority, description))        return BioSeqDatabase(self.adaptor, db_name)",
        "labels_text": "Add a new database to the server and return it"
    },
    {
        "input_text": "summarize: def commit(self):                return self.adaptor.commit()",
        "labels_text": "Commit the current transaction to the database"
    },
    {
        "input_text": "summarize: def rollback(self):                return self.adaptor.rollback()",
        "labels_text": "Rollback the current transaction"
    },
    {
        "input_text": "summarize: def close(self):                return self.adaptor.close()",
        "labels_text": "Close the connection No further activity possible"
    },
    {
        "input_text": "summarize: def execute(self, operation, params=None, multi=False):                self.real_cursor.execute(operation, params, multi)",
        "labels_text": "Execute a sql statement"
    },
    {
        "input_text": "summarize: def executemany(self, operation, params):                self.real_cursor.executemany(operation, params)",
        "labels_text": "Execute many sql statement"
    },
    {
        "input_text": "summarize: def _convert_tuple(self, tuple_):                tuple_list = list(tuple_)        for i, elem in enumerate(tuple_list):            if isinstance(elem, bytes):                tuple_list[i] = elem.decode(\"utf-8\")        return tuple(tuple_list)",
        "labels_text": "Decode any bytestrings present in the row PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, conn, dbutils, wrap_cursor=False):                self.conn = conn        if wrap_cursor:            self.cursor = _CursorWrapper(conn.cursor())        else:            self.cursor = conn.cursor()        self.dbutils = dbutils",
        "labels_text": "Create an Adaptor object Arguments conn A database connection dbutils A BioSQLDBUtils object wrapcursor Optional whether to wrap the cursor object"
    },
    {
        "input_text": "summarize: def last_id(self, table):                return self.dbutils.last_id(self.cursor, table)",
        "labels_text": "Return the last row id for the selected table"
    },
    {
        "input_text": "summarize: def autocommit(self, y=True):                return self.dbutils.autocommit(self.conn, y)",
        "labels_text": "Set the autocommit mode True value enable False value disable"
    },
    {
        "input_text": "summarize: def commit(self):                return self.conn.commit()",
        "labels_text": "Commit the current transaction"
    },
    {
        "input_text": "summarize: def rollback(self):                return self.conn.rollback()",
        "labels_text": "Rollback the current transaction"
    },
    {
        "input_text": "summarize: def close(self):                return self.conn.close()",
        "labels_text": "Close the connection No further activity possible"
    },
    {
        "input_text": "summarize: def fetch_dbid_by_dbname(self, dbname):                self.execute(            \"select biodatabase_id from biodatabase where name = %s\", (dbname,)        )        rv = self.cursor.fetchall()        if not rv:            raise KeyError(f\"Cannot find biodatabase with name {dbname!r}\")        return rv[0][0]",
        "labels_text": "Return the internal id for the subdatabase using it name"
    },
    {
        "input_text": "summarize: def fetch_seqids_by_accession(self, dbid, name):                sql = \"select bioentry_id from bioentry where accession = %s\"        fields = [name]        if dbid:            sql += \" and biodatabase_id = %s\"            fields.append(dbid)        return self.execute_and_fetch_col0(sql, fields)",
        "labels_text": "Return a list internal id using an accession Arguments dbid the internal id for the subdatabase name the accession of the sequence Corresponds to the accession column of the bioentry table of the SQL schema"
    },
    {
        "input_text": "summarize: def list_biodatabase_names(self):                return self.execute_and_fetch_col0(\"SELECT name FROM biodatabase\")",
        "labels_text": "Return a list of all of the subdatabases"
    },
    {
        "input_text": "summarize: def list_bioentry_ids(self, dbid):                return self.execute_and_fetch_col0(            \"SELECT bioentry_id FROM bioentry WHERE biodatabase_id = %s\", (dbid,)        )",
        "labels_text": "Return a list of internal id for all of the sequence in a subdatabae Arguments dbid The internal id for a subdatabase"
    },
    {
        "input_text": "summarize: def list_bioentry_display_ids(self, dbid):                return self.execute_and_fetch_col0(            \"SELECT name FROM bioentry WHERE biodatabase_id = %s\", (dbid,)        )",
        "labels_text": "Return a list of all sequence name in a subdatabae Arguments dbid The internal id for a subdatabase"
    },
    {
        "input_text": "summarize: def list_any_ids(self, sql, args):                return self.execute_and_fetch_col0(sql, args)",
        "labels_text": "Return id given a SQL statement to select for them This assumes that the given SQL doe a SELECT statement that return a list of item This parses them out of the D list they come a and just return them in a list"
    },
    {
        "input_text": "summarize: def execute_one(self, sql, args=None):                self.execute(sql, args or ())        rv = self.cursor.fetchall()        if len(rv) != 1:            raise ValueError(f\"Expected 1 response, got {len(rv)}.\")        return rv[0]",
        "labels_text": "Execute sql that return record and return the record"
    },
    {
        "input_text": "summarize: def execute(self, sql, args=None):                if os.name == \"java\":            sql = sql.replace(\"%s\", \"?\")        self.dbutils.execute(self.cursor, sql, args)",
        "labels_text": "Just execute an sql command"
    },
    {
        "input_text": "summarize: def executemany(self, sql, args):                if os.name == \"java\":            sql = sql.replace(\"%s\", \"?\")        self.dbutils.executemany(self.cursor, sql, args)",
        "labels_text": "Execute many sql command"
    },
    {
        "input_text": "summarize: def execute_and_fetch_col0(self, sql, args=None):                self.execute(sql, args or ())        return [field[0] for field in self.cursor.fetchall()]",
        "labels_text": "Return a list of value from the first column in the row"
    },
    {
        "input_text": "summarize: def execute_and_fetchall(self, sql, args=None):                self.execute(sql, args or ())        return self.cursor.fetchall()",
        "labels_text": "Return a list of tuples of all row"
    },
    {
        "input_text": "summarize: def _bytearray_to_str(s):                if isinstance(s, (bytes, bytearray)):            return s.decode()        return s",
        "labels_text": "If s is byte or bytearray convert to a string PRIVATE"
    },
    {
        "input_text": "summarize: def execute_one(self, sql, args=None):                out = super().execute_one(sql, args)        return tuple(self._bytearray_to_str(v) for v in out)",
        "labels_text": "Execute sql that return record and return the record"
    },
    {
        "input_text": "summarize: def execute_and_fetch_col0(self, sql, args=None):                out = super().execute_and_fetch_col0(sql, args)        return [self._bytearray_to_str(column) for column in out]",
        "labels_text": "Return a list of value from the first column in the row"
    },
    {
        "input_text": "summarize: def execute_and_fetchall(self, sql, args=None):                out = super().execute_and_fetchall(sql, args)        return [tuple(self._bytearray_to_str(v) for v in o) for o in out]",
        "labels_text": "Return a list of tuples of all row"
    },
    {
        "input_text": "summarize: def __init__(self, adaptor, name):                self.adaptor = adaptor        self.name = name        self.dbid = self.adaptor.fetch_dbid_by_dbname(name)",
        "labels_text": "Create a BioDatabase object Arguments adaptor A BioSQLAdaptor object name The name of the subdatabase namespace"
    },
    {
        "input_text": "summarize: def __repr__(self):                return f\"BioSeqDatabase({self.adaptor!r}, {self.name!r})\"",
        "labels_text": "Return a short summary of the BioSeqDatabase"
    },
    {
        "input_text": "summarize: def get_Seq_by_id(self, name):                seqid = self.adaptor.fetch_seqid_by_display_id(self.dbid, name)        return BioSeq.DBSeqRecord(self.adaptor, seqid)",
        "labels_text": "Get a DBSeqRecord object by it name Example seqrec dbgetSeqbyidROAHUMAN The name of this method is misleading since it return a DBSeqRecord rather than a Seq object and presumably wa to mirror BioPerl"
    },
    {
        "input_text": "summarize: def get_Seq_by_acc(self, name):                seqid = self.adaptor.fetch_seqid_by_accession(self.dbid, name)        return BioSeq.DBSeqRecord(self.adaptor, seqid)",
        "labels_text": "Get a DBSeqRecord object by accession number Example seqrec dbgetSeqbyaccX The name of this method is misleading since it return a DBSeqRecord rather than a Seq object and presumably wa to mirror BioPerl"
    },
    {
        "input_text": "summarize: def get_Seq_by_ver(self, name):                seqid = self.adaptor.fetch_seqid_by_version(self.dbid, name)        return BioSeq.DBSeqRecord(self.adaptor, seqid)",
        "labels_text": "Get a DBSeqRecord object by version number Example seqrec dbgetSeqbyverX The name of this method is misleading since it return a DBSeqRecord rather than a Seq object and presumably wa to mirror BioPerl"
    },
    {
        "input_text": "summarize: def get_Seqs_by_acc(self, name):                seqids = self.adaptor.fetch_seqids_by_accession(self.dbid, name)        return [BioSeq.DBSeqRecord(self.adaptor, seqid) for seqid in seqids]",
        "labels_text": "Get a list of DBSeqRecord object by accession number Example seqrecs dbgetSeqbyaccX The name of this method is misleading since it return a list of DBSeqRecord object rather than a list of Seq object and presumably wa to mirror BioPerl"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                record = BioSeq.DBSeqRecord(self.adaptor, key)        if record._biodatabase_id != self.dbid:            raise KeyError(f\"Entry {key!r} does exist, but not in current name space\")        return record",
        "labels_text": "Return a DBSeqRecord for one of the sequence in the subdatabase Arguments key The internal id for the sequence"
    },
    {
        "input_text": "summarize: def __delitem__(self, key):                if key not in self:            raise KeyError(                f\"Entry {key!r} cannot be deleted. It was not found or is invalid\"            )        # Assuming this will automatically cascade to the other tables...        sql = \"DELETE FROM bioentry WHERE biodatabase_id=%s AND bioentry_id=%s;\"        self.adaptor.execute(sql, (self.dbid, key))",
        "labels_text": "Remove an entry and all it annotation"
    },
    {
        "input_text": "summarize: def __len__(self):                sql = \"SELECT COUNT(bioentry_id) FROM bioentry WHERE biodatabase_id=%s;\"        return int(self.adaptor.execute_and_fetch_col0(sql, (self.dbid,))[0])",
        "labels_text": "Return number of record in this namespace sub database"
    },
    {
        "input_text": "summarize: def __iter__(self):                # TODO - Iterate over the cursor, much more efficient        return iter(self.adaptor.list_bioentry_ids(self.dbid))",
        "labels_text": "Iterate over id which may not be meaningful outside this database"
    },
    {
        "input_text": "summarize: def keys(self):                return iter(self)",
        "labels_text": "Iterate over id which may not be meaningful outside this database"
    },
    {
        "input_text": "summarize: def values(self):                for key in self:            yield self[key]",
        "labels_text": "Iterate over DBSeqRecord object in the namespace sub database"
    },
    {
        "input_text": "summarize: def items(self):                for key in self:            yield key, self[key]",
        "labels_text": "Iterate over id DBSeqRecord for the namespace sub database"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Create a Genericdbutils object"
    },
    {
        "input_text": "summarize: def tname(self, table):                if table != \"biosequence\":            return table        else:            return \"bioentry\"",
        "labels_text": "Return the name of the table"
    },
    {
        "input_text": "summarize: def last_id(self, cursor, table):                # XXX: Unsafe without transactions isolation        table = self.tname(table)        sql = f\"select max({table}_id) from {table}\"        cursor.execute(sql)        rv = cursor.fetchone()        return rv[0]",
        "labels_text": "Return the last used id for a table"
    },
    {
        "input_text": "summarize: def execute(self, cursor, sql, args=None):                cursor.execute(sql, args or ())",
        "labels_text": "Just execute an sql command"
    },
    {
        "input_text": "summarize: def executemany(self, cursor, sql, seq):                cursor.executemany(sql, seq)",
        "labels_text": "Execute many sql command"
    },
    {
        "input_text": "summarize: def autocommit(self, conn, y=1):",
        "labels_text": "Set autocommit on the database connection"
    },
    {
        "input_text": "summarize: def _sub_placeholder(self, sql):                return sql.replace(\"%s\", \"?\")",
        "labels_text": "Format the argument placeholder for sqlite PRIVATE"
    },
    {
        "input_text": "summarize: def execute(self, cursor, sql, args=None):                sql = self._sub_placeholder(sql)        cursor.execute(sql, args or ())",
        "labels_text": "Execute SQL command Replaces s with for variable substitution in sqlite"
    },
    {
        "input_text": "summarize: def executemany(self, cursor, sql, seq):                sql = self._sub_placeholder(sql)        cursor.executemany(sql, seq)",
        "labels_text": "Execute many sql statement"
    },
    {
        "input_text": "summarize: def last_id(self, cursor, table):                if os.name == \"java\":            return Generic_dbutils.last_id(self, cursor, table)        try:            # This worked on older versions of MySQL            return cursor.insert_id()        except AttributeError:            # See bug 2390            # Google suggests this is the new way,            # same fix also suggested by Eric Gibert:            return cursor.lastrowid",
        "labels_text": "Return the last used id for a table"
    },
    {
        "input_text": "summarize: def autocommit(self, conn, y=True):                if y:            if os.name == \"java\":                conn.autocommit = 1            else:                conn.set_isolation_level(0)        else:            if os.name == \"java\":                conn.autocommit = 0            else:                conn.set_isolation_level(1)",
        "labels_text": "Set autocommit on the database connection"
    },
    {
        "input_text": "summarize: def autocommit(self, conn, y=True):                raise NotImplementedError(\"pgdb does not support this!\")",
        "labels_text": "Set autocommit on the database connection Currently not implemented"
    },
    {
        "input_text": "summarize: def get_dbutils(module_name):        try:        return _dbutils[module_name]()    except KeyError:        return Generic_dbutils()",
        "labels_text": "Return the correct dbutils object for the database driver"
    },
    {
        "input_text": "summarize: def __init__(self, adaptor, dbid, fetch_NCBI_taxonomy=False):                self.adaptor = adaptor        self.dbid = dbid        self.fetch_NCBI_taxonomy = fetch_NCBI_taxonomy",
        "labels_text": "Initialize with connection information for the database Creating a DatabaseLoader object is normally handled via the BioSeqDatabase DBServer object for example from BioSQL import BioSeqDatabase server BioSeqDatabaseopendatabasedriverMySQLdb usergbrowse passwdbiosql hostlocalhost dbtestbiosql try db servertest except KeyError db servernewdatabasetest descriptionFor testing GBrowse"
    },
    {
        "input_text": "summarize: def _get_ontology_id(self, name, definition=None):                oids = self.adaptor.execute_and_fetch_col0(            \"SELECT ontology_id FROM ontology WHERE name = %s\", (name,)        )        if oids:            return oids[0]        self.adaptor.execute(            \"INSERT INTO ontology(name, definition) VALUES (%s, %s)\", (name, definition)        )        return self.adaptor.last_id(\"ontology\")",
        "labels_text": "Return identifier for the named ontology PRIVATE This look through the onotology table for a the given entry name If it is not found a row is added for this ontology using the definition if supplied In either case the id corresponding to the provided name is returned so that you can reference it in another table"
    },
    {
        "input_text": "summarize: def _add_dbxref(self, dbname, accession, version):                self.adaptor.execute(            \"INSERT INTO dbxref(dbname, accession, version) VALUES (%s, %s, %s)\",            (dbname, accession, version),        )        return self.adaptor.last_id(\"dbxref\")",
        "labels_text": "Insert a dbxref and return it id PRIVATE"
    },
    {
        "input_text": "summarize: def _add_seqfeature_dbxref(self, seqfeature_id, dbxref_id, rank):                sql = (            \"INSERT INTO seqfeature_dbxref \"            '(seqfeature_id, dbxref_id, \"rank\") VALUES'            \"(%s, %s, %s)\"        )        self.adaptor.execute(sql, (seqfeature_id, dbxref_id, rank))        return (seqfeature_id, dbxref_id)",
        "labels_text": "Add DB crossreference PRIVATE Insert a seqfeaturedbxref row and return the seqfeatureid and dbxrefid"
    },
    {
        "input_text": "summarize: def _add_bioentry_dbxref(self, bioentry_id, dbxref_id, rank):                sql = (            \"INSERT INTO bioentry_dbxref \"            '(bioentry_id,dbxref_id,\"rank\") VALUES '            \"(%s, %s, %s)\"        )        self.adaptor.execute(sql, (bioentry_id, dbxref_id, rank))        return (bioentry_id, dbxref_id)",
        "labels_text": "Insert a bioentrydbxref row PRIVATE Returns the seqfeatureid and dbxrefid PRIVATE"
    },
    {
        "input_text": "summarize: def __init__(self, adaptor, dbid):                self.adaptor = adaptor        self.dbid = dbid",
        "labels_text": "Initialize with a database id and adaptor connection"
    },
    {
        "input_text": "summarize: def remove(self):                sql = \"DELETE FROM bioentry WHERE biodatabase_id = %s\"        self.adaptor.execute(sql, (self.dbid,))        sql = \"DELETE FROM biodatabase WHERE biodatabase_id = %s\"        self.adaptor.execute(sql, (self.dbid,))",
        "labels_text": "Remove everything related to the given database id"
    },
    {
        "input_text": "summarize: def _insert_latex_author_line_breaks(author):        names = [x.strip() + \",\" for x in author.split(\",\")]    for i in range(2, len(names) - 1, 3):        # After every three names insert legacy LaTeX \\and        # for a line break in the authors        names[i] += r\" \\and\"    return (\" \".join(names))[:-1]",
        "labels_text": "Perform LaTeX magic to insert author line break"
    },
    {
        "input_text": "summarize: def import_object(self):                ret = super().import_object()        if not issubclass(self.object, Application.AbstractCommandline):            return ret        try:            # If the object is an AbstractCommandline we instantiate it.            self.object()        except TypeError:            # Throws if the object is the base AbstractCommandline class            pass        return ret",
        "labels_text": "Import the class"
    },
    {
        "input_text": "summarize: def setup(app):        app.connect(\"builder-inited\", run_apidoc)    app.add_css_file(\"biopython.css\")    def add_documenter(app, env, docnames):        app.add_autodocumenter(BioPythonAPI, True)    # Over-ride autodoc documenter    app.connect(\"env-before-read-docs\", add_documenter)",
        "labels_text": "Override Sphinx setup to trigger sphinxapidoc"
    },
    {
        "input_text": "summarize: def get_accession_num(seq_record):        accession_atoms = seq_record.id.split(\"|\")    gb_name = accession_atoms[3]    # strip the version info before returning    return gb_name[:-2]",
        "labels_text": "Extract accession number from sequence id"
    },
    {
        "input_text": "summarize: def extract_organisms(file_to_parse, fmt):        all_species = set()    for cur_record in SeqIO.parse(open(file_to_parse), fmt):        # extract the info from the description        new_species = cur_record.description.split()[1]        all_species.add(new_species)    # sorting the species will convert the set to a list    all_species = sorted(all_species)    return all_species",
        "labels_text": "Extract specie name from sequence description line"
    },
    {
        "input_text": "summarize: def get_feature(features, id, tags=(\"locus_tag\", \"gene\", \"old_locus_tag\")):        for f in features:        for key in tags:            # tag may not be present in this feature            for x in f.qualifiers.get(key, []):                if x == id:                    return f    raise KeyError(id)",
        "labels_text": "Search list of SeqFeature object for an identifier under the given tag"
    },
    {
        "input_text": "summarize: def print_usage():        print(            )",
        "labels_text": "Print a help message"
    },
    {
        "input_text": "summarize: def usage():        print(            )",
        "labels_text": "Print a help message"
    },
    {
        "input_text": "summarize: def open_pdb(pdbid, pdb_url=None):        if pdb_url is None:        pdb_url = default_pdb_url    url = pdb_url % pdbid    fn, header = _urlretrieve(url)    return open(fn)",
        "labels_text": "Make a local copy of an online pdb file and return a file handle"
    },
    {
        "input_text": "summarize: def parse_enzyme_records(handle):        while True:        record = read_enzyme_record(handle)        if not record:            break        yield record",
        "labels_text": "Parse ENZYME record This function is for parsing ENZYME file containing multiple record Arguments handle handle to the file"
    },
    {
        "input_text": "summarize: def read_enzyme_record(handle):        record = None    for line in handle:        key, value = line[:2], line[5:].rstrip()        if key == \"ID\":            record = {\"ID\": value}        elif key == \"AC\":            record[\"AC\"] = value        elif key == \"//\":            if record:                return record            else:  # This was the copyright notice                continue    if record:        raise ValueError(\"Unexpected end of stream\")",
        "labels_text": "Read a single Enzyme record Enzyme record read format is adapted from BioExPASyEnzyme but must be able to read an accession field that is not used by BioExPASyEnzyme"
    },
    {
        "input_text": "summarize: def load_enzyme_ids(file) -> dict[str, int]:        with open(file) as in_file:        return {            record[\"ID\"]: int(record[\"AC\"].removeprefix(\"RB\").removesuffix(\";\"))            for record in parse_enzyme_records(in_file)        }",
        "labels_text": "Load enzyme identifier from bairochformat file"
    },
    {
        "input_text": "summarize: def is_palindrome(sequence):        return sequence == sequence.reverse_complement()",
        "labels_text": "Check whether the sequence is a palindrome or not"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "TypeCompiler new TypeCompiler instance"
    },
    {
        "input_text": "summarize: def removestart(self, file):                return list(itertools.dropwhile(lambda line: line.startswith(\"#\"), file))",
        "labels_text": "Remove the header of the file"
    },
    {
        "input_text": "summarize: def getblock(self, file, index):                #        #   emboss_r.txt, separation between blocks is //        #        take = itertools.takewhile        block = list(take(lambda line: not line.startswith(\"//\"), file[index:]))        index += len(block) + 1        return block, index",
        "labels_text": "Get a data block from the embossr file"
    },
    {
        "input_text": "summarize: def standalone():        parser = optparse.OptionParser()    add = parser.add_option    add(        \"-i\",        \"--install\",        action=\"store_true\",        dest=\"i\",        default=False,        help=\"compile and install the newly created file. \"        \"default behaviour (without switch): \"        \"Compile the enzymes and store them in the Updates folder\",    )    options, args = parser.parse_args()    return options, args",
        "labels_text": "Set up for running a main"
    },
    {
        "input_text": "summarize: def clear_output():        input_text.delete(1.0, tk.END)    output_text.delete(1.0, tk.END)",
        "labels_text": "Clear the output window"
    },
    {
        "input_text": "summarize: def set_statusbar(event):        index = main_window.call(event.widget, \"index\", \"active\")    if index == 0:        statustext.set(\"More information about this program\")    elif index == 2:        statustext.set(\"Terminate the program\")    else:        statustext.set(\"This is the statusbar\")",
        "labels_text": "Show statusbar comment from menu selection"
    },
    {
        "input_text": "summarize: def __init__(self, seq, parent=None):                self.seq = seq        self.parent = parent        self.toplevel = tk.Toplevel(parent)        self.toplevel.title(\"BLAST parameters\")        if not self.get_blast_databases() or not self.get_blast_binaries():            return        self.Choices()        self.dbs.bind(\"<<ComboboxSelected>>\", self.Validate)        self.blasts.bind(\"<<ComboboxSelected>>\", self.Validate)",
        "labels_text": "Set up new toplevel window for BLAST search"
    },
    {
        "input_text": "summarize: def database_readable(self, db_paths):                db_names = [entry.split(os.sep)[-1].split(\".\")[0] for entry in db_paths]        return db_names",
        "labels_text": "Return the name of the blast database without path and extension"
    },
    {
        "input_text": "summarize: def convert_dbname_to_dbpath(self, db_name):                database_path = \"\"        for database in self.nin:            if database.endswith(db_name):                database_path = database                break        for database in self.pin:            if database.endswith(db_name):                database_path = database                break        return database_path",
        "labels_text": "Return the full path for a given blast database name"
    },
    {
        "input_text": "summarize: def _Run(self):                command_options = self.option.get()        options = \"\"        if len(command_options.strip()):            options = command_options.strip()        db = self.convert_dbname_to_dbpath(self.dbs.get())        prog = self.blast_path + self.blasts.get()        self.command_data = [self.seq, prog, db, options]        self.Run()",
        "labels_text": "Initialise option for Blast commandline PRIVATE"
    },
    {
        "input_text": "summarize: def Run(self):                self.notepad = NotePad()        tid = self.notepad.tid        self.toplevel.destroy()        blastbg = xbb_blastbg.BlastDisplayer(self.command_data, tid)        blastbg.RunCommand()",
        "labels_text": "Open new notepad and initialize running BLAST"
    },
    {
        "input_text": "summarize: def __init__(self, command_data, text_id=None):                self.command_data = command_data        self.tid = text_id",
        "labels_text": "Take command data and notpad id"
    },
    {
        "input_text": "summarize: def Exit(self):                if os.path.exists(self.outfile):            os.close(self.fh_out)            os.remove(self.outfile)        if os.path.exists(self.infile):            os.close(self.fh_in)            os.remove(self.infile)        del self.worker",
        "labels_text": "Clean up on exit"
    },
    {
        "input_text": "summarize: def __init__(self, blast_command):                self.com = blast_command        threading.Thread.__init__(self)        self.finished = 0",
        "labels_text": "Initialize the worker"
    },
    {
        "input_text": "summarize: def run(self):                try:            self.com()        except Exception as e:            messagebox.showwarning(\"BLAST error\", \"BLAST error:\\n\\n\" + str(e))        self.finished = 1",
        "labels_text": "Start worker"
    },
    {
        "input_text": "summarize: def __init__(self, *args):                tk.Toplevel.__init__(self)        self.tid = scrolledtext.ScrolledText(self)        self.tid.pack(fill=tk.BOTH, expand=1)        self.Styles()        self.Show()",
        "labels_text": "Make toplevel help window"
    },
    {
        "input_text": "summarize: def __init__(self):                self.init_alphabet()        self.sequence = \"\"",
        "labels_text": "Set up the alphabet"
    },
    {
        "input_text": "summarize: def init_alphabet(self):                self.alphabet = ambiguous_dna_values        other = \"\".join(self.alphabet)        self.alphabet[\"N\"] = self.alphabet[\"N\"] + other        for key in self.alphabet:            if key == \"N\":                continue            if key in self.alphabet[key]:                continue            self.alphabet[key] = self.alphabet[key] + key",
        "labels_text": "Expand alphabet value for ambiguous code"
    },
    {
        "input_text": "summarize: def SetSeq(self, seq):                self.sequence = seq",
        "labels_text": "Set sequence"
    },
    {
        "input_text": "summarize: def SetPattern(self, pattern):                self.pattern = pattern        self.rx_pattern = self.IUPAC2regex(pattern)        self.rx = re.compile(self.rx_pattern)",
        "labels_text": "Convert search pattern to regular expression"
    },
    {
        "input_text": "summarize: def IUPAC2regex(self, s):                rx = \"\"        for i in s:            r = self.alphabet.get(i, i)            if len(r) > 1:                rx = f\"{rx}[{r}]\"            else:                rx += r        return rx",
        "labels_text": "Translate search text into pattern"
    },
    {
        "input_text": "summarize: def _Search(self, start=0):                # Only called from SearchAll. Is it used?        pos = self.rx.search(self.sequence, start)        return pos",
        "labels_text": "Search and return MatchObject PRIVAT"
    },
    {
        "input_text": "summarize: def Search(self, start=0):                pos = self.rx.search(self.sequence, start)        if pos:            return pos.start()        else:            return -1",
        "labels_text": "Search for query sequence and return position"
    },
    {
        "input_text": "summarize: def SearchAll(self):                # Doesn't seem to be used...        pos = -1        positions = []        while True:            m = self._Search(pos + 1)            if not m:                break            pos = m.start()            if pos == -1:                break            positions.append(pos)        return positions",
        "labels_text": "Search the whole sequence"
    },
    {
        "input_text": "summarize: def __init__(self, seq=\"\", master=None, highlight=0):                DNAsearch.__init__(self)        self.master = master        self.highlight = highlight        self.colors = []        self.init_graphics()        self.sequence = seq        self.cur_pos = 0",
        "labels_text": "Initialize the search GUI"
    },
    {
        "input_text": "summarize: def change_color(self):                self.config_color()",
        "labels_text": "Call back for color button"
    },
    {
        "input_text": "summarize: def get_pattern(self):                pattern = self.search_entry.get()        return pattern",
        "labels_text": "Retrieve query sequence"
    },
    {
        "input_text": "summarize: def exit(self):                for c in self.colors:            self.master.tag_remove(f\"searched_{c}\", 1.0, tk.END)            self.master.tag_remove(f\"searched_{c}R\", 1.0, tk.END)        self.destroy()        del self",
        "labels_text": "Clean up on exit"
    },
    {
        "input_text": "summarize: def __init__(self):",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def frame1(self, seq, translation_table=1):                return translate(seq, table=translation_table)",
        "labels_text": "Translate first reading frame"
    },
    {
        "input_text": "summarize: def complement(self, seq):                return Seq.complement(Seq(seq))",
        "labels_text": "Return complementary DNA Seq object"
    },
    {
        "input_text": "summarize: def reverse(self, seq):                return seq[::-1]",
        "labels_text": "Reverse the sequence"
    },
    {
        "input_text": "summarize: def antiparallel(self, seq):                return reverse_complement(seq)",
        "labels_text": "Return reverse complementary sequence"
    },
    {
        "input_text": "summarize: def frame(self, seq, frame, translation_table=1):                if frame < 0:            seq = reverse_complement(seq)        seq = seq[(abs(frame) - 1) :]        return translate(seq, table=translation_table)",
        "labels_text": "Translate DNA sequence in a chosen frame"
    },
    {
        "input_text": "summarize: def gc(self, seq):                return 100 * gc_fraction(seq)",
        "labels_text": "Calculate GC content in percent"
    },
    {
        "input_text": "summarize: def text_id(self):                return self.tid",
        "labels_text": "Get reference to notepad window"
    },
    {
        "input_text": "summarize: def insert(self, start, txt):                self.tid.insert(start, txt)",
        "labels_text": "Add text to notepad window"
    },
    {
        "input_text": "summarize: def save(self):                filename = filedialog.asksaveasfilename()        if filename:            with open(filename, \"w\") as fid:                fid.write(self.tid.get(0.0, \"end\"))",
        "labels_text": "Save text from notepad to file"
    },
    {
        "input_text": "summarize: def init_variables(self):                self.seqwidth = 60        self.translation_tables = {}        for i, table in CodonTable.unambiguous_dna_by_id.items():            self.translation_tables[table.names[0]] = i        self.translator = xbb_translations()",
        "labels_text": "Set up some standard value"
    },
    {
        "input_text": "summarize: def set_codon_table(self):                self.current_codon_table_id = self.translation_tables[            self.current_codon_table.get()        ]",
        "labels_text": "Set codon table to selection in Translations menu"
    },
    {
        "input_text": "summarize: def exit(self, *args):                # depending on if this widget is the first created or a child widget        if self.is_a_master:            sys.exit()        else:            self.main_frame.destroy()",
        "labels_text": "Close the program"
    },
    {
        "input_text": "summarize: def create_seqfield(self, parent):                self.sequence_id = tk.Text(parent, wrap=\"char\", width=self.seqwidth)        self.sequence_id.pack(fill=\"both\", expand=1, side=\"right\")",
        "labels_text": "Set up the main text field"
    },
    {
        "input_text": "summarize: def create_bindings(self):                self.sequence_id.bind(\"<Motion>\", self.position)        self.sequence_id.bind(            \"<Leave>\", lambda x, s=self: s.position_ids[\"id\"].configure(text=\"\")        )        self.sequence_id.bind(\"<1>\", self.zero)        self.sequence_id.bind(\"<B1-Motion>\", self.count_selection)        self.sequence_id.bind(\"<Double-Button-1>\", self.select_all)",
        "labels_text": "Bind event to command"
    },
    {
        "input_text": "summarize: def zero(self, event):                for i in [\"from_id\", \"to_id\", \"length_id\"]:            self.position_ids[i].configure(text=\"\")",
        "labels_text": "Remove selection"
    },
    {
        "input_text": "summarize: def get_length(self):                self.sequence_length = len(self.sequence_id.get(1.0, \"end\"))        return self.sequence_length",
        "labels_text": "Return length of sequence"
    },
    {
        "input_text": "summarize: def select_all(self, event):                self.select(1, self.get_length())        self.count_selection(None)",
        "labels_text": "Select the whole sequence"
    },
    {
        "input_text": "summarize: def select(self, a, b):                w = self.sequence_id        w.selection_own()        w.tag_add(\"sel\", \"1.%d\" % (a - 1), f\"1.{b:d}\")        self.count_selection(None)",
        "labels_text": "Select subsequence from a to b"
    },
    {
        "input_text": "summarize: def get_selection_or_sequence(self):                seq = self.get_selection()        if not len(seq):            seq = self.sequence_id.get(1.0, \"end\")        seq = re.sub(\"[^A-Z]\", \"\", seq)        return str(seq)",
        "labels_text": "Return selected sequence or whole sequence if nothing selected Whitespaces digit etc are removed"
    },
    {
        "input_text": "summarize: def get_selection(self):                w = self.sequence_id        try:            return w.selection_get()        except tk.TclError:  # Nothing is selected            return \"\"",
        "labels_text": "Return selected sequence or empty text if nothing selected"
    },
    {
        "input_text": "summarize: def get_self_selection(self):                # Identical to ``get_selection`` from above. Reason for two methods?        w = self.sequence_id        try:            return w.selection_get()        except tk.TclError:  # Nothing is selected            return \"\"",
        "labels_text": "Return selected sequence or empty text if no selection"
    },
    {
        "input_text": "summarize: def position(self, event):                x = event.x        y = event.y        pos = self.sequence_id.index(f\"@{x:d},{y:d}\").split(\".\")        pos = int(pos[1]) + 1        self.position_ids[\"id\"].configure(text=str(pos))",
        "labels_text": "Get position of cursor and display it"
    },
    {
        "input_text": "summarize: def open(self, filename=None):                if not filename:            filename = filedialog.askopenfilename()        if not filename:            return        with open(filename) as handle:            self.insert_sequence(next(SimpleFastaParser(handle)))",
        "labels_text": "Open a file"
    },
    {
        "input_text": "summarize: def insert_sequence(self, name_sequence):                (name, sequence) = name_sequence        self.sequence_id.delete(0.0, \"end\")        self.sequence_id.insert(\"end\", sequence.upper())        self.fix_sequence()        self.update_label(name)",
        "labels_text": "Load new sequence in sequence window"
    },
    {
        "input_text": "summarize: def fix_sequence(self):                seq = str(self.sequence_id.get(1.0, \"end\"))        seq = seq.upper()        seq = re.sub(\"[^A-Z]\", \"\", seq)        self.sequence_id.delete(0.0, \"end\")        self.sequence_id.insert(\"end\", seq)",
        "labels_text": "Do basic formatting of sequence in sequence window"
    },
    {
        "input_text": "summarize: def update_label(self, header):                name = header.split(\" \")[0]        name = name.split(\",\")[0]        self.position_ids[\"label\"].configure(text=name)",
        "labels_text": "Update name label"
    },
    {
        "input_text": "summarize: def export(self):                seq = self.get_selection_or_sequence()        if not seq:            return        np = NotePad()        tid = np.text_id()        tid.insert(\"end\", seq)",
        "labels_text": "Export selected text to new text window"
    },
    {
        "input_text": "summarize: def gcframe(self, direction=\"both\"):                seq = self.get_selection_or_sequence()        if not seq:            return        np = NotePad()        tid = np.text_id()        tid.insert(            \"end\", self.translator.gcframe(seq, self.current_codon_table_id, direction)        )",
        "labels_text": "Run pretty print multiple frame translation"
    },
    {
        "input_text": "summarize: def translate(self):                seq = self.get_selection_or_sequence()        frame = self.frame_int.get()        if not seq:            return        np = NotePad()        tid = np.text_id()        tid.insert(            \"end\", self.translator.frame_nice(seq, frame, self.current_codon_table_id)        )",
        "labels_text": "Run pretty print single frame translation"
    },
    {
        "input_text": "summarize: def extract(self):                seq = self.get_selection_or_sequence()        frame = self.frame_int.get()        if not seq:            return        aa_seq = self.translator.frame(seq, frame, self.current_codon_table_id)        aa_seq = re.sub(\"(.{50})\", \"\\\\1\\n\", str(aa_seq))        np = NotePad()        tid = np.text_id()        tid.insert(\"end\", f\">frame{frame:d}\\n{aa_seq}\")",
        "labels_text": "Make single frame translation and display aa sequence a fasta"
    },
    {
        "input_text": "summarize: def blast(self):                seq = self.get_selection_or_sequence()        self.blaster = BlastIt(seq, self.parent)",
        "labels_text": "Initialize and start BLASTing"
    },
    {
        "input_text": "summarize: def search(self):                seq = self.get_selection_or_sequence()        XDNAsearch(seq, master=self.sequence_id, highlight=1)",
        "labels_text": "Initialize and start search process"
    },
    {
        "input_text": "summarize: def mark(self, start, stop):                self.sequence_id.focus()        self.sequence_id.mark_set(\"insert\", f\"1.{start:d}\")        self.sequence_id.tag_add(\"sel\", f\"1.{start:d}\", f\"1.{stop:d}\")",
        "labels_text": "Mark and put a tag on chosen subsequence from start to stop"
    },
    {
        "input_text": "summarize: def destroy_database():        if DBDRIVER in [\"sqlite3\"]:        if os.path.exists(TESTDB):            os.remove(TESTDB)",
        "labels_text": "Delete any temporary BioSQL sqlite database file"
    },
    {
        "input_text": "summarize: def setUp(self):                load_multi_database(\"GenBank/cor6_6.gb\", \"GenBank/NC_000932.gb\")        self.server = BioSeqDatabase.open_database(            driver=DBDRIVER, user=DBUSER, passwd=DBPASSWD, host=DBHOST, db=TESTDB        )        self.db = self.server[\"biosql-test\"]        self.db2 = self.server[\"biosql-test2\"]",
        "labels_text": "Connect to and load up the database"
    },
    {
        "input_text": "summarize: def test_cross_retrieval_of_items(self):                db = self.db        db2 = self.db2        for db2_id in db2.keys():            with self.assertRaises(KeyError):                db[db2_id]",
        "labels_text": "Test that valid id cant be retrieved between namespaces"
    },
    {
        "input_text": "summarize: def setUp(self):                load_database(\"GenBank/cor6_6.gb\")        self.server = BioSeqDatabase.open_database(            driver=DBDRIVER, user=DBUSER, passwd=DBPASSWD, host=DBHOST, db=TESTDB        )        self.db = self.server[\"biosql-test\"]",
        "labels_text": "Connect to and load up the database"
    },
    {
        "input_text": "summarize: def test_server(self):                server = self.server        self.assertIn(\"biosql-test\", server)        self.assertEqual(1, len(server))        self.assertEqual([\"biosql-test\"], list(server.keys()))        # Check we can delete the namespace...        del server[\"biosql-test\"]        self.assertEqual(0, len(server))        with self.assertRaises(KeyError):            del server[\"non-existant-name\"]",
        "labels_text": "Check BioSeqDatabase method"
    },
    {
        "input_text": "summarize: def setUp(self):                load_database(\"GenBank/cor6_6.gb\")        self.server = BioSeqDatabase.open_database(            driver=DBDRIVER, user=DBUSER, passwd=DBPASSWD, host=DBHOST, db=TESTDB        )        self.db = self.server[\"biosql-test\"]        self.item = self.db.lookup(accession=\"X62281\")        self.item2 = self.db.lookup(accession=\"AJ237582\")",
        "labels_text": "Load a database"
    },
    {
        "input_text": "summarize: def test_convert(self):                test_seq = self.item.seq        other = Seq(test_seq)        self.assertEqual(test_seq, other)        self.assertIsInstance(other, Seq)        other = MutableSeq(test_seq)        self.assertEqual(test_seq, other)        self.assertIsInstance(other, MutableSeq)",
        "labels_text": "Check can turn a Seq object from BioSQL into a Seq or MutableSeq"
    },
    {
        "input_text": "summarize: def test_addition(self):                test_seq = self.item.seq        for other in [Seq(\"ACGT\"), MutableSeq(\"ACGT\"), \"ACGT\", test_seq]:            test = test_seq + other            self.assertEqual(test, str(test_seq) + str(other))            self.assertIsInstance(test, Seq)            test = other + test_seq            self.assertEqual(test, str(other) + str(test_seq))",
        "labels_text": "Check can add Seq object from BioSQL together"
    },
    {
        "input_text": "summarize: def test_record_slicing(self):                new_rec = self.item[400:]        self.assertIsInstance(new_rec, SeqRecord)        self.assertEqual(len(new_rec), 480)        self.assertEqual(len(new_rec.features), 5)",
        "labels_text": "Check that slice of DBSeqRecord are retrieved properly"
    },
    {
        "input_text": "summarize: def setUp(self):                load_database(\"GenBank/cor6_6.gb\")        self.server = BioSeqDatabase.open_database(            driver=DBDRIVER, user=DBUSER, passwd=DBPASSWD, host=DBHOST, db=TESTDB        )        self.db = self.server[\"biosql-test\"]",
        "labels_text": "Connect to and load up the database"
    },
    {
        "input_text": "summarize: def test_server(self):                server = self.server        self.assertIn(\"biosql-test\", server)        self.assertEqual(1, len(server))        self.assertEqual([\"biosql-test\"], list(server.keys()))        # Check we can delete the namespace...        del server[\"biosql-test\"]        self.assertEqual(0, len(server))        with self.assertRaises(KeyError):            del server[\"non-existant-name\"]",
        "labels_text": "Check BioSeqDatabase method"
    },
    {
        "input_text": "summarize: def test_NC_005816(self):                with warnings.catch_warnings():            # BiopythonWarning: order location operators are not fully supported            warnings.simplefilter(\"ignore\", BiopythonWarning)            self.loop(\"GenBank/NC_005816.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file NC"
    },
    {
        "input_text": "summarize: def test_NC_000932(self):                self.loop(\"GenBank/NC_000932.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file NC"
    },
    {
        "input_text": "summarize: def test_NT_019265(self):                self.loop(\"GenBank/NT_019265.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file NT"
    },
    {
        "input_text": "summarize: def test_protein_refseq2(self):                with warnings.catch_warnings():            # BiopythonWarning: order location operators are not fully supported            warnings.simplefilter(\"ignore\", BiopythonWarning)            self.loop(\"GenBank/protein_refseq2.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file proteinrefseq"
    },
    {
        "input_text": "summarize: def test_no_ref(self):                self.loop(\"GenBank/noref.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file noref"
    },
    {
        "input_text": "summarize: def test_one_of(self):                self.loop(\"GenBank/one_of.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file oneof"
    },
    {
        "input_text": "summarize: def test_cor6_6(self):                self.loop(\"GenBank/cor6_6.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file cor"
    },
    {
        "input_text": "summarize: def test_arab1(self):                self.loop(\"GenBank/arab1.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL and back to a GenBank file arab"
    },
    {
        "input_text": "summarize: def test_NC_005816(self):                with warnings.catch_warnings():            # BiopythonWarning: order location operators are not fully supported            warnings.simplefilter(\"ignore\", BiopythonWarning)            self.trans(\"GenBank/NC_005816.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace NC"
    },
    {
        "input_text": "summarize: def test_NC_000932(self):                self.trans(\"GenBank/NC_000932.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace NC"
    },
    {
        "input_text": "summarize: def test_NT_019265(self):                self.trans(\"GenBank/NT_019265.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace NT"
    },
    {
        "input_text": "summarize: def test_protein_refseq2(self):                with warnings.catch_warnings():            # BiopythonWarning: order location operators are not fully supported            warnings.simplefilter(\"ignore\", BiopythonWarning)            self.trans(\"GenBank/protein_refseq2.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace proteinrefseq"
    },
    {
        "input_text": "summarize: def test_no_ref(self):                self.trans(\"GenBank/noref.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace noref"
    },
    {
        "input_text": "summarize: def test_one_of(self):                self.trans(\"GenBank/one_of.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace oneof"
    },
    {
        "input_text": "summarize: def test_cor6_6(self):                self.trans(\"GenBank/cor6_6.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace cor"
    },
    {
        "input_text": "summarize: def test_arab1(self):                self.trans(\"GenBank/arab1.gb\", \"gb\")",
        "labels_text": "From GenBank file to BioSQL then again to a new namespace arab"
    },
    {
        "input_text": "summarize: def test_transfer(self):                # Should be in database already...        db_record = self.db.lookup(accession=\"X55053\")        # Make a new namespace        db2 = self.server.new_database(\"biosql-test-alt\")        # Should be able to load this DBSeqRecord there...        count = db2.load([db_record])        self.assertEqual(count, 1)",
        "labels_text": "Make sure can load record into another namespace"
    },
    {
        "input_text": "summarize: def setUp(self):                db_name = \"biosql-test-seqio\"        server = BioSeqDatabase.open_database(            driver=DBDRIVER, user=DBUSER, passwd=DBPASSWD, host=DBHOST, db=TESTDB        )        self.server = server        if db_name not in server:            self.db = server.new_database(db_name)            server.commit()        self.db = self.server[db_name]",
        "labels_text": "Connect to the database"
    },
    {
        "input_text": "summarize: def share_config(dbdriver, dbtype, dbhost, dbuser, dbpasswd, testdb):        global DBDRIVER, DBTYPE, DBHOST, DBUSER, DBPASSWD, TESTDB, DBSCHEMA    global SYSTEM, SQL_FILE    DBDRIVER = dbdriver    DBTYPE = dbtype    DBHOST = dbhost    DBUSER = dbuser    DBPASSWD = dbpasswd    TESTDB = testdb",
        "labels_text": "Make sure we can access the DB setting from this file"
    },
    {
        "input_text": "summarize: def test_param_names(self):                a = pairwise2.align.alignment_function(\"globalxx\")        a.param_names = [\"Hello\"]        self.assertRaises(ValueError, a.decode, \"Bye\")",
        "labels_text": "Test for unknown parameter in parameter name"
    },
    {
        "input_text": "summarize: def test_warnings(self):                with warnings.catch_warnings(record=True) as w:            # Cause all warnings to always be triggered.            warnings.simplefilter(\"always\")            # Trigger a warning.            pairwise2.align.localxx(\"GA\", \"CGA\", penalize_end_gaps=True)            # Verify some things            self.assertEqual(len(w), 1)            self.assertEqual(w[-1].category, BiopythonWarning)            self.assertIn(\"should not\", str(w[-1].message))",
        "labels_text": "Test for warning"
    },
    {
        "input_text": "summarize: def test_one_alignment_only(self):                aligns = pairwise2.align.globalxx(\"ACCGT\", \"ACG\")        self.assertEqual(len(aligns), 2)        aligns = pairwise2.align.globalxx(\"ACCGT\", \"ACG\", one_alignment_only=True)        self.assertEqual(len(aligns), 1)",
        "labels_text": "Test onealignmentonly parameter"
    },
    {
        "input_text": "summarize: def test_localms(self):                aligns = sorted(            pairwise2.align.localms(\"xxxABCDxxx\", \"zzzABzzCDz\", 1, -0.5, -3, -1)        )        alignment = pairwise2.format_alignment(*aligns[0])        self.assertEqual(            alignment,            ,        )        alignment = pairwise2.format_alignment(*aligns[1])        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Two different local alignment"
    },
    {
        "input_text": "summarize: def test_empty_result(self):                self.assertEqual(pairwise2.align.localxx(\"AT\", \"GC\"), [])",
        "labels_text": "Return no alignment"
    },
    {
        "input_text": "summarize: def test_score_only_global(self):                aligns1 = pairwise2.align.globalxx(\"GAACT\", \"GAT\")        aligns2 = pairwise2.align.globalxx(\"GAACT\", \"GAT\", score_only=True)        self.assertEqual(aligns1[0][2], aligns2)",
        "labels_text": "Test scoreonly in a global alignment"
    },
    {
        "input_text": "summarize: def test_score_only_local(self):                aligns1 = pairwise2.align.localms(\"xxxABCDxxx\", \"zzzABzzCDz\", 1, -0.5, -3, -1)        aligns2 = pairwise2.align.localms(            \"xxxABCDxxx\", \"zzzABzzCDz\", 1, -0.5, -3, -1, score_only=True        )        self.assertEqual(aligns1[0][2], aligns2)",
        "labels_text": "Test scoreonly in a local alignment"
    },
    {
        "input_text": "summarize: def test_match_score_open_penalty3(self):                aligns = pairwise2.align.globalxs(\"GAACT\", \"GAT\", -0.1, 0)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_match_score_open_penalty4(self):                aligns = pairwise2.align.globalms(\"GCT\", \"GATA\", 1, -2, -0.1, 0)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,  # noqa: W291        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_extend_penalty1(self):                aligns = pairwise2.align.globalxs(\"GACT\", \"GT\", -0.5, -0.2)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_extend_penalty2(self):                aligns = pairwise2.align.globalxs(\"GACT\", \"GT\", -1.5, -0.2)        self.assertEqual(len(aligns), 1)        aligns.sort()        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_separate_penalize_end_gaps(self):                align = pairwise2.align.globalms(            \"AT\", \"AGG\", 1.0, -0.5, -1.75, -0.25, penalize_end_gaps=(True, False)        )        self.assertEqual(align[0], (\"A--T\", \"AGG-\", -1.0, 0, 4))",
        "labels_text": "Test alignment where endgaps are differently penalized"
    },
    {
        "input_text": "summarize: def test_separate_gap_penalties2(self):                aligns = pairwise2.align.localxd(\"GAT\", \"GTCT\", -0.5, 0, -0.2, 0)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_match_dictionary2(self):                aligns = pairwise2.align.localds(\"ATAT\", \"ATT\", self.match_dict, -1, 0)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_match_dictionary3(self):                aligns = pairwise2.align.localds(\"ATT\", \"ATAT\", self.match_dict, -1, 0)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test"
    },
    {
        "input_text": "summarize: def test_align_one_char1(self):                aligns = pairwise2.align.localxs(\"abcde\", \"c\", -0.3, -0.1)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,        )",
        "labels_text": "Test sequence with only one match"
    },
    {
        "input_text": "summarize: def test_align_one_char3(self):                aligns = pairwise2.align.globalxs(\"abcde\", \"c\", -0.3, -0.1)        self.assertEqual(len(aligns), 1)        seq1, seq2, score, begin, end = aligns[0]        alignment = pairwise2.format_alignment(seq1, seq2, score, begin, end)        self.assertEqual(            alignment,            ,  # noqa: W291        )",
        "labels_text": "Like test but global alignment"
    },
    {
        "input_text": "summarize: def test_recover_alignments(self):                self.assertEqual(len(pairwise2.align.localxx(\"AC\", \"GA\")), 1)",
        "labels_text": "One possible start position in local alignment is not a match"
    },
    {
        "input_text": "summarize: def _num_difference(obj_a, obj_b):        attrs_a = set(obj_a.__dict__)    attrs_b = set(obj_b.__dict__)    diff = attrs_a.symmetric_difference(attrs_b)    privates = len([x for x in diff if x.startswith(\"_\")])    return len(diff) - privates",
        "labels_text": "Return the number of instance attribute present only in one object"
    },
    {
        "input_text": "summarize: def compare_features(self, old_list, new_list):                self.assertIsInstance(old_list, list)        self.assertIsInstance(new_list, list)        self.assertEqual(len(old_list), len(new_list))        for old_f, new_f in zip(old_list, new_list):            self.compare_feature(old_f, new_f)",
        "labels_text": "Compare two list of SeqFeature object"
    },
    {
        "input_text": "summarize: def compare_records(self, old_list, new_list):                self.assertEqual(len(old_list), len(new_list))        for old_r, new_r in zip(old_list, new_list):            self.compare_record(old_r, new_r)",
        "labels_text": "Compare two list of SeqRecord object"
    },
    {
        "input_text": "summarize: def test_empty_alignment(self):                alignment = MultipleSeqAlignment([])        self.assertEqual(alignment.get_alignment_length(), 0)        self.assertEqual(len(alignment), 0)        alignment = alignment.alignment  # new-style Alignment object        self.assertEqual(alignment.length, 0)        self.assertEqual(len(alignment), 0)",
        "labels_text": "Very simple test on an empty alignment"
    },
    {
        "input_text": "summarize: def test_parsing_empty_files(self):                for t_format in AlignIO._FormatToIterator:            handle = StringIO()            alignments = list(AlignIO.parse(handle, t_format))            self.assertEqual(alignments, [])",
        "labels_text": "Check that parsing an empty file return an empty list"
    },
    {
        "input_text": "summarize: def test_writing_empty_files(self):                for t_format in self.t_formats:            handle = StringIO()            number = AlignIO.write([], handle, t_format)            self.assertEqual(number, 0)",
        "labels_text": "Check that writer can cope with no alignment"
    },
    {
        "input_text": "summarize: def test_writing_not_alignments(self):                path = \"Clustalw/opuntia.aln\"        records = list(AlignIO.read(path, \"clustal\"))        for t_format in self.t_formats:            handle = StringIO()            self.assertRaises(Exception, AlignIO.write, [records], handle, t_format)",
        "labels_text": "Check that writer reject record that are not alignment"
    },
    {
        "input_text": "summarize: def test_empty(self):                self.assertEqual(0, len(list(ClustalIterator(StringIO(\"\")))))",
        "labels_text": "Checking empty file"
    },
    {
        "input_text": "summarize: def test_kalign_header(self):                alignment = next(ClustalIterator(StringIO(aln_example4)))        self.assertEqual(2, len(alignment))",
        "labels_text": "Make sure we can parse the Kalign header"
    },
    {
        "input_text": "summarize: def test_biopython_header(self):                alignment = next(ClustalIterator(StringIO(aln_example5)))        self.assertEqual(2, len(alignment))        self.assertEqual(alignment._version, \"1.80.dev0\")",
        "labels_text": "Make sure we can parse the Biopython header"
    },
    {
        "input_text": "summarize: def test_clustal_to_nexus_without_mol_type(self):                handle = StringIO()        self.assertRaises(            ValueError,            AlignIO.convert,            \"Clustalw/protein.aln\",            \"clustal\",            handle,            \"nexus\",        )",
        "labels_text": "Converting Clustal to NEXUS without a molecule type"
    },
    {
        "input_text": "summarize: def test_clustal_to_nexus_with_mol_type(self):                handle = StringIO()        self.assertEqual(            1,            AlignIO.convert(                \"Clustalw/protein.aln\", \"clustal\", handle, \"nexus\", \"protein\"            ),        )        self.assertIn(\" datatype=protein \", handle.getvalue())",
        "labels_text": "Converting Clustal to NEXUS with a molecule type"
    },
    {
        "input_text": "summarize: def test_empty(self):                stream = StringIO()        alignments = Align.parse(stream, \"a2m\")        with self.assertRaises(ValueError) as cm:            next(alignments)        self.assertEqual(str(cm.exception), \"Empty file.\")",
        "labels_text": "Checking empty file"
    },
    {
        "input_text": "summarize: def test_writing(self):                path = \"Blat/dna_rna.bed\"        with open(path) as stream:            original_data = stream.read()        alignments = Align.parse(path, \"bed\")        stream = StringIO()        n = Align.write(alignments, stream, \"bed\")        self.assertEqual(n, 4)        stream.seek(0)        written_data = stream.read()        stream.close()        self.assertEqual(original_data, written_data)",
        "labels_text": "Test writing the alignment in dnarnabed"
    },
    {
        "input_text": "summarize: def test_reading(self):                alignments = Align.parse(self.path, \"bed\")        self.check_alignments(alignments)",
        "labels_text": "Test reading bigbedtestbed"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bed\")        with tempfile.TemporaryFile(\"w+t\") as output:            Align.write(alignments, output, \"bed\", bedN=6)            output.seek(0)            alignments = Align.parse(output, \"bed\")            self.check_alignments(alignments)",
        "labels_text": "Test writing bigbedtestbed"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments(alignments)",
        "labels_text": "Test writing dnarnabb"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_001(self):                path = \"Blat/psl_34_001.bb\"        alignments = Align.parse(path, \"bigbed\")        self.check_alignments_psl_34_001(alignments)",
        "labels_text": "Test reading pslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_001(self):                path = \"Blat/psl_34_001.bb\"        alignments = Align.parse(path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments_psl_34_001(alignments)",
        "labels_text": "Test writing pslbb"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_003(self):                path = \"Blat/psl_34_003.bb\"        alignments = Align.parse(path, \"bigbed\")        self.check_alignments_psl_34_003(alignments)",
        "labels_text": "Test reading pslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_003(self):                path = \"Blat/psl_34_003.bb\"        alignments = Align.parse(path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments_psl_34_003(alignments)",
        "labels_text": "Test writing pslbb"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_004(self):                path = \"Blat/psl_34_004.bb\"        alignments = Align.parse(path, \"bigbed\")        self.check_alignments_psl_34_004(alignments)",
        "labels_text": "Test reading pslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_004(self):                path = \"Blat/psl_34_004.bb\"        alignments = Align.parse(path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments_psl_34_004(alignments)",
        "labels_text": "Test writing pslbb"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_005(self):                path = \"Blat/psl_34_005.bb\"        alignments = Align.parse(path, \"bigbed\")        self.check_alignments_psl_34_005(alignments)",
        "labels_text": "Test reading pslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_005(self):                path = \"Blat/psl_34_005.bb\"        alignments = Align.parse(path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments_psl_34_005(alignments)",
        "labels_text": "Test writing pslbb"
    },
    {
        "input_text": "summarize: def test_reading_psl_35_001(self):                alignments = Align.parse(self.path, \"bigbed\")        self.check_alignments(alignments)",
        "labels_text": "Test parsing pslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_35_001(self):                alignments = Align.parse(self.path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments(alignments)",
        "labels_text": "Test writing pslbb"
    },
    {
        "input_text": "summarize: def test_reading(self):                for bedN in (3, 4, 5, 6, 7, 8, 9, 12):            filename = \"bed%d.bb\" % bedN            path = os.path.join(\"Blat\", filename)            alignments = Align.parse(path, \"bigbed\")            msg = \"bed%d\" % bedN            self.check_autosql(alignments.declaration, bedN, msg)            self.check_alignments(alignments, bedN, msg)",
        "labels_text": "Test parsing alignment in file format BED through BED"
    },
    {
        "input_text": "summarize: def test_reading(self):                path = \"Blat/bigbed_extended.littleendian.bb\"        alignments = Align.parse(path, \"bigbed\")        self.assertEqual(alignments.byteorder, \"<\")        self.check_alignments(alignments)        path = \"Blat/bigbed_extended.bigendian.bb\"        alignments = Align.parse(path, \"bigbed\")        self.assertEqual(alignments.byteorder, \">\")        self.check_alignments(alignments)",
        "labels_text": "Test parsing bigbedextendedbb"
    },
    {
        "input_text": "summarize: def test_reading(self):                alignments = Align.parse(self.path, \"bigbed\")        self.check_alignments(alignments)",
        "labels_text": "Test reading bigbedtestbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigbed\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigbed\", bedN=6)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigbed\")            self.check_alignments(alignments)",
        "labels_text": "Test writing bigbedtestbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigmaf\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigmaf\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigmaf\")            self.check_alignments(alignments)",
        "labels_text": "Test writing ucsctestbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigmaf\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigmaf\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigmaf\")            self.check_alignments(alignments)",
        "labels_text": "Test writing bundlewithouttargetbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigmaf\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigmaf\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigmaf\")            self.check_alignments(alignments)",
        "labels_text": "Test writing file ucscmmchrbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                alignments = Align.parse(self.path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\")            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_alignments(alignments)",
        "labels_text": "Test writing dnarnapslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_001(self):                path = \"Blat/psl_34_001.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_34_001(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_003(self):                path = \"Blat/psl_34_003.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_34_003(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_004(self):                path = \"Blat/psl_34_004.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_34_004(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_005(self):                path = \"Blat/psl_34_005.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_34_005(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_35_001(self):                path = \"Blat/psl_35_001.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_35_001(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_writing_psl_35_002(self):                path = \"Blat/psl_35_002.psl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", fa=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_psl_35_002(alignments)",
        "labels_text": "Test writing pslpslbb"
    },
    {
        "input_text": "summarize: def test_reading(self):                path = \"Blat/bigPsl.bb\"        alignments = Align.parse(path, \"bigpsl\")        self.check_alignments(alignments)",
        "labels_text": "Test parsing bigPslbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                path = \"Blat/bigPsl.bb\"        alignments = Align.parse(path, \"bigpsl\")        with tempfile.TemporaryFile() as output:            Align.write(alignments, output, \"bigpsl\", cds=True)            output.flush()            output.seek(0)            alignments = Align.parse(output, \"bigpsl\")            self.check_alignments(alignments)",
        "labels_text": "Test writing bigPslbb"
    },
    {
        "input_text": "summarize: def test_writing(self):                path = \"Blat/dna_rna.chain\"        alignments = Align.parse(path, \"chain\")        stream = StringIO()        n = Align.write(alignments, stream, \"chain\")        self.assertEqual(n, 4)        stream.seek(0)        alignments = Align.parse(stream, \"chain\")        self.check_alignments(alignments)",
        "labels_text": "Test writing the alignment in dnarnachain"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_001(self):                # The chain file psl_34_001.chain was generated from the PSL file using:        # pslToChain psl_34_001.psl psl_34_001.chain        path = \"Blat/psl_34_001.chain\"        alignments = Align.parse(path, \"chain\")        self.check_reading_psl_34_001(alignments)",
        "labels_text": "Test parsing pslchain"
    },
    {
        "input_text": "summarize: def test_reading_chain_34_002(self):                # The chain file psl_34_002.chain was generated from the PSL file using:        # pslToChain psl_34_002.psl psl_34_002.chain        path = \"Blat/psl_34_002.chain\"        alignments = Align.parse(path, \"chain\")        self.assertRaises(StopIteration, next, alignments)",
        "labels_text": "Test parsing pslchain"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_002(self):                path = \"Blat/psl_34_002.chain\"        alignments = Align.parse(path, \"chain\")        stream = StringIO()        n = Align.write(alignments, stream, \"chain\")        self.assertEqual(n, 0)        stream.seek(0)        alignments = Align.parse(stream, \"chain\")        self.assertRaises(StopIteration, next, alignments)",
        "labels_text": "Test writing the alignment in pslchain"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_003(self):                # The chain file psl_34_003.chain was generated from the PSL file using:        # pslToChain psl_34_003.psl psl_34_003.chain        path = \"Blat/psl_34_003.chain\"        alignments = Align.parse(path, \"chain\")        self.check_reading_psl_34_003(alignments)",
        "labels_text": "Test parsing pslchain"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_003(self):                path = \"Blat/psl_34_003.chain\"        alignments = Align.parse(path, \"chain\")        stream = StringIO()        n = Align.write(alignments, stream, \"chain\")        self.assertEqual(n, 3)        stream.seek(0)        alignments = Align.parse(stream, \"chain\")        self.check_reading_psl_34_003(alignments)",
        "labels_text": "Test writing the alignment in pslchain"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_004(self):                # The chain file psl_34_004.chain was generated from the PSL file using:        # pslToChain psl_34_004.psl psl_34_004.chain        path = \"Blat/psl_34_004.chain\"        alignments = Align.parse(path, \"chain\")        self.check_reading_psl_34_004(alignments)",
        "labels_text": "Test parsing pslchain"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_005(self):                # The chain file psl_34_005.chain was generated from the PSL file using:        # pslToChain psl_34_005.psl psl_34_005.chain        path = \"Blat/psl_34_005.chain\"        alignments = Align.parse(path, \"chain\")        self.check_reading_psl_34_005(alignments)",
        "labels_text": "Test parsing pslchain"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_005(self):                path = \"Blat/psl_34_005.chain\"        alignments = Align.parse(path, \"chain\")        stream = StringIO()        n = Align.write(alignments, stream, \"chain\")        self.assertEqual(n, 22)        stream.seek(0)        alignments = Align.parse(stream, \"chain\")        self.check_reading_psl_34_005(alignments)",
        "labels_text": "Test writing the alignment in pslchain"
    },
    {
        "input_text": "summarize: def test_empty(self):                stream = StringIO()        with self.assertRaises(ValueError):            Align.parse(stream, \"clustal\")",
        "labels_text": "Checking empty file"
    },
    {
        "input_text": "summarize: def test_empty(self):                stream = StringIO()        alignments = Align.parse(stream, \"fasta\")        with self.assertRaises(ValueError) as cm:            next(alignments)        self.assertEqual(str(cm.exception), \"Empty file.\")",
        "labels_text": "Checking empty file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 32)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 32)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 10)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 8)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 29)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 34)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 16)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_length(self):                stream = open(self.path)        alignments = Align.parse(stream, \"hhr\")        stream.close()        self.assertEqual(len(alignments), 0)",
        "labels_text": "Test getting the number of alignment without parsing the file"
    },
    {
        "input_text": "summarize: def test_reading_missing_signature(self):                path = \"MAF/ucsc_mm9_chr10_big.maf\"        with self.assertRaises(ValueError) as cm:            Align.parse(path, \"maf\")        self.assertEqual(str(cm.exception), \"header line does not start with ##maf\")",
        "labels_text": "Test parsing MAF file ucscmmchrbigmaf with missing signature"
    },
    {
        "input_text": "summarize: def test_reading_bug2453(self):                path = \"MAF/bug2453.maf\"        alignments = Align.parse(path, \"maf\")        self.assertEqual(len(alignments.metadata), 3)        self.check_alignments(alignments)",
        "labels_text": "Test parsing bugmaf"
    },
    {
        "input_text": "summarize: def test_writing_bundle_without_target(self):                path = \"MAF/bundle_without_target.maf\"        alignments = Align.parse(path, \"maf\")        output = StringIO()        n = Align.write(alignments, output, \"maf\")        self.assertEqual(n, 1)        output.seek(0)        with open(path) as stream:            for line1, line2 in zip(output, stream):                self.assertEqual(line1, line2)",
        "labels_text": "Test reading and writing bundlewithouttargetmaf"
    },
    {
        "input_text": "summarize: def test_empty(self):                stream = StringIO()        alignments = Align.parse(stream, \"msf\")        with self.assertRaises(ValueError) as cm:            next(alignments)        self.assertEqual(str(cm.exception), \"Empty file.\")",
        "labels_text": "Checking empty file"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_001(self):                self.check_reading_psl_34_001(\"psl\")        self.check_reading_psl_34_001(\"pslx\")",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_002(self):                path = \"Blat/psl_34_002.psl\"        self.check_reading_psl_34_002(path)        path = \"Blat/pslx_34_002.pslx\"        self.check_reading_psl_34_002(path)",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def check_reading_psl_34_002(self, path):                alignments = Align.parse(path, \"psl\")        self.assertEqual(alignments.metadata[\"psLayout version\"], \"3\")        self.assertRaises(StopIteration, next, alignments)",
        "labels_text": "Check parsing pslpsl or pslxpslx"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_003(self):                self.check_reading_psl_34_003(\"psl\")        self.check_reading_psl_34_003(\"pslx\")",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_004(self):                self.check_reading_psl_34_004(\"psl\")        self.check_reading_psl_34_004(\"pslx\")",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_005(self):                self.check_reading_psl_34_005(\"psl\")        self.check_reading_psl_34_005(\"pslx\")",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def test_reading_psl_35_001(self):                self.check_reading_psl_35_001(\"psl\")        self.check_reading_psl_35_001(\"pslx\")",
        "labels_text": "Test parsing pslpsl and pslxpslx"
    },
    {
        "input_text": "summarize: def test_writing(self):                path = \"Blat/dna_rna.sam\"        alignments = Align.parse(path, \"sam\")        stream = StringIO()        n = Align.write(alignments, stream, \"sam\", md=True)        self.assertEqual(n, 4)        stream.seek(0)        alignments = Align.parse(stream, \"sam\")        self.check_alignments(alignments)        stream.close()",
        "labels_text": "Test writing the alignment in dnarnasam"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_001(self):                path = \"Blat/psl_34_001.sam\"        alignments = Align.parse(path, \"sam\")        self.check_alignments_psl_34_001(alignments)",
        "labels_text": "Test parsing pslsam"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_001(self):                path = \"Blat/psl_34_001.sam\"        alignments = Align.parse(path, \"sam\")        stream = StringIO()        n = Align.write(alignments, stream, \"sam\")        self.assertEqual(n, 22)        stream.seek(0)        alignments = Align.parse(stream, \"sam\")        self.check_alignments_psl_34_001(alignments)        stream.close()",
        "labels_text": "Test writing the alignment in pslsam"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_003(self):                path = \"Blat/psl_34_003.sam\"        alignments = Align.parse(path, \"sam\")        self.check_alignments_psl_34_003(alignments)",
        "labels_text": "Test parsing pslsam"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_003(self):                path = \"Blat/psl_34_003.sam\"        alignments = Align.parse(path, \"sam\")        stream = StringIO()        n = Align.write(alignments, stream, \"sam\")        self.assertEqual(n, 3)        stream.seek(0)        alignments = Align.parse(stream, \"sam\")        self.check_alignments_psl_34_003(alignments)        stream.close()",
        "labels_text": "Test writing the alignment in pslsam"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_004(self):                path = \"Blat/psl_34_004.sam\"        alignments = Align.parse(path, \"sam\")        self.check_alignments_psl_34_004(alignments)",
        "labels_text": "Test parsing pslsam"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_004(self):                path = \"Blat/psl_34_004.sam\"        alignments = Align.parse(path, \"sam\")        stream = StringIO()        n = Align.write(alignments, stream, \"sam\")        self.assertEqual(n, 19)        stream.seek(0)        alignments = Align.parse(stream, \"sam\")        self.check_alignments_psl_34_004(alignments)        stream.close()",
        "labels_text": "Test writing the alignment in pslsam"
    },
    {
        "input_text": "summarize: def test_reading_psl_34_005(self):                path = \"Blat/psl_34_005.sam\"        alignments = Align.parse(path, \"sam\")        self.check_alignments_psl_34_005(alignments)",
        "labels_text": "Test parsing pslsam"
    },
    {
        "input_text": "summarize: def test_writing_psl_34_005(self):                path = \"Blat/psl_34_005.sam\"        alignments = Align.parse(path, \"sam\")        stream = StringIO()        n = Align.write(alignments, stream, \"sam\")        self.assertEqual(n, 22)        stream.seek(0)        alignments = Align.parse(stream, \"sam\")        self.check_alignments_psl_34_005(alignments)        stream.close()",
        "labels_text": "Test writing the alignment in pslsam"
    },
    {
        "input_text": "summarize: def test_pickling(self):                matrix = substitution_matrices.load(\"BLOSUM62\")        pickled = pickle.dumps(matrix)        loaded = pickle.loads(pickled)        self.assertEqual(matrix.alphabet, loaded.alphabet)        for c1 in matrix.alphabet:            for c2 in matrix.alphabet:                self.assertAlmostEqual(matrix[c1, c2], loaded[c1, c2])",
        "labels_text": "Test pickling a substitution matrix"
    },
    {
        "input_text": "summarize: def test_loading(self):                names = substitution_matrices.load()        for name in names:            try:                m = substitution_matrices.load(name)            except Exception:                self.fail(f\"Failed to load substitution matrix '{name}'\")",
        "labels_text": "Confirm that all provided substitution matrix can be loaded"
    },
    {
        "input_text": "summarize: def __init__(self, cmd=\"echo\", **kwargs):                self.parameters = [_Argument([\"text\"], \"Text to echo\")]        AbstractCommandline.__init__(self, cmd, **kwargs)",
        "labels_text": "Initialize wrapper for echo command"
    },
    {
        "input_text": "summarize: def test_random_bam_ex1(self):                self.check_random(\"SamBam/ex1.bam\")",
        "labels_text": "Check random access to SamBamexbam"
    },
    {
        "input_text": "summarize: def test_random_bam_ex1_refresh(self):                self.check_random(\"SamBam/ex1_refresh.bam\")",
        "labels_text": "Check random access to SamBamexrefreshbam"
    },
    {
        "input_text": "summarize: def test_random_bam_ex1_header(self):                self.check_random(\"SamBam/ex1_header.bam\")",
        "labels_text": "Check random access to SamBamexheaderbam"
    },
    {
        "input_text": "summarize: def test_random_wnts_xml(self):                self.check_random(\"Blast/wnts.xml.bgz\")",
        "labels_text": "Check random access to Blastwntsxmlbgz"
    },
    {
        "input_text": "summarize: def test_random_example_fastq(self):                self.check_random(\"Quality/example.fastq.bgz\")",
        "labels_text": "Check random access to Qualityexamplefastqbgz Unix newlines"
    },
    {
        "input_text": "summarize: def test_random_example_dos_fastq(self):                self.check_random(\"Quality/example_dos.fastq.bgz\")",
        "labels_text": "Check random access to Qualityexampledosfastqbgz DOS newlines"
    },
    {
        "input_text": "summarize: def test_random_example_cor6(self):                self.check_random(\"GenBank/cor6_6.gb.bgz\")",
        "labels_text": "Check random access to GenBankcorgbbgz"
    },
    {
        "input_text": "summarize: def test_text_wnts_xml(self):                self.check_text(\"Blast/wnts.xml\", \"Blast/wnts.xml.bgz\")        self.check_text_with(\"Blast/wnts.xml\", \"Blast/wnts.xml.bgz\")",
        "labels_text": "Check text mode access to Blastwntsxmlbgz"
    },
    {
        "input_text": "summarize: def test_text_example_fastq(self):                self.check_text(\"Quality/example.fastq\", \"Quality/example.fastq.bgz\")        self.check_text_with(\"Quality/example.fastq\", \"Quality/example.fastq.bgz\")",
        "labels_text": "Check text mode access to Qualityexamplefastqbgz"
    },
    {
        "input_text": "summarize: def test_iter_wnts_xml(self):                self.check_by_line(\"Blast/wnts.xml\", \"Blast/wnts.xml.bgz\")        self.check_by_char(\"Blast/wnts.xml\", \"Blast/wnts.xml.bgz\")",
        "labels_text": "Check iteration over Blastwntsxmlbgz"
    },
    {
        "input_text": "summarize: def test_iter_example_fastq(self):                self.check_by_line(\"Quality/example.fastq\", \"Quality/example.fastq.bgz\")        self.check_by_char(\"Quality/example.fastq\", \"Quality/example.fastq.bgz\")",
        "labels_text": "Check iteration over Qualityexamplefastqbgz"
    },
    {
        "input_text": "summarize: def test_iter_example_cor6(self):                self.check_by_line(\"GenBank/cor6_6.gb\", \"GenBank/cor6_6.gb.bgz\")        self.check_by_char(\"GenBank/cor6_6.gb\", \"GenBank/cor6_6.gb.bgz\")",
        "labels_text": "Check iteration over GenBankcorgbbgz"
    },
    {
        "input_text": "summarize: def test_iter_example_gb(self):                self.check_by_line(\"GenBank/NC_000932.gb\", \"GenBank/NC_000932.gb.bgz\")        self.check_by_char(\"GenBank/NC_000932.gb\", \"GenBank/NC_000932.gb.bgz\")",
        "labels_text": "Check iteration over GenBankNCgbbgz"
    },
    {
        "input_text": "summarize: def test_bam_ex1(self):                temp_file = self.temp_file        # Note this example is from an old version of samtools        # and all the blocks are full (except the last one)        self.rewrite(\"SamBam/ex1.bam\", temp_file)        # Now check the blocks agree (using the fact that        # this example BAM file has simple block usage)        self.check_blocks(\"SamBam/ex1.bam\", temp_file)",
        "labels_text": "Reproduce BGZF compression for BAM file"
    },
    {
        "input_text": "summarize: def test_iter_bam_ex1(self):                self.check_by_char(\"SamBam/ex1.bam\", \"SamBam/ex1.bam\", True)",
        "labels_text": "Check iteration over SamBamexbam"
    },
    {
        "input_text": "summarize: def test_example_fastq(self):                temp_file = self.temp_file        self.rewrite(\"Quality/example.fastq.gz\", temp_file)        self.check_blocks(\"Quality/example.fastq.bgz\", temp_file)",
        "labels_text": "Reproduce BGZF compression for a FASTQ file"
    },
    {
        "input_text": "summarize: def test_example_gb(self):                temp_file = self.temp_file        self.rewrite(\"GenBank/NC_000932.gb.bgz\", temp_file)        self.check_blocks(\"GenBank/NC_000932.gb.bgz\", temp_file)",
        "labels_text": "Reproduce BGZF compression for NC GenBank file"
    },
    {
        "input_text": "summarize: def test_example_cor6(self):                temp_file = self.temp_file        self.rewrite(\"GenBank/cor6_6.gb.bgz\", temp_file)        self.check_blocks(\"GenBank/cor6_6.gb.bgz\", temp_file)",
        "labels_text": "Reproduce BGZF compression for corgb GenBank file"
    },
    {
        "input_text": "summarize: def test_example_wnts_xml(self):                temp_file = self.temp_file        self.rewrite(\"Blast/wnts.xml.bgz\", temp_file)        self.check_blocks(\"Blast/wnts.xml.bgz\", temp_file)",
        "labels_text": "Reproduce BGZF compression for wntsxml BLAST file"
    },
    {
        "input_text": "summarize: def test_BgzfBlocks_TypeError(self):                for mode in (\"r\", \"rb\"):            with bgzf.open(\"GenBank/cor6_6.gb.bgz\", mode) as decompressed:                with self.assertRaises(TypeError):                    list(bgzf.BgzfBlocks(decompressed))",
        "labels_text": "Check get expected TypeError from BgzfBlocks"
    },
    {
        "input_text": "summarize: def test_reader_with_binary_fileobj(self):                reader = bgzf.BgzfReader(fileobj=io.BytesIO())        self.assertEqual(0, reader.tell())",
        "labels_text": "A BgzfReader must accept a binary mode file object"
    },
    {
        "input_text": "summarize: def test_reader_with_non_binary_fileobj(self):                error = \"^fileobj not opened in binary mode$\"        with self.assertRaisesRegex(ValueError, error):            bgzf.BgzfReader(fileobj=io.StringIO())",
        "labels_text": "A BgzfReader must raise ValueError on a nonbinary file object"
    },
    {
        "input_text": "summarize: def test_writer_with_binary_fileobj(self):                writer = bgzf.BgzfWriter(fileobj=io.BytesIO())        self.assertEqual(0, writer.tell())",
        "labels_text": "A BgzfWriter must accept a binary mode file object"
    },
    {
        "input_text": "summarize: def test_writer_with_non_binary_fileobj(self):                error = \"^fileobj not opened in binary mode$\"        with self.assertRaisesRegex(ValueError, error):            bgzf.BgzfWriter(fileobj=io.StringIO())",
        "labels_text": "A BgzfWriter must raise ValueError on a nonbinary file object"
    },
    {
        "input_text": "summarize: def test_writer_with_non_binary_file(self):                error = \"^fileobj not opened in binary mode$\"        with open(self.temp_file, \"w\") as handle:            with self.assertRaisesRegex(ValueError, error):                bgzf.BgzfWriter(fileobj=handle)",
        "labels_text": "A BgzfWriter must raise ValueError on a nonbinary file handle"
    },
    {
        "input_text": "summarize: def test_writer_passes_on_plain_file_handle(self):                with open(self.temp_file, \"wb\") as handle:            bgzf.BgzfWriter(fileobj=handle)",
        "labels_text": "A BgzfWriter must be able to work with plain file handle"
    },
    {
        "input_text": "summarize: def test_phiblast_writer(self):                filename = \"phiblast.xml\"        path = os.path.join(\"Blast\", filename)        with Blast.parse(path) as records:            stream = io.BytesIO()            n = Blast.write(records, stream)            self.assertEqual(n, 1)            stream.seek(0)            written_records = Blast.parse(stream)            self.check_phiblast_records(written_records)",
        "labels_text": "Writing BLASTP phiblastxml"
    },
    {
        "input_text": "summarize: def test_megablast_legacy_writer(self):                filename = \"megablast_legacy.xml\"        path = os.path.join(\"Blast\", filename)        with Blast.parse(path) as records:            stream = io.BytesIO()            n = Blast.write(records, stream)            self.assertEqual(n, 1)            stream.seek(0)            written_records = Blast.parse(stream)            self.check_megablast_legacy_records(written_records)",
        "labels_text": "Writing megablast Sep megablastlegacyxml"
    },
    {
        "input_text": "summarize: def test_xml_2222_blastx_001_parser(self):                filename = \"xml_2222_blastx_001.xml\"        path = os.path.join(\"Blast\", filename)        with open(path, \"rb\") as stream:            records = Blast.parse(stream)            self.check_xml_2222_blastx_001(records)        with open(path, \"rb\") as stream:            records = Blast.parse(stream)            self.assertEqual(                str(records),                ,            )",
        "labels_text": "Parsing BLASTX xmlblastxxml"
    },
    {
        "input_text": "summarize: def create_fasta_index(self):                cmdline = BwaIndexCommandline(bwa_exe)        cmdline.set_parameter(\"infile\", self.reference_file)        cmdline.set_parameter(\"algorithm\", \"bwtsw\")        stdout, stderr = cmdline()",
        "labels_text": "Test for generating index for fasta file BWA requires an indexed fasta for each alignment operation This should be called to create an index before any alignment operation"
    },
    {
        "input_text": "summarize: def createAlignment(sequences):        return Alignment(        [            SeqRecord(Seq(s), id=\"sequence%i\" % (i + 1))            for (i, s) in enumerate(sequences)        ]    )",
        "labels_text": "Create an Alignment object from a list of sequence"
    },
    {
        "input_text": "summarize: def createMultipleSeqAlignment(sequences):        return MultipleSeqAlignment(        SeqRecord(Seq(s), id=\"sequence%i\" % (i + 1)) for (i, s) in enumerate(sequences)    )",
        "labels_text": "Create a MultipleSeqAlignment object from a list of sequence"
    },
    {
        "input_text": "summarize: def add_file_to_clean(self, filename):                self.files_to_clean.add(filename)",
        "labels_text": "Add a file for deferred removal by the tearDown routine"
    },
    {
        "input_text": "summarize: def add_file_to_clean(self, filename):                self.files_to_clean.add(filename)",
        "labels_text": "Add a file for deferred removal by the tearDown routine"
    },
    {
        "input_text": "summarize: def test_properties(self):                cline = ClustalwCommandline(clustalw_exe)        cline.infile = \"Fasta/f002\"        cline.outfile = \"temp_test.aln\"        cline.align = True        self.standard_test_procedure(cline)",
        "labels_text": "Test passing option via property"
    },
    {
        "input_text": "summarize: def test_simple_fasta(self):                input_file = \"Fasta/f002\"        output_file = \"temp_test.aln\"        cline = ClustalwCommandline(            clustalw_exe, infile=input_file, outfile=output_file        )        self.standard_test_procedure(cline)",
        "labels_text": "Test a simple fasta input file"
    },
    {
        "input_text": "summarize: def test_output_filename_with_spaces(self):                input_file = \"GFF/multi.fna\"        output_file = \"temp with space.aln\"        cline = ClustalwCommandline(            clustalw_exe, infile=input_file, outfile=output_file        )        self.standard_test_procedure(cline)",
        "labels_text": "Test an output filename containing space"
    },
    {
        "input_text": "summarize: def test_addition_CodonAlignment(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", category=BiopythonWarning)            new_aln = self.codon_aln + self.codon_aln        self.assertIsInstance(new_aln, codonalign.CodonAlignment)        for x in range(len(self.codon_aln)):            self.assertEqual(                new_aln[x].seq, self.codon_aln[x].seq + self.codon_aln[x].seq            )",
        "labels_text": "Check addition of CodonAlignment and CodonAlignment"
    },
    {
        "input_text": "summarize: def test_TypeError(self):                for obj in [0, \"string\", [\"str1\", \"str2\"], Seq(\"ATGTCTCGT\")]:            with self.assertRaises(TypeError):                self.codon_aln + obj",
        "labels_text": "Check that TypeError is thrown for non CodonAlignmentMultipleSequenceAlignment object"
    },
    {
        "input_text": "summarize: def test_ambiguous_tables(self):                for key, val in generic_by_name.items():            self.assertIn(key, ambiguous_generic_by_name[key].names)        for key, val in generic_by_id.items():            self.assertEqual(ambiguous_generic_by_id[key].id, key)",
        "labels_text": "Check if all IDs and all name are present in ambiguous table"
    },
    {
        "input_text": "summarize: def test_ambiguous_forward_table(self):                table = ambiguous_dna_by_id[1]        self.assertIsNone(table.forward_table.get(\"ZZZ\"))        with self.assertRaises(KeyError):            table.forward_table[\"ZZZ\"]  # KeyError it's a stop codon            table.forward_table[\"TGA\"]  # KeyError stop codon        with self.assertRaises(TranslationError):            table.forward_table[\"WWW\"]",
        "labels_text": "Raise error in AmbiguousForwardTable"
    },
    {
        "input_text": "summarize: def test_print_table(self):                table = generic_by_id[1]        output = table.__str__()        expected_output =         self.assertEqual(output, expected_output)        table = unambiguous_dna_by_id[1]        table.id = \"\"        output = table.__str__()        expected_output =         self.assertEqual(output, expected_output)        # We need to set table.id to the correct value, otherwise        # following tests may fail!        table.id = 1",
        "labels_text": "Test output of str function"
    },
    {
        "input_text": "summarize: def setUp(self):                output_filename = os.path.join(\"Graphics\", \"spiral_test.pdf\")        self.c = Canvas(output_filename, pagesize=A4)        # coordinates of the centre of the canvas        self.x_0, self.y_0 = 0.5 * A4[0], 0.5 * A4[1]",
        "labels_text": "Set up canvas for drawing"
    },
    {
        "input_text": "summarize: def finish(self):                self.c.save()",
        "labels_text": "Clean up and save image"
    },
    {
        "input_text": "summarize: def setUp(self):                output_filename = os.path.join(\"Graphics\", \"square_test.pdf\")        self.c = Canvas(output_filename, pagesize=(500, 500))",
        "labels_text": "Set up canvas for drawing"
    },
    {
        "input_text": "summarize: def finish(self):                self.c.save()",
        "labels_text": "Clean up and save image"
    },
    {
        "input_text": "summarize: def test_Dialign_simple(self):                # Test using keyword arguments:        cmdline = DialignCommandline(dialign_exe, input=self.infile1)        self.assertEqual(str(cmdline), dialign_exe + \" Fasta/f002\")        stdout, stderr = cmdline()        self.assertEqual(stderr, \"\")        self.assertEqual(stdout, \"\")        self.assertTrue(os.path.exists(self.outfile1))",
        "labels_text": "Simple roundtrip through app with infile"
    },
    {
        "input_text": "summarize: def test_embl_wrong_dr_line(self):                with warnings.catch_warnings(record=True) as w:            warnings.simplefilter(\"always\", BiopythonParserWarning)            record = SeqIO.read(\"EMBL/RepBase23.02.embl\", \"embl\")            self.assertTrue(w, \"Expected parser warning\")            self.assertEqual(                [str(_.message) for _ in w], [\"Malformed DR line in EMBL file.\"]            )",
        "labels_text": "Test file with wrong DR line"
    },
    {
        "input_text": "summarize: def test_genbank(self):                self.check_SeqIO_with_EMBOSS(\"GenBank/cor6_6.gb\", \"genbank\")",
        "labels_text": "Check SeqIO EMBOSS reading each others conversion of a GenBank file"
    },
    {
        "input_text": "summarize: def test_genbank2(self):                self.check_SeqIO_with_EMBOSS(\"GenBank/NC_000932.gb\", \"genbank\")",
        "labels_text": "Check SeqIO EMBOSS reading each others conversion of another GenBank file"
    },
    {
        "input_text": "summarize: def test_embl(self):                self.check_SeqIO_with_EMBOSS(\"EMBL/U87107.embl\", \"embl\")",
        "labels_text": "Check SeqIO EMBOSS reading each others conversion of an EMBL file"
    },
    {
        "input_text": "summarize: def test_ig(self):                # NOTE - EMBOSS considers \"genbank\" to be for nucleotides only,        # and will turn \"X\" into \"N\" for GenBank output.        self.check_SeqIO_to_EMBOSS(            \"IntelliGenetics/VIF_mase-pro.txt\", \"ig\", skip_formats=[\"genbank\", \"embl\"]        )",
        "labels_text": "Check SeqIO EMBOSS reading each others conversion of an ig file"
    },
    {
        "input_text": "summarize: def test_clustalw(self):                self.check_SeqIO_with_EMBOSS(            \"Clustalw/hedgehog.aln\", \"clustal\", skip_formats=[\"embl\", \"genbank\"]        )        self.check_SeqIO_with_EMBOSS(            \"Clustalw/opuntia.aln\", \"clustal\", skip_formats=[\"embl\", \"genbank\"]        )",
        "labels_text": "Check SeqIO EMBOSS reading each others conversion of a Clustalw file"
    },
    {
        "input_text": "summarize: def check(self, sequence):                self.check_emboss_translate(sequence)        for table in [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 21, 22, 23]:            self.check_emboss_translate(sequence, table)",
        "labels_text": "Compare our translation to EMBOSSs using all table Takes a Seq object and a filename containing it"
    },
    {
        "input_text": "summarize: def test_all_unambig_dna_codons(self):                self.translate_all_codons(\"ATCGatcg\")",
        "labels_text": "Run transeq v BioSeq on unambiguous DNA codon inc alt table"
    },
    {
        "input_text": "summarize: def test_all_unambig_rna_codons(self):                self.translate_all_codons(\"AUCGaucg\")",
        "labels_text": "Run transeq v BioSeq on unambiguous RNA codon inc alt table"
    },
    {
        "input_text": "summarize: def test_mixed_unambig_rna_codons(self):                self.translate_all_codons(\"ATUCGatucg\")",
        "labels_text": "Run transeq v BioSeq on unambiguous DNARNA codon inc alt table"
    },
    {
        "input_text": "summarize: def clean_up():        for filename in os.listdir(\"Emboss\"):        if filename.startswith(\"temp_\"):            try:                os.remove(filename)            except Exception:  # TODO - Which exceptions?                pass",
        "labels_text": "Fallback clean up method to remove temp file"
    },
    {
        "input_text": "summarize: def clean_up():        for filename in [\"test_file\", \"Phylip/opuntia.phy\", \"Phylip/hedgehog.phy\"]:        if os.path.isfile(filename):            os.remove(filename)",
        "labels_text": "Delete test file to be used a tearDown function in test fixture"
    },
    {
        "input_text": "summarize: def parse_trees(filename):        # TODO - Can this be removed now?    with open(\"test_file\") as handle:        data = handle.read()    for tree_str in data.split(\";\\n\"):        if tree_str:            yield Trees.Tree(tree_str + \";\")",
        "labels_text": "Parse tree Helper function until we have BioPhylo on trunk"
    },
    {
        "input_text": "summarize: def test_distances_from_phylip_DNA(self):                self.distances_from_alignment(\"Phylip/horses.phy\")",
        "labels_text": "Calculate a distance matrix from an phylip alignment"
    },
    {
        "input_text": "summarize: def test_distances_from_AlignIO_DNA(self):                n = AlignIO.convert(            \"Clustalw/opuntia.aln\", \"clustal\", \"Phylip/opuntia.phy\", \"phylip\"        )        self.assertEqual(n, 1)        self.distances_from_alignment(\"Phylip/opuntia.phy\")",
        "labels_text": "Calculate a distance matrix from an alignment written by AlignIO"
    },
    {
        "input_text": "summarize: def test_distances_from_protein_phylip(self):                self.distances_from_alignment(\"Phylip/interlaced.phy\", DNA=False)",
        "labels_text": "Calculate a distance matrix from phylip protein alignment"
    },
    {
        "input_text": "summarize: def test_distances_from_protein_AlignIO(self):                n = AlignIO.convert(            \"Clustalw/hedgehog.aln\", \"clustal\", \"Phylip/hedgehog.phy\", \"phylip\"        )        self.assertEqual(n, 1)        self.distances_from_alignment(\"Phylip/hedgehog.phy\", DNA=False)",
        "labels_text": "Calculate distance matrix from an AlignIO written protein alignment"
    },
    {
        "input_text": "summarize: def test_parsimony_tree_from_AlignIO_DNA(self):                n = AlignIO.convert(            \"Clustalw/opuntia.aln\", \"clustal\", \"Phylip/opuntia.phy\", \"phylip\"        )        self.assertEqual(n, 1)        self.parsimony_tree(\"Phylip/opuntia.phy\", \"phylip\")",
        "labels_text": "Make a parsimony tree from an alignment written with AlignIO"
    },
    {
        "input_text": "summarize: def test_parsimony_from_AlignIO_protein(self):                n = AlignIO.convert(            \"Clustalw/hedgehog.aln\", \"clustal\", \"Phylip/hedgehog.phy\", \"phylip\"        )        self.parsimony_tree(\"Phylip/interlaced.phy\", \"phylip\", DNA=False)",
        "labels_text": "Make a parsimony tree from protein alignment written with AlignIO"
    },
    {
        "input_text": "summarize: def test_bootstrap_phylip_DNA(self):                self.check_bootstrap(\"Phylip/horses.phy\", \"phylip\")",
        "labels_text": "Pseudosample a phylip DNA alignment"
    },
    {
        "input_text": "summarize: def test_bootstrap_AlignIO_DNA(self):                n = AlignIO.convert(            \"Clustalw/opuntia.aln\", \"clustal\", \"Phylip/opuntia.phy\", \"phylip\"        )        self.assertEqual(n, 1)        self.check_bootstrap(\"Phylip/opuntia.phy\", \"phylip\")",
        "labels_text": "Pseudosample a phylip DNA alignment written with AlignIO"
    },
    {
        "input_text": "summarize: def test_bootstrap_phylip_protein(self):                self.check_bootstrap(\"Phylip/interlaced.phy\", \"phylip\", \"p\")",
        "labels_text": "Pseudosample a phylip protein alignment"
    },
    {
        "input_text": "summarize: def test_bootstrap_AlignIO_protein(self):                n = AlignIO.convert(            \"Clustalw/hedgehog.aln\", \"clustal\", \"Phylip/hedgehog.phy\", \"phylip\"        )        self.check_bootstrap(\"Phylip/hedgehog.phy\", \"phylip\", \"p\")",
        "labels_text": "Pseudosample a phylip protein alignment written with AlignIO"
    },
    {
        "input_text": "summarize: def test_ftreedist(self):                cline = FTreeDistCommandline(            exes[\"ftreedist\"],            intreefile=\"Phylip/horses.tree\",            outfile=\"test_file\",            auto=True,            filter=True,        )        stdout, stderr = cline()        self.assertTrue(os.path.isfile(\"test_file\"))",
        "labels_text": "Calculate the distance between tree with ftreedist"
    },
    {
        "input_text": "summarize: def test_simple_parse(self):                for file in self.test_files:            # First using read...            with open(file) as handle:                Primer3.read(handle)            # Now using parse...            with open(file) as handle:                self.assertEqual(1, len(list(Primer3.parse(handle))))",
        "labels_text": "Make sure that we can use all single target primer file"
    },
    {
        "input_text": "summarize: def test_simple_parse(self):                for file in self.test_files:            with open(file) as handle:                PrimerSearch.read(handle)",
        "labels_text": "Make sure that we can parse all primersearch file"
    },
    {
        "input_text": "summarize: def test_primer_representation(self):                p_info = PrimerSearch.InputRecord()        p_info.add_primer_set(\"Test\", \"GATC\", \"CATG\")        p_info.add_primer_set(\"Test2\", \"AATA\", \"TTAT\")        output = str(p_info)        self.assertEqual(output, \"Test GATC CATG\\nTest2 AATA TTAT\\n\")",
        "labels_text": "Make sure we can output primer information correctly"
    },
    {
        "input_text": "summarize: def get_base_url(parsed):        return parsed.scheme + \"://\" + parsed.netloc + parsed.path",
        "labels_text": "Convert a parsed URL back to string but only include scheme netloc and path omitting query"
    },
    {
        "input_text": "summarize: def mock_httpresponse(code=200, content_type=\"/xml\"):        resp = mock.NonCallableMock()    resp.code = code    resp.headers = HTTPMessage()    resp.headers.add_header(\"Content-Type\", content_type + \"; charset=UTF-8\")    return resp",
        "labels_text": "Create a mocked version of a response object returned by urlopen param int code Value of code attribute param str contenttype Used to set the ContentType header in the header attribute This is checked in Entrezopen to determine if the response data is plain text"
    },
    {
        "input_text": "summarize: def patch_urlopen(**kwargs):        response = mock_httpresponse(**kwargs)    return unittest.mock.patch(\"Bio.Entrez.urlopen\", return_value=response)",
        "labels_text": "Create a context manager which replaces BioEntrezurlopen with a mocked version Within the decorated function BioEntrezurlopen will be replaced with a unittestmockMock object which when called simply record the argument passed to it and return a mocked response object The actual urlopen function will not be called so no request will actually be made"
    },
    {
        "input_text": "summarize: def get_patched_request(patched_urlopen, testcase=None):        args, kwargs = patched_urlopen.call_args    if testcase is not None:        testcase.assertEqual(patched_urlopen.call_count, 1)        testcase.assertEqual(len(args), 1)        testcase.assertEqual(len(kwargs), 0)        testcase.assertIsInstance(args[0], Request)    return args[0]",
        "labels_text": "Get the Request object passed to the patched urlopen function Expects that the patched function should have been called a single time with a Request instance a the only positional argument and no keyword argument param patchedurlopen value returned when entering the context manager created by patchurlopen type patchedurlopen unittestmockMock param testcase Test case currently being run which is used to make asserts type testcase unittestTestCase rtype urlliburlopenRequest"
    },
    {
        "input_text": "summarize: def deconstruct_request(request, testcase=None):        parsed = urlparse(request.full_url)    if request.method == \"GET\":        params = parse_qs(parsed.query)    elif request.method == \"POST\":        data = request.data.decode(\"utf8\")        params = parse_qs(data)    else:        raise ValueError(            \"Expected method to be either GET or POST, got %r\" % request.method        )    return get_base_url(parsed), params",
        "labels_text": "Get the base URL and parsed parameter of a Request object Method may be either GET or POST POST data should be encoded query params param request Request object passed to urlopen type request urllibrequestRequest param testcase Test case currently being run which is used to make asserts type testcase unittestTestCase return baseurl params tuple"
    },
    {
        "input_text": "summarize: def check_request_ids(testcase, params, expected):        testcase.assertEqual(len(params[\"id\"]), 1)    ids_str = params[\"id\"][0]    # Compare up to ordering    testcase.assertCountEqual(ids_str.split(\",\"), expected)",
        "labels_text": "Check that the constructed request parameter contain the correct IDs param testcase Test case currently being run which is used to make asserts type testcase unittestTestCase param params Parsed parameter dictionary returned by deconstructrequest type params dict param expected Expected set of IDs a collection of string"
    },
    {
        "input_text": "summarize: def test_email_warning(self):                email = Entrez.email        Entrez.email = None        try:            with warnings.catch_warnings(record=True) as w:                Entrez._construct_params(params=None)                self.assertEqual(len(w), 1)        finally:            Entrez.email = email",
        "labels_text": "Test issuing warning when user doe not specify email address"
    },
    {
        "input_text": "summarize: def test_construct_cgi_einfo(self):                with patch_urlopen() as patched:            Entrez.einfo()        request = get_patched_request(patched, self)        self.assertEqual(request.method, \"GET\")        base_url, query = deconstruct_request(request, self)        self.assertEqual(base_url, URL_HEAD + \"einfo.fcgi\")        self.assertDictEqual(query, QUERY_DEFAULTS)",
        "labels_text": "Test constructed url for request to Entrez"
    },
    {
        "input_text": "summarize: def test_read_from_url(self):                stream = Entrez.einfo()        rec = Entrez.read(stream)        stream.close()        self.assertIsInstance(rec, dict)        self.assertIn(\"DbList\", rec)        # arbitrary number, just to make sure that DbList has contents        self.assertGreater(len(rec[\"DbList\"]), 5)",
        "labels_text": "Test Entrezread from URL"
    },
    {
        "input_text": "summarize: def test_parse_from_url(self):                stream = Entrez.efetch(            db=\"protein\", id=\"15718680, 157427902, 119703751\", retmode=\"xml\"        )        recs = list(Entrez.parse(stream))        stream.close()        self.assertEqual(3, len(recs))        # arbitrary number, just to make sure the parser works        self.assertTrue(all(len(rec).keys > 5) for rec in recs)",
        "labels_text": "Test Entrezparse from URL"
    },
    {
        "input_text": "summarize: def test_seqio_from_url(self):                stream = Entrez.efetch(            db=\"nucleotide\", id=\"186972394\", rettype=\"gb\", retmode=\"text\"        )        record = SeqIO.read(stream, \"genbank\")        stream.close()        self.assertIsInstance(record, SeqRecord)        self.assertEqual(\"EU490707.1\", record.id)        self.assertEqual(1302, len(record))",
        "labels_text": "Test Entrez into SeqIOread from URL"
    },
    {
        "input_text": "summarize: def test_medline_from_url(self):                stream = Entrez.efetch(            db=\"pubmed\", id=\"19304878\", rettype=\"medline\", retmode=\"text\"        )        record = Medline.read(stream)        stream.close()        self.assertIsInstance(record, dict)        self.assertEqual(\"19304878\", record[\"PMID\"])        self.assertEqual(\"10.1093/bioinformatics/btp163 [doi]\", record[\"LID\"])",
        "labels_text": "Test Entrez into Medlineread from URL"
    },
    {
        "input_text": "summarize: def test_efetch_taxonomy_xml(self):                stream = Entrez.efetch(db=\"taxonomy\", id=3702, retmode=\"XML\")        taxon_record = Entrez.read(stream)        self.assertTrue(1, len(taxon_record))        self.assertIn(\"TaxId\", taxon_record[0])        self.assertTrue(\"3702\", taxon_record[0][\"TaxId\"])        stream.close()",
        "labels_text": "Test Entrez using a integer id like a taxon id"
    },
    {
        "input_text": "summarize: def test_epost(self):                # TODO - check results        stream = Entrez.epost(\"nuccore\", id=\"186972394,160418\")        stream.close()        stream = Entrez.epost(\"nuccore\", id=[\"160418\", \"160351\"])        stream.close()",
        "labels_text": "Test Entrezepost with multiple id both comma separated and a list"
    },
    {
        "input_text": "summarize: def test_espell(self):                stream = Entrez.espell(term=\"biopythooon\")        record = Entrez.read(stream)        stream.close()        self.assertEqual(record[\"Query\"], \"biopythooon\")        self.assertEqual(record[\"CorrectedQuery\"], \"biopython\")",
        "labels_text": "Test misspelling with Entrezespell"
    },
    {
        "input_text": "summarize: def test_efetch_gds_utf8(self):                # See issue #1402 in case any encoding issues occur        stream = Entrez.efetch(db=\"gds\", id=\"200079209\")        text = stream.read()        # Use of Unicode double quotation marks U+201C and U+201D        expected_phrase = \"\u201cfield of injury\u201d\"        self.assertEqual(text[342:359], expected_phrase)        stream.close()",
        "labels_text": "Test correct handling of encoding in Entrezefetch"
    },
    {
        "input_text": "summarize: def test_closed_file(self):                stream = open(\"Entrez/einfo1.xml\", \"rb\")        stream.close()        self.assertRaises(ValueError, Entrez.read, stream)",
        "labels_text": "Test parsing closed file fails gracefully"
    },
    {
        "input_text": "summarize: def test_read_bytes_stream(self):                with open(\"Entrez/pubmed1.xml\", \"rb\") as stream:            record = Entrez.read(stream)        self.assertEqual(len(record), 2)        self.assertIn(\"MedlineCitation\", record[0])",
        "labels_text": "Test reading a file opened in binary mode"
    },
    {
        "input_text": "summarize: def test_parse_bytes_stream(self):                with open(\"Entrez/pubmed1.xml\", \"rb\") as stream:            records = Entrez.parse(stream)            n = 0            for record in records:                self.assertIn(\"MedlineCitation\", record)                n += 1        self.assertEqual(n, 2)",
        "labels_text": "Test parsing a file opened in binary mode"
    },
    {
        "input_text": "summarize: def test_read_text_file(self):                message = \"^the XML file must be opened in binary mode.$\"        with open(\"Entrez/pubmed1.xml\") as stream:            with self.assertRaisesRegex(StreamModeError, message):                Entrez.read(stream)",
        "labels_text": "Test reading a file opened in text mode"
    },
    {
        "input_text": "summarize: def test_parse_text_file(self):                message = \"^the XML file must be opened in binary mode.$\"        with open(\"Entrez/einfo1.xml\") as stream:            records = Entrez.parse(stream)            with self.assertRaisesRegex(StreamModeError, message):                next(records)",
        "labels_text": "Test parsing a file opened in text mode"
    },
    {
        "input_text": "summarize: def test_BytesIO(self):                with open(\"Entrez/einfo1.xml\", \"rb\") as stream:            data = stream.read()        stream = BytesIO(data)        record = Entrez.read(stream)        self.assertIn(\"DbList\", record)        stream.close()",
        "labels_text": "Test parsing a BytesIO stream byte not string"
    },
    {
        "input_text": "summarize: def test_pubmed2(self):                # To create the XML file, use        # >>> Bio.Entrez.einfo(db=\"pubmed\")        # Starting some time in 2010, the results returned by Bio.Entrez        # included some tags that are not part of the corresponding DTD.        from Bio.Entrez import Parser        with open(\"Entrez/einfo3.xml\", \"rb\") as stream:            self.assertRaises(Parser.ValidationError, Entrez.read, stream)",
        "labels_text": "Test validating the XML against the DTD"
    },
    {
        "input_text": "summarize: def test_corrupted(self):                # To create the XML file, use        # >>> Bio.Entrez.einfo()        # and manually delete the last couple of lines        from Bio.Entrez import Parser        with open(\"Entrez/einfo4.xml\", \"rb\") as stream:            self.assertRaises(Parser.CorruptedXMLError, Entrez.read, stream)",
        "labels_text": "Test if corrupted XML is handled correctly"
    },
    {
        "input_text": "summarize: def test_fasta(self):                from Bio.Entrez import Parser        with open(\"Fasta/wisteria.nu\", \"rb\") as stream:            self.assertRaises(Parser.NotXMLError, Entrez.read, stream)        with open(\"Fasta/wisteria.nu\", \"rb\") as stream:            iterator = Entrez.parse(stream)            self.assertRaises(Parser.NotXMLError, next, iterator)",
        "labels_text": "Test error handling when presented with Fasta nonXML data"
    },
    {
        "input_text": "summarize: def test_truncated_xml(self):                from io import BytesIO        from Bio.Entrez.Parser import CorruptedXMLError        truncated_xml = b        stream = BytesIO()        stream.write(truncated_xml)        stream.seek(0)        records = Entrez.parse(stream)        self.assertRaises(CorruptedXMLError, next, records)",
        "labels_text": "Test error handling for a truncated XML declaration"
    },
    {
        "input_text": "summarize: def test_parse_one(self):                with open(\"Enzymes/lipoprotein.txt\") as handle:            records = list(Enzyme.parse(handle))        self.assertEqual(len(records), 1)        self.assertEqual(records[0][\"ID\"], \"3.1.1.34\")",
        "labels_text": "Check parse function with one record"
    },
    {
        "input_text": "summarize: def test_plain(self):                with File._open_for_random_access(\"Quality/example.fastq\") as handle:            self.assertIn(\"r\", handle.mode)            self.assertIn(\"b\", handle.mode)",
        "labels_text": "Test plain text file"
    },
    {
        "input_text": "summarize: def test_bgzf(self):                with File._open_for_random_access(\"Quality/example.fastq.bgz\") as handle:            self.assertIsInstance(handle, bgzf.BgzfReader)",
        "labels_text": "Test BGZF compressed file"
    },
    {
        "input_text": "summarize: def test_gzip(self):                self.assertRaises(            ValueError, File._open_for_random_access, \"Quality/example.fastq.gz\"        )",
        "labels_text": "Test gzip compressed file"
    },
    {
        "input_text": "summarize: def setUp(self):                # Create a directory to work in        self.temp_dir = tempfile.mkdtemp(prefix=\"biopython-test\")",
        "labels_text": "Initialise temporary directory"
    },
    {
        "input_text": "summarize: def tearDown(self):                shutil.rmtree(self.temp_dir)",
        "labels_text": "Remove temporary directory"
    },
    {
        "input_text": "summarize: def test_string_path(self):                p = self._path(\"test_file.fasta\")        mode = \"wb\"        with File.as_handle(p, mode=mode) as handle:            self.assertEqual(p, handle.name)            self.assertEqual(mode, handle.mode)            self.assertFalse(handle.closed)        self.assertTrue(handle.closed)",
        "labels_text": "Test ashandle with a string path argument"
    },
    {
        "input_text": "summarize: def test_path_object(self):                from pathlib import Path        p = Path(self._path(\"test_file.fasta\"))        mode = \"wb\"        with File.as_handle(p, mode=mode) as handle:            self.assertEqual(str(p.absolute()), handle.name)            self.assertEqual(mode, handle.mode)            self.assertFalse(handle.closed)        self.assertTrue(handle.closed)",
        "labels_text": "Test ashandle with a pathlibPath object"
    },
    {
        "input_text": "summarize: def test_stringio(self):                s = StringIO()        with File.as_handle(s) as handle:            self.assertIs(s, handle)",
        "labels_text": "Testing passing StringIO handle"
    },
    {
        "input_text": "summarize: def test_invalid_product_line_raises_value_error(self):                path = \"GenBank/invalid_product.gb\"        self.assertRaises(ValueError, SeqIO.read, path, \"genbank\")",
        "labels_text": "Parsing invalid product line"
    },
    {
        "input_text": "summarize: def test_genbank_read(self):                path = \"GenBank/NC_000932.gb\"        with open(path) as handle:            record = GenBank.read(handle)        self.assertEqual([\"NC_000932\"], record.accession)",
        "labels_text": "GenBankread simple test"
    },
    {
        "input_text": "summarize: def test_genbank_read_multirecord(self):                path = \"GenBank/cor6_6.gb\"        with open(path) as handle:            self.assertRaises(ValueError, GenBank.read, handle)",
        "labels_text": "GenBankread error on multiple record input"
    },
    {
        "input_text": "summarize: def test_genbank_read_invalid(self):                path = \"GenBank/NC_000932.faa\"        with open(path) as handle:            self.assertRaises(ValueError, GenBank.read, handle)",
        "labels_text": "GenBankread error on invalid file eg FASTA file"
    },
    {
        "input_text": "summarize: def test_genbank_read_no_origin_no_end(self):                path = \"GenBank/no_origin_no_end.gb\"        with open(path) as handle:            self.assertRaises(ValueError, GenBank.read, handle)",
        "labels_text": "GenBankread error on malformed file"
    },
    {
        "input_text": "summarize: def test_000_genbank_bad_loc_wrap_warning(self):                path = \"GenBank/bad_loc_wrap.gb\"        with warnings.catch_warnings():            warnings.simplefilter(\"error\", BiopythonParserWarning)            with open(path) as handle:                with self.assertRaises(BiopythonParserWarning) as cm:                    GenBank.read(handle)                self.assertEqual(                    \"Non-standard feature line wrapping (didn't break on comma)?\",                    str(cm.exception),                )",
        "labels_text": "Feature line wrapping warning"
    },
    {
        "input_text": "summarize: def test_001_negative_location_warning(self):                path = \"GenBank/negative_location.gb\"        with warnings.catch_warnings():            warnings.simplefilter(\"error\", BiopythonParserWarning)            with self.assertRaises(BiopythonParserWarning) as cm:                record = SeqIO.read(path, \"genbank\")            self.assertEqual(                \"negative starting position in feature location '-2..492'; setting feature location to None.\",                str(cm.exception),            )",
        "labels_text": "Unparsable feature location warning"
    },
    {
        "input_text": "summarize: def test_negative_location(self):                path = \"GenBank/negative_location.gb\"        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", BiopythonParserWarning)            record = SeqIO.read(path, \"genbank\")            self.assertIsNone(record.features[-1].location)",
        "labels_text": "Negative feature location"
    },
    {
        "input_text": "summarize: def test_dot_lineage(self):                path = \"GenBank/bad_loc_wrap.gb\"        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", BiopythonParserWarning)            record = SeqIO.read(path, \"genbank\")        self.assertEqual(record.annotations[\"organism\"], \".\")        self.assertEqual(record.annotations[\"taxonomy\"], [])",
        "labels_text": "Missing taxonomy lineage"
    },
    {
        "input_text": "summarize: def test_tsa(self):                path = \"GenBank/tsa_acropora.gb\"        record = SeqIO.read(path, \"genbank\")        self.assertIn(\"tsa\", record.annotations)        self.assertEqual(record.annotations[\"tsa\"], [\"GHGH01000001\", \"GHGH01126539\"])",
        "labels_text": "Test TSA annotation parsing"
    },
    {
        "input_text": "summarize: def test_dblink(self):                path = \"GenBank/NC_005816.gb\"        record = SeqIO.read(path, \"gb\")        self.assertEqual(record.dbxrefs, [\"Project:58037\"])        gb = record.format(\"gb\")        self.assertIn(\"\\nDBLINK      Project: 58037\\n\", gb)        embl = record.format(\"embl\")        self.assertIn(\"XX\\nPR   Project:58037;\\nXX\\n\", embl)",
        "labels_text": "Parse GenBank record with old DBLINK project entry"
    },
    {
        "input_text": "summarize: def test_malformed_structured_comment_parsing(self):                path = \"GenBank/invalid_structured_comment.gb\"        with warnings.catch_warnings(record=True) as caught:            warnings.simplefilter(\"always\")            record = SeqIO.read(path, \"genbank\")            self.assertNotIn(\"structured_comment\", record.annotations)            self.assertIn(                \"Structured comment not parsed on malformed header line\",                str(caught[0].message),            )",
        "labels_text": "Test malformed structured comment give warning The comment will be ignored if it is not read by the parser AYW Malformed keyvalue delimiter used Should be but the record us"
    },
    {
        "input_text": "summarize: def test_qualifier_order(self):                record = SeqIO.read(\"GenBank/DS830848.gb\", \"gb\")        f = record.features[0]        self.assertEqual(            list(f.qualifiers),            [\"organism\", \"mol_type\", \"strain\", \"db_xref\", \"dev_stage\"],        )",
        "labels_text": "Check the qualifier order is preserved"
    },
    {
        "input_text": "summarize: def gb_to_l_cds_f(self, filename, tags2id=None):                with open(filename) as handle:            if tags2id:                l_cds_f = list(self.gb_s.parse_cds_features(handle, tags2id=tags2id))            else:                l_cds_f = list(self.gb_s.parse_cds_features(handle))        return l_cds_f",
        "labels_text": "Gb file to Seq list parse CDS feature"
    },
    {
        "input_text": "summarize: def gb_to_l_r(self, filename, do_features=False):                with open(filename) as handle:            l_gb_r = list(self.gb_s.parse_records(handle, do_features=do_features))        return l_gb_r",
        "labels_text": "Gb file to Seq list parse record"
    },
    {
        "input_text": "summarize: def fill_and_border(base_color, alpha=0.5):        try:        c = base_color.clone()        c.alpha = alpha        return c, base_color    except AttributeError:        # Old ReportLab, no transparency and/or no clone        return base_color, base_color",
        "labels_text": "Return fill and border color given a base color"
    },
    {
        "input_text": "summarize: def calc_gc_content(sequence):        d = {}    for nt in [\"A\", \"T\", \"G\", \"C\"]:        d[nt] = sequence.count(nt) + sequence.count(nt.lower())    gc = d.get(\"G\", 0) + d.get(\"C\", 0)    if gc == 0:        return 0    return gc / (d[\"A\"] + d[\"T\"] + gc)",
        "labels_text": "Return the GC content in a passed sequence Arguments sequence a BioSeqSeq object calcgccontentsequence"
    },
    {
        "input_text": "summarize: def calc_at_content(sequence):        d = {}    for nt in [\"A\", \"T\", \"G\", \"C\"]:        d[nt] = sequence.count(nt) + sequence.count(nt.lower())    at = d.get(\"A\", 0) + d.get(\"T\", 0)    if at == 0:        return 0    return at / (d[\"G\"] + d[\"G\"] + at)",
        "labels_text": "Return the AT content in a passed sequence Arguments sequence a BioSeqSeq object calcatcontentsequence"
    },
    {
        "input_text": "summarize: def calc_gc_skew(sequence):        g = sequence.count(\"G\") + sequence.count(\"g\")    c = sequence.count(\"C\") + sequence.count(\"c\")    if g + c == 0:        return 0.0  # TODO - return NaN or None here?    else:        return (g - c) / (g + c)",
        "labels_text": "Return the GCGC GC skew in a passed sequence Arguments sequence a BioSeqSeq object calcgcskewsequence"
    },
    {
        "input_text": "summarize: def calc_at_skew(sequence):        a = sequence.count(\"A\") + sequence.count(\"a\")    t = sequence.count(\"T\") + sequence.count(\"t\")    if a + t == 0:        return 0.0  # TODO - return NaN or None here?    else:        return (a - t) / (a + t)",
        "labels_text": "Return the ATAT AT skew in a passed sequence Arguments sequence a BioSeqSeq object calcatskewsequence"
    },
    {
        "input_text": "summarize: def calc_dinucleotide_counts(sequence):        total = 0    for letter in \"ACTGUactgu\":        total += sequence.count(letter + letter)    return total",
        "labels_text": "Return the total count of dinucleotides repeat eg AA CC This is purely for the sake of generating some nonrandom sequence based score for plotting with no expected biological meaning NOTE Only considers same case pair NOTE AA score AAA score AAAA score etc"
    },
    {
        "input_text": "summarize: def test_slicing(self):                gd = GraphData()        gd.set_data([(1, 10), (5, 15), (20, 40)])        gd.add_point((10, 20))        self.assertEqual(            gd[4:16],            [(5, 15), (10, 20)],  # noqa 231            \"Unable to insert and retrieve points correctly\",        )",
        "labels_text": "Check GraphData slicing"
    },
    {
        "input_text": "summarize: def setUp(self):                self.gdd = Diagram(            \"Test Diagram\",            circular=False,            y=0.01,            yt=0.01,            yb=0.01,            x=0.01,            xl=0.01,            xr=0.01,        )",
        "labels_text": "Start a diagram"
    },
    {
        "input_text": "summarize: def test_label_default(self):                self.add_track_with_sigils()        self.finish(\"labels_default\")",
        "labels_text": "Feature label default"
    },
    {
        "input_text": "summarize: def setUp(self):                self.gdd = Diagram(            \"Test Diagram\",            circular=False,            y=0.01,            yt=0.01,            yb=0.01,            x=0.01,            xl=0.01,            xr=0.01,        )",
        "labels_text": "Initialise diagram"
    },
    {
        "input_text": "summarize: def test_all_sigils(self):                for glyph in [\"BOX\", \"OCTO\", \"JAGGY\", \"ARROW\", \"BIGARROW\"]:            self.add_track_with_sigils(track_caption=f'  sigil=\"{glyph}\"', sigil=glyph)        self.finish(\"GD_sigils\")",
        "labels_text": "All sigils"
    },
    {
        "input_text": "summarize: def test_short_arrow(self):                self.short_sigils(\"ARROW\")",
        "labels_text": "Feature arrow sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_short_bigarrow(self):                self.short_sigils(\"BIGARROW\")",
        "labels_text": "Feature bigarrow sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_short_jaggy(self):                self.short_sigils(\"JAGGY\")",
        "labels_text": "Feature arrow sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_short_octo(self):                self.short_sigils(\"OCTO\")",
        "labels_text": "Feature bigarrow sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_long_arrow_heads(self):                self.long_sigils(\"ARROW\")",
        "labels_text": "Feature ARROW sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_long_bigarrow_heads(self):                self.long_sigils(\"BIGARROW\")",
        "labels_text": "Feature BIGARROW sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_long_octo_heads(self):                self.long_sigils(\"OCTO\")",
        "labels_text": "Feature OCTO sigil head within bounding box"
    },
    {
        "input_text": "summarize: def test_long_jaggy(self):                self.long_sigils(\"JAGGY\")",
        "labels_text": "Feature JAGGY sigil head within bounding box"
    },
    {
        "input_text": "summarize: def setUp(self):                with open(os.path.join(\"GenBank\", \"NC_005816.gb\")) as handle:            self.record = SeqIO.read(handle, \"genbank\")        self.gdd = Diagram(\"Test Diagram\")        # Add a track of features,        self.gdd.new_track(            1, greytrack=True, name=\"CDS Features\", greytrack_labels=0, height=0.5        )",
        "labels_text": "Test setup just load a GenBank file a a SeqRecord"
    },
    {
        "input_text": "summarize: def tearDown(self):                del self.gdd",
        "labels_text": "Release the drawing object"
    },
    {
        "input_text": "summarize: def test_str(self):                expected = (            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Diagram.Diagram'>: Test Diagram>\"            \"\\n1 tracks\"            \"\\nTrack 1: \"            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Track.Track'>: CDS Features>\"            \"\\n0 sets\"            \"\\n\"        )        self.assertEqual(expected, str(self.gdd))",
        "labels_text": "Test diagram info a string"
    },
    {
        "input_text": "summarize: def test_add_track(self):                track = Track(name=\"Annotated Features\")        self.gdd.add_track(track, 2)        self.assertEqual(2, len(self.gdd.get_tracks()))",
        "labels_text": "Add track"
    },
    {
        "input_text": "summarize: def test_add_track_to_occupied_level(self):                new_track = self.gdd.get_tracks()[0]        self.gdd.add_track(new_track, 1)        self.assertEqual(2, len(self.gdd.get_tracks()))",
        "labels_text": "Add track to occupied level"
    },
    {
        "input_text": "summarize: def test_add_track_error(self):                self.assertRaises(ValueError, self.gdd.add_track, None, 1)",
        "labels_text": "Test adding unspecified track"
    },
    {
        "input_text": "summarize: def test_del_tracks(self):                self.gdd.del_track(1)        self.assertEqual(0, len(self.gdd.get_tracks()))",
        "labels_text": "Delete track"
    },
    {
        "input_text": "summarize: def test_get_tracks(self):                self.assertEqual(1, len(self.gdd.get_tracks()))",
        "labels_text": "Get track"
    },
    {
        "input_text": "summarize: def test_move_track(self):                self.gdd.move_track(1, 2)        expected = (            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Diagram.Diagram'>: Test Diagram>\"            \"\\n1 tracks\"            \"\\nTrack 2: \"            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Track.Track'>: CDS Features>\"            \"\\n0 sets\"            \"\\n\"        )        self.assertEqual(expected, str(self.gdd))",
        "labels_text": "Move a track"
    },
    {
        "input_text": "summarize: def test_renumber(self):                self.gdd.renumber_tracks(0)        expected = (            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Diagram.Diagram'>: Test Diagram>\"            \"\\n1 tracks\"            \"\\nTrack 0: \"            \"\\n<<class 'Bio.Graphics.GenomeDiagram._Track.Track'>: CDS Features>\"            \"\\n0 sets\"            \"\\n\"        )        self.assertEqual(expected, str(self.gdd))",
        "labels_text": "Test renumbering track"
    },
    {
        "input_text": "summarize: def test_simple_scatter_plot(self):",
        "labels_text": "Test creation of a simple PNG scatter plot"
    },
    {
        "input_text": "summarize: def get_random_id():        id = \"\"    for n in range(6):        letter = random.choice(letter_choices)        id += letter    return id",
        "labels_text": "Generate a random id number"
    },
    {
        "input_text": "summarize: def test_simple_organism(self):                pdf_organism = BasicChromosome.Organism()        # add chromosomes        for chr_name in [\"I\", \"II\", \"III\", \"IV\"]:            cur_chromosome = load_chromosome(chr_name)            pdf_organism.add(cur_chromosome)        pdf_organism.draw(self.test_file, \"Test organism\")",
        "labels_text": "Test the basic functionality of drawing an organism"
    },
    {
        "input_text": "summarize: def _simple_organism(self, filename, format):                test_organism = BasicChromosome.Organism(format)        test_file = os.path.join(\"Graphics\", filename)        # add chromosomes        for chr_name in [\"I\", \"II\", \"III\", \"IV\"]:            cur_chromosome = load_chromosome(chr_name)            test_organism.add(cur_chromosome)        test_organism.draw(test_file, \"Test organism\")",
        "labels_text": "Output a simple organism to given format"
    },
    {
        "input_text": "summarize: def test_simple_organism_ps(self):                self._simple_organism(\"organism.eps\", \"ps\")",
        "labels_text": "Output a simple organism to a postscript file"
    },
    {
        "input_text": "summarize: def test_simple_organism_pdf(self):                self._simple_organism(\"organism.pdf\", \"pdf\")",
        "labels_text": "Output a simple organism to a PDF file"
    },
    {
        "input_text": "summarize: def test_simple_organism_svg(self):                self._simple_organism(\"organism.svg\", \"svg\")",
        "labels_text": "Output a simple organism to an SVG file"
    },
    {
        "input_text": "summarize: def test_simple_tRNA_tuples(self):                self.check_simple_tRNA(\"Graphics/tRNA_chrom.pdf\", False)",
        "labels_text": "Test subannotations a tuples on a genome segment tRNA for Arabidopsis"
    },
    {
        "input_text": "summarize: def test_simple_tRNA_seqfeatures(self):                self.check_simple_tRNA(\"Graphics/tRNA_chrom_sf.pdf\", True)",
        "labels_text": "Test subannotations a SeqFeatures on a genome segment tRNA for Arabidopsis"
    },
    {
        "input_text": "summarize: def test_add_count(self):                self.count_display.add_count(self.names[1])        self.count_display.add_count(self.names[2], 5)        self.assertRaises(KeyError, self.count_display.add_count, \"Non-existent\")",
        "labels_text": "Add count to specific chromosome segment"
    },
    {
        "input_text": "summarize: def test_add_label(self):                self.count_display.add_label(self.names[1], \"Rules\")        self.assertRaises(            KeyError, self.count_display.add_label, \"Non-existent\", \"elephant\"        )",
        "labels_text": "Add label to chromosome segment"
    },
    {
        "input_text": "summarize: def test_set_scale(self):                self.count_display.set_scale(self.names[1], 1.5)        self.assertRaises(KeyError, self.count_display.set_scale, \"Non-existent\", 5)",
        "labels_text": "Set the scale for a chromosome segment"
    },
    {
        "input_text": "summarize: def test_color_from_count(self):                test_color = self.count_display._color_from_count(3)        self.assertEqual(test_color, colors.blue)        test_color = self.count_display._color_from_count(9)        self.assertEqual(test_color, colors.red)        self.assertRaises(ValueError, self.count_display._color_from_count, 200)",
        "labels_text": "Retrieve a color from a count number with the default color scheme"
    },
    {
        "input_text": "summarize: def test_fill_chromosome(self):                test_chr = BasicChromosome.Chromosome(\"1\")        self.count_display.add_count(self.names[2], 5)        self.count_display.add_count(self.names[1], 2)        self.count_display.add_label(self.names[3], \"Test-Label\")        new_chr = self.count_display.fill_chromosome(test_chr)",
        "labels_text": "Test filling out the information on a chromosome"
    },
    {
        "input_text": "summarize: def random_distribution(min=-5.0, max=5.0, total_items=50):        num_items = random.randrange(5, total_items)    all_info = []    for item in range(num_items):        new_item = random.uniform(min, max)        all_info.append(new_item)    return all_info",
        "labels_text": "Create a series of random distribution information"
    },
    {
        "input_text": "summarize: def test_simple_page(self):                dist_info = []        new_info = random_distribution()        dist_info.append(new_info)        distribution = BarChartDistribution(dist_info)        dist_page = DistributionPage()        dist_page.distributions.append(distribution)        dist_page.draw(self.simple_page, \"Test Bar Chart\")",
        "labels_text": "Test displaying a page with single distribution"
    },
    {
        "input_text": "summarize: def test_simple_scatter_plot_1(self):                compare_plot = ComparativeScatterPlot()        compare_plot.display_info = self._make_random_points(1)        output_file = os.path.join(os.getcwd(), \"Graphics\", \"scatter_test_1.pdf\")        compare_plot.draw_to_file(output_file, \"Testing Scatter Plots\")",
        "labels_text": "Test creation of a simple ScatterPlot with one list"
    },
    {
        "input_text": "summarize: def test_simple_scatter_plot_7(self):                compare_plot = ComparativeScatterPlot()        # There are 6 pre-defined colors and symbols, doing more:        compare_plot.display_info = self._make_random_points(7)        output_file = os.path.join(os.getcwd(), \"Graphics\", \"scatter_test_7.pdf\")        compare_plot.draw_to_file(output_file, \"Testing Scatter Plots\")",
        "labels_text": "Test creation of a simple ScatterPlot with more list"
    },
    {
        "input_text": "summarize: def test_calculate_s_value(self):                previous_vars = {(\"1\", 0): 0.5, (\"2\", 0): 0.7}        s_value = self.dp._calculate_s_value(1, previous_vars)",
        "labels_text": "Testing the calculation of s value"
    },
    {
        "input_text": "summarize: def test_log_likelihood(self):                probs = [0.25, 0.13, 0.12, 0.17]        log_prob = self.test_trainer.log_likelihood(probs)        expected_log_prob = -7.31873556778        self.assertAlmostEqual(expected_log_prob, log_prob)",
        "labels_text": "Calculate log likelihood"
    },
    {
        "input_text": "summarize: def test_irregular(self):                with open(\"KEGG/enzyme.irregular\") as handle:            records = list(Enzyme.parse(handle))        self.assertEqual(len(records), 2)        self.assertEqual(records[0].entry, \"1.14.18.1\")        self.assertEqual(records[-1].entry, \"3.4.21.50\")",
        "labels_text": "enzymeirregular test"
    },
    {
        "input_text": "summarize: def test_new(self):                with open(\"KEGG/enzyme.new\") as handle:            records = list(Enzyme.parse(handle))        self.assertEqual(len(records), 1)        self.assertEqual(records[0].entry, \"6.2.1.25\")",
        "labels_text": "enzymenew test"
    },
    {
        "input_text": "summarize: def test_exceptions(self):                with open(\"KEGG/enzyme.sample\") as handle:            with self.assertRaises(ValueError) as context:                list(Enzyme.read(handle))            self.assertIn(                \"More than one record found in handle\", str(context.exception)            )            records = Enzyme.parse(handle)            for i in range(6):                next(records)            self.assertRaises(StopIteration, next, records)",
        "labels_text": "enzymeexceptions test"
    },
    {
        "input_text": "summarize: def test_irregular(self):                with open(\"KEGG/compound.irregular\") as handle:            records = list(Compound.parse(handle))        self.assertEqual(len(records), 2)        self.assertEqual(records[0].entry, \"C01454\")",
        "labels_text": "compoundirregular test"
    },
    {
        "input_text": "summarize: def test_parse_remote_pathway(self):                with kegg_get(\"ko03070\", \"kgml\") as handle:            pathway = KGML_parser.read(handle)        self.assertEqual(pathway.name, \"path:ko03070\")",
        "labels_text": "Download a KEGG pathway from the KEGG server and parse KGML"
    },
    {
        "input_text": "summarize: def test_parser_roundtrip(self):                with kegg_get(\"ko00680\", \"kgml\") as remote_handle:            pathway = KGML_parser.read(remote_handle)        with io.StringIO(pathway.get_KGML()) as local_handle:            roundtrip = KGML_parser.read(local_handle)        self.assertEqual(pathway.name, roundtrip.name)        self.assertEqual(len(pathway.relations), len(roundtrip.relations))",
        "labels_text": "Download a KEGG pathway write local KGML and check roundtrip"
    },
    {
        "input_text": "summarize: def test_render_KGML_import_map(self):                # We test rendering of the original KEGG KGML using imported files        for p in self.data:            with open(p.infilename) as f:                pathway = read(f)                kgml_map = KGMLCanvas(pathway, import_imagemap=True)                kgml_map.draw(p.output_stem + \"_importmap.pdf\")",
        "labels_text": "Basic rendering of KGML use imported imagemap Uses the URL indicated in the xml file This test may fail if the imagemap is not available eg if there is not a web connection and may look odd if the remote imagemap ha changed since the local KGML file wa downloaded"
    },
    {
        "input_text": "summarize: def __init__(        self,        infilename,        outfilename,        element_counts,        pathway_image,        show_pathway_image=False,    ):                self.infilename = infilename        self.outfilename = outfilename        self.element_counts = element_counts        self.pathway_image = pathway_image        self.show_pathway_image = show_pathway_image",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def show_progress(iteration, loglikelihood):",
        "labels_text": "No action callback function used when training the model"
    },
    {
        "input_text": "summarize: def test_4CUP(self):                self.check_mmtf_vs_cif(\"PDB/4CUP.mmtf\", \"PDB/4CUP.cif\")",
        "labels_text": "Compare parsing CUPmmtf and CUPcif"
    },
    {
        "input_text": "summarize: def test_4ZHL(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            structure = MMTFParser.get_structure(\"PDB/4ZHL.mmtf\")",
        "labels_text": "Parse ZHLmmtf"
    },
    {
        "input_text": "summarize: def test_1A80(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            structure = MMTFParser.get_structure(\"PDB/1A8O.mmtf\")",
        "labels_text": "Parse AOmmtf"
    },
    {
        "input_text": "summarize: def test_from_url(self):                parser = MMTFParser()        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            struct = parser.get_structure_from_url(\"4ZHL\")        atoms = list(struct.get_atoms())        self.assertEqual(len(atoms), 2080)",
        "labels_text": "Check parser can fetch a record from it PDB ID"
    },
    {
        "input_text": "summarize: def test_pfm_parsing(self):                with open(\"motifs/SRF.pfm\") as stream:            m = motifs.read(stream, \"pfm\")        self.assertEqual(m.length, 12)",
        "labels_text": "Test if Biomotifs can parse JASPARstyle pfm file"
    },
    {
        "input_text": "summarize: def test_TFoutput(self):                m = motifs.create([Seq(\"ATATA\")])        with tempfile.TemporaryFile(\"w\") as stream:            stream.write(format(m, \"transfac\"))",
        "labels_text": "Ensure that we can write proper TransFac output file"
    },
    {
        "input_text": "summarize: def test_dna(self):                self.check(            [\"TACAA\", \"TACGC\", \"TACAC\", \"TACCC\", \"AACCC\", \"AATGC\", \"AATGC\"], \"GATCBDSW\"        )",
        "labels_text": "Test Biomotifsweblogo with a DNA sequence"
    },
    {
        "input_text": "summarize: def test_rna(self):                self.check(            [\"UACAA\", \"UACGC\", \"UACAC\", \"UACCC\", \"AACCC\", \"AAUGC\", \"AAUGC\"], \"GAUC\"        )",
        "labels_text": "Test Biomotifsweblogo with an RNA sequence"
    },
    {
        "input_text": "summarize: def test_protein(self):                self.check(            [\"ACDEG\", \"AYCRN\", \"HYLID\", \"AYHEL\", \"ACDEH\", \"AYYRN\", \"HYIID\"],            \"ACDEFGHIKLMNPQRSTVWYBXZJUO\",        )",
        "labels_text": "Test Biomotifsweblogo with a protein sequence"
    },
    {
        "input_text": "summarize: def standard_test_procedure(self, cline):                # Mark output files for later cleanup.        self.add_file_to_clean(cline.outfile)        input_records = SeqIO.to_dict(SeqIO.parse(cline.infile, \"fasta\"))        self.assertEqual(str(eval(repr(cline))), str(cline))        output, error = cline()",
        "labels_text": "Shared testing procedure used by all test"
    },
    {
        "input_text": "summarize: def add_file_to_clean(self, filename):                self.files_to_clean.add(filename)",
        "labels_text": "Add a file for deferred removal by the tearDown routine"
    },
    {
        "input_text": "summarize: def test_invalid_format(self):                input_file = \"Medline/pubmed_result1.txt\"        self.assertTrue(os.path.isfile(input_file))        cline = MSAProbsCommandline(msaprobs_exe, infile=input_file)        try:            stdout, stderr = cline()        except ApplicationError as err:            self.assertEqual(err.returncode, 1)        else:            self.fail(f\"Should have failed, returned:\\n{stdout}\\n{stderr}\")",
        "labels_text": "Test an input file in an invalid format"
    },
    {
        "input_text": "summarize: def test_simple_fasta(self):                input_file = \"Registry/seqs.fasta\"        output_file = \"temp_test.aln\"        cline = MSAProbsCommandline(            msaprobs_exe, infile=input_file, outfile=output_file, clustalw=True        )        self.standard_test_procedure(cline)",
        "labels_text": "Test a simple fasta file"
    },
    {
        "input_text": "summarize: def test_properties(self):                input_file = \"Registry/seqs.fasta\"        output_file = \"temp_test.aln\"        cline = MSAProbsCommandline(msaprobs_exe)        cline.infile = input_file        cline.outfile = output_file        cline.clustalw = True        self.standard_test_procedure(cline)",
        "labels_text": "Test setting option via property"
    },
    {
        "input_text": "summarize: def test_output_filename_with_spaces(self):                input_file = \"Registry/seqs.fasta\"        output_file = \"temp with spaces.aln\"        cline = MSAProbsCommandline(            msaprobs_exe, infile=input_file, outfile=output_file, clustalw=True        )        self.standard_test_procedure(cline)",
        "labels_text": "Test an output filename containing space"
    },
    {
        "input_text": "summarize: def test_requires_dbtype(self):                global exe_names        cline = Applications.NcbimakeblastdbCommandline(            exe_names[\"makeblastdb\"], input_file=\"GenBank/NC_005816.faa\"        )        with self.assertRaises(ValueError):            str(cline)",
        "labels_text": "Check that dbtype throw error if not set"
    },
    {
        "input_text": "summarize: def test_blastx(self):                self.check(\"blastx\", Applications.NcbiblastxCommandline)",
        "labels_text": "Check all blastx argument are supported"
    },
    {
        "input_text": "summarize: def test_blastp(self):                self.check(\"blastp\", Applications.NcbiblastpCommandline)",
        "labels_text": "Check all blastp argument are supported"
    },
    {
        "input_text": "summarize: def test_blastn(self):                self.check(\"blastn\", Applications.NcbiblastnCommandline)",
        "labels_text": "Check all blastn argument are supported"
    },
    {
        "input_text": "summarize: def test_tblastx(self):                self.check(\"tblastx\", Applications.NcbitblastxCommandline)",
        "labels_text": "Check all tblastx argument are supported"
    },
    {
        "input_text": "summarize: def test_tblastn(self):                self.check(\"tblastn\", Applications.NcbitblastnCommandline)",
        "labels_text": "Check all tblastn argument are supported"
    },
    {
        "input_text": "summarize: def test_psiblast(self):                self.check(\"psiblast\", Applications.NcbipsiblastCommandline)",
        "labels_text": "Check all psiblast argument are supported"
    },
    {
        "input_text": "summarize: def test_rpsblast(self):                self.check(\"rpsblast\", Applications.NcbirpsblastCommandline)",
        "labels_text": "Check all rpsblast argument are supported"
    },
    {
        "input_text": "summarize: def test_rpstblastn(self):                self.check(\"rpstblastn\", Applications.NcbirpstblastnCommandline)",
        "labels_text": "Check all rpstblastn argument are supported"
    },
    {
        "input_text": "summarize: def test_makeblastdb(self):                self.check(\"makeblastdb\", Applications.NcbimakeblastdbCommandline)",
        "labels_text": "Check all makeblastdb argument are supported"
    },
    {
        "input_text": "summarize: def test_blast_formatter(self):                        self.check(\"blast_formatter\", Applications.NcbiblastformatterCommandline)",
        "labels_text": "Check all blastformatter argument are supported"
    },
    {
        "input_text": "summarize: def test_deltablast(self):                        self.check(\"deltablast\", Applications.NcbideltablastCommandline)",
        "labels_text": "Check all deltablast argument are supported"
    },
    {
        "input_text": "summarize: def test_error_conditions(self):                self.assertRaises(            ValueError,            NCBIWWW.qblast,            \"megablast\",            \"nt\",            \"ATGCGTACGCAGCTAAAGTAAACCTATCGCGTCTCCT\",        )",
        "labels_text": "Test if exception were properly handled"
    },
    {
        "input_text": "summarize: def test_trees_and_taxa_block(self):                nexus1 = Nexus.Nexus()        nexus1.read(\"Nexus/bats.nex\")",
        "labels_text": "Basic tree file with TREES and TAXA block"
    },
    {
        "input_text": "summarize: def test_data_and_codons_block(self):                nexus2 = Nexus.Nexus()        nexus2.read(\"Nexus/codonposset.nex\")",
        "labels_text": "Simple sequence data file with DATA and CODONS block"
    },
    {
        "input_text": "summarize: def test_data_sets_trees_unknown_block(self):                nexus3 = Nexus.Nexus()        nexus3.read(\"Nexus/test_Nexus_input.nex\")",
        "labels_text": "Sequence data file with DATA SETS TREES and an unknown block"
    },
    {
        "input_text": "summarize: def test_taxa_and_characters_block(self):                nexus4 = Nexus.Nexus()        nexus4.read(\"Nexus/vSysLab_Ganaspidium_multistate.nex\")",
        "labels_text": "Taxa and character multistate block"
    },
    {
        "input_text": "summarize: def test_taxa_and_characters_with_many_codings_one_without_state(self):                nexus5 = Nexus.Nexus()        nexus5.read(\"Nexus/vSysLab_Heptascelio_no-states_10+chars.nex\")",
        "labels_text": "Taxa and chr block over coding character without state"
    },
    {
        "input_text": "summarize: def test_taxa_and_characters_with_many_codings_two_without_state(self):                nexus6 = Nexus.Nexus()        # TODO: Implement continuous datatype:        # Bio.Nexus.Nexus.NexusError: Unsupported datatype: continuous        self.assertRaises(            Nexus.NexusError,            nexus6.read,            \"Nexus/vSysLab_Oreiscelio_discrete+continuous.nex\",        )",
        "labels_text": "Taxa and chr block over coding character without state"
    },
    {
        "input_text": "summarize: def test_WriteToFileName(self):                filename = \"Nexus/test_temp.nex\"        if os.path.isfile(filename):            os.remove(filename)        n = Nexus.Nexus(self.handle)        n.write_nexus_data(filename)        self.assertTrue(os.path.isfile(filename))        os.remove(filename)",
        "labels_text": "Test writing to a given filename"
    },
    {
        "input_text": "summarize: def is_exe(filepath):        return os.path.exists(filepath) and os.access(filepath, os.X_OK)",
        "labels_text": "Test if a file is an executable"
    },
    {
        "input_text": "summarize: def __del__(self):                del_files = self.del_files        for filename in del_files:            if os.path.exists(filename):                os.remove(filename)",
        "labels_text": "Just in case tool creates some junk file do a cleanup"
    },
    {
        "input_text": "summarize: def unpack_all_atoms(self, structure):                return [a for r in structure.get_residues() for a in r.get_unpacked_list()]",
        "labels_text": "Return a list of all atom in the structure"
    },
    {
        "input_text": "summarize: def test_structure_w_disordered_com(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\")            s = self.parser.get_structure(\"b\", \"PDB/disordered.pdb\")        com = s.center_of_mass()        self.assertTrue(np.allclose(com, [54.545, 19.868, 31.212], atol=1e-3))",
        "labels_text": "Calculate center of mass of structure including DisorderedAtoms"
    },
    {
        "input_text": "summarize: def test_empty_disordered(self):                da = DisorderedAtom(\"dummy\")        with self.assertRaises(ValueError):            da.center_of_mass()",
        "labels_text": "Raise ValueError on center of mass calculation of empty DisorderedAtom"
    },
    {
        "input_text": "summarize: def parse_dssp_version(version_string):        match = re.search(r\"\\s*([\\d.]+)\", version_string)    if match:        version = match.group(1)    return tuple(map(int, version.split(\".\")))",
        "labels_text": "Parse the DSSP version into a tuple from the tool output"
    },
    {
        "input_text": "summarize: def will_it_float(s):  # well played, whoever this was :)        try:        return float(s)    except ValueError:        return s",
        "labels_text": "Convert the input into a float if it is a number If the input is a string the output doe not change"
    },
    {
        "input_text": "summarize: def test_dssp(self):                pdbfile = \"PDB/2BEG.pdb\"        model = self.pdbparser.get_structure(\"2BEG\", pdbfile)[0]        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\")  # silence DSSP warnings            dssp = DSSP(model, pdbfile)        self.assertEqual(len(dssp), 130)",
        "labels_text": "Test DSSP generation from PDB"
    },
    {
        "input_text": "summarize: def test_DSSP_file(self):                dssp, keys = make_dssp_dict(\"PDB/2BEG.dssp\")        self.assertEqual(len(dssp), 130)",
        "labels_text": "Test parsing of pregenerated DSSP"
    },
    {
        "input_text": "summarize: def test_DSSP_noheader_file(self):                # New DSSP prints a line containing only whitespace and \".\"        dssp, keys = make_dssp_dict(\"PDB/2BEG_noheader.dssp\")        self.assertEqual(len(dssp), 130)",
        "labels_text": "Test parsing of pregenerated DSSP missing header information"
    },
    {
        "input_text": "summarize: def test_fragment_mapper(self):                p = PDBParser()        pdb1 = \"PDB/1A8O.pdb\"        s = p.get_structure(\"X\", pdb1)        m = s[0]        fm = FragmentMapper(m, 10, 5, \"PDB\")        for r in Selection.unfold_entities(m, \"R\"):            if r in fm:                self.assertTrue(str(fm[r]).startswith(\"<Fragment length=5 id=\"))",
        "labels_text": "Self test for FragmentMapper module"
    },
    {
        "input_text": "summarize: def test_verbatim_block(self):                mmcif_dict = MMCIF2Dict(            io.StringIO(                \"data_verbatim_test\\n\"                \"_test_value\\n\"                \";First line\\n\"                \"    Second line\\n\"                \"Third line\\n\"                \";\\n\"            )        )        self.assertEqual(            mmcif_dict[\"_test_value\"], [\"First line\\n    Second line\\nThird line\"]        )",
        "labels_text": "Verbatim block parsed correctly Verbatim block delimited by should have the final newline stripped Whitespace may be stripped from the end of the line but not the beginning"
    },
    {
        "input_text": "summarize: def test_filehandle(self):                parser = MMCIFParser()        structure = parser.get_structure(\"example\", \"PDB/1A8O.cif\")        self.assertEqual(len(structure), 1)        with open(\"PDB/1A8O.cif\") as handle:            structure = parser.get_structure(\"example\", handle)        self.assertEqual(len(structure), 1)",
        "labels_text": "Test if the parser can handle file handle a well a filename"
    },
    {
        "input_text": "summarize: def test_point_mutations_main(self):                self._run_point_mutation_tests(MMCIFParser(QUIET=True))",
        "labels_text": "Test if MMCIFParser parse point mutation correctly"
    },
    {
        "input_text": "summarize: def test_point_mutations_fast(self):                self._run_point_mutation_tests(FastMMCIFParser(QUIET=True))",
        "labels_text": "Test if FastMMCIFParser can parse point mutation correctly"
    },
    {
        "input_text": "summarize: def test_conversion_not_preserve_numbering(self):                cif_parser = MMCIFParser(QUIET=1)        cif_struct = cif_parser.get_structure(\"example\", \"PDB/a_structure.cif\")        pdb_writer = PDBIO()        pdb_writer.set_structure(cif_struct)        filenumber, filename = tempfile.mkstemp()        pdb_writer.save(filename, preserve_atom_numbering=False)",
        "labels_text": "Convert mmCIF to PDB and renumber atom serial"
    },
    {
        "input_text": "summarize: def test_conversion_preserve_numbering(self):                cif_parser = MMCIFParser(QUIET=1)        cif_struct = cif_parser.get_structure(\"example\", \"PDB/a_structure.cif\")        pdb_writer = PDBIO()        pdb_writer.set_structure(cif_struct)        filenumber, filename = tempfile.mkstemp()        with self.assertRaises(PDBIOException):            pdb_writer.save(filename, preserve_atom_numbering=True)",
        "labels_text": "Convert mmCIF to PDB and preserve original serial numbering"
    },
    {
        "input_text": "summarize: def test_NACCESS_rsa_file(self):                with open(\"PDB/1A8O.rsa\") as rsa:            naccess = process_rsa_data(rsa)        self.assertEqual(len(naccess), 66)",
        "labels_text": "Test parsing of pregenerated rsa NACCESS file"
    },
    {
        "input_text": "summarize: def test_NACCESS_asa_file(self):                with open(\"PDB/1A8O.asa\") as asa:            naccess = process_asa_data(asa)        self.assertEqual(len(naccess), 524)",
        "labels_text": "Test parsing of pregenerated asa NACCESS file"
    },
    {
        "input_text": "summarize: def test_parse_header_line(self):                header = parse_pdb_header(\"PDB/header.pdb\")        self.assertEqual(header[\"head\"], \"structural genomics, unknown function\")        self.assertEqual(header[\"idcode\"], \"3EFG\")        self.assertEqual(header[\"deposition_date\"], \"2008-09-08\")",
        "labels_text": "Unit test for parsing and converting field in HEADER record"
    },
    {
        "input_text": "summarize: def test_parse_title_line(self):                header = parse_pdb_header(\"PDB/1LCD.pdb\")        self.assertEqual(            header[\"name\"],            \"structure of the complex of lac repressor headpiece and an 11 \"            \"base-pair half-operator determined by nuclear magnetic resonance \"            \"spectroscopy and restrained molecular dynamics\",        )",
        "labels_text": "Unit test for correct parsing of multiline title record"
    },
    {
        "input_text": "summarize: def test_parse_no_title(self):                header = parse_pdb_header(\"PDB/occupancy.pdb\")        self.assertEqual(header[\"name\"], \"\")",
        "labels_text": "Unit test for sensible result with no TITLE line"
    },
    {
        "input_text": "summarize: def test_pdbio_write_x_element(self):                struct1 = self.structure        # Change element of one atom        atom = next(struct1.get_atoms())        atom.element = \"X\"  # X is assigned in Atom.py as last resort        self.io.set_structure(struct1)        filenumber, filename = tempfile.mkstemp()        os.close(filenumber)        try:            self.io.save(filename)        finally:            os.remove(filename)",
        "labels_text": "Write a structure with atomic element X with PDBIO"
    },
    {
        "input_text": "summarize: def test_pdbio_write_unk_element(self):                struct1 = self.structure        atom = next(struct1.get_atoms())        atom.element = \"1\"        self.io.set_structure(struct1)        filenumber, filename = tempfile.mkstemp()        os.close(filenumber)        with self.assertRaises(PDBIOException):            self.io.save(filename)        self.assertFalse(os.path.exists(filename))",
        "labels_text": "PDBIO raise PDBIOException when writing unrecognised atomic element"
    },
    {
        "input_text": "summarize: def test_get_recent_changes(self):                # obsolete_pdb declared to prevent from creating the \"obsolete\" directory        pdblist = PDBList(obsolete_pdb=\"unimportant\")        url = pdblist.pdb_server + \"/pub/pdb/data/status/latest/added.pdb\"        entries = pdblist.get_status_list(url)        self.assertIsNotNone(entries)",
        "labels_text": "Tests the BioPDBPDBListgetrecentchanges method"
    },
    {
        "input_text": "summarize: def test_get_all_entries(self):                # obsolete_pdb declared to prevent from creating the \"obsolete\" directory        pdblist = PDBList(obsolete_pdb=\"unimportant\")        entries = pdblist.get_all_entries()        # As number of entries constantly grow, test checks if a certain number was        # exceeded        self.assertGreater(len(entries), 100000)",
        "labels_text": "Tests the BioPDBPDBListgetallentries method"
    },
    {
        "input_text": "summarize: def test_get_all_obsolete(self):                # obsolete_pdb declared to prevent from creating the \"obsolete\" directory        pdblist = PDBList(obsolete_pdb=\"unimportant\")        entries = pdblist.get_all_obsolete()        # As number of obsolete entries constantly grow, test checks if a certain number        # was exceeded        self.assertGreater(len(entries), 3000)",
        "labels_text": "Tests the BioPDBPDBListgetallobsolete method"
    },
    {
        "input_text": "summarize: def test_get_all_assemblies(self):                # obsolete_pdb declared to prevent from creating the \"obsolete\" directory        pdblist = PDBList(obsolete_pdb=\"unimportant\")        entries = pdblist.get_all_assemblies()        # As number of obsolete entries constantly grow, test checks if a certain number        # was exceeded        self.assertGreater(len(entries), 100000)",
        "labels_text": "Tests the BioPDBPDBListgetallassemblies method"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_small_pdb(self):                structure = \"127d\"        self.check(            structure, os.path.join(structure[1:3], f\"pdb{structure}.ent\"), \"pdb\"        )",
        "labels_text": "Tests retrieving the small molecule in pdb format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_large_pdb(self):                structure = \"3k1q\"        self.check(            structure,            os.path.join(structure[1:3], f\"{structure}-pdb-bundle.tar\"),            \"bundle\",        )",
        "labels_text": "Tests retrieving the bundle for large molecule in pdblike format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_obsolete_pdb(self):                structure = \"347d\"        self.check(            structure,            os.path.join(\"obsolete\", structure[1:3], f\"pdb{structure}.ent\"),            \"pdb\",            obsolete=True,        )",
        "labels_text": "Tests retrieving the obsolete molecule in pdb format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_obsolete_mmcif(self):                structure = \"347d\"        self.check(            structure,            os.path.join(\"obsolete\", structure[1:3], f\"{structure}.cif\"),            \"mmCif\",            obsolete=True,        )",
        "labels_text": "Tests retrieving the obsolete molecule in mmcif format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_mmcif(self):                structure = \"127d\"        self.check(structure, os.path.join(structure[1:3], f\"{structure}.cif\"), \"mmCif\")",
        "labels_text": "Tests retrieving the nonobsolete molecule in mmcif format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_obsolete_xml(self):                structure = \"347d\"        self.check(            structure,            os.path.join(\"obsolete\", structure[1:3], f\"{structure}.xml\"),            \"xml\",            obsolete=True,        )",
        "labels_text": "Tests retrieving the obsolete molecule in mmcif format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_xml(self):                structure = \"127d\"        self.check(structure, os.path.join(structure[1:3], f\"{structure}.xml\"), \"xml\")",
        "labels_text": "Tests retrieving the non obsolete molecule in xml format"
    },
    {
        "input_text": "summarize: def test_retrieve_pdb_file_mmtf(self):                structure = \"127d\"        self.check(structure, os.path.join(structure[1:3], f\"{structure}.mmtf\"), \"mmtf\")",
        "labels_text": "Tests retrieving the molecule in mmtf format"
    },
    {
        "input_text": "summarize: def test_double_retrieve_structure(self):                structure = \"127d\"        self.check(structure, os.path.join(\"a\", f\"{structure}.cif\"), \"mmCif\", pdir=\"a\")        self.check(structure, os.path.join(\"b\", f\"{structure}.cif\"), \"mmCif\", pdir=\"b\")",
        "labels_text": "Tests retrieving the same file to different directory"
    },
    {
        "input_text": "summarize: def test_retrieve_assembly_file_mmcif(self):                structure = \"127d\"        assembly_num = \"1\"        self.check(            structure,            assembly_num,            os.path.join(structure[1:3], f\"{structure}-assembly{assembly_num}.cif\"),            \"mmCif\",        )",
        "labels_text": "Tests retrieving a small assembly in mmCif format"
    },
    {
        "input_text": "summarize: def test_retrieve_assembly_file_pdb(self):                structure = \"127d\"        assembly_num = \"1\"        self.check(            structure,            assembly_num,            os.path.join(structure[1:3], f\"{structure}.pdb{assembly_num}\"),            \"pdb\",        )",
        "labels_text": "Tests retrieving a small assembly in pdb format"
    },
    {
        "input_text": "summarize: def test_2_flawedpdb_strict(self):                with warnings.catch_warnings(record=True) as w:            warnings.simplefilter(\"always\", PDBConstructionWarning)            self.assertRaises(                PDBConstructionException,                self.strict.get_structure,                \"example\",                \"PDB/a_structure.pdb\",            )            self.assertEqual(len(w), 4, w)",
        "labels_text": "Parse a flawed PDB file in permissive mode check error"
    },
    {
        "input_text": "summarize: def test_3_bad_xyz_permissive(self):                data = \"ATOM      9  N   ASP A 152      21.554  34.953  27.691  1.00 19.26           N\\n\"        _ = self.permissive.get_structure(\"example\", StringIO(data))",
        "labels_text": "Parse an entry with bad xyz value with PERMISSIVETrue"
    },
    {
        "input_text": "summarize: def test_4_bad_xyz_strict(self):                data = \"ATOM      9  N   ASP A 152      21.ish  34.953  27.691  1.00 19.26           N\\n\"        with self.assertRaises(PDBConstructionException):            self.strict.get_structure(\"example\", StringIO(data))",
        "labels_text": "Parse an entry with bad xyz value with PERMISSIVEFalse"
    },
    {
        "input_text": "summarize: def test_6_missing_occupancy_strict(self):                with self.assertRaises(PDBConstructionException):            _ = self.strict.get_structure(\"test\", \"PDB/occupancy.pdb\")",
        "labels_text": "Parse file with missing occupancy with PERMISSIVEFalse"
    },
    {
        "input_text": "summarize: def test_empty(self):                handle = StringIO()        with self.assertRaises(ValueError) as context_manager:            _ = self.permissive.get_structure(\"MT\", handle)        self.assertEqual(str(context_manager.exception), \"Empty file.\")",
        "labels_text": "Parse an empty file"
    },
    {
        "input_text": "summarize: def check_msms(self, prot_file, first_100_residues):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            s = self.pdbparser.get_structure(\"X\", prot_file)        model = s[0]        rd = ResidueDepth(model)        residues = []        for item in rd.property_list[:100]:            residues.append(item[0].get_resname())        self.assertEqual(\"\".join(residues), first_100_residues)",
        "labels_text": "Wrap call to MSMS and the respective test"
    },
    {
        "input_text": "summarize: def test_default_algorithm(self):                m = copy.deepcopy(self.model)  # modifies atom.sasa        sasa = ShrakeRupley()        sasa.compute(m)        result = [a.sasa for a in m.get_atoms()][:5]        expected = [50.36, 31.40, 10.87, 12.86, 2.42]        for a, b in zip(result, expected):            self.assertAlmostEqual(a, b, places=2)",
        "labels_text": "Run ShrakeRupley with default parameter"
    },
    {
        "input_text": "summarize: def test_level_R(self):                m = copy.deepcopy(self.model)  # modifies atom.sasa        sasa = ShrakeRupley()        sasa.compute(m, level=\"R\")        for r in m.get_residues():            atom_sum = sum(a.sasa for a in r)            self.assertAlmostEqual(atom_sum, r.sasa, places=2)",
        "labels_text": "Run ShrakeRupley with level R"
    },
    {
        "input_text": "summarize: def test_level_C(self):                m = copy.deepcopy(self.model)  # modifies atom.sasa        sasa = ShrakeRupley()        sasa.compute(m, level=\"C\")        for c in m.get_chains():            atom_sum = sum(a.sasa for a in c.get_atoms())            self.assertAlmostEqual(atom_sum, c.sasa, places=2)",
        "labels_text": "Run ShrakeRupley with level C"
    },
    {
        "input_text": "summarize: def test_fail_probe_radius(self):                with self.assertRaisesRegex(ValueError, \"must be a positive number\"):            sasa = ShrakeRupley(probe_radius=-1.40)",
        "labels_text": "Raise exception on bad proberadius parameter"
    },
    {
        "input_text": "summarize: def test_fail_n_points(self):                with self.assertRaisesRegex(ValueError, \"must be larger than 1\"):            sasa = ShrakeRupley(n_points=0)",
        "labels_text": "Raise exception on bad npoints parameter"
    },
    {
        "input_text": "summarize: def test_fail_compute_entity_type(self):                with self.assertRaisesRegex(ValueError, \"Invalid entity type\"):            sasa = ShrakeRupley()            sasa.compute([1, 2, 3, 4, 5])",
        "labels_text": "Raise exception on unsupported entity type"
    },
    {
        "input_text": "summarize: def test_fail_compute_entity_level(self):                atom = list(self.model.get_atoms())[0]        with self.assertRaisesRegex(ValueError, \"Invalid entity type\"):            sasa = ShrakeRupley()            sasa.compute(atom)",
        "labels_text": "Raise exception on input Atom entity"
    },
    {
        "input_text": "summarize: def test_fail_compute_level_1(self):                with self.assertRaisesRegex(ValueError, \"Invalid level\"):            sasa = ShrakeRupley()            sasa.compute(self.model, level=\"X\")",
        "labels_text": "Raise exception on invalid level parameter X"
    },
    {
        "input_text": "summarize: def test_fail_compute_level_2(self):                chain = self.model[\"A\"]        with self.assertRaisesRegex(ValueError, \"be equal or smaller than\"):            sasa = ShrakeRupley()            sasa.compute(chain, level=\"S\")",
        "labels_text": "Raise exception on invalid level parameter S C"
    },
    {
        "input_text": "summarize: def test_fail_empty_entity(self):                sasa = ShrakeRupley()        r = copy.deepcopy(self.model[\"A\"].child_list[0])        for a in list(r):            r.detach_child(a.name)  # empty residue        self.assertEqual(len(r.child_list), 0)        with self.assertRaisesRegex(ValueError, \"Entity has no child atoms\"):            sasa.compute(r)",
        "labels_text": "Raise exception on invalid level parameter S C"
    },
    {
        "input_text": "summarize: def res_full_id(res: Residue):        return (res.get_resname(), *res.get_id())",
        "labels_text": "Return full residue identifier for thoroughly comparing residue"
    },
    {
        "input_text": "summarize: def test_assign_unknown_element(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            a = Atom.Atom(                \"XE1\", None, None, None, None, \" XE1\", None  # serial 5170 - 4CP4            )        self.assertEqual(a.element, \"X\")",
        "labels_text": "Unknown element is assigned X"
    },
    {
        "input_text": "summarize: def test_ions(self):                parser = PDBParser(PERMISSIVE=True)        structure = parser.get_structure(\"X\", \"PDB/ions.pdb\")        # check magnesium atom        atoms = structure[0][\"A\"][(\"H_MG\", 1, \" \")].child_list        self.assertEqual(\"MG\", atoms[0].element)",
        "labels_text": "Element for magnesium is assigned correctly"
    },
    {
        "input_text": "summarize: def test_get_chains(self):                chains = [chain.id for chain in self.structure.get_chains()]        self.assertEqual(chains, [\"A\", \"A\", \"B\", \"C\", \" \"])",
        "labels_text": "Yields chain from different model separately"
    },
    {
        "input_text": "summarize: def test_get_residues(self):                residues = [resi.id for resi in self.structure.get_residues()]        self.assertEqual(len(residues), 179)",
        "labels_text": "Yields all residue from all model"
    },
    {
        "input_text": "summarize: def test_get_atoms(self):                atoms = [            \"%12s\" % str((atom.id, atom.altloc)) for atom in self.structure.get_atoms()        ]        self.assertEqual(len(atoms), 835)",
        "labels_text": "Yields all atom from the structure excluding duplicate and ALTLOCs which are not parsed"
    },
    {
        "input_text": "summarize: def test_change_model_id(self):                for model in self.structure:            break  # Get first model in structure        model.id = 2        self.assertEqual(model.id, 2)        self.assertIn(2, self.structure)        self.assertNotIn(0, self.structure)",
        "labels_text": "Change the id of a model"
    },
    {
        "input_text": "summarize: def test_change_model_id_warns(self):                model = next(iter(self.structure))        with self.assertWarns(BiopythonWarning):            model.id = 1        # make sure children were not overwritten        self.assertEqual(model.id, 1)        self.assertEqual(len(self.structure.child_list), 2)        self.assertIn(1, self.structure)",
        "labels_text": "Warning when changing id to a value already in use by another child"
    },
    {
        "input_text": "summarize: def test_change_chain_id(self):                chain = next(iter(self.structure.get_chains()))        chain.id = \"R\"        self.assertEqual(chain.id, \"R\")        model = next(iter(self.structure))        self.assertIn(\"R\", model)",
        "labels_text": "Change the id of a model"
    },
    {
        "input_text": "summarize: def test_change_id_to_self(self):                chain = next(iter(self.structure.get_chains()))        chain_id = chain.id        chain.id = chain_id        self.assertEqual(chain.id, chain_id)",
        "labels_text": "Changing the id to itself doe nothing doe not raise"
    },
    {
        "input_text": "summarize: def test_change_residue_id(self):                chain = next(iter(self.structure.get_chains()))        res = chain[(\"H_PCA\", 1, \" \")]        res.id = (\" \", 1, \" \")        self.assertEqual(res.id, (\" \", 1, \" \"))        self.assertIn((\" \", 1, \" \"), chain)        self.assertNotIn((\"H_PCA\", 1, \" \"), chain)        self.assertEqual(chain[(\" \", 1, \" \")], res)",
        "labels_text": "Change the id of a residue"
    },
    {
        "input_text": "summarize: def get_total_pos(self, o):                if hasattr(o, \"get_coord\"):            return o.get_coord(), 1        total_pos = np.array((0.0, 0.0, 0.0))        total_count = 0        for p in o.get_list():            pos, count = self.get_total_pos(p)            total_pos += pos            total_count += count        return total_pos, total_count",
        "labels_text": "Sum of position of atom in an entity along with the number of atom"
    },
    {
        "input_text": "summarize: def get_pos(self, o):                pos, count = self.get_total_pos(o)        return pos / count",
        "labels_text": "Average atom position in an entity"
    },
    {
        "input_text": "summarize: def test_entity_copy(self):                for e in (self.s, self.m, self.c, self.r):            ee = e.copy()            self.assertIsNot(e, ee)            self.assertIsNot(e.get_list()[0], ee.get_list()[0])",
        "labels_text": "Make a copy of a residue"
    },
    {
        "input_text": "summarize: def test_structure_com(self):                com = self.structure.center_of_mass()        self.assertTrue(np.allclose(com, [19.870, 25.455, 28.753], atol=1e-3))",
        "labels_text": "Calculate Structure center of mass"
    },
    {
        "input_text": "summarize: def test_structure_cog(self):                cog = self.structure.center_of_mass(geometric=True)        self.assertTrue(np.allclose(cog, [19.882, 25.842, 28.333], atol=1e-3))",
        "labels_text": "Calculate Structure center of geometry"
    },
    {
        "input_text": "summarize: def test_com_empty_structure(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            s = self.parser.get_structure(\"b\", \"PDB/disordered.pdb\")  # smaller        for child in list(s):            s.detach_child(child.id)        with self.assertRaises(ValueError):            s.center_of_mass()",
        "labels_text": "Center of mass of empty structure raise ValueError"
    },
    {
        "input_text": "summarize: def test_division(self):                v = Vector(1, 1, 1) / 2        self.assertEqual(repr(v), \"<Vector 0.50, 0.50, 0.50>\")",
        "labels_text": "Confirm division work"
    },
    {
        "input_text": "summarize: def test_rotmat_180(self):                v1 = Vector([1.0, 0.8, 0])        v2 = Vector([-1.0, -0.8, 0])        rot = rotmat(v1, v2)        v3 = v1.left_multiply(rot)        self.assertTrue(np.allclose(v2.get_array(), v3.get_array()))",
        "labels_text": "Test rotmat when the rotation is deg singularity"
    },
    {
        "input_text": "summarize: def test_rotmat_0(self):                v1 = Vector([1.0, 0.8, 0])        v2 = Vector([1.0, 0.8, 0])        rot = rotmat(v1, v2)        v3 = v1.left_multiply(rot)        self.assertTrue(np.allclose(v1.get_array(), v3.get_array()))",
        "labels_text": "Test rotmat when the rotation is deg singularity"
    },
    {
        "input_text": "summarize: def test_m2rotaxis_90(self):                v1 = Vector(0, 0, 1)        v2 = Vector(0, 1, 0)        rot = rotmat(v1, v2)        angle, axis = m2rotaxis(rot)        self.assertTrue(np.allclose(axis.get_array(), [-1.0, 0.0, 0.0]))        self.assertLess(abs(angle - np.pi / 2), 1e-5)",
        "labels_text": "Test deg rotation"
    },
    {
        "input_text": "summarize: def test_m2rotaxis_180(self):                v1 = Vector([1.0, 0.8, 0])        v2 = Vector([-1.0, -0.8, 0])        rot = rotmat(v1, v2)        angle, axis = m2rotaxis(rot)        self.assertLess(abs(axis * v1), 1e-5)  # axis orthogonal to v1        self.assertLess(abs(angle - np.pi), 1e-5)",
        "labels_text": "Test deg rotation"
    },
    {
        "input_text": "summarize: def test_m2rotaxis_0(self):                v1 = Vector([1.0, 0.8, 0])        v2 = Vector([1.0, 0.8, 0])        rot = rotmat(v1, v2)        angle, axis = m2rotaxis(rot)        self.assertTrue(np.allclose(axis.get_array(), [1, 0, 0]))        self.assertLess(abs(angle), 1e-5)",
        "labels_text": "Test deg rotation Axis must be a per Vector doc"
    },
    {
        "input_text": "summarize: def test_PlateRecord_errors(self):                self.assertRaises(            ValueError, phenotype.phen_micro.PlateRecord, \"test\", [1, 2, 3]        )        self.assertRaises(TypeError, phenotype.phen_micro.PlateRecord, \"test\", 1)",
        "labels_text": "Test bad argument with PlateRecord object"
    },
    {
        "input_text": "summarize: def test_JsonIterator(self):                # Parse file content big enough to trigger issue #3783        handle = StringIO(            '{\"csv_data\": {\"Plate Type\": \"PM-999\"}, \"measurements\": {\"Hour\": 9}}'        )        with self.assertWarnsRegex(UserWarning, \"PM-999\"):            for w in phenotype.phen_micro.JsonIterator(handle):                self.assertEqual(w.id, \"PM999\")",
        "labels_text": "Test basic functionality of JsonIterator file parser"
    },
    {
        "input_text": "summarize: def test_CsvIterator(self):                # Parse file content big enough to trigger issue #3783        handle = StringIO('\"Data File\",3\\n\"Plate Type\",PM-33\\n')        with self.assertWarnsRegex(UserWarning, \"PM-33\"):            for w in phenotype.phen_micro.CsvIterator(handle):                self.assertEqual(w.id, \"PM33\")",
        "labels_text": "Test basic functionality of CsvIterator file parser"
    },
    {
        "input_text": "summarize: def test_newick_read_single1(self):                tree = Phylo.read(EX_NEWICK, \"newick\")        self.assertEqual(len(tree.get_terminals()), 28)",
        "labels_text": "Read first Newick file with one tree"
    },
    {
        "input_text": "summarize: def test_newick_read_single3(self):                tree = Phylo.read(EX_NEXUS2, \"nexus\")        self.assertEqual(len(tree.get_terminals()), 658)",
        "labels_text": "Read Nexus file with one tree"
    },
    {
        "input_text": "summarize: def test_unicode_exception(self):                with open(EX_NEWICK_BOM, encoding=\"utf-8\") as handle:            tree = Phylo.read(handle, \"newick\")        self.assertEqual(len(tree.get_terminals()), 3)",
        "labels_text": "Read a Newick file with a unicode byte order mark BOM"
    },
    {
        "input_text": "summarize: def test_newick_read_multiple(self):                trees = list(Phylo.parse(EX_NEXUS, \"nexus\"))        self.assertEqual(len(trees), 3)        for tree in trees:            self.assertEqual(len(tree.get_terminals()), 9)",
        "labels_text": "Parse a Nexus file with multiple tree"
    },
    {
        "input_text": "summarize: def test_newick_read_scinot(self):                tree = Phylo.read(StringIO(\"(foo:1e-1,bar:0.1)\"), \"newick\")        clade_a = tree.clade[0]        self.assertEqual(clade_a.name, \"foo\")        self.assertAlmostEqual(clade_a.branch_length, 0.1)",
        "labels_text": "Parse Newick branch length in scientific notation"
    },
    {
        "input_text": "summarize: def test_convert_phyloxml_binary(self):                trees = Phylo.parse(\"PhyloXML/phyloxml_examples.xml\", \"phyloxml\")        with tempfile.NamedTemporaryFile(mode=\"wb\") as out_handle:            self.assertRaises(TypeError, Phylo.write, trees, out_handle, \"phyloxml\")",
        "labels_text": "Try writing phyloxml to a binary handle fail on Py"
    },
    {
        "input_text": "summarize: def test_convert_phyloxml_text(self):                trees = Phylo.parse(\"PhyloXML/phyloxml_examples.xml\", \"phyloxml\")        with tempfile.NamedTemporaryFile(mode=\"w\") as out_handle:            count = Phylo.write(trees, out_handle, \"phyloxml\")        self.assertEqual(13, count)",
        "labels_text": "Write phyloxml to a text handle"
    },
    {
        "input_text": "summarize: def test_int_labels(self):                tree = Phylo.read(            StringIO(\"(((0:0.1,1:0.1)0.99:0.1,2:0.1)0.98:0.0);\"), \"newick\"        )        self.assertEqual({leaf.name for leaf in tree.get_terminals()}, {\"0\", \"1\", \"2\"})",
        "labels_text": "Read newick formatted tree with numeric label"
    },
    {
        "input_text": "summarize: def test_str(self):                for source, count in zip((EX_APAF, EX_BCL2, EX_DIST), (386, 747, 15)):            tree = Phylo.read(source, \"phyloxml\")            output = str(tree)            self.assertEqual(len(output.splitlines()), count)",
        "labels_text": "Treestr prettyprint to a string NB The exact line count are liable to change if the object constructor change"
    },
    {
        "input_text": "summarize: def test_get_path(self):                path = self.phylogenies[1].get_path(\"B\")        self.assertEqual(len(path), 2)        self.assertAlmostEqual(path[0].branch_length, 0.06)        self.assertAlmostEqual(path[1].branch_length, 0.23)        self.assertEqual(path[1].name, \"B\")",
        "labels_text": "TreeMixin getpath method"
    },
    {
        "input_text": "summarize: def test_trace(self):                tree = self.phylogenies[1]        path = tree.trace(\"A\", \"C\")        self.assertEqual(len(path), 3)        self.assertAlmostEqual(path[0].branch_length, 0.06)        self.assertAlmostEqual(path[2].branch_length, 0.4)        self.assertEqual(path[2].name, \"C\")",
        "labels_text": "TreeMixin trace method"
    },
    {
        "input_text": "summarize: def test_depths(self):                tree = self.phylogenies[1]        depths = tree.depths()        self.assertEqual(len(depths), 5)        for found, expect in zip(            sorted(depths.values()), [0, 0.060, 0.162, 0.290, 0.400]        ):            self.assertAlmostEqual(found, expect)",
        "labels_text": "TreeMixin depth method"
    },
    {
        "input_text": "summarize: def test_is_bifurcating(self):                for tree, is_b in zip(            self.phylogenies, (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)        ):            self.assertEqual(tree.is_bifurcating(), is_b)",
        "labels_text": "TreeMixin isbifurcating method"
    },
    {
        "input_text": "summarize: def test_total_branch_length(self):                tree = self.phylogenies[1]        self.assertAlmostEqual(tree.total_branch_length(), 0.792)        self.assertAlmostEqual(tree.clade[0].total_branch_length(), 0.392)",
        "labels_text": "TreeMixin totalbranchlength method"
    },
    {
        "input_text": "summarize: def test_collapse(self):                tree = self.phylogenies[1]        parent = tree.collapse(tree.clade[0])        self.assertEqual(len(parent), 3)        for clade, name, blength in zip(parent, (\"C\", \"A\", \"B\"), (0.4, 0.162, 0.29)):            self.assertEqual(clade.name, name)            self.assertAlmostEqual(clade.branch_length, blength)",
        "labels_text": "TreeMixin collapse method"
    },
    {
        "input_text": "summarize: def test_ladderize(self):                def ordered_names(tree):            return [n.name for n in tree.get_terminals()]        tree = self.phylogenies[10]        self.assertEqual(ordered_names(tree), list(\"ABCD\"))        tree.ladderize()        self.assertEqual(ordered_names(tree), list(\"DABC\"))        tree.ladderize(reverse=True)        self.assertEqual(ordered_names(tree), list(\"ABCD\"))",
        "labels_text": "TreeMixin ladderize method"
    },
    {
        "input_text": "summarize: def _test_read_factory(source, count):        fname = os.path.basename(source)    def test_read(self):        phx = PhyloXMLIO.read(source)        self.assertTrue(phx)        self.assertEqual(len(phx), count[0])        self.assertEqual(len(phx.other), count[1])    test_read.__doc__ = f\"Read {fname} to produce a phyloXML object.\"    return test_read",
        "labels_text": "Generate a test method for reading the given source The generated function read an example file to produce a phyloXML object then test for existence of the root node and count the number of phylogeny under the root"
    },
    {
        "input_text": "summarize: def _test_parse_factory(source, count):        fname = os.path.basename(source)    def test_parse(self):        trees = PhyloXMLIO.parse(source)        self.assertEqual(len(list(trees)), count)    test_parse.__doc__ = f\"Parse the phylogenies in {fname}.\"    return test_parse",
        "labels_text": "Generate a test method for parseing the given source The generated function extract each phylogenetic tree using the parse function and count the total number of tree extracted"
    },
    {
        "input_text": "summarize: def test_Phyloxml(self):                phx = PhyloXMLIO.read(EX_PHYLO)        self.assertIsInstance(phx, PX.Phyloxml)        for tree in phx:            self.assertIsInstance(tree, PX.Phylogeny)        for otr in phx.other:            self.assertIsInstance(otr, PX.Other)",
        "labels_text": "Instantiation of Phyloxml object"
    },
    {
        "input_text": "summarize: def test_Annotation(self):                tree = list(PhyloXMLIO.parse(EX_PHYLO))[3]        ann = tree.clade[1].sequences[0].annotations[0]        self.assertIsInstance(ann, PX.Annotation)        self.assertEqual(ann.desc, \"alcohol dehydrogenase\")        self.assertAlmostEqual(ann.confidence.value, 0.67)        self.assertEqual(ann.confidence.type, \"probability\")",
        "labels_text": "Instantiation of Annotation object"
    },
    {
        "input_text": "summarize: def test_CladeRelation(self):                tree = list(PhyloXMLIO.parse(EX_PHYLO))[6]        crel = tree.clade_relations[0]        self.assertIsInstance(crel, PX.CladeRelation)        self.assertEqual(crel.id_ref_0, \"b\")        self.assertEqual(crel.id_ref_1, \"c\")        self.assertEqual(crel.type, \"network_connection\")",
        "labels_text": "Instantiation of CladeRelation object"
    },
    {
        "input_text": "summarize: def test_Events(self):                tree = list(PhyloXMLIO.parse(EX_PHYLO))[4]        event_s = tree.clade.events        self.assertIsInstance(event_s, PX.Events)        self.assertEqual(event_s.speciations, 1)        event_d = tree.clade[0].events        self.assertIsInstance(event_d, PX.Events)        self.assertEqual(event_d.duplications, 1)",
        "labels_text": "Instantiation of Events object"
    },
    {
        "input_text": "summarize: def test_Reference(self):                with open(EX_DOLLO) as handle:            tree = next(PhyloXMLIO.parse(handle))        reference = tree.clade[0, 0, 0, 0, 0, 0].references[0]        self.assertIsInstance(reference, PX.Reference)        self.assertEqual(reference.doi, \"10.1038/nature06614\")        self.assertIsNone(reference.desc)",
        "labels_text": "Instantiation of Reference object"
    },
    {
        "input_text": "summarize: def test_Uri(self):                tree = list(PhyloXMLIO.parse(EX_PHYLO))[9]        uri = tree.clade.taxonomies[0].uri        self.assertIsInstance(uri, PX.Uri)        self.assertEqual(uri.desc, \"EMBL REPTILE DATABASE\")        self.assertEqual(            uri.value, \"http://www.embl-heidelberg.de/~uetz/families/Varanidae.html\"        )",
        "labels_text": "Instantiation of Uri object"
    },
    {
        "input_text": "summarize: def test_apaf(self):                global EX_APAF        orig_fname = EX_APAF        try:            EX_APAF = DUMMY            self._rewrite_and_call(                orig_fname,                (                    (                        ParseTests,                        [\"test_read_apaf\", \"test_parse_apaf\", \"test_shape_apaf\"],                    ),                    (TreeTests, [\"test_DomainArchitecture\"]),                ),            )        finally:            EX_APAF = orig_fname",
        "labels_text": "Roundtrip parsing and serialization of apafxml"
    },
    {
        "input_text": "summarize: def test_bcl2(self):                global EX_BCL2        orig_fname = EX_BCL2        try:            EX_BCL2 = DUMMY            self._rewrite_and_call(                orig_fname,                (                    (                        ParseTests,                        [\"test_read_bcl2\", \"test_parse_bcl2\", \"test_shape_bcl2\"],                    ),                    (TreeTests, [\"test_Confidence\"]),                ),            )        finally:            EX_BCL2 = orig_fname",
        "labels_text": "Roundtrip parsing and serialization of bclxml"
    },
    {
        "input_text": "summarize: def test_made(self):                global EX_MADE        orig_fname = EX_MADE        try:            EX_MADE = DUMMY            self._rewrite_and_call(                orig_fname,                (                    (ParseTests, [\"test_read_made\", \"test_parse_made\"]),                    (TreeTests, [\"test_Confidence\", \"test_Polygon\"]),                ),            )        finally:            EX_MADE = orig_fname",
        "labels_text": "Roundtrip parsing and serialization of madeupxml"
    },
    {
        "input_text": "summarize: def test_dollo(self):                global EX_DOLLO        orig_fname = EX_DOLLO        try:            EX_DOLLO = DUMMY            self._rewrite_and_call(                orig_fname,                (                    (ParseTests, [\"test_read_dollo\", \"test_parse_dollo\"]),                    (TreeTests, [\"test_BinaryCharacters\"]),                ),            )        finally:            EX_DOLLO = orig_fname",
        "labels_text": "Roundtrip parsing and serialization of otolddolloxml"
    },
    {
        "input_text": "summarize: def test_clade_to_phylogeny(self):                clade = self.phyloxml.phylogenies[0].clade[0]        tree = clade.to_phylogeny(rooted=True)        self.assertIsInstance(tree, PX.Phylogeny)",
        "labels_text": "Convert a Clade object to a new Phylogeny"
    },
    {
        "input_text": "summarize: def test_phylogeny_to_phyloxml(self):                tree = self.phyloxml.phylogenies[0]        doc = tree.to_phyloxml_container()        self.assertIsInstance(doc, PX.Phyloxml)",
        "labels_text": "Convert a Phylogeny object to a new Phyloxml"
    },
    {
        "input_text": "summarize: def test_phyloxml_getitem(self):                self.assertIs(self.phyloxml.phylogenies[9], self.phyloxml[9])        self.assertIs(self.phyloxml[\"monitor lizards\"], self.phyloxml[9])        self.assertEqual(len(self.phyloxml[:]), len(self.phyloxml))",
        "labels_text": "Phyloxmlgetitem get phylogeny by name or index"
    },
    {
        "input_text": "summarize: def test_color_hex(self):                black = PX.BranchColor(0, 0, 0)        self.assertEqual(black.to_hex(), \"#000000\")        white = PX.BranchColor(255, 255, 255)        self.assertEqual(white.to_hex(), \"#ffffff\")        green = PX.BranchColor(14, 192, 113)        self.assertEqual(green.to_hex(), \"#0ec071\")",
        "labels_text": "BranchColor tohex method"
    },
    {
        "input_text": "summarize: def _test_parse_factory(source):        filename = os.path.join(\"CDAO/\", source)    def test_parse(self):        trees = list(bp._io.parse(filename, \"cdao\"))    test_parse.__doc__ = f\"Parse the phylogenies in {source}.\"    return test_parse",
        "labels_text": "Generate a test method for parseing the given source The generated function extract each phylogenetic tree using the parse function"
    },
    {
        "input_text": "summarize: def test_to_igraph_exd_ollo(self):                tree = Phylo.read(EX_DOLLO, \"phyloxml\")        graph = Phylo.to_igraph(tree)        self.assertEqual(graph.vcount(), 659)        self.assertEqual(graph.ecount(), graph.vcount() - 1)",
        "labels_text": "Tree to Graph conversion if networkx is available"
    },
    {
        "input_text": "summarize: def test_draw_ascii(self):                handle = StringIO()        tree = Phylo.read(EX_APAF, \"phyloxml\")        Phylo.draw_ascii(tree, file=handle)        Phylo.draw_ascii(tree, file=handle, column_width=120)        handle.close()",
        "labels_text": "Tree to Graph conversion"
    },
    {
        "input_text": "summarize: def test_to_networkx(self):                tree = Phylo.read(EX_DOLLO, \"phyloxml\")        G = Phylo.to_networkx(tree)        self.assertEqual(len(G.nodes()), 659)",
        "labels_text": "Tree to Graph conversion if networkx is available"
    },
    {
        "input_text": "summarize: def test_parse(self):                for filename in nexml_files:            count = tree_counts.get(filename, 1)            path = os.path.join(\"NeXML\", filename)            msg = f\"Failed parser test for {path}\"            trees = list(Phylo.parse(path, \"nexml\"))            self.assertEqual(len(trees), count, msg=msg)",
        "labels_text": "Extract and count phylogenetic tree using Phyloparse"
    },
    {
        "input_text": "summarize: def test_write(self):                for filename in nexml_files:            count = tree_counts.get(filename, 1)            if count > 0:                path = os.path.join(\"NeXML\", filename)                self.check(path)",
        "labels_text": "Test for serialization of object to NeXML format"
    },
    {
        "input_text": "summarize: def test_allele_genotype_frequencies(self):                ctrl = GenePopController()        path = os.path.join(\"PopGen\", \"big.gen\")        pop_iter, locus_iter = ctrl.calc_allele_genotype_freqs(path)",
        "labels_text": "Test genepop execution on basic allele and genotype frequency"
    },
    {
        "input_text": "summarize: def test_fst_all(self):                ctrl = GenePopController()        path = os.path.join(\"PopGen\", \"c2line.gen\")        (allFis, allFst, allFit), itr = ctrl.calc_fst_all(path)        results = list(itr)        self.assertEqual(len(results), 3)        self.assertEqual(results[0][0], \"136255903\")        self.assertAlmostEqual(results[1][3], 0.335846)",
        "labels_text": "Test genepop execution on all fst"
    },
    {
        "input_text": "summarize: def test_haploidy(self):                ctrl = GenePopController()        path = os.path.join(\"PopGen\", \"haplo.gen\")        (allFis, allFst, allFit), itr = ctrl.calc_fst_all(path)        litr = list(itr)        self.assertNotIsInstance(allFst, int)        self.assertEqual(len(litr), 37)        self.assertEqual(litr[36][0], \"Locus37\")",
        "labels_text": "Test haploidy"
    },
    {
        "input_text": "summarize: def setUp(self):                # Genepop likes to be on the directory where the file is.        os.chdir(\"PopGen\")        self.ctrl = EasyController(\"big.gen\")",
        "labels_text": "Change working directory"
    },
    {
        "input_text": "summarize: def tearDown(self):                os.chdir(cur_dir)",
        "labels_text": "Restore working directory"
    },
    {
        "input_text": "summarize: def test_basic_info(self):                pops, loci = self.ctrl.get_basic_info()        self.assertEqual(len(pops), 10)        self.assertEqual(len(loci), 37)",
        "labels_text": "Test basic info"
    },
    {
        "input_text": "summarize: def test_get_heterozygosity_info(self):                hz_info = self.ctrl.get_heterozygosity_info(0, \"Locus2\")        self.assertEqual(hz_info[1], 24)        self.assertEqual(hz_info[3], 7)",
        "labels_text": "Test heterozygosity info"
    },
    {
        "input_text": "summarize: def test_get_alleles(self):                # Returns keys of a dict, so order is Python implementation dependent        self.assertCountEqual(self.ctrl.get_alleles(0, \"Locus3\"), [3, 20])",
        "labels_text": "Test get allele"
    },
    {
        "input_text": "summarize: def test_get_alleles_all_pops(self):                self.assertEqual(self.ctrl.get_alleles_all_pops(\"Locus4\"), [1, 3])",
        "labels_text": "Test get allele for all population"
    },
    {
        "input_text": "summarize: def test_get_fis(self):                alleles, overall = self.ctrl.get_fis(0, \"Locus2\")        self.assertEqual(alleles[3][0], 55)        self.assertEqual(overall[0], 62)",
        "labels_text": "Test get Fis"
    },
    {
        "input_text": "summarize: def test_get_allele_frequency(self):                tot_genes, alleles = self.ctrl.get_allele_frequency(0, \"Locus2\")        self.assertEqual(tot_genes, 62)        self.assertLess(abs(alleles[20] - 0.113), 0.05)",
        "labels_text": "Test allele frequency"
    },
    {
        "input_text": "summarize: def test_get_genotype_count(self):                self.assertEqual(len(self.ctrl.get_genotype_count(0, \"Locus2\")), 3)",
        "labels_text": "Test genotype count"
    },
    {
        "input_text": "summarize: def test_estimate_nm(self):                nms = self.ctrl.estimate_nm()        self.assertEqual(nms[0], 28.0)",
        "labels_text": "Test Nm estimation"
    },
    {
        "input_text": "summarize: def test_hwe_excess(self):                hwe_excess = self.ctrl.test_hw_pop(0, \"excess\")        self.assertEqual(hwe_excess[\"Locus1\"], (0.4955, None, -0.16, -0.1623, 5))",
        "labels_text": "Test HardyWeinberg Equilibrium"
    },
    {
        "input_text": "summarize: def test_get_avg_fis(self):                self.ctrl.get_avg_fis()",
        "labels_text": "Test average Fis"
    },
    {
        "input_text": "summarize: def test_get_multilocus_f_stats(self):                mf = self.ctrl.get_multilocus_f_stats()        self.assertEqual(len(mf), 3)        self.assertLess(mf[0], 0.1)",
        "labels_text": "Test multilocus F stats"
    },
    {
        "input_text": "summarize: def test_get_f_stats(self):                fs = self.ctrl.get_f_stats(\"Locus2\")        self.assertEqual(len(fs), 5)        self.assertLess(fs[0], 0)",
        "labels_text": "Test F stats"
    },
    {
        "input_text": "summarize: def test_record_basic(self):                r = GenePop.Record()        self.assertIsInstance(r.marker_len, int)        self.assertIsInstance(r.comment_line, str)        self.assertIsInstance(r.loci_list, list)        self.assertIsInstance(r.populations, list)",
        "labels_text": "Basic test on Record"
    },
    {
        "input_text": "summarize: def test_wrong_file_parser(self):                with open(os.path.join(\"PopGen\", \"README\")) as f:            self.assertRaises(ValueError, GenePop.read, f)",
        "labels_text": "Testing the ability to deal with wrongly formatted file"
    },
    {
        "input_text": "summarize: def test_wrong_file_parser(self):                with open(os.path.join(\"PopGen\", \"README\")) as f:            self.assertRaises(ValueError, GenePop.read, f)",
        "labels_text": "Testing the ability to deal with wrongly formatted file"
    },
    {
        "input_text": "summarize: def test_bad_xyz(self):                # Atom Entry        data = \"ATOM      1  N   PRO     1      00abc1  02.000 3.0000 -0.1000  1.0000       N\\n\"        # Get sole atom of this structure        parser = PDBParser(is_pqr=True)  # default initialization        self.assertRaises(            PDBConstructionException, parser.get_structure, \"example\", StringIO(data)        )",
        "labels_text": "Test if bad coordinate exception is raised"
    },
    {
        "input_text": "summarize: def test_Prank_simple(self):                cmdline = PrankCommandline(prank_exe)        cmdline.set_parameter(\"d\", self.infile1)        self.assertEqual(str(cmdline), _escape_filename(prank_exe) + \" -d=Fasta/fa01\")        self.assertEqual(str(eval(repr(cmdline))), str(cmdline))        output, error = cmdline()        self.assertEqual(error, \"\")        self.assertIn(\"Total time\", output)",
        "labels_text": "Simple roundtrip through app with infile output file written to cwd no way to redirect"
    },
    {
        "input_text": "summarize: def test_convert_to_fasta(self):                self.conversion(8, \"fas\", \"fasta\")",
        "labels_text": "Convert FASTA to FASTA format"
    },
    {
        "input_text": "summarize: def test_convert_to_phylip(self):                self.conversion(12, \"phy\", \"phylip\")",
        "labels_text": "Convert FASTA to PHYLIP format"
    },
    {
        "input_text": "summarize: def test_count_amino_acids(self):                for analysis in self.analyses:            count_dict = analysis.count_amino_acids()            for i in count_dict:                self.assertEqual(count_dict[i], self.text.count(i))",
        "labels_text": "Calculate amino acid count"
    },
    {
        "input_text": "summarize: def test_get_amino_acids_percent(self):                with self.assertWarns(BiopythonDeprecationWarning):            for analysis in self.analyses:                percent_dict = analysis.get_amino_acids_percent()                seq_len = len(self.text)                for i in percent_dict:                    self.assertAlmostEqual(                        percent_dict[i], self.text.count(i) / seq_len                    )",
        "labels_text": "Calculate amino acid percentage DEPRECATED"
    },
    {
        "input_text": "summarize: def test_amino_acids_percent(self):                for analysis in self.analyses:            seq_len = len(self.text)            for i in analysis.amino_acids_percent:                self.assertAlmostEqual(                    analysis.amino_acids_percent[i],                    (self.text.count(i) * 100 / seq_len),                )",
        "labels_text": "Calculate amino acid percentage"
    },
    {
        "input_text": "summarize: def test_get_molecular_weight(self):                for analysis in self.analyses:            self.assertAlmostEqual(analysis.molecular_weight(), 17103.16, 2)",
        "labels_text": "Calculate protein molecular weight"
    },
    {
        "input_text": "summarize: def test_get_monoisotopic_molecular_weight(self):                for sequence in self.sequences:            analysis = ProtParam.ProteinAnalysis(sequence, monoisotopic=True)            self.assertAlmostEqual(analysis.molecular_weight(), 17092.61, 2)",
        "labels_text": "Calculate monoisotopic molecular weight"
    },
    {
        "input_text": "summarize: def test_get_molecular_weight_identical(self):                # This test is somehow useless, since ProteinAnalysis.molecular_weight        # is internally calling SeqUtils.molecular_weight.        mw_2 = molecular_weight(self.text, seq_type=\"protein\")        for analysis in self.analyses:            mw_1 = analysis.molecular_weight()            self.assertAlmostEqual(mw_1, mw_2)",
        "labels_text": "Confirm protein molecular weight agrees with calculation from BioSeqUtils"
    },
    {
        "input_text": "summarize: def test_aromaticity(self):                for analysis in self.analyses:            # Old test used a number rounded to two digits, so use the same            self.assertAlmostEqual(analysis.aromaticity(), 0.10, 2)",
        "labels_text": "Calculate protein aromaticity"
    },
    {
        "input_text": "summarize: def test_instability_index(self):                for analysis in self.analyses:            # Old test used a number rounded to two digits, so use the same            self.assertAlmostEqual(analysis.instability_index(), 41.98, 2)",
        "labels_text": "Calculate protein instability index"
    },
    {
        "input_text": "summarize: def test_isoelectric_point(self):                for analysis in self.analyses:            # Old test used a number rounded to two digits, so use the same            self.assertAlmostEqual(analysis.isoelectric_point(), 7.72, 2)",
        "labels_text": "Calculate the isoelectric point"
    },
    {
        "input_text": "summarize: def test_charge_at_pH(self):                for analysis in self.analyses:            self.assertAlmostEqual(analysis.charge_at_pH(7.72), 0.00, 2)",
        "labels_text": "Test chargeatpH function"
    },
    {
        "input_text": "summarize: def test_secondary_structure_fraction(self):                for analysis in self.analyses:            helix, turn, sheet = analysis.secondary_structure_fraction()            # Old test used numbers rounded to two digits, so use the same            self.assertAlmostEqual(helix, 0.33, 2)            self.assertAlmostEqual(turn, 0.29, 2)            self.assertAlmostEqual(sheet, 0.37, 2)",
        "labels_text": "Calculate secondary structure fraction"
    },
    {
        "input_text": "summarize: def test_molar_extinction_coefficient(self):                for analysis in self.analyses:            self.assertAlmostEqual(                analysis.molar_extinction_coefficient()[0], 17420, places=5            )            self.assertAlmostEqual(                analysis.molar_extinction_coefficient()[1], 17545, places=5            )",
        "labels_text": "Molar extinction coefficient"
    },
    {
        "input_text": "summarize: def test_sequence_object(self):                with self.assertRaises(TypeError):            seq = FormattedSeq(\"GATC\")        seq = FormattedSeq(Seq(\"TAGC\"))        seq = FormattedSeq(MutableSeq(\"AGTC\"))        seq = FormattedSeq(seq)        with self.assertRaises(TypeError):            EcoRI.search(\"GATC\")        EcoRI.search(Seq(\"ATGC\"))        EcoRI.search(MutableSeq(\"TCAG\"))",
        "labels_text": "Test if sequence must be a Seq or MutableSeq object"
    },
    {
        "input_text": "summarize: def test_non_iupac_letters(self):                with self.assertRaises(TypeError):            seq = FormattedSeq(Seq(\"GATCZ\"))",
        "labels_text": "Test if nonIUPAC letter raise a TypeError"
    },
    {
        "input_text": "summarize: def test_init(self):                with self.assertRaises(ValueError) as ve:            Restriction.OneCut(\"bla-me\", (Restriction.RestrictionType,), {})            self.assertIn(\"hyphen\", str(ve.exception))",
        "labels_text": "Check for error during init"
    },
    {
        "input_text": "summarize: def setUp(self):                base_seq = Seq(\"AAAA\")        self.ecosite_seq = base_seq + Seq(EcoRI.site) + base_seq        self.smasite_seq = base_seq + Seq(SmaI.site) + base_seq        self.kpnsite_seq = base_seq + Seq(KpnI.site) + base_seq",
        "labels_text": "Set up some sequence for later use"
    },
    {
        "input_text": "summarize: def test_ear_cutting(self):                self.assertFalse(EarI.is_palindromic())        self.assertFalse(EarI.is_defined())        self.assertTrue(EarI.is_ambiguous())        self.assertFalse(EarI.is_unknown())        self.assertEqual(EarI.elucidate(), \"CTCTTCN^NNN_N\")",
        "labels_text": "Test basic cutting with EarI ambiguous overhang"
    },
    {
        "input_text": "summarize: def test_shortcuts(self):                self.assertEqual(EcoRI / self.ecosite_seq, [6])        self.assertEqual(self.ecosite_seq / EcoRI, [6])        self.assertEqual(len(EcoRI // self.ecosite_seq), 2)        self.assertEqual(len(self.ecosite_seq // EcoRI), 2)",
        "labels_text": "Check if and work a search and catalyse"
    },
    {
        "input_text": "summarize: def test_recognition_site_on_both_strands(self):                seq = Seq(\"CTCTTCGAAGAG\")        self.assertEqual(EarI.search(seq), [3, 8])",
        "labels_text": "Check if recognition site on both strand are properly handled"
    },
    {
        "input_text": "summarize: def test_overlapping_cut_sites(self):                seq = Seq(\"CATGCACGCATGCATGCACGC\")        self.assertEqual(SphI.search(seq), [13, 17])",
        "labels_text": "Check if overlapping recognition site are properly handled"
    },
    {
        "input_text": "summarize: def createAnalysis(self, seq_str, batch_ary):                rb = Restriction.RestrictionBatch(batch_ary)        seq = Seq(seq_str)        return Restriction.Analysis(rb, seq)",
        "labels_text": "RestrictionAnalysis creation helper method"
    },
    {
        "input_text": "summarize: def assertAnalysisFormat(self, analysis, expected):                dct = analysis.mapping        ls, nc = [], []        for k, v in dct.items():            if v:                ls.append((k, v))            else:                nc.append(k)        result = analysis.make_format(ls, \"\", [], \"\")        self.assertEqual(result.replace(\" \", \"\"), expected.replace(\" \", \"\"))",
        "labels_text": "Test makeformat Test that the RestrictionAnalysis makeformatprintthat match some string"
    },
    {
        "input_text": "summarize: def test_batch_analysis(self):                seq = Seq(\"AAAA\" + EcoRV.site + \"AAAA\" + EcoRI.site + \"AAAA\")        batch = RestrictionBatch([EcoRV, EcoRI])        hits = batch.search(seq)        self.assertEqual(hits[EcoRV], [8])        self.assertEqual(hits[EcoRI], [16])",
        "labels_text": "Sequence analysis with a restriction batch"
    },
    {
        "input_text": "summarize: def test_premade_batches(self):                self.assertEqual(len(AllEnzymes), (len(CommOnly) + len(NonComm)))        self.assertTrue(len(AllEnzymes) > len(CommOnly) > len(NonComm))",
        "labels_text": "Test content of premade batch CommOnly NoComm AllEnzymes"
    },
    {
        "input_text": "summarize: def create_fasta_index(self):                cmdline = SamtoolsFaidxCommandline(samtools_exe)        cmdline.set_parameter(\"reference\", self.reference)        stdout, stderr = cmdline()",
        "labels_text": "Create index for reference fasta sequence"
    },
    {
        "input_text": "summarize: def create_bam_index(self, input_bam):                cmdline = SamtoolsIndexCommandline(samtools_exe)        cmdline.set_parameter(\"input_bam\", input_bam)        stdout, stderr = cmdline()",
        "labels_text": "Create index of an input bam file"
    },
    {
        "input_text": "summarize: def testParse(self):                count = 0        with open(self.filename) as f:            records = Cla.parse(f)            for record in records:                count += 1        self.assertEqual(count, 14)",
        "labels_text": "Test if all record in a CLA file are being read"
    },
    {
        "input_text": "summarize: def testError(self):                corruptRec = \"49268\\tsp\\tb.1.2.1\\t-\\n\"        self.assertRaises(ValueError, Cla.Record, corruptRec)",
        "labels_text": "Test if a corrupt record raise the appropriate exception"
    },
    {
        "input_text": "summarize: def testIndex(self):                index = Cla.Index(self.filename)        self.assertEqual(len(index), 14)        self.assertIn(\"d4hbia_\", index)        rec = index[\"d1hbia_\"]        self.assertEqual(rec.sunid, 14996)",
        "labels_text": "Test CLA file indexing"
    },
    {
        "input_text": "summarize: def testParse(self):                count = 0        with open(self.filename) as f:            records = Des.parse(f)            for record in records:                count += 1        self.assertEqual(count, 20)",
        "labels_text": "Test if all record in a DES file are being read"
    },
    {
        "input_text": "summarize: def testStr(self):                with open(self.filename) as f:            for line in f:                record = Des.Record(line)                # End of line is platform dependent. Strip it off                self.assertEqual(str(record).rstrip(), line.rstrip())",
        "labels_text": "Test if we can convert each record to a string correctly"
    },
    {
        "input_text": "summarize: def testError(self):                corruptRec = \"49268\\tsp\\tb.1.2.1\\t-\\n\"        self.assertRaises(ValueError, Des.Record, corruptRec)",
        "labels_text": "Test if a corrupt record raise the appropriate exception"
    },
    {
        "input_text": "summarize: def testParse(self):                count = 0        with open(self.filename) as f:            for record in Dom.parse(f):                count += 1        self.assertEqual(count, 10)",
        "labels_text": "Test if all record in a DOM file are being read"
    },
    {
        "input_text": "summarize: def testStr(self):                with open(self.filename) as f:            for line in f:                record = Dom.Record(line)                # End of line is platform dependent. Strip it off                self.assertEqual(str(record).rstrip(), line.rstrip())",
        "labels_text": "Test if we can convert each record to a string correctly"
    },
    {
        "input_text": "summarize: def testError(self):                corruptDom = \"49xxx268\\tsp\\tb.1.2.1\\t-\\n\"        self.assertRaises(ValueError, Dom.Record, corruptDom)",
        "labels_text": "Test if a corrupt record raise the appropriate exception"
    },
    {
        "input_text": "summarize: def testParse(self):                count = 0        with open(self.filename) as f:            for record in Hie.parse(f):                count += 1        self.assertEqual(count, 21)",
        "labels_text": "Test if all record in a HIE file are being read"
    },
    {
        "input_text": "summarize: def testStr(self):                with open(self.filename) as f:            for line in f:                record = Hie.Record(line)                # End of line is platform dependent. Strip it off                self.assertEqual(str(record).rstrip(), line.rstrip())",
        "labels_text": "Test if we can convert each record to a string correctly"
    },
    {
        "input_text": "summarize: def testError(self):                corruptRec = \"4926sdfhjhfgyjdfyg\"        self.assertRaises(ValueError, Hie.Record, corruptRec)",
        "labels_text": "Test if a corrupt record raise the appropriate exception"
    },
    {
        "input_text": "summarize: def testAstralParse(self):                for loc in self.res:            r = Residues(\"(\" + loc[0] + \")\")            self.assertEqual(r.fragments, loc[1])",
        "labels_text": "Test if we can parse residue subset enclosed in bracket"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_tab_2226_tblastn_002(self):                xml_file = get_file(\"tab_2226_tblastn_002.txt\")        qresults = parse(xml_file, FMT)        # check if we've finished iteration over qresults        self.assertRaises(StopIteration, next, qresults)",
        "labels_text": "Test parsing TBLASTN tabular output tabtblastn"
    },
    {
        "input_text": "summarize: def test_tab_2226_tblastn_005_comments_false(self):                tab_file = get_file(\"tab_2226_tblastn_005.txt\")        exc_msg = (            \"Encountered unexpected character '#' at the beginning of a line. \"            \"Set comments=True if the file is a commented file.\"        )        qresults = parse(tab_file, FMT)        with self.assertRaises(ValueError, msg=exc_msg):            next(qresults)",
        "labels_text": "Test parsing TBLASTN tabular output with comment tabtblastn"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_multiple_first(self):                filename = \"Blast/tab_2226_tblastn_001.txt\"        raw =         self.check_raw(filename, \"gi|16080617|ref|NP_391444.1|\", raw)",
        "labels_text": "Test blasttab raw string retrieval BLAST multiple query first tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_multiple_last(self):                filename = \"Blast/tab_2226_tblastn_001.txt\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw)",
        "labels_text": "Test blasttab raw string retrieval BLAST multiple query last tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_single(self):                filename = \"Blast/tab_2226_tblastn_004.txt\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw)",
        "labels_text": "Test blasttab raw string retrieval BLAST single query tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_multiple_first_commented(self):                filename = \"Blast/tab_2226_tblastn_005.txt\"        raw =         self.check_raw(filename, \"random_s00\", raw, comments=True)",
        "labels_text": "Test blasttab raw string retrieval BLAST multiple query first commented tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_multiple_middle_commented(self):                filename = \"Blast/tab_2226_tblastn_005.txt\"        raw =         self.check_raw(filename, \"gi|16080617|ref|NP_391444.1|\", raw, comments=True)",
        "labels_text": "Test blasttab raw string retrieval BLAST multiple query middle commented tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_multiple_last_commented(self):                filename = \"Blast/tab_2226_tblastn_005.txt\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw, comments=True)",
        "labels_text": "Test blasttab raw string retrieval BLAST multiple query last commented tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_single_commented(self):                filename = \"Blast/tab_2226_tblastn_008.txt\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw, comments=True)",
        "labels_text": "Test blasttab raw string retrieval BLAST single query commented tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_001(self):                filename = \"Blast/tab_2226_tblastn_001.txt\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blasttab indexing BLAST multiple query"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_002(self):                filename = \"Blast/tab_2226_tblastn_002.txt\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blasttab indexing BLAST single query no hit"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_004(self):                filename = \"Blast/tab_2226_tblastn_004.txt\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blasttab indexing BLAST single query multiple hit"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_005(self):                filename = \"Blast/tab_2226_tblastn_005.txt\"        self.check_index(filename, self.fmt, comments=True)",
        "labels_text": "Test blasttab indexing BLAST multiple query commented"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_006(self):                filename = \"Blast/tab_2226_tblastn_006.txt\"        self.check_index(filename, self.fmt, comments=True)",
        "labels_text": "Test blasttab indexing BLAST single query no hit commented"
    },
    {
        "input_text": "summarize: def test_blasttab_comment_sing(self):                filename = \"Blast/tab_2226_tblastn_008.txt\"        self.check_index(filename, self.fmt, comments=True)",
        "labels_text": "Test blasttab indexing BLAST single query multiple hit commented"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_009(self):                filename = \"Blast/tab_2226_tblastn_009.txt\"        self.check_index(filename, self.fmt, fields=[\"qseqid\", \"sseqid\"])",
        "labels_text": "Test blasttab indexing BLAST custom column"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_010(self):                filename = \"Blast/tab_2226_tblastn_010.txt\"        self.check_index(filename, self.fmt, comments=True)",
        "labels_text": "Test blasttab indexing BLAST custom column commented"
    },
    {
        "input_text": "summarize: def test_blasttab_2226_tblastn_011(self):                filename = \"Blast/tab_2226_tblastn_011.txt\"        self.check_index(filename, self.fmt, comments=True)",
        "labels_text": "Test blasttab indexing BLAST all column commented"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_multiple_first(self):                filename = \"Blast/xml_2226_blastp_001.xml\"        raw =         self.check_raw(filename, \"random_s00\", raw)",
        "labels_text": "Test blastxml raw string retrieval BLAST multiple query first xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_multiple_middle(self):                filename = \"Blast/xml_2226_blastp_001.xml\"        raw =         self.check_raw(filename, \"gi|16080617|ref|NP_391444.1|\", raw)",
        "labels_text": "Test blastxml raw string retrieval BLAST multiple query middle xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_multiple_last(self):                filename = \"Blast/xml_2226_blastp_001.xml\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw)",
        "labels_text": "Test blastxml raw string retrieval BLAST multiple query last xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_single(self):                filename = \"Blast/xml_2226_blastp_004.xml\"        raw =         self.check_raw(filename, \"gi|11464971:4-101\", raw)",
        "labels_text": "Test blastxml raw string retrieval BLAST single query xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_blastxml_2212L_blastp_001(self):                filename = \"Blast/xml_2212L_blastp_001.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BLAST"
    },
    {
        "input_text": "summarize: def test_blastxml_2218_blastp_001(self):                filename = \"Blast/xml_2218_blastp_001.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BLAST"
    },
    {
        "input_text": "summarize: def test_blastxml_2222_blastx_001(self):                filename = \"Blast/xml_2222_blastx_001.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BLAST"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_tblastn_001(self):                filename = \"Blast/xml_2226_tblastn_001.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BLAST multiple query"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_tblastn_002(self):                filename = \"Blast/xml_2226_tblastn_002.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BlAST single query no hit"
    },
    {
        "input_text": "summarize: def test_blastxml_2226_tblastn_004(self):                filename = \"Blast/xml_2226_tblastn_004.xml\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test blastxml indexing BLAST single query multiple hit"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_psl_34_002(self, testf=\"psl_34_002.psl\", pslx=False):                blat_file = get_file(testf)        self.qresults = list(parse(blat_file, FMT, pslx=pslx))        self.assertEqual(0, len(self.qresults))",
        "labels_text": "Test parsing blat output pslpsl"
    },
    {
        "input_text": "summarize: def test_pslx_34_002(self, testf=\"pslx_34_002.pslx\"):                BlatPslCases.test_psl_34_002(self, \"pslx_34_002.pslx\", pslx=True)",
        "labels_text": "Test parsing blat output pslxpslx"
    },
    {
        "input_text": "summarize: def test_psl_34_001(self):                filename = os.path.join(\"Blat\", \"psl_34_001.psl\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test blatpsl indexing multiple query"
    },
    {
        "input_text": "summarize: def test_psl_34_002(self):                filename = os.path.join(\"Blat\", \"psl_34_002.psl\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test blatpsl indexing single query no hit"
    },
    {
        "input_text": "summarize: def test_psl_34_003(self):                filename = os.path.join(\"Blat\", \"psl_34_003.psl\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test blatpsl indexing single query single hit"
    },
    {
        "input_text": "summarize: def test_psl_34_004(self):                filename = os.path.join(\"Blat\", \"psl_34_004.psl\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test blatpsl indexing single query multiple hit with multiple hsps"
    },
    {
        "input_text": "summarize: def test_psl_34_005(self):                filename = os.path.join(\"Blat\", \"psl_34_005.psl\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test blatpsl indexing multiple query no header"
    },
    {
        "input_text": "summarize: def test_psl_34_006(self):                filename = os.path.join(\"Blat\", \"pslx_34_001.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx indexing multiple query"
    },
    {
        "input_text": "summarize: def test_psl_34_007(self):                filename = os.path.join(\"Blat\", \"pslx_34_002.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx indexing single query no hit"
    },
    {
        "input_text": "summarize: def test_psl_34_008(self):                filename = os.path.join(\"Blat\", \"pslx_34_003.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx indexing single query single hit"
    },
    {
        "input_text": "summarize: def test_psl_34_009(self):                filename = os.path.join(\"Blat\", \"pslx_34_004.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx indexing single query multiple hit with multiple hsps"
    },
    {
        "input_text": "summarize: def test_psl_34_010(self):                filename = os.path.join(\"Blat\", \"pslx_34_005.pslx\")        self.check_index(filename, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx indexing multiple query no header"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_vulgar_text_similar_g2g(self):                self.check_vulgar_text(\"exn_22_o_vulgar.exn\", \"exn_22_m_genome2genome.exn\")",
        "labels_text": "Compares vulgartext coordinate parsing for the genomegenome model"
    },
    {
        "input_text": "summarize: def test_vulgar_text_similar_c2c(self):                self.check_vulgar_text(            \"exn_22_o_vulgar_fshifts.exn\", \"exn_22_m_coding2coding_fshifts.exn\"        )",
        "labels_text": "Compares vulgartext coordinate parsing for the codingcoding model"
    },
    {
        "input_text": "summarize: def test_vulgar_text_similar_p2d(self):                self.check_vulgar_text(            \"exn_22_o_vulgar_fshifts2.exn\", \"exn_22_m_protein2dna_fshifts.exn\"        )",
        "labels_text": "Compares vulgartext coordinate parsing for the proteindna model"
    },
    {
        "input_text": "summarize: def test_exn_22_q_none(self):                exn_file = get_file(\"exn_22_q_none.exn\")        qresults = parse(exn_file, \"exonerate-text\")        self.assertRaises(StopIteration, next, qresults)",
        "labels_text": "Test parsing exonerate output exnqnoneexn"
    },
    {
        "input_text": "summarize: def test_exn_22_m_est2genome(self):                filename = os.path.join(\"Exonerate\", \"exn_22_m_est2genome.exn\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test exoneratetext indexing single"
    },
    {
        "input_text": "summarize: def test_exn_22_q_multiple(self):                filename = os.path.join(\"Exonerate\", \"exn_22_q_multiple.exn\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test exoneratetext indexing single"
    },
    {
        "input_text": "summarize: def test_exn_22_m_est2genome(self):                filename = os.path.join(\"Exonerate\", \"exn_22_o_vulgar.exn\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test exoneratevulgar indexing single"
    },
    {
        "input_text": "summarize: def test_exn_22_q_multiple(self):                filename = os.path.join(\"Exonerate\", \"exn_22_q_multiple_vulgar.exn\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test exoneratevulgar indexing single"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_output_002(self):                filename = os.path.join(\"Fasta\", \"output002.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta multiple query"
    },
    {
        "input_text": "summarize: def test_output_001(self):                filename = os.path.join(\"Fasta\", \"output001.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta multiple query"
    },
    {
        "input_text": "summarize: def test_output_005(self):                filename = os.path.join(\"Fasta\", \"output005.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing ssearch multiple query"
    },
    {
        "input_text": "summarize: def test_output_008(self):                filename = os.path.join(\"Fasta\", \"output008.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing tfastx multiple query"
    },
    {
        "input_text": "summarize: def test_output_009(self):                filename = os.path.join(\"Fasta\", \"output009.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta multiple query"
    },
    {
        "input_text": "summarize: def test_output_010(self):                filename = os.path.join(\"Fasta\", \"output010.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta single query no hit"
    },
    {
        "input_text": "summarize: def test_output_011(self):                filename = os.path.join(\"Fasta\", \"output011.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta single query hit with single hsp"
    },
    {
        "input_text": "summarize: def test_output_012(self):                filename = os.path.join(\"Fasta\", \"output012.m10\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test fastam indexing fasta single query with multiple hsps"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_2uvo_onlyheader(self):                txt_file = get_file(\"2uvo_hhblits_onlyheader.hhr\")        qresults = parse(txt_file, FMT)        with self.assertRaises(RuntimeError):            next(qresults)",
        "labels_text": "Parsing uvo with only header present"
    },
    {
        "input_text": "summarize: def test_2uvo_emptytable(self):                txt_file = get_file(\"2uvo_hhblits_emptytable.hhr\")        qresults = parse(txt_file, FMT)        with self.assertRaises(RuntimeError):            next(qresults)",
        "labels_text": "Parsing uvo with empty result table"
    },
    {
        "input_text": "summarize: def test_hmmpfam_23_no_match(self):                results = parse(path.join(\"Hmmer\", \"text_23_hmmpfam_002.out\"), self.fmt)        res = next(results)        self.assertEqual(\"SEQ0001\", res.id)        self.assertEqual(0, len(res.hits))        res = next(results)        self.assertEqual(\"SEQ0002\", res.id)        self.assertEqual(0, len(res.hits))",
        "labels_text": "Test parsing hmmpfam file texthmmpfamout"
    },
    {
        "input_text": "summarize: def test_hmmpfam_23_break_in_end_of_seq(self):                results = parse(path.join(\"Hmmer\", \"text_23_hmmpfam_004.out\"), self.fmt)        res = next(results)        self.assertEqual(\"PKSI-KS\", res[0].id)        self.assertEqual(\"PKSI-FK\", res[1].id)",
        "labels_text": "Test parsing hmmpfam file with a line break in the end of seq marker file texthmmpfamout"
    },
    {
        "input_text": "summarize: def test_hmmer2text_22_single_hmmsearch(self):                filename = os.path.join(\"Hmmer\", \"text_22_hmmsearch_001.out\")        raw =   # noqa : W291        self.check_raw(filename, \"Peptidase_C1\", raw)",
        "labels_text": "Test hmmertext raw string retrieval single query hmmsearch"
    },
    {
        "input_text": "summarize: def test_hmmer2text_22_single_hmmpfam(self):                filename = os.path.join(\"Hmmer\", \"text_22_hmmpfam_001.out\")        raw =   # noqa : W291        self.check_raw(filename, \"gi|1522636|gb|AAC37060.1|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval single query hmmpfam"
    },
    {
        "input_text": "summarize: def test_hmmer2text_22_multiple_first_hmmpfam(self):                filename = os.path.join(\"Hmmer\", \"text_24_hmmpfam_001.out\")        raw =   # noqa : W291        self.check_raw(filename, \"random_s00\", raw)",
        "labels_text": "Test hmmertext raw string retrieval multiple query hmmpfam"
    },
    {
        "input_text": "summarize: def test_hmmer2text_22_multiple_middle_hmmpfam(self):                filename = os.path.join(\"Hmmer\", \"text_24_hmmpfam_001.out\")        raw =   # noqa : W291        self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval multiple query hmmpfam"
    },
    {
        "input_text": "summarize: def test_hmmer2text_22_multiple_last_hmmpfam(self):                filename = os.path.join(\"Hmmer\", \"text_24_hmmpfam_001.out\")        raw =   # noqa : W291        self.check_raw(filename, \"gi|125490392|ref|NP_038661.2|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval multiple query hmmpfam"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_21_hmmpfam_001(self):                filename = os.path.join(\"Hmmer\", \"text_21_hmmpfam_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_22_hmmpfam_001(self):                filename = os.path.join(\"Hmmer\", \"text_22_hmmpfam_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_23_hmmpfam_001(self):                filename = os.path.join(\"Hmmer\", \"text_23_hmmpfam_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_24_hmmpfam_001(self):                filename = os.path.join(\"Hmmer\", \"text_24_hmmpfam_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_22_hmmsearch_001(self):                filename = os.path.join(\"Hmmer\", \"text_22_hmmsearch_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_domtab_30_hmmscan_002(self):                tab_file = get_file(\"domtab_30_hmmscan_002.out\")        qresults = parse(tab_file, self.fmt)        self.assertRaises(StopIteration, next, qresults)",
        "labels_text": "Parsing hmmscandomtab hmmscan single query no hit domtabhmmscan"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_multiple_first(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "labels_text": "Test hmmscandomtab raw string retrieval HMMER multiple query first domtabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_multiple_middle(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|126362951:116-221\", raw)",
        "labels_text": "Test hmmscandomtab raw string retrieval HMMER multiple query middle domtabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_multiple_last(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|125490392|ref|NP_038661.2|\", raw)",
        "labels_text": "Test hmmscandomtab raw string retrieval HMMER multiple query last domtabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_single(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_004.out\")        raw =         self.check_raw(filename, \"gi|126362951:116-221\", raw)",
        "labels_text": "Test hmmscandomtab raw string retrieval HMMER single query domtabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_hmmscan_001(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_001.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "labels_text": "Test hmmscandomtab indexing HMMER multiple query"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_hmmscan_002(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_002.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "labels_text": "Test hmmscandomtab indexing HMMER single query no hit"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_hmmscan_003(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_003.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "labels_text": "Test hmmscandomtab indexing HMMER single query multiple hit"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_hmmscan_004(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_004.out\")        self.check_index(filename, \"hmmscan3-domtab\")",
        "labels_text": "Test hmmscandomtab indexing HMMER single query no alignment"
    },
    {
        "input_text": "summarize: def test_hmmerdomtab_30_hmmsearch_001(self):                filename = os.path.join(\"Hmmer\", \"domtab_30_hmmsearch_001.out\")        self.check_index(filename, \"hmmsearch3-domtab\")",
        "labels_text": "Test hmmsearchdomtab indexing HMMER single query no alignment"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_30_hmmscan_002(self):                tab_file = get_file(\"tab_30_hmmscan_002.out\")        qresults = parse(tab_file, FMT)        self.assertRaises(StopIteration, next, qresults)",
        "labels_text": "Test parsing hmmertab hmmscan single query no hit tabhmmscan"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_multiple_first(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "labels_text": "Test hmmertab raw string retrieval HMMER multiple query first tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_multiple_middle(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|126362951:116-221\", raw)",
        "labels_text": "Test hmmertab raw string retrieval HMMER multiple query middle tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_multiple_last(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_001.out\")        raw =         self.check_raw(filename, \"gi|125490392|ref|NP_038661.2|\", raw)",
        "labels_text": "Test hmmertab raw string retrieval HMMER multiple query last tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_single(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_004.out\")        raw =         self.check_raw(filename, \"gi|126362951:116-221\", raw)",
        "labels_text": "Test hmmertab raw string retrieval HMMER single query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_hmmscan_001(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_001.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertab indexing HMMER multiple query"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_hmmscan_002(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_002.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertab indexing HMMER single query no hit"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_hmmscan_003(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_003.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertab indexing HMMER single query multiple hit"
    },
    {
        "input_text": "summarize: def test_hmmer3tab_30_hmmscan_004(self):                filename = os.path.join(\"Hmmer\", \"tab_30_hmmscan_004.out\")        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertab indexing HMMER single query no alignment"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_hmmer3text_30_multiple_first(self):                filename = \"Hmmer/text_30_hmmscan_001.out\"        raw =         self.check_raw(filename, \"random_s00\", raw)",
        "labels_text": "Test hmmertext raw string retrieval HMMER multiple query first texthmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3text_30_multiple_middle(self):                filename = \"Hmmer/text_30_hmmscan_001.out\"        raw =   # noqa : W291        self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval HMMER multiple query middle texthmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3text_30_multiple_last(self):                filename = \"Hmmer/text_30_hmmscan_001.out\"        raw =   # noqa : W291        self.check_raw(filename, \"gi|125490392|ref|NP_038661.2|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval HMMER multiple query last texthmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmer3text_30_single(self):                filename = \"Hmmer/text_30_hmmscan_003.out\"        raw =   # noqa : W291        self.check_raw(filename, \"gi|4885477|ref|NP_005359.1|\", raw)",
        "labels_text": "Test hmmertext raw string retrieval HMMER single query texthmmscanout"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmscan_001(self):                filename = \"Hmmer/text_30_hmmscan_001.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER multiple query"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmscan_002(self):                filename = \"Hmmer/text_30_hmmscan_002.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER single query no hit"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmscan_006(self):                filename = \"Hmmer/text_30_hmmscan_006.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER single query multiple hit"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmscan_007(self):                filename = \"Hmmer/text_30_hmmscan_007.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER single query no alignment"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmscan_008(self):                filename = \"Hmmer/text_30_hmmscan_008.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER single query no alignment width"
    },
    {
        "input_text": "summarize: def test_hmmertext_text_30_hmmsearch_005(self):                filename = \"Hmmer/text_30_hmmsearch_005.out\"        self.check_index(filename, self.fmt)",
        "labels_text": "Test hmmertext indexing HMMER multiple query"
    },
    {
        "input_text": "summarize: def get_file(filename):        return os.path.join(TEST_DIR, filename)",
        "labels_text": "Return the path of a test file"
    },
    {
        "input_text": "summarize: def test_pickle(self):                buf = BytesIO()        pickle.dump(self.qresult, buf)        unp = pickle.loads(buf.getvalue())        self.compare_search_obj(self.qresult, unp)",
        "labels_text": "Test pickling and unpickling of QueryResult"
    },
    {
        "input_text": "summarize: def test_init_none(self):                qresult = QueryResult()        self.assertIsNone(qresult.id)        self.assertIsNone(qresult.description)",
        "labels_text": "Test QueryResultinit no argument"
    },
    {
        "input_text": "summarize: def test_init_id_only(self):                qresult = QueryResult(id=\"query1\")        self.assertEqual(\"query1\", qresult.id)        self.assertIsNone(qresult.description)",
        "labels_text": "Test QueryResultinit with ID only"
    },
    {
        "input_text": "summarize: def test_init_hits_only(self):                qresult = QueryResult([hit11, hit21, hit31])        self.assertEqual(\"query1\", qresult.id)        self.assertEqual(\"<unknown description>\", qresult.description)",
        "labels_text": "Test QueryResultinit with hit only"
    },
    {
        "input_text": "summarize: def test_repr(self):                self.assertEqual(\"QueryResult(id='query1', 3 hits)\", repr(self.qresult))",
        "labels_text": "Test QueryResultrepr"
    },
    {
        "input_text": "summarize: def test_iter(self):                # iteration should return hits contained        for counter, hit in enumerate(self.qresult):            self.assertIn(hit, (hit11, hit21, hit31))        self.assertEqual(2, counter)",
        "labels_text": "Test QueryResultiter"
    },
    {
        "input_text": "summarize: def test_hits(self):                # hits should return hits contained in qresult        hits = list(self.qresult.hits)        self.assertEqual([hit11, hit21, hit31], hits)",
        "labels_text": "Test QueryResulthits"
    },
    {
        "input_text": "summarize: def test_hit_keys(self):                # hit_keys should return hit keys (which default to hit ids)        hit_keys = list(self.qresult.hit_keys)        self.assertEqual([\"hit1\", \"hit2\", \"hit3\"], hit_keys)",
        "labels_text": "Test QueryResulthitkeys"
    },
    {
        "input_text": "summarize: def test_items(self):                # items should return tuples of hit key, hit object pair        items = list(self.qresult.items)        self.assertEqual([(\"hit1\", hit11), (\"hit2\", hit21), (\"hit3\", hit31)], items)",
        "labels_text": "Test QueryResultitems"
    },
    {
        "input_text": "summarize: def test_hsps(self):                # hsps should return all hsps contained in qresult        hsps = self.qresult.hsps        self.assertEqual([hsp111, hsp112, hsp113, hsp114, hsp211, hsp311, hsp312], hsps)",
        "labels_text": "Test QueryResulthsps"
    },
    {
        "input_text": "summarize: def test_fragments(self):                # fragments should return all fragments contained in qresult        frags = self.qresult.fragments        self.assertEqual(            [                frag111,                frag112,                frag113,                frag113b,                frag114,                frag114b,                frag211,                frag311,                frag312,            ],            frags,        )",
        "labels_text": "Test QueryResultfragments"
    },
    {
        "input_text": "summarize: def test_contains(self):                # contains should work with hit ids or hit objects        self.assertIn(\"hit1\", self.qresult)        self.assertIn(hit21, self.qresult)        self.assertNotIn(\"hit5\", self.qresult)        self.assertNotIn(hit41, self.qresult)",
        "labels_text": "Test QueryResultcontains"
    },
    {
        "input_text": "summarize: def test_contains_alt(self):                # contains should work with alternative hit IDs        hit11._id_alt = [\"alt1\"]        query = QueryResult([hit11])        self.assertIn(\"alt1\", query)        hit11._id_alt = []",
        "labels_text": "Test QueryResultcontains with alternative IDs"
    },
    {
        "input_text": "summarize: def test_len(self):                # len() should return the number of hits contained        self.assertEqual(3, len(self.qresult))",
        "labels_text": "Test QueryResultlen"
    },
    {
        "input_text": "summarize: def test_bool(self):                # should return true only if the qresult has hits        self.assertTrue(self.qresult)        blank_qresult = QueryResult()        self.assertFalse(blank_qresult)",
        "labels_text": "Test QueryResultbool"
    },
    {
        "input_text": "summarize: def test_setitem_ok(self):                # hit objects assignment should work with arbitrary string keys        self.qresult[\"hit4\"] = hit41        self.assertEqual([hit11, hit21, hit31, hit41], list(self.qresult.hits))        # and if the key already exist, the object should be overwritten        self.qresult[\"hit4\"] = hit11        self.assertEqual([hit11, hit21, hit31, hit11], list(self.qresult.hits))",
        "labels_text": "Test QueryResultsetitem"
    },
    {
        "input_text": "summarize: def test_setitem_wrong_key_type(self):                # item assignment should fail if the key is not string        self.assertRaises(TypeError, self.qresult.__setitem__, 0, hit41)        self.assertRaises(            TypeError, self.qresult.__setitem__, slice(0, 2), [hit41, hit31]        )",
        "labels_text": "Test QueryResultsetitem wrong key type"
    },
    {
        "input_text": "summarize: def test_setitem_wrong_type(self):                # item assignment should fail if the object assigned is not a hit object        self.assertRaises(TypeError, self.qresult.__setitem__, \"hit4\", hsp111)        self.assertRaises(TypeError, self.qresult.__setitem__, \"hit5\", \"hit5\")",
        "labels_text": "Test QueryResultsetitem wrong type"
    },
    {
        "input_text": "summarize: def test_setitem_wrong_query_id(self):                # item assignment should fail if the hit object does not have the same        # query id        self.assertRaises(ValueError, self.qresult.__setitem__, \"hit4\", hit12)",
        "labels_text": "Test QueryResultsetitem wrong query ID"
    },
    {
        "input_text": "summarize: def test_getitem_default_ok(self):                # hits should be retrievable by their keys (default to id)        self.assertEqual(hit21, self.qresult[\"hit2\"])        self.assertEqual(hit11, self.qresult[\"hit1\"])",
        "labels_text": "Test QueryResultgetitem"
    },
    {
        "input_text": "summarize: def test_getitem_int_ok(self):                # hits should be retrievable by their index        self.assertEqual(hit21, self.qresult[1])        self.assertEqual(hit31, self.qresult[-1])",
        "labels_text": "Test QueryResultgetitem with integer"
    },
    {
        "input_text": "summarize: def test_getitem_alt_ok(self):                hit11._id_alt = [\"alt1\"]        query = QueryResult([hit11])        self.assertEqual(hit11, query[\"hit1\"])        self.assertEqual(hit11, query[\"alt1\"])        self.assertNotEqual(hit11.id, \"alt1\")        hit11._id_alt = []",
        "labels_text": "Test QueryResultgetitem single item with alternative ID"
    },
    {
        "input_text": "summarize: def test_delitem_string_ok(self):                # delitem should work with string index        del self.qresult[\"hit1\"]        self.assertEqual(2, len(self.qresult))        self.assertTrue([hit21, hit31], list(self.qresult.hits))",
        "labels_text": "Test QueryResultgetitem with string"
    },
    {
        "input_text": "summarize: def test_delitem_int_ok(self):                # delitem should work with int index        del self.qresult[-1]        self.assertEqual(2, len(self.qresult))        self.assertEqual([hit11, hit21], list(self.qresult.hits))        del self.qresult[0]        self.assertEqual(1, len(self.qresult))        self.assertTrue([hit21], list(self.qresult.hits))",
        "labels_text": "Test QueryResultdelitem"
    },
    {
        "input_text": "summarize: def test_delitem_slice_ok(self):                # delitem should work with slice objects        del self.qresult[:-1]        self.assertEqual(1, len(self.qresult))        self.assertTrue([hit31], self.qresult.hits)",
        "labels_text": "Test QueryResultdelitem with slice"
    },
    {
        "input_text": "summarize: def test_append_ok(self):                # append should work with Hit objects        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))        self.qresult.append(hit41)        self.assertEqual([hit11, hit21, hit31, hit41], list(self.qresult.hits))        self.assertEqual([\"hit1\", \"hit2\", \"hit3\", \"hit4\"], list(self.qresult.hit_keys))",
        "labels_text": "Test QueryResultappend"
    },
    {
        "input_text": "summarize: def test_append_id_exists(self):                # append should raise an error if hit_key already exists        self.assertRaises(ValueError, self.qresult.append, hit11)",
        "labels_text": "Test QueryResultappend when ID exists"
    },
    {
        "input_text": "summarize: def test_append_alt_id_exists(self):                # append should raise an error if hit_key already exists as alt ID        hit11._id_alt = [\"alt\"]        hit21._id_alt = [\"alt\"]        qresult = QueryResult([hit11])        self.assertRaises(ValueError, qresult.append, hit21)        hit11._id_alt = []        hit21._id_alt = []",
        "labels_text": "Test QueryResultappend when alt ID exists"
    },
    {
        "input_text": "summarize: def test_append_alt_id_exists_alt(self):                # append should raise an error if alt ID already exists as primary ID        hit21._id_alt = [\"hit1\"]        qresult = QueryResult([hit11])        self.assertRaises(ValueError, qresult.append, hit21)        hit21._id_alt = []",
        "labels_text": "Test QueryResultappend when alt ID exists a primary"
    },
    {
        "input_text": "summarize: def test_hit_filter_no_func(self):                # when given no arguments, hit_filter should create a new object with        # the same contents        filtered = self.qresult.hit_filter()        self.compare_search_obj(filtered, self.qresult)        self.assertNotEqual(id(filtered), id(self.qresult))        self.assertEqual(1102, filtered.seq_len)        self.assertEqual(\"refseq_rna\", filtered.target)",
        "labels_text": "Test QueryResulthitfilter without argument"
    },
    {
        "input_text": "summarize: def test_hit_map_no_func(self):                # when given no arguments, hit_map should create a new object with        # the same contents        mapped = self.qresult.hit_map()        self.compare_search_obj(mapped, self.qresult)        self.assertNotEqual(id(mapped), id(self.qresult))        self.assertEqual(1102, mapped.seq_len)        self.assertEqual(\"refseq_rna\", mapped.target)",
        "labels_text": "Test QueryResulthitmap without argument"
    },
    {
        "input_text": "summarize: def test_hsp_filter_no_func(self):                # when given no arguments, hsp_filter should create a new object with        # the same contents        filtered = self.qresult.hsp_filter()        self.compare_search_obj(filtered, self.qresult)        self.assertNotEqual(id(filtered), id(self.qresult))        self.assertEqual(1102, filtered.seq_len)        self.assertEqual(\"refseq_rna\", filtered.target)",
        "labels_text": "Test QueryResulthspfilter no argument"
    },
    {
        "input_text": "summarize: def test_hsp_map_no_func(self):                # when given no arguments, hit_map should create a new object with        # the same contents        mapped = self.qresult.hsp_map()        self.compare_search_obj(mapped, self.qresult)        self.assertNotEqual(id(mapped), id(self.qresult))        self.assertEqual(1102, mapped.seq_len)        self.assertEqual(\"refseq_rna\", mapped.target)",
        "labels_text": "Test QueryResulthspmap without argument"
    },
    {
        "input_text": "summarize: def test_pop_nonexistent_with_default(self):                default = \"An arbitrary default return value for this test only.\"        nonexistent_key = \"neither a standard nor alternative key\"        hit = self.qresult.pop(nonexistent_key, default)        self.assertEqual(hit, default)",
        "labels_text": "Test QueryResultpop with default for nonexistent key"
    },
    {
        "input_text": "summarize: def test_pop_nonexistent_key(self):                nonexistent_key = \"neither a standard nor alternative key\"        self.assertRaises(KeyError, self.qresult.pop, nonexistent_key)",
        "labels_text": "Test QueryResultpop with default for nonexistent key"
    },
    {
        "input_text": "summarize: def test_pop_ok(self):                self.assertEqual(3, len(self.qresult))        hit = self.qresult.pop()        self.assertEqual(hit, hit31)        self.assertEqual([hit11, hit21], list(self.qresult.hits))",
        "labels_text": "Test QueryResultpop"
    },
    {
        "input_text": "summarize: def test_pop_int_index_ok(self):                # pop should work if given an int index        self.assertEqual(3, len(self.qresult))        hit = self.qresult.pop(1)        self.assertEqual(hit, hit21)        self.assertEqual([hit11, hit31], list(self.qresult.hits))",
        "labels_text": "Test QueryResultpop with integer index"
    },
    {
        "input_text": "summarize: def test_pop_string_index_ok(self):                # pop should work if given a string index        self.assertEqual(3, len(self.qresult))        hit = self.qresult.pop(\"hit2\")        self.assertEqual(hit, hit21)        self.assertEqual([hit11, hit31], list(self.qresult.hits))",
        "labels_text": "Test QueryResultpop with string index"
    },
    {
        "input_text": "summarize: def test_pop_string_alt_ok(self):                # pop should work with alternative index        hit11._id_alt = [\"alt1\"]        hit21._id_alt = [\"alt2\"]        qresult = QueryResult([hit11, hit21])        hit = qresult.pop(\"alt1\")        self.assertEqual(hit, hit11)        self.assertEqual([hit21], list(qresult))        self.assertNotIn(\"hit1\", qresult)        hit11._id_alt = []        hit21._id_alt = []",
        "labels_text": "Test QueryResultpop with alternative ID"
    },
    {
        "input_text": "summarize: def test_index(self):                # index should accept hit objects or hit key strings        self.assertEqual(2, self.qresult.index(\"hit3\"))        self.assertEqual(2, self.qresult.index(hit31))",
        "labels_text": "Test QueryResultindex"
    },
    {
        "input_text": "summarize: def test_index_alt(self):                # index should work with alt IDs        hit11._id_alt = [\"alt1\"]        qresult = QueryResult([hit21, hit11])        self.assertEqual(1, qresult.index(\"alt1\"))        hit11._id_alt = []",
        "labels_text": "Test QueryResultindex with alt ID"
    },
    {
        "input_text": "summarize: def test_index_not_present(self):                self.assertRaises(ValueError, self.qresult.index, \"hit4\")        self.assertRaises(ValueError, self.qresult.index, hit41)",
        "labels_text": "Test QueryResultindex when index is not present"
    },
    {
        "input_text": "summarize: def test_sort_ok(self):                # sort without any arguments should keep the Hits in the same order        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))        self.qresult.sort()        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))",
        "labels_text": "Test QueryResultsort"
    },
    {
        "input_text": "summarize: def test_sort_not_in_place_ok(self):                # sort without any arguments should keep the Hits in the same order        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))        sorted_qresult = self.qresult.sort(in_place=False)        self.assertEqual([hit11, hit21, hit31], list(sorted_qresult.hits))        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))",
        "labels_text": "Test QueryResultsort not in place"
    },
    {
        "input_text": "summarize: def test_sort_reverse_ok(self):                # sorting with reverse=True should return a QueryResult with Hits reversed        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))        self.qresult.sort(reverse=True)        self.assertEqual([hit31, hit21, hit11], list(self.qresult.hits))",
        "labels_text": "Test QueryResultsort reverse"
    },
    {
        "input_text": "summarize: def test_sort_key_ok(self):                # if custom key is given, sort using it        key = lambda hit: len(hit)  # noqa: E731        self.assertEqual([hit11, hit21, hit31], list(self.qresult.hits))        self.qresult.sort(key=key)        self.assertEqual([hit21, hit31, hit11], list(self.qresult.hits))",
        "labels_text": "Test QueryResultsort with custom key"
    },
    {
        "input_text": "summarize: def test_pickle(self):                buf = BytesIO()        pickle.dump(self.hit, buf)        unp = pickle.loads(buf.getvalue())        self.compare_search_obj(self.hit, unp)",
        "labels_text": "Test pickling and unpickling of Hit"
    },
    {
        "input_text": "summarize: def test_init_none(self):                hit = Hit()        self.assertIsNone(hit.id)        self.assertIsNone(hit.description)        self.assertIsNone(hit.query_id)        self.assertIsNone(hit.query_description)",
        "labels_text": "Test Hitinit no argument"
    },
    {
        "input_text": "summarize: def test_init_id_only(self):                hit = Hit(id=\"hit1\")        self.assertEqual(\"hit1\", hit.id)        self.assertIsNone(hit.description)        self.assertIsNone(hit.query_id)        self.assertIsNone(hit.query_description)",
        "labels_text": "Test Hitinit with ID only"
    },
    {
        "input_text": "summarize: def test_init_hsps_only(self):                hit = Hit([hsp111, hsp112, hsp113])        self.assertEqual(\"hit1\", hit.id)        self.assertEqual(\"<unknown description>\", hit.description)        self.assertEqual(\"query1\", hit.query_id)  # set from the HSPs        self.assertEqual(\"<unknown description>\", hit.query_description)",
        "labels_text": "Test Hitinit with hsps only"
    },
    {
        "input_text": "summarize: def test_repr(self):                # test for cases with 1 or other alignment numbers        self.assertEqual(\"Hit(id='hit1', query_id='query1', 3 hsps)\", repr(self.hit))",
        "labels_text": "Test Hitrepr"
    },
    {
        "input_text": "summarize: def test_hsps(self):                # hsps should return the list of hsps contained        self.assertEqual([hsp111, hsp112, hsp113], self.hit.hsps)",
        "labels_text": "Test Hithsps"
    },
    {
        "input_text": "summarize: def test_fragments(self):                # fragments should return the list of fragments in each hsps        # as a flat list        self.assertEqual([frag111, frag112, frag113, frag113b], self.hit.fragments)",
        "labels_text": "Test Hitfragments"
    },
    {
        "input_text": "summarize: def test_iter(self):                # iteration should return hsps contained        for counter, hsp in enumerate(self.hit):            self.assertIn(hsp, [hsp111, hsp112, hsp113])        self.assertEqual(2, counter)",
        "labels_text": "Test Hititer"
    },
    {
        "input_text": "summarize: def test_len(self):                # len() on Hit objects should return how many hsps it has        self.assertEqual(3, len(self.hit))",
        "labels_text": "Test Hitlen"
    },
    {
        "input_text": "summarize: def test_bool(self):                # bool() on Hit objects should return True only if hsps is filled        # which is always true        self.assertTrue(self.hit)",
        "labels_text": "Test Hitbool"
    },
    {
        "input_text": "summarize: def test_setitem_single(self):                # test regular setitem overwrite        self.hit[1] = hsp114        self.assertEqual(self.hit.hsps, [hsp111, hsp114, hsp113])",
        "labels_text": "Test Hitsetitem single item"
    },
    {
        "input_text": "summarize: def test_item_multiple(self):                # test iterable setitem        self.hit[:] = [hsp113, hsp112, hsp111]        self.assertEqual(self.hit.hsps, [hsp113, hsp112, hsp111])",
        "labels_text": "Test Hitsetitem multiple item"
    },
    {
        "input_text": "summarize: def test_getitem_single(self):                # getitem using integer index should return a hsp object        hsp1 = self.hit[0]        self.assertEqual(hsp111, hsp1)        hsp3 = self.hit[-1]        self.assertEqual(hsp113, hsp3)",
        "labels_text": "Test Hitgetitem single item"
    },
    {
        "input_text": "summarize: def test_delitem(self):                # test delitem        del self.hit[0]        self.assertEqual(2, len(self.hit))        self.assertEqual([hsp112, hsp113], self.hit.hsps)",
        "labels_text": "Test Hitdelitem"
    },
    {
        "input_text": "summarize: def test_validate_hsp_ok(self):                # validation should pass if item is an hsp object with matching        # query and hit ids        # if validation passes, None is returned        self.assertIsNone(self.hit._validate_hsp(hsp114))",
        "labels_text": "Test Hitvalidatehsp"
    },
    {
        "input_text": "summarize: def test_validate_hsp_wrong_type(self):                # validation should fail if item is not an hsp object        self.assertRaises(TypeError, self.hit._validate_hsp, 1)        self.assertRaises(TypeError, self.hit._validate_hsp, Seq(\"\"))",
        "labels_text": "Test Hitvalidatehsp wrong type"
    },
    {
        "input_text": "summarize: def test_validate_hsp_wrong_query_id(self):                # validation should fail if query id does not match        self.assertRaises(ValueError, self.hit._validate_hsp, hsp211)",
        "labels_text": "Test Hitvalidatehsp wrong query ID"
    },
    {
        "input_text": "summarize: def test_validate_hsp_wrong_hit_id(self):                # validation should vail if hit id does not match        self.assertRaises(ValueError, self.hit._validate_hsp, hsp121)",
        "labels_text": "Test Hitvalidatehsp wrong hit ID"
    },
    {
        "input_text": "summarize: def test_append(self):                # append should add hits to the last position        self.hit.append(hsp114)        self.assertEqual(4, len(self.hit))        self.assertEqual(hsp114, self.hit[-1])",
        "labels_text": "Test Hitappend"
    },
    {
        "input_text": "summarize: def test_filter_no_func(self):                # when given no arguments, filter should create a new object with        # the same contents        filtered = self.hit.filter()        self.compare_search_obj(filtered, self.hit)        self.assertNotEqual(id(filtered), id(self.hit))        self.assertEqual(5e-10, filtered.evalue)        self.assertEqual(\"test\", filtered.name)",
        "labels_text": "Test Hitfilter without argument"
    },
    {
        "input_text": "summarize: def test_filter_no_filtered(self):                # when the filter filters out all hits, it should return None        filter_func = lambda hsp: len(hsp[0]) > 50  # noqa: E731        filtered = self.hit.filter(filter_func)        self.assertIsNone(filtered)",
        "labels_text": "Test Hithitfilter all hit filtered out"
    },
    {
        "input_text": "summarize: def test_index(self):                # index should accept hsp objects        self.assertEqual(1, self.hit.index(hsp112))",
        "labels_text": "Test Hitindex"
    },
    {
        "input_text": "summarize: def test_index_not_present(self):                self.assertRaises(ValueError, self.hit.index, hsp114)",
        "labels_text": "Test Hitindex when index is not present"
    },
    {
        "input_text": "summarize: def test_hsp_map_no_func(self):                # when given no arguments, map should create a new object with        # the same contents        mapped = self.hit.map()        self.compare_search_obj(mapped, self.hit)        self.assertNotEqual(id(mapped), id(self.hit))        self.assertEqual(5e-10, mapped.evalue)        self.assertEqual(\"test\", mapped.name)",
        "labels_text": "Test Hitmap without argument"
    },
    {
        "input_text": "summarize: def test_pop(self):                # pop should return the last item by default        self.assertEqual(hsp113, self.hit.pop())        self.assertEqual(hsp111, self.hit.pop(0))",
        "labels_text": "Test Hitpop"
    },
    {
        "input_text": "summarize: def test_sort(self):                self.assertEqual([hsp111, hsp112, hsp113], self.hit.hsps)        # sort by hsp length        key = lambda batch_hsp: len(batch_hsp[0])  # noqa: E731        self.hit.sort(key=key)        self.assertEqual([hsp112, hsp113, hsp111], self.hit.hsps)",
        "labels_text": "Test Hitsort"
    },
    {
        "input_text": "summarize: def test_init_no_fragment(self):                self.assertRaises(ValueError, HSP, [])",
        "labels_text": "Test HSPinit without fragment"
    },
    {
        "input_text": "summarize: def test_len(self):                self.assertEqual(1, len(self.hsp))",
        "labels_text": "Test HSPlen"
    },
    {
        "input_text": "summarize: def test_fragment(self):                self.assertIs(self.frag, self.hsp.fragment)",
        "labels_text": "Test HSPfragment property"
    },
    {
        "input_text": "summarize: def test_is_fragmented(self):                self.assertFalse(self.hsp.is_fragmented)",
        "labels_text": "Test HSPisfragmented property"
    },
    {
        "input_text": "summarize: def test_seq(self):                self.assertEqual(\"ATCAGT\", self.hsp.hit.seq)        self.assertEqual(\"AT-ACT\", self.hsp.query.seq)",
        "labels_text": "Test HSP sequence property"
    },
    {
        "input_text": "summarize: def test_alignment(self):                aln = self.hsp.aln        self.assertIsInstance(aln, MultipleSeqAlignment)        self.assertEqual(2, len(aln))        self.assertTrue(\"ATCAGT\", aln[0].seq)        self.assertTrue(\"AT-ACT\", aln[1].seq)",
        "labels_text": "Test HSPalignment property"
    },
    {
        "input_text": "summarize: def test_aln_span(self):                self.assertEqual(6, self.hsp.aln_span)",
        "labels_text": "Test HSPalnspan property"
    },
    {
        "input_text": "summarize: def test_span(self):                self.assertEqual(5, self.hsp.hit_span)        self.assertEqual(6, self.hsp.query_span)",
        "labels_text": "Test HSP span property"
    },
    {
        "input_text": "summarize: def test_range(self):                self.assertEqual((15, 20), self.hsp.hit_range)        self.assertEqual((0, 6), self.hsp.query_range)",
        "labels_text": "Test HSP range property"
    },
    {
        "input_text": "summarize: def test_pickle(self):                buf = BytesIO()        pickle.dump(self.hsp, buf)        unp = pickle.loads(buf.getvalue())        self.compare_search_obj(self.hsp, unp)",
        "labels_text": "Test pickling and unpickling of HSP"
    },
    {
        "input_text": "summarize: def test_len(self):                self.assertEqual(2, len(self.hsp))",
        "labels_text": "Test HSPlen"
    },
    {
        "input_text": "summarize: def test_getitem(self):                self.assertIs(self.frag1, self.hsp[0])        self.assertIs(self.frag2, self.hsp[1])",
        "labels_text": "Test HSPgetitem"
    },
    {
        "input_text": "summarize: def test_setitem_single(self):                frag3 = HSPFragment(\"hit_id\", \"query_id\", \"AAA\", \"AAT\")        self.hsp[1] = frag3        self.assertEqual(2, len(self.hsp))        self.assertIs(self.frag1, self.hsp[0])        self.assertIs(frag3, self.hsp[1])",
        "labels_text": "Test HSPsetitem single item"
    },
    {
        "input_text": "summarize: def test_setitem_multiple(self):                frag3 = HSPFragment(\"hit_id\", \"query_id\", \"AAA\", \"AAT\")        frag4 = HSPFragment(\"hit_id\", \"query_id\", \"GGG\", \"GAG\")        self.hsp[:2] = [frag3, frag4]        self.assertEqual(2, len(self.hsp))        self.assertIs(frag3, self.hsp[0])        self.assertIs(frag4, self.hsp[1])",
        "labels_text": "Test HSPsetitem multiple item"
    },
    {
        "input_text": "summarize: def test_delitem(self):                del self.hsp[0]        self.assertEqual(1, len(self.hsp))        self.assertIs(self.frag2, self.hsp[0])",
        "labels_text": "Test HSPdelitem"
    },
    {
        "input_text": "summarize: def test_contains(self):                frag3 = HSPFragment(\"hit_id\", \"query_id\", \"AAA\", \"AAT\")        self.assertIn(self.frag1, self.hsp)        self.assertNotIn(frag3, self.hsp)",
        "labels_text": "Test HSPcontains"
    },
    {
        "input_text": "summarize: def test_fragments(self):                self.assertEqual([self.frag1, self.frag2], self.hsp.fragments)",
        "labels_text": "Test HSPfragments property"
    },
    {
        "input_text": "summarize: def test_is_fragmented(self):                self.assertTrue(self.hsp.is_fragmented)",
        "labels_text": "Test HSPisfragmented property"
    },
    {
        "input_text": "summarize: def test_seqs(self):                self.assertEqual([\"ATCAGT\", \"GGG\"], [x.seq for x in self.hsp.hit_all])        self.assertEqual([\"AT-ACT\", \"CCC\"], [x.seq for x in self.hsp.query_all])",
        "labels_text": "Test HSP sequence property"
    },
    {
        "input_text": "summarize: def test_molecule_type(self):                self.assertIsNone(self.hsp.molecule_type)",
        "labels_text": "Test HSPmoleculetype getter"
    },
    {
        "input_text": "summarize: def test_range(self):                # range on HSP with multiple fragment should give the        # min start and max end coordinates        self.assertEqual((15, 161), self.hsp.hit_range)        self.assertEqual((0, 13), self.hsp.query_range)",
        "labels_text": "Test HSP range property"
    },
    {
        "input_text": "summarize: def test_ranges(self):                self.assertEqual([(15, 20), (158, 161)], self.hsp.hit_range_all)        self.assertEqual([(0, 6), (10, 13)], self.hsp.query_range_all)",
        "labels_text": "Test HSP range property"
    },
    {
        "input_text": "summarize: def test_span(self):                # span is always end - start        self.assertEqual(146, self.hsp.hit_span)        self.assertEqual(13, self.hsp.query_span)",
        "labels_text": "Test HSP span property"
    },
    {
        "input_text": "summarize: def test_seqmodel(self):                # all query, hit, and alignment objects should be None        self.assertIsNone(self.fragment.query)        self.assertIsNone(self.fragment.hit)        self.assertIsNone(self.fragment.aln)",
        "labels_text": "Test HSPFragment sequence attribute no alignment"
    },
    {
        "input_text": "summarize: def test_len(self):                self.assertRaises(TypeError, len, self)        # len is a shorthand for .aln_span, and it can be set manually        self.fragment.aln_span = 5        self.assertEqual(5, len(self.fragment))",
        "labels_text": "Test HSPFragmentlen no alignment"
    },
    {
        "input_text": "summarize: def test_repr(self):                # test for minimum repr        self.assertEqual(            \"HSPFragment(hit_id='hit_id', query_id='query_id')\", repr(self.fragment)        )        self.fragment.aln_span = 5        self.assertEqual(            \"HSPFragment(hit_id='hit_id', query_id='query_id', 5 columns)\",            repr(self.fragment),        )",
        "labels_text": "Test HSPFragmentrepr no alignment"
    },
    {
        "input_text": "summarize: def test_getitem(self):                # getitem not supported without alignment        self.assertRaises(TypeError, self.fragment.__getitem__, 0)        self.assertRaises(TypeError, self.fragment.__getitem__, slice(0, 2))",
        "labels_text": "Test HSPFragmentgetitem no alignment"
    },
    {
        "input_text": "summarize: def test_getitem_only_query(self):                # getitem should work if only query is present        self.fragment.query = \"AATCG\"        self.assertEqual(\"ATCG\", self.fragment[1:].query.seq)",
        "labels_text": "Test HSPFragmentgetitem only query"
    },
    {
        "input_text": "summarize: def test_getitem_only_hit(self):                # getitem should work if only query is present        self.fragment.hit = \"CATGC\"        self.assertEqual(\"ATGC\", self.fragment[1:].hit.seq)",
        "labels_text": "Test HSPFragmentgetitem only hit"
    },
    {
        "input_text": "summarize: def test_iter(self):                # iteration not supported        self.assertRaises(TypeError, iter, self)",
        "labels_text": "Test HSPiter no alignment"
    },
    {
        "input_text": "summarize: def test_pickle(self):                buf = BytesIO()        pickle.dump(self.fragment, buf)        unp = pickle.loads(buf.getvalue())        self.compare_search_obj(self.fragment, unp)",
        "labels_text": "Test pickling and unpickling of HSPFragment"
    },
    {
        "input_text": "summarize: def test_init_wrong_seqtypes(self):                # init should only work with string or seqrecords        wrong_query = Seq(\"ATGC\")        wrong_hit = Seq(\"ATGC\")        self.assertRaises(            TypeError, HSPFragment, \"hit_id\", \"query_id\", wrong_hit, wrong_query        )",
        "labels_text": "Test HSPFragmentinit wrong sequence argument type"
    },
    {
        "input_text": "summarize: def test_molecule_type_no_seq(self):                self.assertIsNone(self.fragment.molecule_type)        self.fragment.molecule_type = \"DNA\"        self.assertEqual(self.fragment.molecule_type, \"DNA\")",
        "labels_text": "Test HSPFragment moleculetype property query and hit sequence not present"
    },
    {
        "input_text": "summarize: def test_seq_unequal_hit_query_len(self):                for seq_type in (\"hit\", \"query\"):            opp_type = \"query\" if seq_type == \"hit\" else \"hit\"            # reset values first            fragment = HSPFragment(\"hit_id\", \"query_id\")            # and test it against the opposite            setattr(fragment, seq_type, \"ATGCACAACAGGA\")            self.assertRaises(ValueError, setattr, fragment, opp_type, \"ATGCGA\")",
        "labels_text": "Test HSPFragment sequence setter with unequal hit and query length"
    },
    {
        "input_text": "summarize: def test_len(self):                # len should equal alignment column length        self.assertEqual(12, len(self.fragment))",
        "labels_text": "Test HSPFragmentlen"
    },
    {
        "input_text": "summarize: def test_repr(self):                # test for minimum repr        self.assertEqual(            \"HSPFragment(hit_id='hit_id', query_id='query_id', 12 columns)\",            repr(self.fragment),        )",
        "labels_text": "Test HSPFragmentrepr"
    },
    {
        "input_text": "summarize: def test_getitem(self):                # getitem is supported when alignment is present        sliced_fragment = self.fragment[:5]        self.assertIsInstance(sliced_fragment, HSPFragment)        self.assertEqual(5, len(sliced_fragment))        self.assertEqual(\"ATGCT\", sliced_fragment.hit.seq)        self.assertEqual(\"ATG--\", sliced_fragment.query.seq)",
        "labels_text": "Test HSPFragmentgetitem"
    },
    {
        "input_text": "summarize: def test_getitem_alignment_annot(self):                # the alignment is annotated, it should be sliced accordingly        # and transferred to the new object        setattr(self.fragment, \"aln_annotation\", {\"test\": \"182718738172\"})        new_hsp = self.fragment[:5]        self.assertEqual(\"18271\", new_hsp.aln_annotation[\"test\"])",
        "labels_text": "Test HSPFragmentgetitem with alignment annotation"
    },
    {
        "input_text": "summarize: def test_frame_set_ok(self):                attr = \"frame\"        for seq_type in (\"query\", \"hit\"):            attr_name = f\"{seq_type}_{attr}\"            for value in (-3, -2, -1, 0, 1, 2, 3, None):                setattr(self.fragment, attr_name, value)                self.assertEqual(value, getattr(self.fragment, attr_name))",
        "labels_text": "Test HSPFragment query and hit frame setter"
    },
    {
        "input_text": "summarize: def test_frame_set_error(self):                attr = \"frame\"        for seq_type in (\"query\", \"hit\"):            func_name = f\"_{seq_type}_{attr}_set\"            func = getattr(self.fragment, func_name)            for value in (\"3\", \"+3\", \"-2\", \"plus\"):                self.assertRaises(ValueError, func, value)",
        "labels_text": "Test HSPFragment query and hit frame setter invalid value"
    },
    {
        "input_text": "summarize: def test_strand_set_ok(self):                attr = \"strand\"        for seq_type in (\"query\", \"hit\"):            attr_name = f\"{seq_type}_{attr}\"            for value in (-1, 0, 1, None):                setattr(self.fragment, attr_name, value)                self.assertEqual(value, getattr(self.fragment, attr_name))",
        "labels_text": "Test HSPFragment query and hit strand setter"
    },
    {
        "input_text": "summarize: def test_strand_set_error(self):                attr = \"strand\"        for seq_type in (\"query\", \"hit\"):            func_name = f\"_{seq_type}_{attr}_set\"            func = getattr(self.fragment, func_name)            for value in (3, \"plus\", \"minus\", \"-\", \"+\"):                self.assertRaises(ValueError, func, value)",
        "labels_text": "Test HSPFragment query and hit strand setter invalid value"
    },
    {
        "input_text": "summarize: def test_strand_set_from_plus_frame(self):                for seq_type in (\"query\", \"hit\"):            attr_name = f\"{seq_type}_strand\"            self.assertIsNone(getattr(self.fragment, attr_name))            setattr(self.fragment, f\"{seq_type}_frame\", 3)            self.assertEqual(1, getattr(self.fragment, attr_name))",
        "labels_text": "Test HSPFragment query and hit strand getters from plus frame"
    },
    {
        "input_text": "summarize: def test_strand_set_from_minus_frame(self):                for seq_type in (\"query\", \"hit\"):            attr_name = f\"{seq_type}_strand\"            self.assertIsNone(getattr(self.fragment, attr_name))            setattr(self.fragment, f\"{seq_type}_frame\", -2)            self.assertEqual(-1, getattr(self.fragment, attr_name))",
        "labels_text": "Test HSPFragment query and hit strand getters from minus frame"
    },
    {
        "input_text": "summarize: def test_strand_set_from_zero_frame(self):                for seq_type in (\"query\", \"hit\"):            attr_name = f\"{seq_type}_strand\"            self.assertIsNone(getattr(self.fragment, attr_name))            setattr(self.fragment, f\"{seq_type}_frame\", 0)            self.assertEqual(0, getattr(self.fragment, attr_name))",
        "labels_text": "Test HSPFragment query and hit strand getters from zero frame"
    },
    {
        "input_text": "summarize: def test_coords_setters_readonly(self):                read_onlies = (\"range\", \"span\")        for seq_type in (\"query\", \"hit\"):            for attr in read_onlies:                self.assertRaises(                    AttributeError,                    setattr,                    self.fragment,                    f\"{seq_type}_{attr}\",                    5,                )",
        "labels_text": "Test HSPFragment query and hit coordinaterelated readonly getters"
    },
    {
        "input_text": "summarize: def read_write_and_compare(        self, source_file, source_format, out_file, out_format, **kwargs    ):                source_qresult = SearchIO.read(source_file, source_format, **kwargs)        SearchIO.write(source_qresult, out_file, out_format, **kwargs)        out_qresult = SearchIO.read(out_file, out_format, **kwargs)        self.compare_search_obj(source_qresult, out_qresult)",
        "labels_text": "Compare read QueryResults after it ha been written to a file"
    },
    {
        "input_text": "summarize: def test_write_single_from_blastxml(self):                source = os.path.join(\"Blast\", \"xml_2226_blastp_004.xml\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blastxml writing from blastxml BLAST single query xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_blastxml(self):                source = os.path.join(\"Blast\", \"xml_2226_blastp_001.xml\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blastxml writing from blastxml BLAST multiple query xmlblastpxml"
    },
    {
        "input_text": "summarize: def test_write_single_from_blasttab(self):                source = os.path.join(\"Blast\", \"tab_2226_tblastn_004.txt\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blasttab writing from blasttab BLAST single query tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_blasttab(self):                source = os.path.join(\"Blast\", \"tab_2226_tblastn_001.txt\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blasttab writing from blasttab BLAST multiple query tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_write_single_from_blasttabc(self):                source = os.path.join(\"Blast\", \"tab_2226_tblastn_008.txt\")        self.parse_write_and_compare(            source, self.fmt, self.out, self.fmt, comments=True        )        self.read_write_and_compare(source, self.fmt, self.out, self.fmt, comments=True)",
        "labels_text": "Test blasttabc writing from blasttabc BLAST single query tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_blasttabc(self):                source = os.path.join(\"Blast\", \"tab_2226_tblastn_005.txt\")        self.parse_write_and_compare(            source, self.fmt, self.out, self.fmt, comments=True        )",
        "labels_text": "Test blasttabc writing from blasttabc BLAST multiple query tabtblastntxt"
    },
    {
        "input_text": "summarize: def test_write_single_from_hmmertab(self):                source = os.path.join(\"Hmmer\", \"tab_30_hmmscan_004.out\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test hmmertab writing from hmmertab HMMER single query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_hmmertab(self):                source = os.path.join(\"Hmmer\", \"tab_30_hmmscan_001.out\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test hmmertab writing from hmmertab HMMER multiple query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_write_single_from_hmmscandomtab(self):                source = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_004.out\")        fmt = \"hmmscan3-domtab\"        self.parse_write_and_compare(source, fmt, self.out, fmt)        self.read_write_and_compare(source, fmt, self.out, fmt)",
        "labels_text": "Test hmmscandomtab writing from hmmscandomtab HMMER single query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_hmmscandomtab(self):                source = os.path.join(\"Hmmer\", \"domtab_30_hmmscan_001.out\")        fmt = \"hmmscan3-domtab\"        self.parse_write_and_compare(source, fmt, self.out, fmt)",
        "labels_text": "Test hmmscandomtab writing from hmmscandomtab HMMER multiple query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_write_single_from_hmmsearchdomtab(self):                source = os.path.join(\"Hmmer\", \"domtab_30_hmmsearch_001.out\")        fmt = \"hmmsearch3-domtab\"        self.parse_write_and_compare(source, fmt, self.out, fmt)        self.read_write_and_compare(source, fmt, self.out, fmt)",
        "labels_text": "Test hmmsearchdomtab writing from hmmsearchdomtab HMMER single query tabhmmscanout"
    },
    {
        "input_text": "summarize: def test_write_single_from_blatpsl(self):                source = os.path.join(\"Blat\", \"psl_34_004.psl\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blatpsl writing from blatpsl single query pslpsl"
    },
    {
        "input_text": "summarize: def test_write_single_from_blatpsl_protein_query(self):                source = os.path.join(\"Blat\", \"psl_35_002.psl\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blatpsl writing from blatpsl single query pslpsl"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_blatpsl(self):                source = os.path.join(\"Blat\", \"psl_34_001.psl\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt)",
        "labels_text": "Test blatpsl writing from blatpsl multiple query pslpsl"
    },
    {
        "input_text": "summarize: def test_write_single_from_blatpslx(self):                source = os.path.join(\"Blat\", \"pslx_34_004.pslx\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt, pslx=True)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx writing from blatpslx single query pslxpslx"
    },
    {
        "input_text": "summarize: def test_write_single_from_blatpslx_protein_query(self):                source = os.path.join(\"Blat\", \"pslx_35_002.pslx\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt, pslx=True)        self.read_write_and_compare(source, self.fmt, self.out, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx writing from blatpslx single query pslxpslx"
    },
    {
        "input_text": "summarize: def test_write_multiple_from_blatpslx(self):                source = os.path.join(\"Blat\", \"pslx_34_001.pslx\")        self.parse_write_and_compare(source, self.fmt, self.out, self.fmt, pslx=True)",
        "labels_text": "Test blatpslx writing from blatpslx multiple query pslxpslx"
    },
    {
        "input_text": "summarize: def test_as_string(self):                self.assertEqual(\"TCAAAAGGATGCATCATG\", self.s)",
        "labels_text": "Test converting Seq to string"
    },
    {
        "input_text": "summarize: def test_repr(self):                self.assertEqual(\"Seq('TCAAAAGGATGCATCATG')\", repr(self.s))",
        "labels_text": "Test representation of Seq object"
    },
    {
        "input_text": "summarize: def test_length(self):                self.assertEqual(18, len(self.s))",
        "labels_text": "Test len method on Seq object"
    },
    {
        "input_text": "summarize: def test_first_nucleotide(self):                self.assertEqual(\"T\", self.s[0])",
        "labels_text": "Test getting first nucleotide of Seq"
    },
    {
        "input_text": "summarize: def test_last_nucleotide(self):                self.assertEqual(\"G\", self.s[-1])",
        "labels_text": "Test getting last nucleotide of Seq"
    },
    {
        "input_text": "summarize: def test_slicing(self):                self.assertEqual(\"AA\", self.s[3:5])",
        "labels_text": "Test slicing of Seq"
    },
    {
        "input_text": "summarize: def test_reverse(self):                self.assertEqual(\"GTACTACGTAGGAAAACT\", self.s[::-1])",
        "labels_text": "Test reverse using stride"
    },
    {
        "input_text": "summarize: def test_extract_third_nucleotide(self):                self.assertEqual(\"TAGTAA\", self.s[0::3])        self.assertEqual(\"CAGGTT\", self.s[1::3])        self.assertEqual(\"AAACCG\", self.s[2::3])",
        "labels_text": "Test extracting every third nucleotide slicing with stride"
    },
    {
        "input_text": "summarize: def test_not_equal_comparsion(self):                self.assertNotEqual(Seq.Seq(\"TCAAA\"), Seq.Seq(\"TCAAAA\"))",
        "labels_text": "Test ne comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_comparison(self):                self.assertLess(self.s[:-1], self.s)",
        "labels_text": "Test lt comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_comparison_of_incompatible_types(self):                with self.assertRaises(TypeError):            self.s < 1",
        "labels_text": "Test incompatible type lt comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_or_equal_comparison(self):                self.assertLessEqual(self.s, self.s)",
        "labels_text": "Test le comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_or_equal_comparison_of_incompatible_types(self):                with self.assertRaises(TypeError):            self.s <= 1",
        "labels_text": "Test incompatible type le comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_comparison(self):                self.assertGreater(self.s, self.s[:-1])",
        "labels_text": "Test gt comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_comparison_of_incompatible_types(self):                with self.assertRaises(TypeError):            self.s > 1",
        "labels_text": "Test incompatible type gt comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_or_equal_comparison(self):                self.assertGreaterEqual(self.s, self.s)",
        "labels_text": "Test ge comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_or_equal_comparison_of_incompatible_types(self):                with self.assertRaises(TypeError):            self.s >= 1",
        "labels_text": "Test incompatible type ge comparison method"
    },
    {
        "input_text": "summarize: def test_mul_method(self):                for seq in test_seqs + protein_seqs:            self.assertEqual(seq * 3, seq + seq + seq)        if np is not None:            factor = np.intc(3)  # numpy integer            for seq in test_seqs + protein_seqs:                self.assertEqual(seq * factor, seq + seq + seq)",
        "labels_text": "Test mul method relies on addition method"
    },
    {
        "input_text": "summarize: def test_mul_method_exceptions(self):                for seq in test_seqs + protein_seqs:            with self.assertRaises(TypeError):                seq * 3.0            with self.assertRaises(TypeError):                seq * \"\"",
        "labels_text": "Test mul method exception"
    },
    {
        "input_text": "summarize: def test_rmul_method(self):                for seq in test_seqs + protein_seqs:            self.assertEqual(3 * seq, seq + seq + seq)        if np is not None:            factor = np.intc(3)  # numpy integer            for seq in test_seqs + protein_seqs:                self.assertEqual(factor * seq, seq + seq + seq)",
        "labels_text": "Test rmul method relies on addition method"
    },
    {
        "input_text": "summarize: def test_rmul_method_exceptions(self):                for seq in test_seqs + protein_seqs:            with self.assertRaises(TypeError):                3.0 * seq            with self.assertRaises(TypeError):                \"\" * seq",
        "labels_text": "Test rmul method exception"
    },
    {
        "input_text": "summarize: def test_imul_method_exceptions(self):                for seq in test_seqs + protein_seqs:            with self.assertRaises(TypeError):                seq *= 3.0            with self.assertRaises(TypeError):                seq *= \"\"",
        "labels_text": "Test imul method exception"
    },
    {
        "input_text": "summarize: def test_equal_comparison(self):                self.assertEqual(self.mutable_s, \"TCAAAAGGATGCATCATG\")",
        "labels_text": "Test eq comparison method"
    },
    {
        "input_text": "summarize: def test_not_equal_comparison(self):                self.assertNotEqual(self.mutable_s, \"other thing\")",
        "labels_text": "Test ne comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_comparison(self):                self.assertLess(self.mutable_s[:-1], self.mutable_s)",
        "labels_text": "Test lt comparison method"
    },
    {
        "input_text": "summarize: def test_less_than_or_equal_comparison(self):                self.assertLessEqual(self.mutable_s[:-1], self.mutable_s)",
        "labels_text": "Test le comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_comparison(self):                self.assertGreater(self.mutable_s, self.mutable_s[:-1])",
        "labels_text": "Test gt comparison method"
    },
    {
        "input_text": "summarize: def test_greater_than_or_equal_comparison(self):                self.assertGreaterEqual(self.mutable_s, self.mutable_s)",
        "labels_text": "Test ge comparison method"
    },
    {
        "input_text": "summarize: def test_add_method(self):                with self.assertRaises(TypeError):            self.mutable_s + 1234",
        "labels_text": "Test adding wrong type to MutableSeq"
    },
    {
        "input_text": "summarize: def test_reverse(self):                self.mutable_s.reverse()        self.assertEqual(Seq.MutableSeq(\"GTACTACGTAGGAAAACT\"), self.mutable_s)",
        "labels_text": "Test using reverse method"
    },
    {
        "input_text": "summarize: def test_reverse_with_stride(self):                self.assertEqual(Seq.MutableSeq(\"GTACTACGTAGGAAAACT\"), self.mutable_s[::-1])",
        "labels_text": "Test reverse using stride"
    },
    {
        "input_text": "summarize: def test_extract_third_nucleotide(self):                self.assertEqual(Seq.MutableSeq(\"TAGTAA\"), self.mutable_s[0::3])        self.assertEqual(Seq.MutableSeq(\"CAGGTT\"), self.mutable_s[1::3])        self.assertEqual(Seq.MutableSeq(\"AAACCG\"), self.mutable_s[2::3])",
        "labels_text": "Test extracting every third nucleotide slicing with stride"
    },
    {
        "input_text": "summarize: def test_ambiguous_values(self):                self.assertNotIn(\"-\", ambiguous_dna_values)        self.assertNotIn(\"?\", ambiguous_dna_values)",
        "labels_text": "Test that other test do not introduce character to our value"
    },
    {
        "input_text": "summarize: def test_translation_wrong_type(self):                seq = Seq.Seq(\"ATCGTA\")        with self.assertRaises(ValueError):            seq.translate(table=ambiguous_dna_complement)",
        "labels_text": "Test translation table cannot be CodonTable"
    },
    {
        "input_text": "summarize: def test_translation_checks_cds(self):                seq = Seq.Seq(\"GGTTACACTTACCGATAATGTCTCTGATGA\")        f = SeqFeature(SimpleLocation(0, 30), type=\"CDS\")        f.qualifiers[\"transl_table\"] = [11]        with self.assertRaises(TranslationError):            f.translate(seq)",
        "labels_text": "Test that a CDS feature is subject to respective check"
    },
    {
        "input_text": "summarize: def test_reference_in_location_sequence(self):                parent_sequence = Seq.Seq(\"actg\")        another_sequence = Seq.Seq(\"gtcagctac\")        location = SimpleLocation(5, 8, ref=\"ANOTHER.7\")        sequence = location.extract(            parent_sequence, references={\"ANOTHER.7\": another_sequence}        )        self.assertEqual(type(sequence), Seq.Seq)        self.assertEqual(sequence, \"cta\")",
        "labels_text": "Test location with reference to another sequence"
    },
    {
        "input_text": "summarize: def get_mode(cls, fmt):                mode = cls.modes.get(fmt)        if mode is not None:            return mode        for mode, stream in ((\"t\", StringIO()), (\"b\", BytesIO())):            try:                SeqIO.read(stream, fmt)            except StreamModeError:                continue            except ValueError:  # SeqIO.read will complain that the stream is empty                pass            cls.modes[fmt] = mode            return mode        raise RuntimeError(f\"Failed to find file mode for {fmt}\")",
        "labels_text": "Determine if file mode should be text t or binary b based on format"
    },
    {
        "input_text": "summarize: def compare_records(self, old_list, new_list, *args, **kwargs):                self.assertEqual(len(old_list), len(new_list))        for old, new in zip(old_list, new_list):            self.compare_record(old, new, *args, **kwargs)",
        "labels_text": "Check if two list of SeqRecords are equal"
    },
    {
        "input_text": "summarize: def __init__(self, handle):                self._handle = handle",
        "labels_text": "Initialize the class"
    },
    {
        "input_text": "summarize: def __iter__(self):                return iter(self._handle)",
        "labels_text": "Iterate"
    },
    {
        "input_text": "summarize: def __next__(self):                return next(self._handle)",
        "labels_text": "Get the next line"
    },
    {
        "input_text": "summarize: def test_gzip_fastq(self):                with gzip.open(\"Quality/example.fastq.gz\", \"rt\") as handle:            self.assertEqual(3, len(list(SeqIO.parse(handle, \"fastq\"))))        with gzip.open(\"Quality/example.fastq.gz\") as handle:            with self.assertRaisesRegex(                ValueError, \"Fastq files must be opened in text mode\"            ):                list(SeqIO.parse(handle, \"fastq\"))",
        "labels_text": "Testing FASTQ with gzip"
    },
    {
        "input_text": "summarize: def test_gzip_fasta(self):                with gzip.open(\"Fasta/flowers.pro.gz\", \"rt\") as handle:            self.assertEqual(3, len(list(SeqIO.parse(handle, \"fasta\"))))        with gzip.open(\"Fasta/flowers.pro.gz\") as handle:            with self.assertRaisesRegex(                ValueError, \"Fasta files must be opened in text mode\"            ):                list(SeqIO.parse(handle, \"fasta\"))",
        "labels_text": "Testing FASTA with gzip"
    },
    {
        "input_text": "summarize: def test_genbank22(self):                record = SeqIO.read(\"GenBank/protein_refseq.gb\", \"genbank\")        db_source = record.annotations.get(\"db_source\")        handle = StringIO()        SeqIO.write(record, handle, \"genbank\")        handle.seek(0)        read_record = SeqIO.read(handle, \"genbank\")        read_db_source = read_record.annotations.get(\"db_source\")        self.assertEqual(db_source, read_db_source)",
        "labels_text": "Test that genbank format write doesnt destroy dbsource in annotation"
    },
    {
        "input_text": "summarize: def test_fasta_to_seqxml_without_mol_type(self):                handle = BytesIO()        self.assertRaises(            ValueError, SeqIO.convert, \"Fasta/rosemary.pro\", \"fasta\", handle, \"seqxml\"        )",
        "labels_text": "Convert FASTA to SeqXML without molecule type"
    },
    {
        "input_text": "summarize: def test_fasta_to_seqxml_with_mol_type(self):                handle = BytesIO()        self.assertEqual(            1, SeqIO.convert(\"Fasta/rosemary.pro\", \"fasta\", handle, \"seqxml\", \"protein\")        )        self.assertIn(            b'<property name=\"molecule_type\" value=\"protein\">', handle.getvalue()        )",
        "labels_text": "Convert FASTA to SeqXML with molecule type"
    },
    {
        "input_text": "summarize: def test_clustal_to_nexus_without_mol_type(self):                handle = StringIO()        self.assertRaises(            ValueError,            SeqIO.convert,            \"Clustalw/protein.aln\",            \"clustal\",            handle,            \"nexus\",        )",
        "labels_text": "Convert Clustal to NEXUS without molecule type"
    },
    {
        "input_text": "summarize: def test_clustal_to_nexus_with_mol_type(self):                handle = StringIO()        self.assertEqual(            20,            SeqIO.convert(                \"Clustalw/protein.aln\", \"clustal\", handle, \"nexus\", \"protein\"            ),        )        self.assertIn(\" datatype=protein \", handle.getvalue())",
        "labels_text": "Convert Clustal to NEXUS with molecule type"
    },
    {
        "input_text": "summarize: def test_file_type(self):                for trace in test_data:            self.assertEqual(test_data[trace][\"handle\"].read(4), b\"ABIF\")",
        "labels_text": "Test if filetype is ABIF"
    },
    {
        "input_text": "summarize: def test_trim(self):                for trace in test_data:            record = SeqIO.read(test_data[trace][\"handle\"], \"abi-trim\")            if trace != \"data_empty\" and trace != \"test_fsa\":                self.assertNotEqual(record.seq, test_data[trace][\"seq\"])                self.assertIn(str(record.seq), test_data[trace][\"seq\"])            else:                self.assertEqual(record.seq, test_data[trace][\"seq\"])",
        "labels_text": "Test if trim work"
    },
    {
        "input_text": "summarize: def test_file_mode(self):                for trace in test_data:            self.assertRaises(ValueError, SeqIO.read, test_data[trace][\"handle\"], \"abi\")",
        "labels_text": "Test if exception is raised if file is not opened in rb mode"
    },
    {
        "input_text": "summarize: def test_file_type(self):                for trace in test_data_fake:            self.assertRaises(                ValueError, SeqIO.read, test_data_fake[trace][\"handle\"], \"abi\"            )",
        "labels_text": "Test if error is raised if filetype is not ABIF"
    },
    {
        "input_text": "summarize: def read_title_and_seq(filename):        with open(filename) as handle:        title = handle.readline().rstrip()        assert title.startswith(\">\")        seq = \"\"        for line in handle:            if line.startswith(\">\"):                break            seq += line.strip()    return title[1:], seq",
        "labels_text": "Crude parser that get the first record from a FASTA file"
    },
    {
        "input_text": "summarize: def test_fails(self):                self.assertRaises(ValueError, SeqIO.read, \"Fasta/aster.pro\", \"fasta-2line\")",
        "labels_text": "Test case which should fail"
    },
    {
        "input_text": "summarize: def test_no_name(self):                handle = StringIO(\">\\nACGT\")        record = SeqIO.read(handle, \"fasta\")        handle.close()        self.assertEqual(record.seq, \"ACGT\")        self.assertEqual(\"\", record.id)        self.assertEqual(\"\", record.name)        self.assertEqual(\"\", record.description)",
        "labels_text": "Test FASTA record with no identifier"
    },
    {
        "input_text": "summarize: def test_single_nucleic_files(self):                paths = (            \"Fasta/lupine.nu\",            \"Fasta/elderberry.nu\",            \"Fasta/phlox.nu\",            \"Fasta/centaurea.nu\",            \"Fasta/wisteria.nu\",            \"Fasta/sweetpea.nu\",            \"Fasta/lavender.nu\",            \"Fasta/f001\",        )        for path in paths:            self.simple_check(path)",
        "labels_text": "Test Fasta file containing a single nucleotide sequence"
    },
    {
        "input_text": "summarize: def test_single_proteino_files(self):                paths = (            \"Fasta/aster.pro\",            \"Fasta/rosemary.pro\",            \"Fasta/rose.pro\",            \"Fasta/loveliesbleeding.pro\",        )        for path in paths:            self.simple_check(path)",
        "labels_text": "Test Fasta file containing a single protein sequence"
    },
    {
        "input_text": "summarize: def test_regular_FastaTwoLineParser(self):                for inp, out in zip(self.ins_two_line, self.outs_two_line):            handle1 = StringIO(inp)            handle2 = StringIO(inp + \"\\n\")            self.assertEqual(list(FastaTwoLineParser(handle1)), out)            self.assertEqual(list(FastaTwoLineParser(handle2)), out)",
        "labels_text": "Test regular FastaTwoLineParser case"
    },
    {
        "input_text": "summarize: def test_edgecases_SimpleFastaParser(self):                for inp, out in zip(self.ins_two_line_edges, self.outs_two_line_edges):            handle = StringIO(inp)            self.assertEqual(list(SimpleFastaParser(handle)), out)        for inp, out in zip(self.ins_simple_edges, self.outs_simple_edges):            handle = StringIO(inp)            self.assertEqual(list(SimpleFastaParser(handle)), out)",
        "labels_text": "Test SimpleFastaParser edgecases"
    },
    {
        "input_text": "summarize: def test_edgecases_FastaTwoLineParser(self):                for inp, out in zip(self.ins_two_line_edges, self.outs_two_line_edges):            handle = StringIO(inp)            self.assertEqual(list(FastaTwoLineParser(handle)), out)",
        "labels_text": "Test FastaTwoLineParser edgecases"
    },
    {
        "input_text": "summarize: def test_exceptions_FastaTwoLineParser(self):                for inp in self.ins_multiline + self.ins_simple_edges:            handle = StringIO(inp)            with self.assertRaises(ValueError):                list(FastaTwoLineParser(handle))",
        "labels_text": "Test FastaTwoLineParser exception"
    },
    {
        "input_text": "summarize: def test_fasta_blast(self):                expected = self.expected        record = SeqIO.read(\"Fasta/aster_blast.pro\", \"fasta-blast\")        self.assertEqual(expected.id, record.id)        self.assertEqual(expected.name, record.name)        self.assertEqual(expected.description, record.description)        self.assertEqual(expected.seq, record.seq)",
        "labels_text": "Test FastaBlastIterator"
    },
    {
        "input_text": "summarize: def test_fasta_pearson(self):                expected = self.expected        record = SeqIO.read(\"Fasta/aster_pearson.pro\", \"fasta-pearson\")        self.assertEqual(expected.id, record.id)        self.assertEqual(expected.name, record.name)        self.assertEqual(expected.description, record.description)        self.assertEqual(expected.seq, record.seq)",
        "labels_text": "Test FastaPearsonIterator"
    },
    {
        "input_text": "summarize: def test_valueerrors(self):                self.assertRaises(            ValueError, SeqIO.read, \"Fasta/aster_pearson.pro\", \"fasta-blast\"        )        with self.assertWarns(BiopythonDeprecationWarning):            record = SeqIO.read(\"Fasta/aster_pearson.pro\", \"fasta\")        with self.assertWarns(BiopythonDeprecationWarning):            record = SeqIO.read(\"Fasta/aster_blast.pro\", \"fasta\")",
        "labels_text": "Test if ValueErrors are raised if comment are found unexpectedly"
    },
    {
        "input_text": "summarize: def test_simple_rna(self):                s = Seq(\"GAUCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10))        self.assertIsNone(f.location.strand)        self.check(s, f, \"YWSMK\", \"6..10\")",
        "labels_text": "Feature on RNA simple default strand"
    },
    {
        "input_text": "summarize: def test_simple_dna(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10))        self.check(s, f, \"YWSMK\", \"6..10\")",
        "labels_text": "Feature on DNA simple default strand"
    },
    {
        "input_text": "summarize: def test_single_letter_dna(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 6))        self.check(s, f, \"Y\", \"6\")",
        "labels_text": "Feature on DNA single letter default strand"
    },
    {
        "input_text": "summarize: def test_zero_len_dna(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 5))        self.check(s, f, \"\", \"5^6\")",
        "labels_text": "Feature on DNA between location zero length default strand"
    },
    {
        "input_text": "summarize: def test_zero_len_dna_end(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(15, 15))        self.check(s, f, \"\", \"15^1\")",
        "labels_text": "Feature on DNA between location at end zero length default strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_strand0(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10, strand=0))        self.check(s, f, \"YWSMK\", \"6..10\")",
        "labels_text": "Feature on DNA simple strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_strand_none(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10, strand=None))        self.check(s, f, \"YWSMK\", \"6..10\")",
        "labels_text": "Feature on DNA simple strand None"
    },
    {
        "input_text": "summarize: def test_simple_dna_strand1(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10, strand=1))        self.assertEqual(f.location.strand, +1)        self.check(s, f, \"YWSMK\", \"6..10\")",
        "labels_text": "Feature on DNA simple strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_strand_minus(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f = SeqFeature(SimpleLocation(5, 10, strand=-1))        self.assertEqual(f.location.strand, -1)        self.check(s, f, \"MKSWR\", \"complement(6..10)\")",
        "labels_text": "Feature on DNA simple strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_join(self):                s = Seq(\"GATCRYWSMKHBVDN\")        f1 = SeqFeature(SimpleLocation(5, 10, strand=1))        f2 = SeqFeature(SimpleLocation(12, 15, strand=1))        f = make_join_feature([f1, f2])        self.check(s, f, \"YWSMKVDN\", \"join(6..10,13..15)\")",
        "labels_text": "Feature on DNA join strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_join_strand_minus(self):                s = Seq(\"AAAAACCCCCTTTTTGGGGG\")        f1 = SeqFeature(SimpleLocation(5, 10, strand=-1))        f2 = SeqFeature(SimpleLocation(12, 15, strand=-1))        f = make_join_feature([f1, f2])        self.check(            s, f, reverse_complement(\"CCCCCTTT\"), \"complement(join(6..10,13..15))\"        )",
        "labels_text": "Feature on DNA join strand"
    },
    {
        "input_text": "summarize: def test_simple_dna_join_before(self):                s = Seq(\"AAAAACCCCCTTTTTGGGGG\")        f1 = SeqFeature(SimpleLocation(BeforePosition(5), 10, strand=-1))        f2 = SeqFeature(SimpleLocation(12, 15, strand=-1))        f = make_join_feature([f1, f2])        self.check(            s, f, reverse_complement(\"CCCCCTTT\"), \"complement(join(<6..10,13..15))\"        )",
        "labels_text": "Feature on DNA join strand before position"
    },
    {
        "input_text": "summarize: def test_simple_dna_join_after(self):                s = Seq(\"AAAAACCCCCTTTTTGGGGG\")        f1 = SeqFeature(SimpleLocation(5, 10, strand=-1))        f2 = SeqFeature(SimpleLocation(12, AfterPosition(15), strand=-1))        f = make_join_feature([f1, f2])        self.check(            s, f, reverse_complement(\"CCCCCTTT\"), \"complement(join(6..10,13..>15))\"        )",
        "labels_text": "Feature on DNA join strand after position"
    },
    {
        "input_text": "summarize: def test_protein_simple(self):                s = Seq(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")        f = SeqFeature(SimpleLocation(5, 10))        self.check(s, f, \"FGHIJ\", \"6..10\")",
        "labels_text": "Feature on protein simple"
    },
    {
        "input_text": "summarize: def test_protein_join(self):                s = Seq(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")        f1 = SeqFeature(SimpleLocation(5, 10))        f2 = SeqFeature(SimpleLocation(15, 20))        f = make_join_feature([f1, f2])        self.check(s, f, \"FGHIJPQRST\", \"join(6..10,16..20)\")",
        "labels_text": "Feature on protein join"
    },
    {
        "input_text": "summarize: def test_protein_between(self):                s = Seq(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")        f = SeqFeature(SimpleLocation(5, 5))        self.check(s, f, \"\", \"5^6\")",
        "labels_text": "Feature on protein between location zero length"
    },
    {
        "input_text": "summarize: def test_qualifiers(self):                f = SeqFeature(SimpleLocation(10, 20, strand=+1), type=\"CDS\")        self.assertEqual(f.qualifiers, {})        f = SeqFeature(            SimpleLocation(10, 20, strand=+1),            type=\"CDS\",            qualifiers={\"test\": [\"a test\"]},        )        self.assertEqual(f.qualifiers[\"test\"], [\"a test\"])",
        "labels_text": "Pass in qualifier to SeqFeatures"
    },
    {
        "input_text": "summarize: def test_NC_000932(self):                self.write_read(os.path.join(\"GenBank\", \"NC_000932.gb\"), \"gb\")",
        "labels_text": "Write and read back NCgb"
    },
    {
        "input_text": "summarize: def test_NC_005816(self):                self.write_read(os.path.join(\"GenBank\", \"NC_005816.gb\"), \"gb\")",
        "labels_text": "Write and read back NCgb"
    },
    {
        "input_text": "summarize: def test_gbvrl1_start(self):                self.write_read(os.path.join(\"GenBank\", \"gbvrl1_start.seq\"), \"gb\")",
        "labels_text": "Write and read back gbvrlstartseq"
    },
    {
        "input_text": "summarize: def test_NT_019265(self):                self.write_read(os.path.join(\"GenBank\", \"NT_019265.gb\"), \"gb\")",
        "labels_text": "Write and read back NTgb"
    },
    {
        "input_text": "summarize: def test_cor6(self):                self.write_read(os.path.join(\"GenBank\", \"cor6_6.gb\"), \"gb\")",
        "labels_text": "Write and read back corgb"
    },
    {
        "input_text": "summarize: def test_arab1(self):                self.write_read(os.path.join(\"GenBank\", \"arab1.gb\"), \"gb\")",
        "labels_text": "Write and read back arabgb"
    },
    {
        "input_text": "summarize: def test_one_of(self):                self.write_read(os.path.join(\"GenBank\", \"one_of.gb\"), \"gb\")",
        "labels_text": "Write and read back ofonegb"
    },
    {
        "input_text": "summarize: def test_pri1(self):                self.write_read(os.path.join(\"GenBank\", \"pri1.gb\"), \"gb\")",
        "labels_text": "Write and read back prigb"
    },
    {
        "input_text": "summarize: def test_noref(self):                self.write_read(os.path.join(\"GenBank\", \"noref.gb\"), \"gb\")",
        "labels_text": "Write and read back norefgb"
    },
    {
        "input_text": "summarize: def test_origin_line(self):                self.write_read(os.path.join(\"GenBank\", \"origin_line.gb\"), \"gb\")",
        "labels_text": "Write and read back originlinegb"
    },
    {
        "input_text": "summarize: def test_dbsource_wrap(self):                with warnings.catch_warnings():            # Ignore warning about over long DBSOURCE line            warnings.simplefilter(\"ignore\", category=BiopythonWarning)            self.write_read(os.path.join(\"GenBank\", \"dbsource_wrap.gb\"), \"gb\", [\"gb\"])",
        "labels_text": "Write and read back dbsourcewrapgb"
    },
    {
        "input_text": "summarize: def test_blank_seq(self):                self.write_read(os.path.join(\"GenBank\", \"blank_seq.gb\"), \"gb\", [\"gb\"])",
        "labels_text": "Write and read back blankseqgb"
    },
    {
        "input_text": "summarize: def test_extra_keywords(self):                self.write_read(os.path.join(\"GenBank\", \"extra_keywords.gb\"), \"gb\")",
        "labels_text": "Write and read back extrakeywordsgb"
    },
    {
        "input_text": "summarize: def test_protein_refseq(self):                self.write_read(os.path.join(\"GenBank\", \"protein_refseq.gb\"), \"gb\", [\"gb\"])",
        "labels_text": "Write and read back proteinrefseqgb"
    },
    {
        "input_text": "summarize: def test_protein_refseq2(self):                self.write_read(os.path.join(\"GenBank\", \"protein_refseq2.gb\"), \"gb\", [\"gb\"])",
        "labels_text": "Write and read back proteinrefseqgb"
    },
    {
        "input_text": "summarize: def test_AAA03323(self):                self.write_read(os.path.join(\"EMBL\", \"AAA03323.embl\"), \"embl\")",
        "labels_text": "Write and read back AAAembl"
    },
    {
        "input_text": "summarize: def test_AE017046(self):                self.write_read(os.path.join(\"EMBL\", \"AE017046.embl\"), \"embl\")",
        "labels_text": "Write and read back AEembl"
    },
    {
        "input_text": "summarize: def test_DD231055_edited(self):                self.write_read(os.path.join(\"EMBL\", \"DD231055_edited.embl\"), \"embl\")",
        "labels_text": "Write and read back DDeditedembl"
    },
    {
        "input_text": "summarize: def test_Human_contigs(self):                self.write_read(os.path.join(\"EMBL\", \"Human_contigs.embl\"), \"embl\")",
        "labels_text": "Write and read back Humancontigsembl"
    },
    {
        "input_text": "summarize: def test_SC10H5(self):                self.write_read(os.path.join(\"EMBL\", \"SC10H5.embl\"), \"embl\")",
        "labels_text": "Write and read back SCHembl"
    },
    {
        "input_text": "summarize: def test_TRBG361(self):                self.write_read(os.path.join(\"EMBL\", \"TRBG361.embl\"), \"embl\")",
        "labels_text": "Write and read back TRBGembl"
    },
    {
        "input_text": "summarize: def test_U87107(self):                self.write_read(os.path.join(\"EMBL\", \"U87107.embl\"), \"embl\")",
        "labels_text": "Write and read back Uembl"
    },
    {
        "input_text": "summarize: def test_read(self):                stream = BytesIO(b\"tiny\")        with self.assertRaisesRegex(            ValueError, \"Improper header, cannot read 24 bytes from stream\"        ):            SeqIO.read(stream, \"gck\")        stream.close()",
        "labels_text": "Read a file with an incomplete header"
    },
    {
        "input_text": "summarize: def test_read_GFA2(self):                records = list(SeqIO.parse(\"GFA/fake_gfa2.gfa\", \"gfa2\"))        self.assertEqual(len(records), 1)        self.assertEqual(records[0].seq, \"AAA\")",
        "labels_text": "Test parsing valid GFA file"
    },
    {
        "input_text": "summarize: def test_corrupt_gfa2(self):                with self.assertRaises(ValueError):            list(SeqIO.parse(\"GFA/seq.gfa\", \"gfa2\"))",
        "labels_text": "Check a GFA x file doe not parse in GFA"
    },
    {
        "input_text": "summarize: def test_corrupt_segment_fields(self):                with self.assertRaises(ValueError):            list(SeqIO.parse(\"GFA/corrupt_segment_fields.gfa\", \"gfa1\"))",
        "labels_text": "Check a GFA file with invalid field on a segment line"
    },
    {
        "input_text": "summarize: def test_corrupt_len(self):                with self.assertWarns(BiopythonWarning):            list(SeqIO.parse(\"GFA/corrupt_len.gfa\", \"gfa1\"))",
        "labels_text": "Check a GFA file with an incorrect length"
    },
    {
        "input_text": "summarize: def test_corrupt_checksum(self):                with self.assertWarns(BiopythonWarning):            list(SeqIO.parse(\"GFA/corrupt_checksum.gfa\", \"gfa1\"))",
        "labels_text": "Check a GFA file with an incorrect checksum"
    },
    {
        "input_text": "summarize: def test_corrupt_tag_name(self):                with self.assertWarns(BiopythonWarning):            list(SeqIO.parse(\"GFA/corrupt_tag_name.gfa\", \"gfa1\"))",
        "labels_text": "Check a GFA file with an invalid tag name"
    },
    {
        "input_text": "summarize: def test_corrupt_tag_type(self):                with self.assertWarns(BiopythonWarning):            list(SeqIO.parse(\"GFA/corrupt_tag_type.gfa\", \"gfa1\"))",
        "labels_text": "Check a GFA file with an incorrect tag type"
    },
    {
        "input_text": "summarize: def test_old(self):                        d = SeqIO.index_db(\"Roche/triple_sff.idx\")            self.assertEqual(54, len(d))            self.assertRaises(FileNotFoundError, d.get_raw, \"alpha\")",
        "labels_text": "Load existing index with no option from parent directory"
    },
    {
        "input_text": "summarize: def test_pathobj(self):                        d = SeqIO.index_db(Path(\"Roche/triple_sff.idx\"))            self.assertEqual(54, len(d))",
        "labels_text": "Load existing index from a pathlibPath object"
    },
    {
        "input_text": "summarize: def test_old_check_same_thread(self):                        d = SeqIO.index_db(\"Roche/triple_sff_rel_paths.idx\")            def reader_thread():                try:                    d[\"alpha\"]                except sqlite3.ProgrammingError:                    self.fail(                        \"Raised sqlite3.ProgrammingError in violation of check_same_thread=False\"                    )            reader = threading.Thread(target=reader_thread)            reader.start()            reader.join()",
        "labels_text": "Setting checksamethread to False doesnt raise an exception"
    },
    {
        "input_text": "summarize: def test_old_rel(self):                        d = SeqIO.index_db(\"Roche/triple_sff_rel_paths.idx\")            self.assertEqual(54, len(d))            self.assertEqual(395, len(d[\"alpha\"]))",
        "labels_text": "Load existing index with relative path with no option from parent directory"
    },
    {
        "input_text": "summarize: def test_old_same_dir(self):                        os.chdir(\"Roche\")            d = SeqIO.index_db(\"triple_sff.idx\")            self.assertEqual(54, len(d))            self.assertEqual(395, len(d[\"alpha\"]))",
        "labels_text": "Load existing index with no option from same directory"
    },
    {
        "input_text": "summarize: def test_old_same_dir_rel(self):                        os.chdir(\"Roche\")            d = SeqIO.index_db(\"triple_sff_rel_paths.idx\")            self.assertEqual(54, len(d))            self.assertEqual(395, len(d[\"alpha\"]))",
        "labels_text": "Load existing index with relative path with no option from same directory"
    },
    {
        "input_text": "summarize: def test_old_format(self):                        d = SeqIO.index_db(\"Roche/triple_sff.idx\", format=\"sff\")            self.assertEqual(54, len(d))",
        "labels_text": "Load existing index with correct format"
    },
    {
        "input_text": "summarize: def test_old_format_wrong(self):                        self.assertRaises(                ValueError, SeqIO.index_db, \"Roche/triple_sff.idx\", format=\"fasta\"            )",
        "labels_text": "Load existing index with wrong format"
    },
    {
        "input_text": "summarize: def test_old_files(self):                        d = SeqIO.index_db(                \"Roche/triple_sff.idx\",                [\"E3MFGYR02_no_manifest.sff\", \"greek.sff\", \"paired.sff\"],            )            self.assertEqual(54, len(d))            self.assertRaises(FileNotFoundError, d.get_raw, \"alpha\")",
        "labels_text": "Load existing index with correct file from parent directory"
    },
    {
        "input_text": "summarize: def test_old_files_same_dir(self):                        os.chdir(\"Roche\")            d = SeqIO.index_db(                \"triple_sff.idx\",                [\"E3MFGYR02_no_manifest.sff\", \"greek.sff\", \"paired.sff\"],            )            self.assertEqual(54, len(d))            self.assertEqual(395, len(d[\"alpha\"]))",
        "labels_text": "Load existing index with correct file from same directory"
    },
    {
        "input_text": "summarize: def test_old_files_wrong(self):                        self.assertRaises(                ValueError,                SeqIO.index_db,                \"Roche/triple_sff.idx\",                [\"a.sff\", \"b.sff\", \"c.sff\"],            )",
        "labels_text": "Load existing index with wrong file"
    },
    {
        "input_text": "summarize: def test_old_files_wrong2(self):                        self.assertRaises(                ValueError,                SeqIO.index_db,                \"Roche/triple_sff.idx\",                [\"E3MFGYR02_no_manifest.sff\", \"greek.sff\"],            )",
        "labels_text": "Load existing index with wrong number of file"
    },
    {
        "input_text": "summarize: def add_prefix(self, key):                return \"id_\" + key",
        "labels_text": "Sample keyfunction for testing index code"
    },
    {
        "input_text": "summarize: def test_alpha_fails_db(self):                        # In historic usage, alphabet=... would be a Bio.Alphabet object.            self.assertRaises(                ValueError,                SeqIO.index_db,                \":memory:\",                [\"Fasta/dups.fasta\"],                \"fasta\",                alphabet=\"XXX\",            )",
        "labels_text": "Reject alphabet argument in BioSeqIOindexdb"
    },
    {
        "input_text": "summarize: def test_alpha_fails(self):                # In historic usage, alphabet=... would be a Bio.Alphabet object.        self.assertRaises(            ValueError, SeqIO.index, \"Fasta/dups.fasta\", \"fasta\", alphabet=\"XXX\"        )",
        "labels_text": "Reject alphabet argument in BioSeqIOindex"
    },
    {
        "input_text": "summarize: def test_duplicates_index_db(self):                        self.assertRaises(                ValueError, SeqIO.index_db, \":memory:\", [\"Fasta/dups.fasta\"], \"fasta\"            )",
        "labels_text": "Index file with duplicate identifier with BioSeqIOindexdb"
    },
    {
        "input_text": "summarize: def test_duplicates_index(self):                self.assertRaises(ValueError, SeqIO.index, \"Fasta/dups.fasta\", \"fasta\")",
        "labels_text": "Index file with duplicate identifier with BioSeqIOindex"
    },
    {
        "input_text": "summarize: def test_duplicates_to_dict(self):                with open(\"Fasta/dups.fasta\") as handle:            iterator = SeqIO.parse(handle, \"fasta\")            self.assertRaises(ValueError, SeqIO.to_dict, iterator)",
        "labels_text": "Index file with duplicate identifier with BioSeqIOtodict"
    },
    {
        "input_text": "summarize: def test_order_to_dict(self):                d = SeqIO.to_dict(SeqIO.parse(self.f, \"fasta\"))        self.assertEqual(self.ids, list(d))",
        "labels_text": "Check todict preserve order in indexed file"
    },
    {
        "input_text": "summarize: def test_order_index(self):                d = SeqIO.index(self.f, \"fasta\")        self.assertEqual(self.ids, list(d))",
        "labels_text": "Check index preserve order in indexed file"
    },
    {
        "input_text": "summarize: def test_order_index_db(self):                        d = SeqIO.index_db(\":memory:\", [self.f], \"fasta\")            self.assertEqual(self.ids, list(d))",
        "labels_text": "Check indexdb preserve ordering indexed file"
    },
    {
        "input_text": "summarize: def test_order_index_db(self):                        files = [\"GenBank/NC_000932.faa\", \"GenBank/NC_005816.faa\"]            ids = []            for f in files:                ids.extend(r.id for r in SeqIO.parse(f, \"fasta\"))            d = SeqIO.index_db(\":memory:\", files, \"fasta\")            self.assertEqual(ids, list(d))",
        "labels_text": "Check indexdb preserve order in multiple indexed file"
    },
    {
        "input_text": "summarize: def test_annotation1(self):                record = SeqIO.read(\"EMBL/TRBG361.embl\", \"embl\")        self.assertEqual(len(record), 1859)        # Single keyword:        self.assertEqual(record.annotations[\"keywords\"], [\"beta-glucosidase\"])        self.assertEqual(record.annotations[\"topology\"], \"linear\")",
        "labels_text": "Check parsing of annotation from EMBL file"
    },
    {
        "input_text": "summarize: def test_annotation3(self):                record = SeqIO.read(\"EMBL/AE017046.embl\", \"embl\")        self.assertEqual(len(record), 9609)        # TODO: Should this be an empty list, or simply absent?        self.assertEqual(record.annotations[\"keywords\"], [\"\"])        self.assertEqual(record.annotations[\"topology\"], \"circular\")",
        "labels_text": "Check parsing of annotation from EMBL file"
    },
    {
        "input_text": "summarize: def test_annotation4(self):                with self.assertWarns(BiopythonParserWarning):            record = SeqIO.read(\"EMBL/location_wrap.embl\", \"embl\")        self.assertEqual(len(record), 120)        self.assertNotIn(\"keywords\", record.annotations)        # The ID line has the topology as unspecified:        self.assertNotIn(\"topology\", record.annotations)",
        "labels_text": "Check parsing of annotation from EMBL file"
    },
    {
        "input_text": "summarize: def test_annotation1(self):                with warnings.catch_warnings():            warnings.simplefilter(\"error\")            self.check_rewrite(\"EMBL/TRBG361.embl\")",
        "labels_text": "Check writingandparsing EMBL file"
    },
    {
        "input_text": "summarize: def test_annotation2(self):                with warnings.catch_warnings():            warnings.simplefilter(\"error\")            self.check_rewrite(\"EMBL/DD231055_edited.embl\")",
        "labels_text": "Check writingandparsing EMBL file"
    },
    {
        "input_text": "summarize: def test_annotation3(self):                with warnings.catch_warnings():            warnings.simplefilter(\"error\")            self.check_rewrite(\"EMBL/AE017046.embl\")",
        "labels_text": "Check writingandparsing EMBL file"
    },
    {
        "input_text": "summarize: def test_atom_read_noheader(self):                with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", PDBConstructionWarning)            warnings.simplefilter(\"ignore\", BiopythonParserWarning)            chain = SeqIO.read(\"PDB/a_structure.pdb\", \"pdb-atom\")        self.assertEqual(chain.id, \"????:A\")        self.assertEqual(chain.annotations[\"chain\"], \"A\")        self.assertEqual(chain.seq, \"Q\")",
        "labels_text": "Read a singlechain PDB without a header by ATOM entry"
    },
    {
        "input_text": "summarize: def test_atom_with_insertion(self):                chain = SeqIO.read(\"PDB/2n0n_M1.pdb\", \"pdb-atom\")        self.assertEqual(chain.seq, \"HAEGKFTSEF\")",
        "labels_text": "Read a PDB with residue insertion code"
    },
    {
        "input_text": "summarize: def test_original(self):                self.check_sff(\"Roche/E3MFGYR02_random_10_reads.sff\")",
        "labels_text": "Test converting EMFGYRrandomreadssff into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_no_manifest(self):                self.check_sff(\"Roche/E3MFGYR02_no_manifest.sff\")",
        "labels_text": "Test converting EMFGYRnomanifestsff into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_alt_index_at_start(self):                self.check_sff(\"Roche/E3MFGYR02_alt_index_at_start.sff\")",
        "labels_text": "Test converting EMFGYRaltindexatstart into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_alt_index_in_middle(self):                self.check_sff(\"Roche/E3MFGYR02_alt_index_in_middle.sff\")",
        "labels_text": "Test converting EMFGYRaltindexinmiddle into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_alt_index_at_end(self):                self.check_sff(\"Roche/E3MFGYR02_alt_index_at_end.sff\")",
        "labels_text": "Test converting EMFGYRaltindexatend into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_index_at_start(self):                self.check_sff(\"Roche/E3MFGYR02_index_at_start.sff\")",
        "labels_text": "Test converting EMFGYRindexatstart into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_index_at_end(self):                self.check_sff(\"Roche/E3MFGYR02_index_in_middle.sff\")",
        "labels_text": "Test converting EMFGYRindexinmiddle into FASTAQUAL"
    },
    {
        "input_text": "summarize: def test_paired(self):                with open(\"Quality/example.fasta\") as f, open(\"Quality/example.qual\") as q:            records1 = list(QualityIO.PairedFastaQualIterator(f, q))        records2 = list(SeqIO.parse(\"Quality/example.fastq\", \"fastq\"))        self.compare_records(records1, records2)",
        "labels_text": "Check FASTQ parsing match FASTAQUAL parsing"
    },
    {
        "input_text": "summarize: def test_qual(self):                records1 = list(SeqIO.parse(\"Quality/example.qual\", \"qual\"))        records2 = list(SeqIO.parse(\"Quality/example.fastq\", \"fastq\"))        # Will ignore the unknown sequences :)        self.compare_records(records1, records2)",
        "labels_text": "Check FASTQ parsing match QUAL parsing"
    },
    {
        "input_text": "summarize: def test_qual_out(self):                records = SeqIO.parse(\"Quality/example.fastq\", \"fastq\")        h = StringIO()        SeqIO.write(records, h, \"qual\")        with open(\"Quality/example.qual\") as expected:            self.assertEqual(h.getvalue(), expected.read())",
        "labels_text": "Check FASTQ to QUAL output"
    },
    {
        "input_text": "summarize: def test_fasta(self):                records1 = list(SeqIO.parse(\"Quality/example.fasta\", \"fasta\"))        records2 = list(SeqIO.parse(\"Quality/example.fastq\", \"fastq\"))        self.compare_records(records1, records2)",
        "labels_text": "Check FASTQ parsing match FASTA parsing"
    },
    {
        "input_text": "summarize: def test_fasta_out(self):                records = SeqIO.parse(\"Quality/example.fastq\", \"fastq\")        h = StringIO()        SeqIO.write(records, h, \"fasta\")        with open(\"Quality/example.fasta\") as expected:            self.assertEqual(h.getvalue(), expected.read())",
        "labels_text": "Check FASTQ to FASTA output"
    },
    {
        "input_text": "summarize: def test_fastq_2000(self):                data = f\"@{'id descr goes here'}\\n{'ACGT' * 500}\\n+\\n{'!@a~' * 500}\\n\"        handle = StringIO()        self.assertEqual(            1, SeqIO.write(SeqIO.parse(StringIO(data), \"fastq\"), handle, \"fastq\")        )        self.assertEqual(data, handle.getvalue())",
        "labels_text": "Read and write back simple example with upper case bp read"
    },
    {
        "input_text": "summarize: def test_fastq_1000(self):                data = \"@%s\\n%s\\n+\\n%s\\n\" % (            \"id descr goes here\",            \"ACGTNncgta\" * 100,            \"abcd!!efgh\" * 100,        )        handle = StringIO()        self.assertEqual(            1, SeqIO.write(SeqIO.parse(StringIO(data), \"fastq\"), handle, \"fastq\")        )        self.assertEqual(data, handle.getvalue())",
        "labels_text": "Read and write back simple example with mixed case bp read"
    },
    {
        "input_text": "summarize: def test_tricky(self):                self.check(            os.path.join(\"Quality\", \"tricky.fastq\"),            \"fastq\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back trickyfastq"
    },
    {
        "input_text": "summarize: def test_sanger_faked(self):                self.check(            os.path.join(\"Quality\", \"sanger_faked.fastq\"),            \"fastq\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back sangerfakedfastq"
    },
    {
        "input_text": "summarize: def test_example_fasta(self):                self.write_read(os.path.join(\"Quality\", \"example.fasta\"), \"fasta\", \"fasta\")",
        "labels_text": "Write and read back examplefasta"
    },
    {
        "input_text": "summarize: def test_example_fastq(self):                self.check(            os.path.join(\"Quality\", \"example.fastq\"),            \"fastq\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back examplefastq"
    },
    {
        "input_text": "summarize: def test_example_qual(self):                self.check(            os.path.join(\"Quality\", \"example.qual\"),            \"qual\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back examplequal"
    },
    {
        "input_text": "summarize: def test_solexa_faked(self):                self.check(            os.path.join(\"Quality\", \"solexa_faked.fastq\"),            \"fastq-solexa\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back solexafakedfastq"
    },
    {
        "input_text": "summarize: def test_solexa_example(self):                self.check(            os.path.join(\"Quality\", \"solexa_example.fastq\"),            \"fastq-solexa\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back solexaexamplefastq"
    },
    {
        "input_text": "summarize: def test_illumina_faked(self):                self.check(            os.path.join(\"Quality\", \"illumina_faked.fastq\"),            \"fastq-illumina\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back illuminafakedfastq"
    },
    {
        "input_text": "summarize: def test_greek_sff(self):                self.check(            os.path.join(\"Roche\", \"greek.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back greeksff"
    },
    {
        "input_text": "summarize: def test_paired_sff(self):                self.check(            os.path.join(\"Roche\", \"paired.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back pairedsff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_random_10_reads.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRrandomreadssff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_no_manifest(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_no_manifest.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRnomanifestsff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_index_at_start(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_index_at_start.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRindexatstartsff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_index_in_middle(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_index_in_middle.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRindexinmiddlesff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_alt_index_at_start(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_alt_index_at_start.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRaltindexatstartsff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_alt_index_in_middle(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_alt_index_in_middle.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRaltindexinmiddlesff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_alt_index_at_end(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_alt_index_at_end.sff\"),            \"sff\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",                \"sff\",            ],        )",
        "labels_text": "Write and read back EMFGYRaltindexatendsff"
    },
    {
        "input_text": "summarize: def test_E3MFGYR02_trimmed(self):                self.check(            os.path.join(\"Roche\", \"E3MFGYR02_random_10_reads.sff\"),            \"sff-trim\",            [                \"fastq\",                \"fastq-sanger\",                \"fastq-illumina\",                \"fastq-solexa\",                \"fasta\",                \"qual\",                \"phd\",            ],        )",
        "labels_text": "Write and read back EMFGYRrandomreadssff trimmed"
    },
    {
        "input_text": "summarize: def test_special_characters_desc(self):                self.assertEqual(            self.records[\"dna\"][2].description,            'some special characters in the description\\n<tag> \"quoted string\"',        )",
        "labels_text": "Read special XML character in description"
    },
    {
        "input_text": "summarize: def test_unicode_characters_desc(self):                self.assertEqual(self.records[\"rna\"][2].description, \"\u00e5\u00c5\u00fc\u00f6\u00d6\u00df\u00f8\u00e4\u00a2\u00a3$\u20ac\u9999\u80a0\")",
        "labels_text": "Test special unicode character in the description"
    },
    {
        "input_text": "summarize: def test_full_characters_set_read(self):                self.assertEqual(self.records[\"dna\"][1].seq, \"ACGTMRWSYKVHDBXN.-\")        self.assertEqual(self.records[\"rna\"][1].seq, \"ACGUMRWSYKVHDBXN.-\")        self.assertEqual(            self.records[\"protein\"][1].seq, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ.-*\"        )",
        "labels_text": "Read full character set for each type"
    },
    {
        "input_text": "summarize: def test_duplicated_property(self):                self.assertEqual(            self.records[\"protein\"][2].annotations[\"test\"], [\"1\", \"2\", \"3\"]        )",
        "labels_text": "Read property with multiple value"
    },
    {
        "input_text": "summarize: def test_duplicated_dbxref(self):                self.assertEqual(            self.records[\"protein\"][2].dbxrefs, [\"someDB:G001\", \"someDB:G002\"]        )",
        "labels_text": "Read multiple cross reference to a single source"
    },
    {
        "input_text": "summarize: def test_local_source_definition(self):                self.assertEqual(self.records[\"protein\"][4].annotations[\"source\"], \"Uniprot\")",
        "labels_text": "Check local source"
    },
    {
        "input_text": "summarize: def test_empty_description(self):                self.assertEqual(            self.records[\"rna\"][4].description,            SeqRecord(id=\"\", seq=Seq(\"\")).description,        )",
        "labels_text": "Check empty description"
    },
    {
        "input_text": "summarize: def test_check_dna_header(self):                records = SeqIO.parse(\"SeqXML/dna_example.xml\", \"seqxml\")        self.assertEqual(records.source, \"Ensembl\")        self.assertEqual(records.sourceVersion, \"56\")        self.assertEqual(records.seqXMLversion, \"0.4\")",
        "labels_text": "Check if the header information is parsed"
    },
    {
        "input_text": "summarize: def test_check_rna_header(self):                records = SeqIO.parse(\"SeqXML/rna_example.xml\", \"seqxml\")        self.assertEqual(records.source, \"Ensembl\")        self.assertEqual(records.sourceVersion, \"56\")        self.assertEqual(records.seqXMLversion, \"0.3\")",
        "labels_text": "Check if the header information is parsed"
    },
    {
        "input_text": "summarize: def test_check_protein_header(self):                records = SeqIO.parse(\"SeqXML/protein_example.xml\", \"seqxml\")        self.assertEqual(records.source, \"Ensembl\")        self.assertEqual(records.sourceVersion, \"56\")        self.assertEqual(records.seqXMLversion, \"0.4\")",
        "labels_text": "Check if the header information is parsed"
    },
    {
        "input_text": "summarize: def test_check_global_species_example_header(self):                records = SeqIO.parse(\"SeqXML/global_species_example.xml\", \"seqxml\")        self.assertEqual(records.speciesName, \"Mus musculus\")        self.assertEqual(records.ncbiTaxID, \"10090\")        self.assertEqual(records.source, \"Ensembl\")        self.assertEqual(records.sourceVersion, \"56\")        self.assertEqual(records.seqXMLversion, \"0.4\")",
        "labels_text": "Check if the header information is parsed"
    },
    {
        "input_text": "summarize: def test_read_write_rna(self):                read1_records = list(SeqIO.parse(\"SeqXML/rna_example.xml\", \"seqxml\"))        self._write_parse_and_compare(read1_records)",
        "labels_text": "Read and write RNA"
    },
    {
        "input_text": "summarize: def test_read_write_dna(self):                read1_records = list(SeqIO.parse(\"SeqXML/dna_example.xml\", \"seqxml\"))        self._write_parse_and_compare(read1_records)",
        "labels_text": "Read and write DNA"
    },
    {
        "input_text": "summarize: def test_read_write_protein(self):                read1_records = list(SeqIO.parse(\"SeqXML/protein_example.xml\", \"seqxml\"))        self._write_parse_and_compare(read1_records)",
        "labels_text": "Read and write protein"
    },
    {
        "input_text": "summarize: def test_read_write_globalSpecies(self):                read1_records = list(SeqIO.parse(\"SeqXML/global_species_example.xml\", \"seqxml\"))        self._write_parse_and_compare(read1_records)",
        "labels_text": "Read and write global specie"
    },
    {
        "input_text": "summarize: def test_wrong_version(self):                def f(path):            records = SeqIO.parse(path, \"seqxml\")            for record in records:                pass        self.assertRaises(ValueError, f, \"SeqXML/wrong_version1.xml\")        self.assertRaises(ValueError, f, \"SeqXML/wrong_version2.xml\")        self.assertRaises(ValueError, f, \"SeqXML/wrong_version3.xml\")",
        "labels_text": "Handling of wrong version"
    },
    {
        "input_text": "summarize: def test_missing_dna(self):                # Simulate a missing DNA packet by changing the tag byte to an        # unknown packet type, so that the parser will skip the packet.        h = self.munge_buffer(19, 0x80)        with self.assertRaisesRegex(ValueError, \"No DNA packet in file\"):            SeqIO.read(h, \"snapgene\")        h.close()",
        "labels_text": "Read a file without a DNA packet"
    },
    {
        "input_text": "summarize: def test_extra_dna(self):                # Fabricate a file with a duplicated DNA packet        buf = bytearray(self.buffer)        buf.extend(self.buffer[19:1025])  # Append duplicated DNA packet        h = BytesIO(buf)        with self.assertRaisesRegex(            ValueError, \"The file contains more than one DNA packet\"        ):            SeqIO.read(h, \"snapgene\")        h.close()",
        "labels_text": "Read a file with supernumerary DNA packet"
    },
    {
        "input_text": "summarize: def test_truncated_packet(self):                # Truncate before the end of the length bytes        h = BytesIO(self.buffer[3:])        with self.assertRaisesRegex(ValueError, \"Unexpected end of packet\"):            SeqIO.read(h, \"snapgene\")        h.close()        # Truncate before the end of the data        h = BytesIO(self.buffer[10:])        with self.assertRaisesRegex(ValueError, \"Unexpected end of packet\"):            SeqIO.read(h, \"snapgene\")        h.close()",
        "labels_text": "Read a file with incomplete packet"
    },
    {
        "input_text": "summarize: def test_unsupported_version(self):                h = self.munge_buffer(0, 0x01)  # Change version byte        with self.assertRaisesRegex(ValueError, \"Unsupported XDNA version\"):            SeqIO.read(h, \"xdna\")        h.close()",
        "labels_text": "Read a file with unexpected version number"
    },
    {
        "input_text": "summarize: def test_invalid_sequence_type(self):                h = self.munge_buffer(1, 0x0A)  # Change type byte        with self.assertRaisesRegex(ValueError, \"Unknown sequence type\"):            SeqIO.read(h, \"xdna\")        h.close()",
        "labels_text": "Read a file with an unknown sequence type"
    },
    {
        "input_text": "summarize: def test_missing_features(self):                # Set a larger number of features than the file actually contains        # Offset of the features number byte:        # header + length of sequence + length of comment + overhangs        feature_byte = 112 + 1000 + len(\"Sample sequence A\") + 5        h = self.munge_buffer(feature_byte, 3)        with self.assertRaisesRegex(ValueError, \"Cannot read 1 bytes from handle\"):            SeqIO.read(h, \"xdna\")        h.close()",
        "labels_text": "Read a file with an incorrect number of feature"
    },
    {
        "input_text": "summarize: def test_annotations(self):                rec = SeqRecord(Seq(\"ACGT\"), id=\"Test\", name=\"Test\", description=\"Test\")        self.assertEqual(rec.annotations, {})        rec = SeqRecord(            Seq(\"ACGT\"),            id=\"Test\",            name=\"Test\",            description=\"Test\",            annotations={\"test\": [\"a test\"]},        )        self.assertEqual(rec.annotations[\"test\"], [\"a test\"])",
        "labels_text": "Pass in annotation to SeqRecords"
    },
    {
        "input_text": "summarize: def test_slice_zero(self):                rec = self.record        self.assertEqual(len(rec), 26)        self.assertEqual(len(rec[2:-2]), 22)        self.assertEqual(len(rec[5:2]), 0)        self.assertEqual(len(rec[5:2][2:-2]), 0)",
        "labels_text": "Zero slice"
    },
    {
        "input_text": "summarize: def test_str_count(self):                self._test_method(\"count\", start_end=True)        self.assertEqual(Seq(\"AC777GT\").count(\"7\"), 3)        self.assertRaises(TypeError, Seq(\"AC777GT\").count, 7)        self.assertRaises(TypeError, Seq(\"AC777GT\").count, None)",
        "labels_text": "Check match the python string count method"
    },
    {
        "input_text": "summarize: def test_count_overlap(self):                self.assertEqual(Seq(\"AC777GT\").count(\"77\"), 1)        self.assertEqual(Seq(\"AC777GT\").count_overlap(\"77\"), 2)        self.assertEqual(Seq(\"AC777GT\").count_overlap(\"7\"), 3)        self.assertRaises(TypeError, Seq(\"AC777GT\").count_overlap, 7)        self.assertRaises(TypeError, Seq(\"AC777GT\").count_overlap, None)",
        "labels_text": "Check countoverlap exception match python string count method"
    },
    {
        "input_text": "summarize: def test_str_find(self):                self._test_method(\"find\", start_end=True)        self.assertEqual(Seq(\"AC7GT\").find(\"7\"), 2)        self.assertRaises(TypeError, Seq(\"AC7GT\").find, 7)        self.assertRaises(TypeError, Seq(\"ACGT\").find, None)",
        "labels_text": "Check match the python string find method"
    },
    {
        "input_text": "summarize: def test_str_rfind(self):                self._test_method(\"rfind\", start_end=True)        self.assertEqual(Seq(\"AC7GT\").rfind(\"7\"), 2)        self.assertRaises(TypeError, Seq(\"AC7GT\").rfind, 7)        self.assertRaises(TypeError, Seq(\"ACGT\").rfind, None)",
        "labels_text": "Check match the python string rfind method"
    },
    {
        "input_text": "summarize: def test_str_split(self):                self._test_method(\"split\")        self.assertEqual(Seq(\"AC7GT\").split(\"7\"), \"AC7GT\".split(\"7\"))        self.assertRaises(TypeError, Seq(\"AC7GT\").split, 7)        self.assertEqual(MutableSeq(\"AC7GT\").split(\"7\"), \"AC7GT\".split(\"7\"))        self.assertRaises(TypeError, MutableSeq(\"AC7GT\").split, 7)",
        "labels_text": "Check match the python string split method"
    },
    {
        "input_text": "summarize: def test_str_rsplit(self):                self._test_method(\"rsplit\")        self.assertEqual(Seq(\"AC7GT\").rsplit(\"7\"), \"AC7GT\".rsplit(\"7\"))        self.assertRaises(TypeError, Seq(\"AC7GT\").rsplit, 7)        self.assertEqual(MutableSeq(\"AC7GT\").rsplit(\"7\"), \"AC7GT\".rsplit(\"7\"))        self.assertRaises(TypeError, MutableSeq(\"AC7GT\").rsplit, 7)",
        "labels_text": "Check match the python string rsplit method"
    },
    {
        "input_text": "summarize: def test_str_length(self):                for example1 in self._examples:            str1 = str(example1)            self.assertEqual(len(example1), len(str1))",
        "labels_text": "Check match the python string len method"
    },
    {
        "input_text": "summarize: def test_str_upper(self):                for example1 in self._examples:            str1 = str(example1)            example1 = example1.upper()            self.assertEqual(example1, str1.upper())        with self.assertRaises(TypeError) as cm:            Seq(\"abcd\").upper(inplace=True)        self.assertEqual(str(cm.exception), \"Sequence is immutable\")",
        "labels_text": "Check match the python string upper method"
    },
    {
        "input_text": "summarize: def test_str_lower(self):                for example1 in self._examples:            str1 = str(example1)            example1 = example1.lower()            self.assertEqual(example1, str1.lower())        with self.assertRaises(TypeError) as cm:            Seq(\"ABCD\").lower(inplace=True)        self.assertEqual(str(cm.exception), \"Sequence is immutable\")",
        "labels_text": "Check match the python string lower method"
    },
    {
        "input_text": "summarize: def test_str_isupper(self):                for example1 in self._examples:            str1 = str(example1)            if isinstance(example1, _UndefinedSequenceData):                with self.assertRaises(UndefinedSequenceError):                    example1.isupper()            else:                example1 = example1.isupper()            self.assertEqual(example1, str1.isupper())",
        "labels_text": "Check match the python string isupper method"
    },
    {
        "input_text": "summarize: def test_str_islower(self):                for example1 in self._examples:            str1 = str(example1)            if isinstance(example1, _UndefinedSequenceData):                with self.assertRaises(UndefinedSequenceError):                    example1.islower()            else:                example1 = example1.islower()            self.assertEqual(example1, str1.islower())",
        "labels_text": "Check match the python string islower method"
    },
    {
        "input_text": "summarize: def test_str_encode(self):                for example1 in self._examples:            str1 = str(example1)            self.assertEqual(bytes(example1), str1.encode(\"ascii\"))",
        "labels_text": "Check match the python string encode method"
    },
    {
        "input_text": "summarize: def test_tomutable(self):                for example1 in self._examples:            mut = MutableSeq(example1)            self.assertIsInstance(mut, MutableSeq)            self.assertEqual(mut, example1)",
        "labels_text": "Check creating a MutableSeq object"
    },
    {
        "input_text": "summarize: def test_toseq(self):                for example1 in self._examples:            seq = Seq(example1)            self.assertIsInstance(seq, Seq)            self.assertEqual(seq, example1)",
        "labels_text": "Check creating a Seq object"
    },
    {
        "input_text": "summarize: def test_the_translation_of_invalid_codons(self):                for codon in [\"TA?\", \"N-N\", \"AC_\", \"Ac_\"]:            msg = f\"Translating {codon} should fail\"            nuc = Seq(codon)            with self.assertRaises(TranslationError, msg=msg):                nuc.translate()            nuc = MutableSeq(codon)            with self.assertRaises(TranslationError, msg=msg):                nuc.translate()",
        "labels_text": "Check objtranslate method with invalid codon"
    },
    {
        "input_text": "summarize: def test_Seq_init_error(self):                self.assertRaises(TypeError, Seq, (\"A\", \"C\", \"G\", \"T\"))        self.assertRaises(TypeError, Seq, [\"A\", \"C\", \"G\", \"T\"])        self.assertRaises(TypeError, Seq, 1)        self.assertRaises(TypeError, Seq, 1.0)        self.assertRaises(ValueError, Seq, None)",
        "labels_text": "Check Seq init raise the appropriate exception"
    },
    {
        "input_text": "summarize: def test_MutableSeq_init_error(self):                self.assertRaises(TypeError, MutableSeq, (\"A\", \"C\", \"G\", \"T\"))        self.assertRaises(TypeError, MutableSeq, [\"A\", \"C\", \"G\", \"T\"])        self.assertRaises(TypeError, MutableSeq, 1)        self.assertRaises(TypeError, MutableSeq, 1.0)        self.assertRaises(TypeError, MutableSeq, array.array(\"i\", [1, 2, 3, 4]))",
        "labels_text": "Check MutableSeq init raise the appropriate exception"
    },
    {
        "input_text": "summarize: def test_join_Seq_TypeError(self):                # No iterable types which contain non-accepted types either.        spacer = Seq(\"NNNNN\")        self.assertRaises(TypeError, spacer.join, 5)        self.assertRaises(TypeError, spacer.join, SeqRecord(Seq(\"ATG\")))        self.assertRaises(TypeError, spacer.join, [\"ATG\", \"ATG\", 5, \"ATG\"])",
        "labels_text": "Checks that a TypeError is thrown for all noniterable type"
    },
    {
        "input_text": "summarize: def test_join_MutableSeq_TypeError_iter(self):                # No iterable types which contain non-accepted types either.        spacer = MutableSeq(\"MMMMM\")        self.assertRaises(TypeError, spacer.join, 5)        self.assertRaises(TypeError, spacer.join, [\"ATG\", \"ATG\", 5, \"ATG\"])",
        "labels_text": "Checks that a TypeError is thrown for all noniterable type"
    },
    {
        "input_text": "summarize: def test_join_MutableSeq_mixed(self):                spacer = MutableSeq(\"NNNNN\")        self.assertEqual(            \"N\" * 15, spacer.join([MutableSeq(\"NNNNN\"), MutableSeq(\"NNNNN\")])        )        self.assertRaises(TypeError, spacer.join([Seq(\"NNNNN\"), MutableSeq(\"NNNNN\")]))",
        "labels_text": "Check MutableSeq object can be joined"
    },
    {
        "input_text": "summarize: def test_unknown_seq(self):                rec = SeqIO.read(\"GenBank/NT_019265.gb\", \"genbank\")        self.assertIsInstance(rec.seq, Seq)        self.assertRaises(UndefinedSequenceError, bytes, rec.seq)        feature = rec.features[1]        seq = feature.extract(rec.seq)        self.assertIsInstance(seq, Seq)        self.assertEqual(len(seq), len(feature))        self.assertRaises(UndefinedSequenceError, bytes, seq)",
        "labels_text": "Test if feature extraction work properly for unknown sequence"
    },
    {
        "input_text": "summarize: def test_ft_line(self):                filename = \"O23729.txt\"        datafile = os.path.join(\"SwissProt\", filename)        with open(datafile) as test_handle:            record = SwissProt.read(test_handle)",
        "labels_text": "Parsing SwissProt file O which ha a newstyle FT line"
    },
    {
        "input_text": "summarize: def test_invalid_database(self):                self.assertRaises(            IOError, TogoWS._get_fields, \"http://togows.dbcls.jp/entry/invalid?fields\"        )",
        "labels_text": "Check asking for field of invalid database fails"
    },
    {
        "input_text": "summarize: def test_pubmed(self):                fields = set(TogoWS._get_entry_fields(\"pubmed\"))        self.assertTrue(            fields.issuperset(                [\"abstract\", \"au\", \"authors\", \"doi\", \"mesh\", \"so\", \"title\"]            ),            fields,        )",
        "labels_text": "Check supported field for pubmed database"
    },
    {
        "input_text": "summarize: def test_uniprot(self):                fields = set(TogoWS._get_entry_fields(\"uniprot\"))        self.assertTrue(fields.issuperset([\"definition\", \"entry_id\", \"seq\"]), fields)",
        "labels_text": "Check supported field for uniprot database"
    },
    {
        "input_text": "summarize: def test_pdb(self):                fields = set(TogoWS._get_entry_fields(\"pdb\"))        self.assertTrue(            fields.issuperset([\"accession\", \"chains\", \"keywords\", \"models\"]), fields        )",
        "labels_text": "Check supported field for pdb database"
    },
    {
        "input_text": "summarize: def test_pubmed_16381885_ti(self):                handle = TogoWS.entry(\"pubmed\", \"16381885\", field=\"title\")        data = handle.read().strip()        handle.close()        self.assertEqual(            data, \"From genomics to chemical genomics: new developments in KEGG.\"        )",
        "labels_text": "BioTogoWSentrypubmed fieldtitle"
    },
    {
        "input_text": "summarize: def test_pubmed_16381885_title(self):                handle = TogoWS.entry(\"pubmed\", \"16381885\", field=\"title\")        data = handle.read().strip()        handle.close()        self.assertEqual(            data, \"From genomics to chemical genomics: new developments in KEGG.\"        )",
        "labels_text": "BioTogoWSentrypubmed fieldtitle"
    },
    {
        "input_text": "summarize: def test_pubmed_16381885_invalid_field(self):                self.assertRaises(            ValueError, TogoWS.entry, \"pubmed\", \"16381885\", field=\"invalid_for_testing\"        )",
        "labels_text": "BioTogoWSentrypubmed fieldinvalidfortesting"
    },
    {
        "input_text": "summarize: def test_pubmed_16381885_invalid_format(self):                self.assertRaises(            ValueError, TogoWS.entry, \"pubmed\", \"16381885\", format=\"invalid_for_testing\"        )",
        "labels_text": "BioTogoWSentrypubmed formatinvalidfortesting"
    },
    {
        "input_text": "summarize: def test_pubmed_invalid_id(self):                self.assertRaises(IOError, TogoWS.entry, \"pubmed\", \"invalid_for_testing\")",
        "labels_text": "BioTogoWSentrypubmed invalidfortesting"
    },
    {
        "input_text": "summarize: def test_invalid_db(self):                self.assertRaises(ValueError, TogoWS.entry, \"invalid_db\", \"invalid_id\")",
        "labels_text": "BioTogoWSentryinvaliddb invalidid"
    },
    {
        "input_text": "summarize: def test_ddbj_genbank_length(self):                handle = TogoWS.entry(\"ddbj\", \"X52960\", field=\"length\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"248\")",
        "labels_text": "BioTogoWSentryddbj X fieldlength"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_length(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"length\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"248\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldlength"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_seq(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"seq\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(seguid(data), \"Ktxz0HgMlhQmrKTuZpOxPZJ6zGU\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldseq"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_definition(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"definition\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"Coleus blumei viroid 1 (CbVd) RNA.\")",
        "labels_text": "BioTogoWSentrynucleotide X fielddefinition"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_accession(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"accession\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"X52960\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldaccession"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_version(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"version\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"1\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldversion"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_acc_version(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"acc_version\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"X52960.1\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldaccversion"
    },
    {
        "input_text": "summarize: def test_nucleotide_genbank_organism(self):                handle = TogoWS.entry(\"nucleotide\", \"X52960\", field=\"organism\")        data = handle.read().strip()  # ignore trailing \\n        handle.close()        self.assertEqual(data, \"Coleus blumei viroid 1\")",
        "labels_text": "BioTogoWSentrynucleotide X fieldorganism"
    },
    {
        "input_text": "summarize: def test_ddbj_genbank_invalid_field(self):                self.assertRaises(            ValueError,            TogoWS.entry,            \"nucleotide\",            \"X52960\",            field=\"invalid_for_testing\",        )",
        "labels_text": "BioTogoWSentrynucleotide X fieldinvalidfortesting"
    },
    {
        "input_text": "summarize: def test_nucleotide_invalid_format(self):                self.assertRaises(            ValueError,            TogoWS.entry,            \"nucleotide\",            \"X52960\",            format=\"invalid_for_testing\",        )",
        "labels_text": "BioTogoWSentrynucleotide X formatinvalidfortesting"
    },
    {
        "input_text": "summarize: def test_ddbj_gff3(self):                handle = TogoWS.entry(\"ddbj\", \"X52960\", format=\"gff\")        data = handle.read()        handle.close()        self.assertTrue(data.startswith(\"##gff-version 3\\nX52960\\tDDBJ\\t\"), data)",
        "labels_text": "BioTogoWSentryddbj X formatgff"
    },
    {
        "input_text": "summarize: def test_genbank_gff3(self):                # Note - Using manual URL with genbank instead of nucleotide works        handle = TogoWS.entry(\"nucleotide\", \"X52960\", format=\"gff\")        data = handle.read()        handle.close()        self.assertTrue(data.startswith(\"##gff-version 3\\nX52960\\tGenbank\\t\"), data)",
        "labels_text": "BioTogoWSentrynucleotide X formatgff"
    },
    {
        "input_text": "summarize: def test_bad_args_just_limit(self):                self.assertRaises(ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", limit=10)",
        "labels_text": "Reject BioTogoWSsearch with just limit"
    },
    {
        "input_text": "summarize: def test_bad_args_just_offset(self):                self.assertRaises(ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=10)",
        "labels_text": "Reject BioTogoWSsearch with just offset"
    },
    {
        "input_text": "summarize: def test_bad_args_zero_limit(self):                self.assertRaises(            ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=1, limit=0        )",
        "labels_text": "Reject BioTogoWSsearch with zero limit"
    },
    {
        "input_text": "summarize: def test_bad_args_zero_offset(self):                self.assertRaises(            ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=0, limit=10        )",
        "labels_text": "Reject BioTogoWSsearch with zero offset"
    },
    {
        "input_text": "summarize: def test_bad_args_non_int_offset(self):                self.assertRaises(            ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=\"test\", limit=10        )",
        "labels_text": "Reject BioTogoWSsearch with noninteger offset"
    },
    {
        "input_text": "summarize: def test_bad_args_non_int_limit(self):                self.assertRaises(            ValueError, TogoWS.search, \"pubmed\", \"lung+cancer\", offset=1, limit=\"lots\"        )",
        "labels_text": "Reject BioTogoWSsearch with noninteger limit"
    },
    {
        "input_text": "summarize: def test_pubmed_search_togows(self):                self.check(\"pubmed\", \"TogoWS\", [\"20472643\"])",
        "labels_text": "BioTogoWSsearchiterpubmed TogoWS etc"
    },
    {
        "input_text": "summarize: def test_pubmed_search_bioruby(self):                self.check(            \"pubmed\",            \"BioRuby\",            [\"22994508\", \"22399473\", \"20739307\", \"20015970\", \"14693808\"],        )",
        "labels_text": "BioTogoWSsearchiterpubmed BioRuby etc"
    },
    {
        "input_text": "summarize: def test_pubmed_search_porin(self):                self.check(\"pubmed\", \"human porin\", [\"21189321\", \"21835183\"])",
        "labels_text": "BioTogoWSsearchiterpubmed human porin etc Count wa at time of writing this wa chosen to be larger than the default chunk size for iteration but still not too big to download the full list"
    },
    {
        "input_text": "summarize: def test_uniprot_search_lung_cancer(self):                self.check(\"uniprot\", \"terminal+lung+cancer\", limit=150)",
        "labels_text": "BioTogoWSsearchiteruniprot terminallungcancer limit etc Search count wa at time of writing a bit large to download all the result in a unit test Want to use a limit larger than the batch size to ensure at least two batch"
    },
    {
        "input_text": "summarize: def test_invalid_format(self):                self.assertRaises(            ValueError,            TogoWS.convert,            StringIO(\"PLACEHOLDER\"),            \"genbank\",            \"invalid_for_testing\",        )        self.assertRaises(            ValueError,            TogoWS.convert,            StringIO(\"PLACEHOLDER\"),            \"invalid_for_testing\",            \"fasta\",        )",
        "labels_text": "Check convert file format checking"
    },
    {
        "input_text": "summarize: def test_genbank_to_fasta(self):                filename = \"GenBank/NC_005816.gb\"        old = SeqIO.read(filename, \"gb\")        with open(filename) as handle:            new = SeqIO.read(TogoWS.convert(handle, \"genbank\", \"fasta\"), \"fasta\")        self.assertEqual(old.seq, new.seq)",
        "labels_text": "Conversion of GenBank to FASTA"
    },
    {
        "input_text": "summarize: def check_deps(dependencies):        missing = []    for dep in dependencies:        if dep == \"internet\":            if not online:                missing.append(\"internet\")        else:            assert dep.startswith(\"lib:\"), dep            lib = dep[4:]            try:                tmp = __import__(lib)                del tmp            except ImportError:                missing.append(lib)    return missing",
        "labels_text": "Check libXXX and internet dependency are met"
    },
    {
        "input_text": "summarize: def test_Q13639(self):                old = SeqIO.read(\"SwissProt/Q13639.txt\", \"swiss\")        new = SeqIO.read(\"SwissProt/Q13639.xml\", \"uniprot-xml\")        self.compare_txt_xml(old, new)",
        "labels_text": "Compare SwissProt text and uniprot XML version of Q"
    },
    {
        "input_text": "summarize: def test_H2CNN8(self):                old = SeqIO.read(\"SwissProt/H2CNN8.txt\", \"swiss\")        new = SeqIO.read(\"SwissProt/H2CNN8.xml\", \"uniprot-xml\")        self.compare_txt_xml(old, new)",
        "labels_text": "Compare SwissProt text and uniprot XML version of HCNN"
    },
    {
        "input_text": "summarize: def test_P84001(self):                xml = list(SeqIO.parse(\"SwissProt/P84001.xml\", \"uniprot-xml\"))[0]        self.assertEqual(xml.id, \"P84001\")        self.assertEqual(len(xml.annotations[\"comment_massspectrometry\"]), 1)        self.assertEqual(            xml.annotations[\"comment_massspectrometry\"][0],            \"undefined:9571|Electrospray\",        )",
        "labels_text": "Parse mass spec structured comment with unknown loc"
    },
    {
        "input_text": "summarize: def test_submittedName_allowed(self):                with open(\"SwissProt/R5HY77.xml\") as handle:            for entry in SeqIO.parse(handle, \"uniprot-xml\"):                self.assertEqual(entry.id, \"R5HY77\")                self.assertEqual(entry.description, \"Elongation factor Ts\")",
        "labels_text": "Checks if parser support new XML Element submittedName"
    },
    {
        "input_text": "summarize: def copy_and_mark_for_cleanup(self, path):                filename = os.path.split(path)[1]        shutil.copyfile(path, filename)        self.add_file_to_clean(filename)        return filename",
        "labels_text": "Copy file to working directory and mark it for removal XXmotif currently only handle a canonical filename a input no path This method copy the specified file in the specified path to the current working directory and mark it for removal"
    },
    {
        "input_text": "summarize: def add_file_to_clean(self, filename):                self.files_to_clean.add(filename)",
        "labels_text": "Add a file for deferred removal by the tearDown routine"
    },
    {
        "input_text": "summarize: def test_empty_file(self):                input_file = \"does_not_exist.fasta\"        self.assertFalse(os.path.isfile(input_file))        cline = XXmotifCommandline(outdir=self.out_dir, seqfile=input_file)        try:            stdout, stderr = cline()        except ApplicationError as err:            self.assertEqual(err.returncode, 255)        else:            self.fail(f\"Should have failed, returned:\\n{stdout}\\n{stderr}\")",
        "labels_text": "Test a nonexisting input file"
    },
    {
        "input_text": "summarize: def test_invalid_format(self):                input_file = self.copy_and_mark_for_cleanup(\"Medline/pubmed_result1.txt\")        cline = XXmotifCommandline(outdir=self.out_dir, seqfile=input_file)        try:            stdout, stderr = cline()        except ApplicationError as err:            self.assertEqual(err.returncode, 255)        else:            self.fail(f\"Should have failed, returned:\\n{stdout}\\n{stderr}\")",
        "labels_text": "Test an input file in an invalid format"
    },
    {
        "input_text": "summarize: def test_output_directory_with_space(self):                temp_out_dir = \"xxmotif test\"        input_file = self.copy_and_mark_for_cleanup(\"Fasta/f002\")        try:            XXmotifCommandline(outdir=temp_out_dir, seqfile=input_file)        except ValueError:            pass        else:            self.fail(\"expected ValueError\")",
        "labels_text": "Test an output directory containing a space"
    },
    {
        "input_text": "summarize: def test_properties(self):                input_file = self.copy_and_mark_for_cleanup(\"Fasta/f002\")        cline = XXmotifCommandline(outdir=self.out_dir, seqfile=input_file)        cline.revcomp = True        cline.pseudo = 20        cline.startmotif = \"ACGGGT\"        self.standard_test_procedure(cline)",
        "labels_text": "Test setting option via property"
    },
    {
        "input_text": "summarize: def update_circuit(instance, **kwargs):        termination_name = f'termination_{instance.term_side.lower()}'    instance.circuit.refresh_from_db()    setattr(instance.circuit, termination_name, instance)    instance.circuit.save()",
        "labels_text": "When a CircuitTermination ha been modified update it parent Circuit"
    },
    {
        "input_text": "summarize: def rebuild_cablepaths(instance, raw=False, **kwargs):        if not raw:        peer_termination = instance.get_peer_termination()        if peer_termination:            rebuild_paths([peer_termination])",
        "labels_text": "Rebuild any CablePaths which traverse the peer CircuitTermination"
    },
    {
        "input_text": "summarize: def clear_events_queue(sender, **kwargs):        logger = logging.getLogger('events')    logger.info(f\"Clearing {len(events_queue.get())} queued events ({sender})\")    events_queue.set({})",
        "labels_text": "Delete any queued event eg because of an aborted bulk transaction"
    },
    {
        "input_text": "summarize: def auto_sync(instance, **kwargs):        from .models import AutoSyncRecord    for autosync in AutoSyncRecord.objects.filter(datafile__source=instance).prefetch_related('object'):        autosync.object.sync(save=True)",
        "labels_text": "Automatically synchronize any DataFiles with AutoSyncRecords after synchronizing a DataSource"
    },
    {
        "input_text": "summarize: def update_config(sender, instance, **kwargs):        instance.activate()",
        "labels_text": "Update the cached NetBox configuration when a new ConfigRevision is created"
    },
    {
        "input_text": "summarize: def get_serializer_ref_name(self, serializer):        # from drf-yasg.utils                serializer_meta = getattr(serializer, 'Meta', None)        serializer_name = type(serializer).__name__        if hasattr(serializer_meta, 'ref_name'):            ref_name = serializer_meta.ref_name        else:            ref_name = serializer_name            if ref_name.endswith('Serializer'):                ref_name = ref_name[: -len('Serializer')]        return ref_name",
        "labels_text": "Get serializers refname param serializer Serializer instance return Serializers refname or None for inline serializer rtype str or None"
    },
    {
        "input_text": "summarize: def _generate_description(self):                model_name = self.view.queryset.model._meta.verbose_name        # Determine if the method is for list or detail.        if '{id}' in self.path:            return f\"{self.method.capitalize()} a {model_name} object.\"        return f\"{self.method.capitalize()} a list of {model_name} objects.\"",
        "labels_text": "Generate a docstring for the method It also take into account whether the method is for list or detail"
    },
    {
        "input_text": "summarize: def get_changed_object(self, obj):                if obj.changed_object is None:            return None        try:            serializer = get_serializer_for_model(obj.changed_object)        except SerializerNotFound:            return obj.object_repr        data = serializer(obj.changed_object, nested=True, context={'request': self.context['request']}).data        return data",
        "labels_text": "Serialize a nested representation of the changed object"
    },
    {
        "input_text": "summarize: def handle(self, *args, **kwargs):                if not kwargs['check_changes'] and not settings.DEVELOPER:            raise CommandError(                \"This command is available for development purposes only. It will\\n\"                \"NOT resolve any issues with missing or unapplied migrations. For assistance,\\n\"                \"please post to the NetBox discussion forum on GitHub:\\n\"                \"    https://github.com/netbox-community/netbox/discussions\"            )        super().handle(*args, **kwargs)",
        "labels_text": "This builtin management command enables the creation of new database schema migration file which should never be required by and ordinary user We prevent this command from executing unless the configuration indicates that the user is a developer ie configurationDEVELOPER True or it wa run with check"
    },
    {
        "input_text": "summarize: def diff_exclude_fields(self):                model = self.changed_object_type.model_class()        attrs = set()        # Exclude auto-populated change tracking fields        if issubclass(model, ChangeLoggingMixin):            attrs.update({'created', 'last_updated'})        # Exclude MPTT-internal fields        if issubclass(model, MPTTModel):            attrs.update({'level', 'lft', 'rght', 'tree_id'})        return attrs",
        "labels_text": "Return a set of attribute which should be ignored when calculating a diff between the pre and postchange data For instance it would not make sense to compare the last updated time a these are expected to differ"
    },
    {
        "input_text": "summarize: def get_clean_data(self, prefix):                ret = {}        change_data = getattr(self, f'{prefix}_data') or {}        for k, v in change_data.items():            if k not in self.diff_exclude_fields and not k.startswith('_'):                ret[k] = v        return ret",
        "labels_text": "Return only the prepostchange attribute which are relevant for calculating a diff"
    },
    {
        "input_text": "summarize: def activate(self):                cache.set('config', self.data, None)        cache.set('config_version', self.pk, None)",
        "labels_text": "Cache the configuration data"
    },
    {
        "input_text": "summarize: def public(self):                q = Q()        for app_label, models in registry['models'].items():            q |= Q(app_label=app_label, model__in=models)        return self.get_queryset().filter(q)",
        "labels_text": "Filter the base queryset to return only ContentTypes corresponding to public model those which are listed in registrymodels and intended for reference by other object"
    },
    {
        "input_text": "summarize: def with_feature(self, feature):                if feature not in registry['model_features']:            raise KeyError(                f\"{feature} is not a registered model feature! Valid features are: {registry['model_features'].keys()}\"            )        q = Q()        for app_label, models in registry['model_features'][feature].items():            q |= Q(app_label=app_label, model__in=models)        return self.get_queryset().filter(q)",
        "labels_text": "Return the ContentTypes only for model which are registered a supporting the specified feature For example we can find all ContentTypes for model which support webhooks with ContentTypeobjectswithfeatureeventrules"
    },
    {
        "input_text": "summarize: def _ignore(self, filename):                if filename.startswith('.'):            return True        for rule in self.ignore_rules.splitlines():            if fnmatchcase(filename, rule):                return True        return False",
        "labels_text": "Returns a boolean indicating whether the file should be ignored per the DataSources configured ignore rule"
    },
    {
        "input_text": "summarize: def get_data(self):                # TODO: Something more robust        return yaml.safe_load(self.data_as_string)",
        "labels_text": "Attempt to read the file data a JSONYAML and return a native Python object"
    },
    {
        "input_text": "summarize: def write_to_disk(self, path, overwrite=False):                # Check whether file already exists        if os.path.isfile(path) and not overwrite:            raise FileExistsError()        with open(path, 'wb+') as new_file:            new_file.write(self.data)",
        "labels_text": "Write the object data to disk at the specified path"
    },
    {
        "input_text": "summarize: def start(self):                if self.started is not None:            return        # Start the job        self.started = timezone.now()        self.status = JobStatusChoices.STATUS_RUNNING        self.save()        # Send signal        job_start.send(self)",
        "labels_text": "Record the job start time and update it status to running"
    },
    {
        "input_text": "summarize: def get_for_device_type(self, queryset, name, value):                return queryset.filter(Q(manufacturer=None) | Q(manufacturer__device_types=value))",
        "labels_text": "Return all Platforms available for a specific manufacturer based on device type and Platforms not assigned any manufacturer"
    },
    {
        "input_text": "summarize: def handle_rack_site_change(instance, created, **kwargs):        if not created:        Device.objects.filter(rack=instance).update(site=instance.site, location=instance.location)",
        "labels_text": "Update child Devices if Site or Location assignment ha changed"
    },
    {
        "input_text": "summarize: def assign_virtualchassis_master(instance, created, **kwargs):        if created and instance.master:        master = Device.objects.get(pk=instance.master.pk)        master.virtual_chassis = instance        master.vc_position = 1        master.save()",
        "labels_text": "When a VirtualChassis is created automatically assign it master device if any to the VC"
    },
    {
        "input_text": "summarize: def clear_virtualchassis_members(instance, **kwargs):        devices = Device.objects.filter(virtual_chassis=instance.pk)    for device in devices:        device.vc_position = None        device.vc_priority = None        device.save()",
        "labels_text": "When a VirtualChassis is deleted nullify the vcposition and vcpriority field of it prior member"
    },
    {
        "input_text": "summarize: def retrace_cable_paths(instance, **kwargs):        for cablepath in CablePath.objects.filter(_nodes__contains=instance):        cablepath.retrace()",
        "labels_text": "When a Cable is deleted check for and update it connected endpoint"
    },
    {
        "input_text": "summarize: def extend_rearport_cable_paths(instance, created, raw, **kwargs):        if created and not raw:        rearport = instance.rear_port        for cablepath in CablePath.objects.filter(_nodes__contains=rearport):            cablepath.retrace()",
        "labels_text": "When a new FrontPort is created add it to any CablePaths which end at it corresponding RearPort"
    },
    {
        "input_text": "summarize: def object_to_path_node(obj):        ct = ContentType.objects.get_for_model(obj)    return compile_path_node(ct.pk, obj.pk)",
        "labels_text": "Return a representation of an object suitable for inclusion in a CablePath path Node representation is in the form ContentType IDObject ID"
    },
    {
        "input_text": "summarize: def path_node_to_object(repr):        ct_id, object_id = decompile_path_node(repr)    ct = ContentType.objects.get_for_id(ct_id)    return ct.model_class().objects.filter(pk=object_id).first()",
        "labels_text": "Given the string representation of a path node return the corresponding instance If the object no longer exists return None"
    },
    {
        "input_text": "summarize: def create_cablepath(terminations):        from dcim.models import CablePath    cp = CablePath.from_origin(terminations)    if cp:        cp.save()",
        "labels_text": "Create CablePaths for all path originating from the specified set of node param termination Iterable of CableTermination object"
    },
    {
        "input_text": "summarize: def rebuild_paths(terminations):        from dcim.models import CablePath    for obj in terminations:        cable_paths = CablePath.objects.filter(_nodes__contains=obj)        with transaction.atomic():            for cp in cable_paths:                cp.delete()                create_cablepath(cp.origins)",
        "labels_text": "Rebuild all CablePaths which traverse the specified node"
    },
    {
        "input_text": "summarize: def save_object(self, object_form, request):                instance = object_form.save(commit=False)        instance.user = request.user        instance.save()        return instance",
        "labels_text": "Assign the currently authenticated user to the RackReservation"
    },
    {
        "input_text": "summarize: def paths(self, request, pk):                obj = get_object_or_404(self.queryset, pk=pk)        cablepaths = CablePath.objects.filter(_nodes__contains=obj)        serializer = serializers.CablePathSerializer(cablepaths, context={'request': request}, many=True)        return Response(serializer.data)",
        "labels_text": "Return all CablePaths which traverse a given passthrough port"
    },
    {
        "input_text": "summarize: def get_serializer_class(self):                request = self.get_serializer_context()['request']        if self.brief or 'config_context' in request.query_params.get('exclude', []):            return serializers.DeviceSerializer        return serializers.DeviceWithConfigContextSerializer",
        "labels_text": "Select the specific serializer based on the request context If the brief query param equates to True return the NestedDeviceSerializer If the exclude query param includes configcontext a a value return the DeviceSerializer Else return the DeviceWithConfigContextSerializer"
    },
    {
        "input_text": "summarize: def get_connected_endpoints(self, obj):                if endpoints := obj.connected_endpoints:            serializer = get_serializer_for_model(endpoints[0])            context = {'request': self.context['request']}            return serializer(endpoints, nested=True, many=True, context=context).data",
        "labels_text": "Return the appropriate serializer for the type of connected object"
    },
    {
        "input_text": "summarize: def get_link_peers_type(self, obj):                if not obj.cable:            return None        if obj.link_peers:            return f'{obj.link_peers[0]._meta.app_label}.{obj.link_peers[0]._meta.model_name}'        return None",
        "labels_text": "Return the type of the peer link termination or None"
    },
    {
        "input_text": "summarize: def get_link_peers(self, obj):                if not obj.link_peers:            return []        # Return serialized peer termination objects        serializer = get_serializer_for_model(obj.link_peers[0])        context = {'request': self.context['request']}        return serializer(obj.link_peers, nested=True, many=True, context=context).data",
        "labels_text": "Return the appropriate serializer for the link termination model"
    },
    {
        "input_text": "summarize: def draw_progress_bar(self, percentage):                bar_size = int(percentage / 5)        self.stdout.write(f\"\\r  [{'#' * bar_size}{' ' * (20 - bar_size)}] {int(percentage)}%\", ending='')",
        "labels_text": "Draw a simple progress bar increment wide illustrating the specified percentage"
    },
    {
        "input_text": "summarize: def path_objects(self):                if not hasattr(self, '_path_objects'):            self._path_objects = self._get_path()        return self._path_objects",
        "labels_text": "Cache and return the complete path a list of object derived from their annotation within the path"
    },
    {
        "input_text": "summarize: def origins(self):                return self.path_objects[0]",
        "labels_text": "Return the list of originating object"
    },
    {
        "input_text": "summarize: def destinations(self):                if not self.is_complete:            return []        return self.path_objects[-1]",
        "labels_text": "Return the list of destination object if the path is complete"
    },
    {
        "input_text": "summarize: def retrace(self):                _new = self.from_origin(self.origins)        if _new:            self.path = _new.path            self.is_complete = _new.is_complete            self.is_active = _new.is_active            self.is_split = _new.is_split            self.save()        else:            self.delete()",
        "labels_text": "Retrace the path from the currentlydefined originating termination"
    },
    {
        "input_text": "summarize: def get_cable_ids(self):                cable_ct = ObjectType.objects.get_for_model(Cable).pk        cable_ids = []        for node in self._nodes:            ct, id = decompile_path_node(node)            if ct == cable_ct:                cable_ids.append(id)        return cable_ids",
        "labels_text": "Return all Cable IDs within the path"
    },
    {
        "input_text": "summarize: def get_total_length(self):                cable_ids = self.get_cable_ids()        cables = Cable.objects.filter(id__in=cable_ids, _abs_length__isnull=False)        total_length = cables.aggregate(total=Sum('_abs_length'))['total']        is_definitive = len(cables) == len(cable_ids)        return total_length, is_definitive",
        "labels_text": "Return a tuple containing the sum of the length of each cable in the path and a flag indicating whether the length is definitive"
    },
    {
        "input_text": "summarize: def get_asymmetric_nodes(self):                from circuits.models import CircuitTermination        asymmetric_nodes = []        for nodes in self.path_objects:            if type(nodes[0]) in [RearPort, FrontPort, CircuitTermination]:                asymmetric_nodes.extend([node for node in nodes if node.link is None])        return asymmetric_nodes",
        "labels_text": "Return all available next segment in a split cable path"
    },
    {
        "input_text": "summarize: def update_interface_bridges(device, interface_templates, module=None):        for interface_template in interface_templates.exclude(bridge=None):        interface = Interface.objects.get(device=device, name=interface_template.resolve_name(module=module))        if interface_template.bridge:            interface.bridge = Interface.objects.get(device=device, name=interface_template.bridge.resolve_name(module=module))            interface.full_clean()            interface.save()",
        "labels_text": "Used for device and module instantiation Iterates all InterfaceTemplates with a bridge assigned and applies it to the actual interface"
    },
    {
        "input_text": "summarize: def identifier(self):                if self.name is not None:            return self.name        return '{{{}}}'.format(self.pk)",
        "labels_text": "Return the device name if set otherwise return the Devices primary key a pk"
    },
    {
        "input_text": "summarize: def get_vc_master(self):                return self.virtual_chassis.master if self.virtual_chassis else None",
        "labels_text": "If this Device is a VirtualChassis member return the VC master Otherwise return None"
    },
    {
        "input_text": "summarize: def vc_interfaces(self, if_master=True):                filter = Q(device=self) if self.pk else Q()        if self.virtual_chassis and (self.virtual_chassis.master == self or not if_master):            filter |= Q(device__virtual_chassis=self.virtual_chassis, mgmt_only=False)        return Interface.objects.filter(filter)",
        "labels_text": "Return a QuerySet matching all Interfaces assigned to this Device or if this Device is a VC master to another Device belonging to the same VirtualChassis param ifmaster If True return VC member interface only if this Device is the VC master"
    },
    {
        "input_text": "summarize: def get_children(self):                return Device.objects.filter(parent_bay__device=self.pk)",
        "labels_text": "Return the set of child Devices installed in DeviceBays within this Device"
    },
    {
        "input_text": "summarize: def link(self):                return self.cable",
        "labels_text": "Generic wrapper for a Cable WirelessLink or some other relation to a connected termination"
    },
    {
        "input_text": "summarize: def connected_endpoints(self):                return self._path.destinations if self._path else []",
        "labels_text": "Caching accessor for the attached CablePaths destination if any"
    },
    {
        "input_text": "summarize: def instantiate(self, device):                raise NotImplementedError()",
        "labels_text": "Instantiate a new component on the specified Device"
    },
    {
        "input_text": "summarize: def get_config_template(self):                if self.config_template:            return self.config_template        if self.role and self.role.config_template:            return self.role.config_template        if self.platform and self.platform.config_template:            return self.platform.config_template",
        "labels_text": "Return the appropriate ConfigTemplate if any for this Device"
    },
    {
        "input_text": "summarize: def units(self):                if self.desc_units:            return drange(decimal.Decimal(self.starting_unit), self.u_height + self.starting_unit, 0.5)        return drange(self.u_height + decimal.Decimal(0.5) + self.starting_unit - 1, 0.5 + self.starting_unit - 1, -0.5)",
        "labels_text": "Return a list of unit number top to bottom"
    },
    {
        "input_text": "summarize: def copy_racktype_attrs(self):                if self.rack_type:            for field_name in self.RACKTYPE_FIELDS:                setattr(self, field_name, getattr(self.rack_type, field_name))",
        "labels_text": "Copy physical attribute from the assigned RackType if any"
    },
    {
        "input_text": "summarize: def units(self):                if self.desc_units:            return drange(decimal.Decimal(self.starting_unit), self.u_height + self.starting_unit, 0.5)        return drange(self.u_height + decimal.Decimal(0.5) + self.starting_unit - 1, 0.5 + self.starting_unit - 1, -0.5)",
        "labels_text": "Return a list of unit number top to bottom"
    },
    {
        "input_text": "summarize: def get_reserved_units(self):                reserved_units = {}        for reservation in self.reservations.all():            for u in reservation.units:                reserved_units[u] = reservation        return reserved_units",
        "labels_text": "Return a dictionary mapping all reserved unit within the rack to their reservation"
    },
    {
        "input_text": "summarize: def _get_color(cls, instance):                if hasattr(instance, 'parent_object'):            # Termination            return getattr(instance, 'color', 'f0f0f0') or 'f0f0f0'        if hasattr(instance, 'role'):            # Device            return instance.role.color        elif instance._meta.model_name == 'circuit' and instance.type.color:            return instance.type.color        else:            # Other parent object            return 'e0e0e0'",
        "labels_text": "Return the appropriate fill color for an object within a cable path"
    },
    {
        "input_text": "summarize: def draw_fanin(self, target, terminations, color):                for term in terminations:            points = (                term.bottom_center,                (term.bottom_center[0], term.bottom_center[1] + FANOUT_LEG_HEIGHT),                target,            )            self.connectors.extend((                Polyline(points=points, class_='cable-shadow'),                Polyline(points=points, style=f'stroke: #{color}'),            ))",
        "labels_text": "Draw the faninlines from each of the termination to the targetpoint"
    },
    {
        "input_text": "summarize: def draw_fanout(self, start, terminations, color):                for term in terminations:            points = (                term.top_center,                (term.top_center[0], term.top_center[1] - FANOUT_LEG_HEIGHT),                start,            )            self.connectors.extend((                Polyline(points=points, class_='cable-shadow'),                Polyline(points=points, style=f'stroke: #{color}'),            ))",
        "labels_text": "Draw the fanoutlines from the startpoint to each of the termination"
    },
    {
        "input_text": "summarize: def draw_attachment(self):                group = Group(class_='connector')        # Draw attachment (line)        start = (OFFSET + self.center, OFFSET + self.cursor)        height = PADDING * 2 + LINE_HEIGHT + PADDING * 2        end = (start[0], start[1] + height)        line = Line(start=start, end=end, class_='attachment')        group.add(line)        self.cursor += PADDING * 4        return group",
        "labels_text": "Return an SVG group containing a line element and Attachment label"
    },
    {
        "input_text": "summarize: def draw_device_front(self, device, coords, size):                color = device.role.color        image = device.device_type.front_image        self._draw_device(device, coords, size, color=color, image=image)",
        "labels_text": "Draw the front mounted face of a device"
    },
    {
        "input_text": "summarize: def draw_device_rear(self, device, coords, size):                image = device.device_type.rear_image        self._draw_device(device, coords, size, image=image)",
        "labels_text": "Draw the rear opposite face of a device"
    },
    {
        "input_text": "summarize: def draw_border(self):                border_width = RACK_ELEVATION_BORDER_WIDTH        border_offset = RACK_ELEVATION_BORDER_WIDTH / 2        frame = Rect(            insert=(self.legend_width + border_offset, border_offset),            size=(self.unit_width + border_width, self.rack.u_height * self.unit_height + border_width),            class_='rack'        )        self.drawing.add(frame)",
        "labels_text": "Draw a border around the collection of rack unit"
    },
    {
        "input_text": "summarize: def render(self, face):                # Initialize the drawing        self.drawing = self._setup_drawing()        # Draw the empty rack, legend, and margin        self.draw_legend()        self.draw_background(face)        self.draw_margin()        # Draw the rack face        self.draw_face(face)        # Draw the rack border last        self.draw_border()        return self.drawing",
        "labels_text": "Return an SVG document representing a rack elevation"
    },
    {
        "input_text": "summarize: def test_get_rack_elevation_svg(self):                rack = Rack.objects.first()        self.add_permissions('dcim.view_rack')        url = '{}?render=svg'.format(reverse('dcim-api:rack-elevation', kwargs={'pk': rack.pk}))        response = self.client.get(url, **self.header)        self.assertHttpStatus(response, status.HTTP_200_OK)        self.assertEqual(response.get('Content-Type'), 'image/svg+xml')",
        "labels_text": "GET a single rack elevation in SVG format"
    },
    {
        "input_text": "summarize: def test_config_context_included_by_default_in_list_view(self):                self.add_permissions('dcim.view_device')        url = reverse('dcim-api:device-list') + '?slug=device-with-context-data'        response = self.client.get(url, **self.header)        self.assertEqual(response.data['results'][0].get('config_context', {}).get('A'), 1)",
        "labels_text": "Check that config context data is included by default in the device list"
    },
    {
        "input_text": "summarize: def test_config_context_excluded(self):                self.add_permissions('dcim.view_device')        url = reverse('dcim-api:device-list') + '?exclude=config_context'        response = self.client.get(url, **self.header)        self.assertFalse('config_context' in response.data['results'][0])",
        "labels_text": "Check that config context data can be excluded by passing excludeconfigcontext"
    },
    {
        "input_text": "summarize: def _get_cablepath(self, nodes, **kwargs):                path = []        for step in nodes:            if type(step) in (list, tuple):                path.append([object_to_path_node(node) for node in step])            else:                path.append([object_to_path_node(step)])        return CablePath.objects.filter(path=path, **kwargs).first()",
        "labels_text": "Return a given cable path param node Iterable of step with each step being either a single node or a list of node return The matching CablePath if any"
    },
    {
        "input_text": "summarize: def assertPathExists(self, nodes, **kwargs):                cablepath = self._get_cablepath(nodes, **kwargs)        self.assertIsNotNone(cablepath, msg='CablePath not found')        return cablepath",
        "labels_text": "Assert that a CablePath from origin to destination with a specific intermediate path exists Returns the first matching CablePath if found param node Iterable of step with each step being either a single node or a list of node"
    },
    {
        "input_text": "summarize: def assertPathDoesNotExist(self, nodes, **kwargs):                cablepath = self._get_cablepath(nodes, **kwargs)        self.assertIsNone(cablepath, msg='Unexpected CablePath found')",
        "labels_text": "Assert that a specific CablePath doe not exist param node Iterable of step with each step being either a single node or a list of node"
    },
    {
        "input_text": "summarize: def assertPathIsSet(self, origin, cablepath, msg=None):                if msg is None:            msg = f\"Path #{cablepath.pk} not set on originating endpoint {origin}\"        self.assertEqual(origin._path_id, cablepath.pk, msg=msg)",
        "labels_text": "Assert that a specific CablePath instance is set a the path on the origin param origin The originating path endpoint param cablepath The CablePath instance originating from this endpoint param msg Custom failure message optional"
    },
    {
        "input_text": "summarize: def assertPathIsNotSet(self, origin, msg=None):                if msg is None:            msg = f\"Path #{origin._path_id} set as origin on {origin}; should be None!\"        self.assertIsNone(origin._path_id, msg=msg)",
        "labels_text": "Assert that a specific CablePath instance is set a the path on the origin param origin The originating path endpoint param msg Custom failure message optional"
    },
    {
        "input_text": "summarize: def test_interface_label_count_valid(self):                interface_data = {            'device': self.device.pk,            'name': 'eth[0-9]',            'label': 'Interface[0-9]',            'type': InterfaceTypeChoices.TYPE_1GE_GBIC,        }        form = InterfaceCreateForm(interface_data)        self.assertTrue(form.is_valid())",
        "labels_text": "Test that generating an equal number of name and label pass form validation"
    },
    {
        "input_text": "summarize: def test_interface_label_count_mismatch(self):                bad_interface_data = {            'device': self.device.pk,            'name': 'eth[0-9]',            'label': 'Interface[0-1]',            'type': InterfaceTypeChoices.TYPE_1GE_GBIC,        }        form = InterfaceCreateForm(bad_interface_data)        self.assertFalse(form.is_valid())        self.assertIn('label', form.errors)",
        "labels_text": "Check that attempting to generate a differing number of name and label result in a validation error"
    },
    {
        "input_text": "summarize: def test_mount_zero_ru(self):                site = Site.objects.first()        rack = Rack.objects.first()        Device(            name='Device 1',            role=DeviceRole.objects.first(),            device_type=DeviceType.objects.first(),            site=site,            rack=rack        ).save()",
        "labels_text": "Check that a RU device can be mounted in a rack with no faceposition"
    },
    {
        "input_text": "summarize: def test_cable_validates_same_parent_object(self):                interface1 = Interface.objects.get(device__name='TestDevice1', name='eth0')        powerport1 = PowerPort.objects.get(device__name='TestDevice2', name='psu1')        cable = Cable(a_terminations=[interface1], b_terminations=[powerport1])        with self.assertRaises(ValidationError):            cable.clean()",
        "labels_text": "The clean method should ensure that all termination at either end of a Cable belong to the same parent object"
    },
    {
        "input_text": "summarize: def test_cable_validates_compatible_types(self):                interface1 = Interface.objects.get(device__name='TestDevice1', name='eth0')        powerport1 = PowerPort.objects.get(device__name='TestDevice2', name='psu1')        # An interface cannot be connected to a power port, for example        cable = Cable(a_terminations=[interface1], b_terminations=[powerport1])        with self.assertRaises(ValidationError):            cable.clean()",
        "labels_text": "The clean method should have a check to ensure only compatible port type can be connected by a cable"
    },
    {
        "input_text": "summarize: def test_cable_cannot_terminate_to_a_provider_network_circuittermination(self):                interface3 = Interface.objects.get(device__name='TestDevice2', name='eth1')        circuittermination3 = CircuitTermination.objects.get(circuit__cid='2', term_side='A')        cable = Cable(a_terminations=[interface3], b_terminations=[circuittermination3])        with self.assertRaises(ValidationError):            cable.clean()",
        "labels_text": "Neither side of a cable can be terminated to a CircuitTermination which is attached to a ProviderNetwork"
    },
    {
        "input_text": "summarize: def test_list_rack_elevations(self):                response = self.client.get(reverse('dcim:rack_elevation_list'))        self.assertHttpStatus(response, 200)",
        "labels_text": "Test viewing the list of rack elevation"
    },
    {
        "input_text": "summarize: def is_ruleset(data):        return type(data) is dict and len(data) == 1 and list(data.keys())[0] in (AND, OR)",
        "labels_text": "Determine whether the given dictionary look like a rule set"
    },
    {
        "input_text": "summarize: def eval(self, data):                def _get(obj, key):            if isinstance(obj, list):                return [dict.get(i, key) for i in obj]            return dict.get(obj, key)        try:            value = functools.reduce(_get, self.attr.split('.'), data)        except TypeError:            # Invalid key path            value = None        result = self.eval_func(value)        if self.negate:            return not result        return result",
        "labels_text": "Evaluate the provided data to determine whether it match the condition"
    },
    {
        "input_text": "summarize: def eval(self, data):                func = any if self.logic == 'or' else all        return func(d.eval(data) for d in self.conditions)",
        "labels_text": "Evaluate the provided data to determine whether it match this set of condition"
    },
    {
        "input_text": "summarize: def serialize_for_event(instance):        serializer_class = get_serializer_for_model(instance.__class__)    serializer_context = {        'request': None,    }    serializer = serializer_class(instance, context=serializer_context)    return serializer.data",
        "labels_text": "Return a serialized representation of the given instance suitable for use in a queued event"
    },
    {
        "input_text": "summarize: def flush_events(events):        if events:        for name in settings.EVENTS_PIPELINE:            try:                func = import_string(name)                func(events)            except Exception as e:                logger.error(_(\"Cannot import events pipeline {name} error: {error}\").format(name=name, error=e))",
        "labels_text": "Flush a list of object representation to RQ for event processing"
    },
    {
        "input_text": "summarize: def annotate_config_context_data(self):                from extras.models import ConfigContext        return self.annotate(            config_context_data=Subquery(                ConfigContext.objects.filter(                    self._get_config_context_filters()                ).annotate(                    _data=EmptyGroupByJSONBAgg('data', ordering=['weight', 'name'])                ).values(\"_data\").order_by()            )        ).distinct()",
        "labels_text": "Attach the subquery annotation to the base queryset"
    },
    {
        "input_text": "summarize: def unread(self):                return self.filter(read__isnull=True)",
        "labels_text": "Return only unread notification"
    },
    {
        "input_text": "summarize: def as_field(self):                form_field = self.form_field(**self.field_attrs)        if not isinstance(form_field.widget, forms.CheckboxInput):            if form_field.widget.attrs and 'class' in form_field.widget.attrs.keys():                form_field.widget.attrs['class'] += ' form-control'            else:                form_field.widget.attrs['class'] = 'form-control'        return form_field",
        "labels_text": "Render the variable a a Django form field"
    },
    {
        "input_text": "summarize: def run(self, data, commit):                # Backward compatibility for legacy Reports        self.pre_run()        self.run_tests()        self.post_run()",
        "labels_text": "Override this method with custom script logic"
    },
    {
        "input_text": "summarize: def get_job_data(self):                return {            'log': self.messages,            'output': self.output,            'tests': self.tests,        }",
        "labels_text": "Return a dictionary of data to attach to the script Job"
    },
    {
        "input_text": "summarize: def load_yaml(self, filename):                try:            from yaml import CLoader as Loader        except ImportError:            from yaml import Loader        file_path = os.path.join(settings.SCRIPTS_ROOT, filename)        with open(file_path, 'r') as datafile:            data = yaml.load(datafile, Loader=Loader)        return data",
        "labels_text": "Return data from a YAML file"
    },
    {
        "input_text": "summarize: def load_json(self, filename):                file_path = os.path.join(settings.SCRIPTS_ROOT, filename)        with open(file_path, 'r') as datafile:            data = json.load(datafile)        return data",
        "labels_text": "Return data from a JSON file"
    },
    {
        "input_text": "summarize: def run_tests(self):                self.logger.info(f\"Running report\")        try:            for test_name in self.tests:                self._current_test = test_name                test_method = getattr(self, test_name)                test_method()                self._current_test = None        except Exception as e:            self._current_test = None            self.post_run()            raise e",
        "labels_text": "Run the report and save it result Each test method will be executed in order"
    },
    {
        "input_text": "summarize: def pre_run(self):                pass",
        "labels_text": "Legacy method for operation performed immediately prior to running a Report"
    },
    {
        "input_text": "summarize: def post_run(self):                pass",
        "labels_text": "Legacy method for operation performed immediately after running a Report"
    },
    {
        "input_text": "summarize: def is_variable(obj):        return isinstance(obj, ScriptVariable)",
        "labels_text": "Returns True if the object is a ScriptVariable"
    },
    {
        "input_text": "summarize: def handle_cf_added_obj_types(instance, action, pk_set, **kwargs):        if action == 'post_add':        instance.populate_initial_data(ContentType.objects.filter(pk__in=pk_set))",
        "labels_text": "Handle the population of defaultnull value when a CustomField is added to one or more ContentTypes"
    },
    {
        "input_text": "summarize: def handle_cf_removed_obj_types(instance, action, pk_set, **kwargs):        if action == 'post_remove':        instance.remove_stale_data(ContentType.objects.filter(pk__in=pk_set))",
        "labels_text": "Handle the cleanup of old custom field data when a CustomField is removed from one or more ContentTypes"
    },
    {
        "input_text": "summarize: def handle_cf_renamed(instance, created, **kwargs):        if not created and instance.name != instance._name:        instance.rename_object_data(old_name=instance._name, new_name=instance.name)",
        "labels_text": "Handle the renaming of custom field data on object when a CustomField is renamed"
    },
    {
        "input_text": "summarize: def handle_cf_deleted(instance, **kwargs):        instance.remove_stale_data(instance.object_types.all())",
        "labels_text": "Handle the cleanup of old custom field data when a CustomField is deleted"
    },
    {
        "input_text": "summarize: def run_save_validators(sender, instance, **kwargs):        model_name = f'{sender._meta.app_label}.{sender._meta.model_name}'    validators = get_config().CUSTOM_VALIDATORS.get(model_name, [])    run_validators(instance, validators)",
        "labels_text": "Run any custom validation rule for the model prior to calling save"
    },
    {
        "input_text": "summarize: def process_job_start_event_rules(sender, **kwargs):        event_rules = EventRule.objects.filter(        event_types__contains=[JOB_STARTED],        enabled=True,        object_types=sender.object_type    )    username = sender.user.username if sender.user else None    process_event_rules(        event_rules=event_rules,        object_type=sender.object_type,        event_type=JOB_STARTED,        data=sender.data,        username=username    )",
        "labels_text": "Process event rule for job starting"
    },
    {
        "input_text": "summarize: def process_job_end_event_rules(sender, **kwargs):        event_rules = EventRule.objects.filter(        event_types__contains=[JOB_COMPLETED],        enabled=True,        object_types=sender.object_type    )    username = sender.user.username if sender.user else None    process_event_rules(        event_rules=event_rules,        object_type=sender.object_type,        event_type=JOB_COMPLETED,        data=sender.data,        username=username    )",
        "labels_text": "Process event rule for job terminating"
    },
    {
        "input_text": "summarize: def is_taggable(obj):        if hasattr(obj, 'tags'):        if issubclass(obj.tags.__class__, _TaggableManager):            return True    return False",
        "labels_text": "Return True if the instance can have Tags assigned to it False otherwise"
    },
    {
        "input_text": "summarize: def is_script(obj):        from .reports import Report    from .scripts import Script    try:        return (issubclass(obj, Report) and obj != Report) or (issubclass(obj, Script) and obj != Script)    except TypeError:        return False",
        "labels_text": "Returns True if the object is a Script or Report"
    },
    {
        "input_text": "summarize: def is_report(obj):        from .reports import Report    try:        return issubclass(obj, Report) and obj != Report    except TypeError:        return False",
        "labels_text": "Returns True if the given object is a Report"
    },
    {
        "input_text": "summarize: def get_validator(self, descriptor, value):                if descriptor not in self.VALIDATORS:            raise NotImplementedError(                f\"Unknown validation type for {self.__class__.__name__}: '{descriptor}'\"            )        validator_cls = self.VALIDATORS.get(descriptor)        return validator_cls(value)",
        "labels_text": "Instantiate and return the appropriate validator based on the descriptor given For example min return MinValueValidatorvalue"
    },
    {
        "input_text": "summarize: def validate(self, instance, request):                return",
        "labels_text": "Custom validation method to be overridden by the user Validation failure should raise a ValidationError exception"
    },
    {
        "input_text": "summarize: def fail(self, message, field=None):                if field is not None:            raise ValidationError({field: message})        raise ValidationError(message)",
        "labels_text": "Raise a ValidationError exception Associate the provided message with a formserializer field if specified"
    },
    {
        "input_text": "summarize: def get_queryset(self, request):                queryset = SavedFilter.objects.all()        user = request.user        if user.is_superuser:            return queryset        if user.is_anonymous:            return queryset.filter(shared=True)        return queryset.filter(            Q(shared=True) | Q(user=user)        )",
        "labels_text": "Return only shared SavedFilters or those owned by the current user unless this is a superuser"
    },
    {
        "input_text": "summarize: def _get_script_class(self, script):                if script_class := script.python_class:            return script_class()",
        "labels_text": "Return an instance of the Scripts Python class"
    },
    {
        "input_text": "summarize: def generate_signature(request_body, secret):        hmac_prep = hmac.new(        key=secret.encode('utf8'),        msg=request_body,        digestmod=hashlib.sha512    )    return hmac_prep.hexdigest()",
        "labels_text": "Return a cryptographic signature that can be used to verify the authenticity of webhook data"
    },
    {
        "input_text": "summarize: def _get_custom_fields(self):                if not hasattr(self, '_custom_fields'):            object_type = ObjectType.objects.get_for_model(self.parent.Meta.model)            self._custom_fields = CustomField.objects.filter(object_types=object_type)        return self._custom_fields",
        "labels_text": "Cache CustomFields assigned to this model to avoid redundant database query"
    },
    {
        "input_text": "summarize: def get_queryset(self):                queryset = super().get_queryset()        request = self.get_serializer_context()['request']        if self.brief or 'config_context' in request.query_params.get('exclude', []):            return queryset        return queryset.annotate_config_context_data()",
        "labels_text": "Build the proper queryset based on the request context If the brief query param equates to True or the exclude query param includes configcontext a a value return the base queryset Else return the queryset annotated with config context data"
    },
    {
        "input_text": "summarize: def render(self, request, pk):                configtemplate = self.get_object()        context = request.data        return self.render_configtemplate(request, configtemplate, context)",
        "labels_text": "Render a ConfigTemplate using the context data provided if any If the client request textplain data return the raw rendered content rather than serialized JSON"
    },
    {
        "input_text": "summarize: def register_widget(cls):        app_label = cls.__module__.split('.', maxsplit=1)[0]    label = f'{app_label}.{cls.__name__}'    registry['widgets'][label] = cls    return cls",
        "labels_text": "Decorator for registering a DashboardWidget class"
    },
    {
        "input_text": "summarize: def get_widget_class(name):        try:        return registry['widgets'][name]    except KeyError:        raise ValueError(_(\"Unregistered widget class: {name}\").format(name=name))",
        "labels_text": "Return a registered DashboardWidget class identified by it name"
    },
    {
        "input_text": "summarize: def get_dashboard(user):        if user.is_anonymous:        dashboard = get_default_dashboard()    else:        try:            dashboard = user.dashboard        except ObjectDoesNotExist:            # Create a dashboard for this user            dashboard = get_default_dashboard()            dashboard.user = user            dashboard.save()    return dashboard",
        "labels_text": "Return the Dashboard for a given User if one exists or generate a default dashboard"
    },
    {
        "input_text": "summarize: def render(self, request):                raise NotImplementedError(_(\"{class_name} must define a render() method.\").format(            class_name=self.__class__        ))",
        "labels_text": "This method is called to render the widget content Params request The current request"
    },
    {
        "input_text": "summarize: def get_full_path(scriptmodule):        root_path = ROOT_PATHS[scriptmodule.file_root]    return os.path.join(root_path, scriptmodule.file_path)",
        "labels_text": "Return the full path to a ScriptModules file on disk"
    },
    {
        "input_text": "summarize: def get_python_name(scriptmodule):        path, filename = os.path.split(scriptmodule.file_path)    return os.path.splitext(filename)[0]",
        "labels_text": "Return the Python name of a ScriptModules file on disk"
    },
    {
        "input_text": "summarize: def is_script(obj):        from extras.scripts import Script    from extras.reports import Report    try:        if issubclass(obj, Report) and obj != Report:            return True        if issubclass(obj, Script) and obj != Script:            return True    except TypeError:        pass    return False",
        "labels_text": "Returns True if the passed Python object is a Script or Report"
    },
    {
        "input_text": "summarize: def sync_data(self):                self.data = self.data_file.get_data()",
        "labels_text": "Synchronize context data from the designated DataFile if any"
    },
    {
        "input_text": "summarize: def sync_data(self):                self.template_code = self.data_file.data_as_string",
        "labels_text": "Synchronize template content from the designated DataFile if any"
    },
    {
        "input_text": "summarize: def get_for_model(self, model):                content_type = ObjectType.objects.get_for_model(model._meta.concrete_model)        return self.get_queryset().filter(object_types=content_type)",
        "labels_text": "Return all CustomFields assigned to the given model"
    },
    {
        "input_text": "summarize: def get_defaults_for_model(self, model):                custom_fields = self.get_for_model(model).filter(default__isnull=False)        return {            cf.name: cf.default for cf in custom_fields        }",
        "labels_text": "Return a dictionary of serialized default value for all CustomFields applicable to the given model"
    },
    {
        "input_text": "summarize: def populate_initial_data(self, content_types):                for ct in content_types:            model = ct.model_class()            instances = model.objects.exclude(**{f'custom_field_data__contains': self.name})            for instance in instances:                instance.custom_field_data[self.name] = self.default            model.objects.bulk_update(instances, ['custom_field_data'], batch_size=100)",
        "labels_text": "Populate initial custom field data upon either a the creation of a new CustomField or b the assignment of an existing CustomField to new object type"
    },
    {
        "input_text": "summarize: def remove_stale_data(self, content_types):                for ct in content_types:            model = ct.model_class()            instances = model.objects.filter(custom_field_data__has_key=self.name)            for instance in instances:                del instance.custom_field_data[self.name]            model.objects.bulk_update(instances, ['custom_field_data'], batch_size=100)",
        "labels_text": "Delete custom field data which is no longer relevant either because the CustomField is no longer assigned to a model or because it ha been deleted"
    },
    {
        "input_text": "summarize: def choices(self):                if not hasattr(self, '_choices'):            self._choices = []            if self.base_choices:                self._choices.extend(CHOICE_SETS.get(self.base_choices))            if self.extra_choices:                self._choices.extend(self.extra_choices)        if self.order_alphabetically:            self._choices = sorted(self._choices, key=lambda x: x[0])        return self._choices",
        "labels_text": "Returns a concatenation of the base and extra choice"
    },
    {
        "input_text": "summarize: def values(self):                return (x[0] for x in self.choices)",
        "labels_text": "Returns an iterator of the valid choice value"
    },
    {
        "input_text": "summarize: def get_widget(self, id):                id = str(id)        config = dict(self.config[id])  # Copy to avoid mutating instance data        widget_class = get_widget_class(config.pop('class'))        return widget_class(id=id, **config)",
        "labels_text": "Instantiate and return a widget by it ID"
    },
    {
        "input_text": "summarize: def get_layout(self):                widgets = []        for grid_item in self.layout:            widget = self.get_widget(grid_item['id'])            widget.set_layout(grid_item)            widgets.append(widget)        return widgets",
        "labels_text": "Return the dashboard configured layout suitable for rendering with gridstackjs"
    },
    {
        "input_text": "summarize: def add_widget(self, widget, x=None, y=None):                id = str(widget.id)        self.config[id] = {            'class': widget.name,            'title': widget.title,            'color': widget.color,            'config': widget.config,        }        self.layout.append({            'id': id,            'h': widget.height,            'w': widget.width,            'x': x,            'y': y,        })",
        "labels_text": "Add a widget to the dashboard optionally specifying it X Y coordinate"
    },
    {
        "input_text": "summarize: def delete_widget(self, id):                id = str(id)        del self.config[id]        self.layout = [            item for item in self.layout if item['id'] != id        ]",
        "labels_text": "Delete a widget from the dashboard"
    },
    {
        "input_text": "summarize: def get_jobs(self, name):                return self.jobs.filter(            name=name        )",
        "labels_text": "Returns a list of Jobs associated with this specific script or report module param name The class name of the script or report return List of Jobs associated with this"
    },
    {
        "input_text": "summarize: def eval_conditions(self, data):                if not self.conditions:            return True        return ConditionSet(self.conditions).eval(data)",
        "labels_text": "Test whether the given data meet the condition of the event rule if any Return True if met or no condition are specified"
    },
    {
        "input_text": "summarize: def render_headers(self, context):                if not self.additional_headers:            return {}        ret = {}        data = render_jinja2(self.additional_headers, context)        for line in data.splitlines():            header, value = line.split(':', 1)            ret[header.strip()] = value.strip()        return ret",
        "labels_text": "Render additionalheaders and return a dict of Header Value pair"
    },
    {
        "input_text": "summarize: def render_body(self, context):                if self.body_template:            return render_jinja2(self.body_template, context)        else:            return json.dumps(context, cls=JSONEncoder)",
        "labels_text": "Render the body template if defined Otherwise jump the context a a JSON object"
    },
    {
        "input_text": "summarize: def render_payload_url(self, context):                return render_jinja2(self.payload_url, context)",
        "labels_text": "Render the payload URL"
    },
    {
        "input_text": "summarize: def sync_data(self):                self.template_code = self.data_file.data_as_string",
        "labels_text": "Synchronize template content from the designated DataFile if any"
    },
    {
        "input_text": "summarize: def render(self, queryset):                context = {            'queryset': queryset        }        output = render_jinja2(self.template_code, context)        # Replace CRLF-style line terminators        output = output.replace('\\r\\n', '\\n')        return output",
        "labels_text": "Render the content of the template"
    },
    {
        "input_text": "summarize: def size(self):                expected_exceptions = [OSError]        try:            from botocore.exceptions import ClientError            expected_exceptions.append(ClientError)        except ImportError:            pass        try:            return self.image.size        except tuple(expected_exceptions):            return None",
        "labels_text": "Wrapper around imagesize to suppress an OSError in case the file is inaccessible Also opportunistically catch other exception that we know other storage backends to throw"
    },
    {
        "input_text": "summarize: def get_event_type_choices():        return [        (name, event.text)        for name, event in registry['event_types'].items()    ]",
        "labels_text": "Compile a list of choice from all registered event type"
    },
    {
        "input_text": "summarize: def event(self):                return registry['event_types'].get(self.event_type)",
        "labels_text": "Returns the registered Event which triggered this Notification"
    },
    {
        "input_text": "summarize: def members(self):                return self.users.union(            User.objects.filter(groups__in=self.groups.all())        ).order_by('username')",
        "labels_text": "Return all Users who belong to this notification group"
    },
    {
        "input_text": "summarize: def notify(self, **kwargs):                Notification.objects.bulk_create([            Notification(user=member, **kwargs)            for member in self.members        ])",
        "labels_text": "Bulkcreate Notifications for all member of this group"
    },
    {
        "input_text": "summarize: def display_attrs(self):                indexer = get_indexer(self.object_type)        attrs = {}        for attr in indexer.display_attrs:            name = self.object._meta.get_field(attr).verbose_name            if value := getattr(self.object, attr):                if display_func := getattr(self.object, f'get_{attr}_display', None):                    attrs[name] = display_func()                else:                    attrs[name] = value        return attrs",
        "labels_text": "Render any display attribute associated with this search result"
    },
    {
        "input_text": "summarize: def log_level(level):        return {        'name': dict(LogLevelChoices)[level],        'class': LogLevelChoices.colors.get(level)    }",
        "labels_text": "Display a label indicating a syslog severity eg info warning etc"
    },
    {
        "input_text": "summarize: def test_invalid_choice_items(self):                self.add_permissions('extras.add_customfieldchoiceset')        data = {            \"name\": \"test\",            \"extra_choices\": [                [\"choice1\"],                [\"choice2\"],                [\"choice3\"],            ]        }        response = self.client.post(self._get_list_url(), data, format='json', **self.header)        self.assertEqual(response.status_code, 400)",
        "labels_text": "Attempting to define each choice a a singleitem list should return a error"
    },
    {
        "input_text": "summarize: def test_invalid_name(self):                with self.assertRaises(ValidationError):            # Invalid character            CustomField(name='?', type=CustomFieldTypeChoices.TYPE_TEXT).full_clean()        with self.assertRaises(ValidationError):            # Double underscores not permitted            CustomField(name='foo__bar', type=CustomFieldTypeChoices.TYPE_TEXT).full_clean()",
        "labels_text": "Try creating a CustomField with an invalid name"
    },
    {
        "input_text": "summarize: def test_import_missing_required(self):                # Set one of our CustomFields to required        CustomField.objects.filter(name='text').update(required=True)        form_data = {            'name': 'Site 1',            'slug': 'site-1',        }        form = SiteImportForm(data=form_data)        self.assertFalse(form.is_valid())        self.assertIn('cf_text', form.errors)",
        "labels_text": "Attempt to import an object missing a required custom field"
    },
    {
        "input_text": "summarize: def test_import_invalid_choice(self):                form_data = {            'name': 'Site 1',            'slug': 'site-1',            'cf_select': 'Choice X'        }        form = SiteImportForm(data=form_data)        self.assertFalse(form.is_valid())        self.assertIn('cf_select', form.errors)",
        "labels_text": "Attempt to import an object with an invalid choice selection"
    },
    {
        "input_text": "summarize: def test_cf_data(self):                site = Site(name='Test Site', slug='test-site')        # Check custom field data on new instance        site.custom_field_data['foo'] = 'abc'        self.assertEqual(site.cf['foo'], 'abc')        # Check custom field data from database        site.save()        site = Site.objects.get(name='Test Site')        self.assertEqual(site.cf['foo'], 'abc')",
        "labels_text": "Check that custom field data is present on the instance immediately after being set and after being fetched from the database"
    },
    {
        "input_text": "summarize: def test_invalid_data(self):                site = Site(name='Test Site', slug='test-site')        # Set custom field data        site.custom_field_data['foo'] = 'abc'        site.custom_field_data['bar'] = 'def'        with self.assertRaises(ValidationError):            site.clean()        del site.custom_field_data['bar']        site.clean()",
        "labels_text": "Setting custom field data for a nonapplicable or nonexistent CustomField should raise a ValidationError"
    },
    {
        "input_text": "summarize: def test_plain_data(self):                with self.assertRaises(ValidationError):            Site(name='abcd', slug='abcd').clean()        Site(name='abcde', slug='abcde').clean()",
        "labels_text": "Test custom validator configuration using plain data a opposed to a CustomValidator class"
    },
    {
        "input_text": "summarize: def test_dotted_path(self):                Site(name='foo', slug='foo').clean()        with self.assertRaises(ValidationError):            Site(name='bar', slug='bar').clean()",
        "labels_text": "Test custom validator configuration using a dotted path string reference to a CustomValidator class"
    },
    {
        "input_text": "summarize: def test_plain_data(self):                # Create a site with a protected status        site = Site(name='Site 1', slug='site-1', status=SiteStatusChoices.STATUS_ACTIVE)        site.save()        # Try to delete it        with self.assertRaises(AbortRequest):            with transaction.atomic():                site.delete()        # Change its status to an allowed value        site.status = SiteStatusChoices.STATUS_DECOMMISSIONING        site.save()        # Deletion should now succeed        site.delete()",
        "labels_text": "Test custom validator configuration using plain data a opposed to a CustomValidator class"
    },
    {
        "input_text": "summarize: def test_dotted_path(self):                # Create a site with a protected name        site = Site(name='bar', slug='bar')        site.save()        # Try to delete it        with self.assertRaises(AbortRequest):            with transaction.atomic():                site.delete()        # Change the name to an allowed value        site.name = site.slug = 'foo'        site.save()        # Deletion should now succeed        site.delete()",
        "labels_text": "Test custom validator configuration using a dotted path string reference to a CustomValidator class"
    },
    {
        "input_text": "summarize: def test_tags_validation(self):                data = {            'name': 'Provider 1',            'slug': 'provider-1',        }        form = ProviderForm(data)        self.assertFalse(form.is_valid())        tags = create_tags('Tag1', 'Tag2', 'Tag3')        data['tags'] = [tag.pk for tag in tags]        form = ProviderForm(data)        self.assertTrue(form.is_valid())",
        "labels_text": "Check that custom validation rule work for tag assignment"
    },
    {
        "input_text": "summarize: def test_tags_validation(self):                data = {            'name': 'Provider 1',            'slug': 'provider-1',        }        serializer = ProviderSerializer(data=data)        self.assertFalse(serializer.is_valid())        tags = create_tags('Tag1', 'Tag2', 'Tag3')        data['tags'] = [tag.pk for tag in tags]        serializer = ProviderSerializer(data=data)        self.assertTrue(serializer.is_valid())",
        "labels_text": "Check that custom validation rule work for tag assignment"
    },
    {
        "input_text": "summarize: def test_empty_values(self):                form = SiteForm({            'name': 'Site 1',            'slug': 'site-1',            'status': 'active',        })        self.assertTrue(form.is_valid())        instance = form.save()        for field_type, _ in CustomFieldTypeChoices.CHOICES:            self.assertIn(field_type, instance.custom_field_data)            self.assertIsNone(instance.custom_field_data[field_type])",
        "labels_text": "Test that empty custom field value are stored a null"
    },
    {
        "input_text": "summarize: def test_basic_submit(self):                form = SavedFilterForm({            'name': 'test-sf',            'slug': 'test-sf',            'object_types': [ObjectType.objects.get_for_model(Site).pk],            'weight': 100,            'parameters': {                \"status\": [                    \"active\"                ]            }        })        self.assertTrue(form.is_valid())        form.save()",
        "labels_text": "Test form submission and validation"
    },
    {
        "input_text": "summarize: def parse_inet_addresses(self, value):                parsed = []        for addr in value:            if netaddr.valid_ipv4(addr) or netaddr.valid_ipv6(addr):                parsed.append(addr)                continue            try:                network = netaddr.IPNetwork(addr)                parsed.append(str(network))            except (AddrFormatError, ValueError):                continue        return parsed",
        "labels_text": "Parse network or IP address and cast to a format acceptable by the Postgres inet type Skips invalid value"
    },
    {
        "input_text": "summarize: def filter_contains_vid(self, queryset, name, value):                table_name = VLANGroup._meta.db_table        # TODO: See if this can be optimized without compromising queryset integrity        # Expand VLAN ID ranges to query by integer        groups = VLANGroup.objects.raw(            f'SELECT id FROM {table_name}, unnest(vid_ranges) vid_range WHERE %s <@ vid_range',            params=(value,)        )        return queryset.filter(            pk__in=[g.id for g in groups]        )",
        "labels_text": "Return all VLANGroups which contain the given VLAN ID"
    },
    {
        "input_text": "summarize: def get_queryset(self):                return super().get_queryset().order_by(Inet(Host('address')))",
        "labels_text": "By default PostgreSQL will order INETs with shorter larger prefix length ahead of those with longer smaller mask This make no sense when ordering IPs which should be ordered solely by family and host address We can use HOST to extract just the host portion of the address ignoring it mask but we must then recast this value to INET so that record will be ordered properly We are essentially recasting each IP address a a or"
    },
    {
        "input_text": "summarize: def update_parents_children(prefix):        parents = prefix.get_parents(include_self=True).annotate_hierarchy()    for parent in parents:        parent._children = parent.hierarchy_children    Prefix.objects.bulk_update(parents, ['_children'], batch_size=100)",
        "labels_text": "Update depth on prefix containing prefix"
    },
    {
        "input_text": "summarize: def update_children_depth(prefix):        children = prefix.get_children(include_self=True).annotate_hierarchy()    for child in children:        child._depth = child.hierarchy_depth    Prefix.objects.bulk_update(children, ['_depth'], batch_size=100)",
        "labels_text": "Update child count on prefix contained prefix"
    },
    {
        "input_text": "summarize: def clear_primary_ip(instance, **kwargs):        field_name = f'primary_ip{instance.family}'    if device := Device.objects.filter(**{field_name: instance}).first():        device.snapshot()        setattr(device, field_name, None)        device.save()    if virtualmachine := VirtualMachine.objects.filter(**{field_name: instance}).first():        virtualmachine.snapshot()        setattr(virtualmachine, field_name, None)        virtualmachine.save()",
        "labels_text": "When an IPAddress is deleted trigger save on any DevicesVirtualMachines for which it wa a primary IP"
    },
    {
        "input_text": "summarize: def clear_oob_ip(instance, **kwargs):        if device := Device.objects.filter(oob_ip=instance).first():        device.snapshot()        device.oob_ip = None        device.save()",
        "labels_text": "When an IPAddress is deleted trigger save on any Devices for which it wa a OOB IP"
    },
    {
        "input_text": "summarize: def add_available_vlans(vlans, vlan_group):        new_vlans = []    for vid_range in vlan_group.vid_ranges:        new_vlans.extend(available_vlans_from_range(vlans, vlan_group, vid_range))    vlans = list(vlans) + new_vlans    vlans.sort(key=lambda v: v.vid if type(v) is VLAN else v['vid'])    return vlans",
        "labels_text": "Create fake record for all gap between used VLANs"
    },
    {
        "input_text": "summarize: def get_next_available_prefix(ipset, prefix_size):        for available_prefix in ipset.iter_cidrs():        if prefix_size >= available_prefix.prefixlen:            allocated_prefix = f\"{available_prefix.network}/{prefix_size}\"            ipset.remove(allocated_prefix)            return allocated_prefix    return None",
        "labels_text": "Given a prefix length allocate the next available prefix from an IPSet"
    },
    {
        "input_text": "summarize: def get_results_limit(request):        config = get_config()    try:        limit = int(request.query_params.get('limit', config.PAGINATE_COUNT)) or config.MAX_PAGE_SIZE    except ValueError:        limit = config.PAGINATE_COUNT    if config.MAX_PAGE_SIZE:        limit = min(limit, config.MAX_PAGE_SIZE)    return limit",
        "labels_text": "Return the lesser of the specified limit if any and the configured MAXPAGESIZE"
    },
    {
        "input_text": "summarize: def get_parent(self, request, pk):                raise NotImplemented()",
        "labels_text": "Return the parent object"
    },
    {
        "input_text": "summarize: def get_available_objects(self, parent, limit=None):                raise NotImplemented()",
        "labels_text": "Return all available object for the parent"
    },
    {
        "input_text": "summarize: def get_extra_context(self, parent):                return {}",
        "labels_text": "Return any extra context data for the serializer"
    },
    {
        "input_text": "summarize: def check_sufficient_available(self, requested_objects, available_objects):                return len(requested_objects) <= len(available_objects)",
        "labels_text": "Check if there exist a sufficient number of available object to satisfy the request"
    },
    {
        "input_text": "summarize: def prep_object_data(self, requested_objects, available_objects, parent):                return requested_objects",
        "labels_text": "Prepare data by setting any programmatically determined object attribute eg next available VLAN ID on the request data"
    },
    {
        "input_text": "summarize: def set_vid_ranges(apps, schema_editor):        VLANGroup = apps.get_model('ipam', 'VLANGroup')    for group in VLANGroup.objects.all():        group.vid_ranges = [            NumericRange(group.min_vid, group.max_vid, bounds='[]')        ]        group._total_vlan_ids = group.max_vid - group.min_vid + 1        group.save()",
        "labels_text": "Convert the minvid maxvid field to a range in the new vidranges ArrayField"
    },
    {
        "input_text": "summarize: def get_available_asns(self):                range = set(self.range)        existing_asns = set(self.get_child_asns().values_list('asn', flat=True))        available_asns = sorted(range - existing_asns)        return available_asns",
        "labels_text": "Return all available ASNs within this range"
    },
    {
        "input_text": "summarize: def asn_asdot(self):                if self.asn > 65535:            return f'{self.asn // 65536}.{self.asn % 65536}'        return self.asn",
        "labels_text": "Return ASDOT notation for AS number greater than bit"
    },
    {
        "input_text": "summarize: def asn_with_asdot(self):                if self.asn > 65535:            return f'{self.asn} ({self.asn // 65536}.{self.asn % 65536})'        else:            return self.asn",
        "labels_text": "Return both plain and ASDOT notation where applicable"
    },
    {
        "input_text": "summarize: def get_available_prefixes(self):                params = {            'prefix__net_contained': str(self.prefix)        }        if hasattr(self, 'vrf'):            params['vrf'] = self.vrf        child_prefixes = Prefix.objects.filter(**params).values_list('prefix', flat=True)        return netaddr.IPSet(self.prefix) - netaddr.IPSet(child_prefixes)",
        "labels_text": "Return all available prefix within this Aggregate or Prefix a an IPSet"
    },
    {
        "input_text": "summarize: def get_first_available_prefix(self):                available_prefixes = self.get_available_prefixes()        if not available_prefixes:            return None        return available_prefixes.iter_cidrs()[0]",
        "labels_text": "Return the first available child prefix within the prefix or None"
    },
    {
        "input_text": "summarize: def get_child_prefixes(self):                return Prefix.objects.filter(prefix__net_contained=str(self.prefix))",
        "labels_text": "Return all Prefixes within this Aggregate"
    },
    {
        "input_text": "summarize: def get_utilization(self):                queryset = Prefix.objects.filter(prefix__net_contained_or_equal=str(self.prefix))        child_prefixes = netaddr.IPSet([p.prefix for p in queryset])        utilization = float(child_prefixes.size) / self.prefix.size * 100        return min(utilization, 100)",
        "labels_text": "Determine the prefix utilization of the aggregate and return it a a percentage"
    },
    {
        "input_text": "summarize: def _set_prefix_length(self, value):                if self.prefix is not None:            self.prefix.prefixlen = value",
        "labels_text": "Expose the IPNetwork object prefixlen attribute on the parent model so that it can be manipulated directly eg for bulk editing"
    },
    {
        "input_text": "summarize: def get_parents(self, include_self=False):                lookup = 'net_contains_or_equals' if include_self else 'net_contains'        return Prefix.objects.filter(**{            'vrf': self.vrf,            f'prefix__{lookup}': self.prefix        })",
        "labels_text": "Return all containing Prefixes in the hierarchy"
    },
    {
        "input_text": "summarize: def get_children(self, include_self=False):                lookup = 'net_contained_or_equal' if include_self else 'net_contained'        return Prefix.objects.filter(**{            'vrf': self.vrf,            f'prefix__{lookup}': self.prefix        })",
        "labels_text": "Return all covered Prefixes in the hierarchy"
    },
    {
        "input_text": "summarize: def get_child_prefixes(self):                if self.vrf is None and self.status == PrefixStatusChoices.STATUS_CONTAINER:            return Prefix.objects.filter(prefix__net_contained=str(self.prefix))        else:            return Prefix.objects.filter(prefix__net_contained=str(self.prefix), vrf=self.vrf)",
        "labels_text": "Return all Prefixes within this Prefix and VRF If this Prefix is a container in the global table return child Prefixes belonging to any VRF"
    },
    {
        "input_text": "summarize: def get_child_ranges(self):                return IPRange.objects.filter(            vrf=self.vrf,            start_address__net_host_contained=str(self.prefix),            end_address__net_host_contained=str(self.prefix)        )",
        "labels_text": "Return all IPRanges within this Prefix and VRF"
    },
    {
        "input_text": "summarize: def get_child_ips(self):                if self.vrf is None and self.status == PrefixStatusChoices.STATUS_CONTAINER:            return IPAddress.objects.filter(address__net_host_contained=str(self.prefix))        else:            return IPAddress.objects.filter(address__net_host_contained=str(self.prefix), vrf=self.vrf)",
        "labels_text": "Return all IPAddresses within this Prefix and VRF If this Prefix is a container in the global table return child IPAddresses belonging to any VRF"
    },
    {
        "input_text": "summarize: def get_first_available_ip(self):                available_ips = self.get_available_ips()        if not available_ips:            return None        return '{}/{}'.format(next(available_ips.__iter__()), self.prefix.prefixlen)",
        "labels_text": "Return the first available IP within the prefix or None"
    },
    {
        "input_text": "summarize: def _set_prefix_length(self, value):                self.start_address.prefixlen = value        self.end_address.prefixlen = value",
        "labels_text": "Expose the IPRange object prefixlen attribute on the parent model so that it can be manipulated directly eg for bulk editing"
    },
    {
        "input_text": "summarize: def get_child_ips(self):                return IPAddress.objects.filter(            address__gte=self.start_address,            address__lte=self.end_address,            vrf=self.vrf        )",
        "labels_text": "Return all IPAddresses within this IPRange and VRF"
    },
    {
        "input_text": "summarize: def get_available_ips(self):                range = netaddr.IPRange(self.start_address.ip, self.end_address.ip)        child_ips = netaddr.IPSet([ip.address.ip for ip in self.get_child_ips()])        return netaddr.IPSet(range) - child_ips",
        "labels_text": "Return all available IPs within this range a an IPSet"
    },
    {
        "input_text": "summarize: def first_available_ip(self):                available_ips = self.get_available_ips()        if not available_ips:            return None        return '{}/{}'.format(next(available_ips.__iter__()), self.start_address.prefixlen)",
        "labels_text": "Return the first available IP within the range or None"
    },
    {
        "input_text": "summarize: def utilization(self):                if self.mark_utilized:            return 100        # Compile an IPSet to avoid counting duplicate IPs        child_count = netaddr.IPSet([            ip.address.ip for ip in self.get_child_ips()        ]).size        return min(float(child_count) / self.size * 100, 100)",
        "labels_text": "Determine the utilization of the range and return it a a percentage"
    },
    {
        "input_text": "summarize: def get_related_ips(self):                return IPAddress.objects.exclude(address=str(self.address)).filter(            vrf=self.vrf, address__net_contained_or_equal=str(self.address)        )",
        "labels_text": "Return all IPAddresses belonging to the same VRF"
    },
    {
        "input_text": "summarize: def _set_mask_length(self, value):                if self.address is not None:            self.address.prefixlen = value",
        "labels_text": "Expose the IPNetwork object prefixlen attribute on the parent model so that it can be manipulated directly eg for bulk editing"
    },
    {
        "input_text": "summarize: def get_available_vids(self):                available_vlans = set()        for vlan_range in self.vid_ranges:            available_vlans = available_vlans.union({                vid for vid in range(vlan_range.lower, vlan_range.upper)            })        available_vlans -= set(VLAN.objects.filter(group=self).values_list('vid', flat=True))        return sorted(available_vlans)",
        "labels_text": "Return all available VLANs within this group"
    },
    {
        "input_text": "summarize: def get_next_available_vid(self):                available_vids = self.get_available_vids()        if available_vids:            return available_vids[0]        return None",
        "labels_text": "Return the first available VLAN ID in the group"
    },
    {
        "input_text": "summarize: def get_child_vlans(self):                return VLAN.objects.filter(group=self).order_by('vid')",
        "labels_text": "Return all VLANs within this group"
    },
    {
        "input_text": "summarize: def setUpTestData(cls):                vrfs = (            VRF(name='VRF 1'),            VRF(name='VRF 2'),            VRF(name='VRF 3'),        )        VRF.objects.bulk_create(vrfs)",
        "labels_text": "Setup the VRFs for the class a a whole"
    },
    {
        "input_text": "summarize: def _compare(self, queryset, objectset):                for i, obj in enumerate(queryset):            self.assertEqual(obj, objectset[i])",
        "labels_text": "Perform the comparison of the queryset object and the object used to instantiate the queryset"
    },
    {
        "input_text": "summarize: def _compare_ne(self, queryset, objectset):                for i, obj in enumerate(queryset):            self.assertNotEqual(obj, objectset[i])",
        "labels_text": "Perform the comparison of the queryset object and the object used to instantiate the queryset"
    },
    {
        "input_text": "summarize: def event_tracking(request):        current_request.set(request)    events_queue.set({})    yield    # Flush queued webhooks to RQ    if events := list(events_queue.get().values()):        flush_events(events)    # Clear context vars    current_request.set(None)    events_queue.set({})",
        "labels_text": "Queue interesting event in memory while processing a request then flush that queue for processing by the event pipline before returning the response param request WSGIRequest object with a unique id set"
    },
    {
        "input_text": "summarize: def config(request):        return {        'config': get_config(),    }",
        "labels_text": "Adds NetBox configuration parameter to the template context Example configBANNERLOGIN"
    },
    {
        "input_text": "summarize: def preferences(request):        user_preferences = request.user.config if request.user.is_authenticated else {}    return {        'preferences': user_preferences,        'htmx_navigation': user_preferences.get('ui.htmx_navigation', False) == 'true'    }",
        "labels_text": "Adds preference for the current user if authenticated to the template context Example preferencesgetkeypaginationplacement"
    },
    {
        "input_text": "summarize: def registry(request):        return {        'registry': registry_,    }",
        "labels_text": "Adds NetBox registry item to the template context Example registrymodelscore"
    },
    {
        "input_text": "summarize: def settings(request):        return {        'settings': django_settings,    }",
        "labels_text": "Adds Django setting to the template context Example settingsDEBUG"
    },
    {
        "input_text": "summarize: def init_config(self):                return",
        "labels_text": "A hook to initialize the instance configuration The data returned by this method is assigned to the instance config attribute upon initialization which can be referenced by the fetch method"
    },
    {
        "input_text": "summarize: def fetch(self):                raise NotImplemented()",
        "labels_text": "A context manager which performs the following Handles all setup and synchronization Yields the local path at which data ha been replicated Performs any necessary cleanup"
    },
    {
        "input_text": "summarize: def register(model, field_name, mappings):        logger.debug(f'Registering denormalized field {model}.{field_name}')    field = model._meta.get_field(field_name)    rel_model = field.related_model    registry['denormalized_fields'][rel_model].append(        (model, field_name, mappings)    )",
        "labels_text": "Register a denormalized model field to ensure that it is kept uptodate with the related object Args model The class being updated fieldname The name of the field related to the triggering instance mapping Dictionary mapping of local to remote field"
    },
    {
        "input_text": "summarize: def get_filters(cls):                filters = super().get_filters()        additional_filters = {}        for existing_filter_name, existing_filter in filters.items():            additional_filters.update(cls.get_additional_lookups(existing_filter_name, existing_filter))        filters.update(additional_filters)        return filters",
        "labels_text": "Override filter generation to support dynamic lookup expression for certain filter type For specific filter type new filter are created based on defined lookup expression in the form fieldnamelookupexpr"
    },
    {
        "input_text": "summarize: def search(self, queryset, name, value):                return queryset",
        "labels_text": "Override this method to apply a generalpurpose search logic"
    },
    {
        "input_text": "summarize: def __init__(self, job):                self.job = job",
        "labels_text": "Args job The specific Job this JobRunner is executing"
    },
    {
        "input_text": "summarize: def run(self, *args, **kwargs):                pass",
        "labels_text": "Run the job A JobRunner class need to implement this method to execute all command of the job"
    },
    {
        "input_text": "summarize: def get_jobs(cls, instance=None):                jobs = Job.objects.filter(name=cls.name)        if instance:            object_type = ObjectType.objects.get_for_model(instance, for_concrete_model=False)            jobs = jobs.filter(                object_type=object_type,                object_id=instance.pk,            )        return jobs",
        "labels_text": "Get all job of this JobRunner related to a specific instance"
    },
    {
        "input_text": "summarize: def enqueue(cls, *args, **kwargs):                return Job.enqueue(cls.handle, name=cls.name, *args, **kwargs)",
        "labels_text": "Enqueue a new Job This method is a wrapper of Jobenqueue using handle a function callback See it documentation for parameter"
    },
    {
        "input_text": "summarize: def _set_session_type(allow_write):                with connection.cursor() as cursor:            mode = 'READ WRITE' if allow_write else 'READ ONLY'            cursor.execute(f'SET SESSION CHARACTERISTICS AS TRANSACTION {mode};')",
        "labels_text": "Prevent any writerelated database operation Args allowwrite bool If True write operation will be permitted"
    },
    {
        "input_text": "summarize: def process_exception(self, request, exception):                if get_config().MAINTENANCE_MODE and isinstance(exception, InternalError):            error_message = 'NetBox is currently operating in maintenance mode and is unable to perform write ' \\                            'operations. Please try again later.'            if is_api_request(request):                return handle_rest_api_exception(request, error=error_message)            messages.error(request, error_message)            return HttpResponseRedirect(request.path_info)",
        "labels_text": "Prevent any writerelated database operation if an exception is raised"
    },
    {
        "input_text": "summarize: def pre_delete_handler(self, sender, instance, **kwargs):                key = self.get_key_for_instance(instance)        object_type = instance._meta.verbose_name        # Delete an existing object        logger.debug(f\"[{self.branch}] Staging deletion of {object_type} {instance} (PK: {instance.pk})\")        self.queue[key] = (ChangeActionChoices.ACTION_DELETE, None)",
        "labels_text": "Hooks to the predelete signal when a branch is active to queue delete action"
    },
    {
        "input_text": "summarize: def register_data_backend():        def _wrapper(cls):        registry['data_backends'][cls.name] = cls        return cls    return _wrapper",
        "labels_text": "Decorator for registering a DataBackend class"
    },
    {
        "input_text": "summarize: def sync(self, request, pk):                permission = get_permission_for_model(self.queryset.model, 'sync')        if not request.user.has_perm(permission):            raise PermissionDenied(f\"Missing permission: {permission}\")        obj = get_object_or_404(self.queryset, pk=pk)        if obj.data_file:            obj.sync(save=True)        serializer = self.serializer_class(obj, context={'request': request})        return Response(serializer.data)",
        "labels_text": "Provide a sync API endpoint to synchronize an object data from it associated DataFile if any"
    },
    {
        "input_text": "summarize: def get_api_root_view(self, api_urls=None):                api_root_dict = {}        list_name = self.routes[0].name        for prefix, viewset, basename in sorted(self.registry, key=lambda x: x[0]):            api_root_dict[prefix] = list_name.format(basename=basename)        return self.APIRootView.as_view(api_root_dict=api_root_dict)",
        "labels_text": "Wrap DRFs DefaultRouter to return an alphabetized list of endpoint"
    },
    {
        "input_text": "summarize: def fields(self):                if not self._requested_fields:            return super().fields        fields = BindingDict(self)        for key, value in self.get_fields().items():            if key in self._requested_fields:                fields[key] = value        return fields",
        "labels_text": "Override the field property to check for requested field If defined return only the applicable field"
    },
    {
        "input_text": "summarize: def _validate_objects(self, instance):                if type(instance) is list:            # Check that all instances are still included in the view's queryset            conforming_count = self.queryset.filter(pk__in=[obj.pk for obj in instance]).count()            if conforming_count != len(instance):                raise ObjectDoesNotExist        elif not self.queryset.filter(pk=instance.pk).exists():            raise ObjectDoesNotExist",
        "labels_text": "Check that the provided instance or list of instance are matched by the current queryset This confirms that any newly created or modified object abide by the attribute granted by any applicable ObjectPermissions"
    },
    {
        "input_text": "summarize: def get_object_with_snapshot(self):                obj = super().get_object()        if hasattr(obj, 'snapshot'):            obj.snapshot()        return obj",
        "labels_text": "Save a prechange snapshot of the object immediately after retrieving it This snapshot will be used to record the before data in the changelog"
    },
    {
        "input_text": "summarize: def get_auth_backend_display(name):        return AUTH_BACKEND_ATTRS.get(name, (name, None))",
        "labels_text": "Return the userfriendly name and icon name for a remote authentication backend if known Obtained from the default dictionary AUTHBACKENDATTRS overridden by the setting SOCIALAUTHBACKENDATTRS Defaults to the raw backend name and no icon"
    },
    {
        "input_text": "summarize: def get_config():        if not hasattr(_thread_locals, 'config'):        _thread_locals.config = Config()        logger.debug(\"Initialized configuration\")    return _thread_locals.config",
        "labels_text": "Return the current NetBox configuration pulling it from cache if not already loaded in memory"
    },
    {
        "input_text": "summarize: def clear_config():        if hasattr(_thread_locals, 'config'):        del _thread_locals.config        logger.debug(\"Cleared configuration\")",
        "labels_text": "Delete the currently loaded configuration if any"
    },
    {
        "input_text": "summarize: def _populate_from_cache(self):                self.config = cache.get('config') or {}        self.version = cache.get('config_version')        if self.config:            logger.debug(\"Loaded configuration data from cache\")",
        "labels_text": "Populate config data from Redis cache"
    },
    {
        "input_text": "summarize: def _post_clean(self):                self.instance._m2m_values = {}        for field in self.instance._meta.local_many_to_many:            if field.name in self.cleaned_data:                self.instance._m2m_values[field.name] = list(self.cleaned_data[field.name])        return super()._post_clean()",
        "labels_text": "Override BaseModelForms postclean to store manytomany field value on the model instance"
    },
    {
        "input_text": "summarize: def _get_content_type(self):                if not getattr(self, 'model', None):            raise NotImplementedError(_(\"{class_name} must specify a model class.\").format(                class_name=self.__class__.__name__            ))        return ObjectType.objects.get_for_model(self.model)",
        "labels_text": "Return the ObjectType of the form model"
    },
    {
        "input_text": "summarize: def serialize_object(self, exclude=None):                return serialize_object(self, exclude=exclude or [])",
        "labels_text": "Return a JSON representation of the instance Models can override this method to replace or extend the default serialization logic provided by the serializeobject utility function Args exclude An iterable of attribute name to omit from the serialized output"
    },
    {
        "input_text": "summarize: def snapshot(self):                exclude_fields = []        if get_config().CHANGELOG_SKIP_EMPTY_CHANGES:            exclude_fields = ['last_updated',]        self._prechange_snapshot = self.serialize_object(exclude=exclude_fields)",
        "labels_text": "Save a snapshot of the object current state in preparation for modification The snapshot is saved a prechangesnapshot on the instance"
    },
    {
        "input_text": "summarize: def cf(self):                return {            cf.name: cf.deserialize(self.custom_field_data.get(cf.name))            for cf in self.custom_fields        }",
        "labels_text": "Return a dictionary mapping each custom field for this instance to it deserialized value python tenant Tenantobjectsfirst tenantcf primarysite Site DMNYC custid DMI isactive True"
    },
    {
        "input_text": "summarize: def custom_fields(self):                from extras.models import CustomField        return CustomField.objects.get_for_model(self)",
        "labels_text": "Return the QuerySet of CustomFields assigned to this model python tenant Tenantobjectsfirst tenantcustomfields RestrictedQuerySet CustomField Primary site CustomField Customer ID CustomField Is active"
    },
    {
        "input_text": "summarize: def populate_custom_field_defaults(self):                for cf in self.custom_fields:            self.custom_field_data[cf.name] = cf.default",
        "labels_text": "Apply the default value for each custom field"
    },
    {
        "input_text": "summarize: def get_latest_jobs(self):                return {            job.name: job            for job in self.jobs.filter(                status__in=JobStatusChoices.TERMINAL_STATE_CHOICES            ).order_by('name', '-created').distinct('name').defer('data')        }",
        "labels_text": "Return a dictionary mapping of the most recent job for this instance"
    },
    {
        "input_text": "summarize: def resolve_data_file(self):                from core.models import DataFile        if self.data_source and self.data_path:            try:                return DataFile.objects.get(source=self.data_source, path=self.data_path)            except DataFile.DoesNotExist:                pass",
        "labels_text": "Determine the designated DataFile object identified by it parent DataSource and it path Returns None if either attribute is unset or if no matching DataFile is found"
    },
    {
        "input_text": "summarize: def sync(self, save=False):                self.sync_data()        self.data_synced = timezone.now()        if save:            self.save()",
        "labels_text": "Synchronize the object from it assigned DataFile if any This wrap syncdata and update the synceddata timestamp param save If true save will be called after data ha been synchronized"
    },
    {
        "input_text": "summarize: def sync_data(self):                raise NotImplementedError(_(\"{class_name} must implement a sync_data() method.\").format(            class_name=self.__class__        ))",
        "labels_text": "Inheriting model must override this method with specific logic to copy data from the assigned DataFile to the local instance This method should NOT call save on the instance"
    },
    {
        "input_text": "summarize: def register_graphql_schema(graphql_schema):        registry['plugins']['graphql_schemas'].extend(graphql_schema)",
        "labels_text": "Register a GraphQL schema class for inclusion in NetBoxs GraphQL API"
    },
    {
        "input_text": "summarize: def register_user_preferences(plugin_name, preferences):        registry['plugins']['preferences'][plugin_name] = preferences",
        "labels_text": "Register a list of user preference defined by a plugin"
    },
    {
        "input_text": "summarize: def render(self, template_name, extra_context=None):                if extra_context is None:            extra_context = {}        elif not isinstance(extra_context, dict):            raise TypeError(_(\"extra_context must be a dictionary\"))        return get_template(template_name).render({**self.context, **extra_context})",
        "labels_text": "Convenience method for rendering the specified Django template using the default context data An additional context dictionary may be passed a extracontext"
    },
    {
        "input_text": "summarize: def navbar(self):                raise NotImplementedError",
        "labels_text": "Content that will be rendered inside the top navigation menu Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def list_buttons(self):                raise NotImplementedError",
        "labels_text": "Buttons that will be rendered and added to the existing list of button on the list view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def buttons(self):                raise NotImplementedError",
        "labels_text": "Buttons that will be rendered and added to the existing list of button on the detail page view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def alerts(self):                raise NotImplementedError",
        "labels_text": "Arbitrary content to be inserted at the top of an object detail view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def left_page(self):                raise NotImplementedError",
        "labels_text": "Content that will be rendered on the left of the detail page view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def right_page(self):                raise NotImplementedError",
        "labels_text": "Content that will be rendered on the right of the detail page view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def full_width_page(self):                raise NotImplementedError",
        "labels_text": "Content that will be rendered within the full width of the detail page view Content should be returned a an HTML string Note that content doe not need to be marked a safe because this is automatically handled"
    },
    {
        "input_text": "summarize: def get_installed_plugins():        plugins = {}    for plugin_name in settings.PLUGINS:        plugin_name = plugin_name.rsplit('.', 1)[-1]        plugin_config = apps.get_app_config(plugin_name)        plugins[plugin_name] = getattr(plugin_config, 'version', None)    return dict(sorted(plugins.items()))",
        "labels_text": "Return a dictionary mapping the name of installed plugins to their version"
    },
    {
        "input_text": "summarize: def get_plugin_config(plugin_name, parameter, default=None):        try:        plugin_config = settings.PLUGINS_CONFIG[plugin_name]        return plugin_config.get(parameter, default)    except KeyError:        raise ImproperlyConfigured(f\"Plugin {plugin_name} is not registered.\")",
        "labels_text": "Return the value of the specified plugin configuration parameter Args pluginname The name of the plugin parameter The name of the configuration parameter default The value to return if the parameter is not defined default None"
    },
    {
        "input_text": "summarize: def search(self, value, user=None, object_types=None, lookup=DEFAULT_LOOKUP_TYPE):                raise NotImplementedError",
        "labels_text": "Search cached object representation for the given value"
    },
    {
        "input_text": "summarize: def caching_handler(self, sender, instance, created, **kwargs):                self.cache(instance, remove_existing=not created)",
        "labels_text": "Receiver for the postsave signal responsible for caching object creationchanges"
    },
    {
        "input_text": "summarize: def removal_handler(self, sender, instance, **kwargs):                self.remove(instance)",
        "labels_text": "Receiver for the postdelete signal responsible for caching object deletion"
    },
    {
        "input_text": "summarize: def cache(self, instances, indexer=None, remove_existing=True):                raise NotImplementedError",
        "labels_text": "Create or update the cached representation of an instance"
    },
    {
        "input_text": "summarize: def remove(self, instance):                raise NotImplementedError",
        "labels_text": "Delete any cached representation of an instance"
    },
    {
        "input_text": "summarize: def clear(self, object_types=None):                raise NotImplementedError",
        "labels_text": "Delete all cached data optionally filtered by object type"
    },
    {
        "input_text": "summarize: def count(self, object_types=None):                raise NotImplementedError",
        "labels_text": "Return a count of all cache entry optionally filtered by object type"
    },
    {
        "input_text": "summarize: def size(self):                return None",
        "labels_text": "Return a total number of cached entry The meaning of this value will be backenddependent"
    },
    {
        "input_text": "summarize: def get_backend():        try:        backend_cls = import_string(settings.SEARCH_BACKEND)    except AttributeError:        raise ImproperlyConfigured(f\"Failed to import configured SEARCH_BACKEND: {settings.SEARCH_BACKEND}\")    # Initialize and return the backend instance    return backend_cls()",
        "labels_text": "Initializes and return the configured search backend"
    },
    {
        "input_text": "summarize: def get_indexer(object_type):        identifier = object_type_identifier(object_type)    return registry['search'].get(identifier)",
        "labels_text": "Return the registered search indexer for the given ContentType"
    },
    {
        "input_text": "summarize: def get_field_type(instance, field_name):                field_cls = instance._meta.get_field(field_name).__class__        if issubclass(field_cls, (models.FloatField, models.DecimalField)):            return FieldTypes.FLOAT        if issubclass(field_cls, IPAddressField):            return FieldTypes.INET        if issubclass(field_cls, IPNetworkField):            return FieldTypes.CIDR        if issubclass(field_cls, models.IntegerField):            return FieldTypes.INTEGER        return FieldTypes.STRING",
        "labels_text": "Return the data type of the specified model field"
    },
    {
        "input_text": "summarize: def get_attr_type(instance, field_name):                value = getattr(instance, field_name)        if type(value) is str:            return FieldTypes.STRING        if type(value) is int:            return FieldTypes.INTEGER        if type(value) in (float, Decimal):            return FieldTypes.FLOAT        if type(value) is IPNetwork:            return FieldTypes.CIDR        if type(value) is IPAddress:            return FieldTypes.INET        return FieldTypes.STRING",
        "labels_text": "Return the data type of the specified object attribute"
    },
    {
        "input_text": "summarize: def get_field_value(instance, field_name):                if value := getattr(instance, field_name):            return str(value)",
        "labels_text": "Return the value of the specified model field a a string or None"
    },
    {
        "input_text": "summarize: def get_indexer(model):        label = f'{model._meta.app_label}.{model._meta.model_name}'    return registry['search'][label]",
        "labels_text": "Get the SearchIndex class for the given model"
    },
    {
        "input_text": "summarize: def register_search(cls):        model = cls.model    label = f'{model._meta.app_label}.{model._meta.model_name}'    registry['search'][label] = cls    return cls",
        "labels_text": "Decorator for registering a SearchIndex class"
    },
    {
        "input_text": "summarize: def __init__(self, export_raw=False, **kwargs):                super().__init__(**kwargs)        self.export_raw = export_raw",
        "labels_text": "Args exportraw If true data export return the raw field value rather than the rendered template Default False"
    },
    {
        "input_text": "summarize: def objects_count(self):                if not hasattr(self, '_objects_count'):            self._objects_count = sum(1 for obj in self.data if hasattr(obj, 'pk'))        return self._objects_count",
        "labels_text": "Return the total number of real object represented by the Table This is useful when dealing with prefixesIP addressesetc where some table row may represent available address space"
    },
    {
        "input_text": "summarize: def htmx_url(self):                if self.embedded:            viewname = get_viewname(self._meta.model, action='list')            try:                return reverse(viewname)            except NoReverseMatch:                pass        return ''",
        "labels_text": "Return the base HTML request URL for embedded table"
    },
    {
        "input_text": "summarize: def test_remote_auth_disabled(self):                headers = {            'HTTP_REMOTE_USER': 'remoteuser1',        }        self.assertFalse(settings.REMOTE_AUTH_ENABLED)        self.assertEqual(settings.REMOTE_AUTH_HEADER, 'HTTP_REMOTE_USER')        # Client should not be authenticated        response = self.client.get(reverse('home'), follow=True, **headers)        self.assertNotIn('_auth_user_id', self.client.session)",
        "labels_text": "Test enabling remote authentication with the default configuration"
    },
    {
        "input_text": "summarize: def setUp(self):                self.user = User.objects.create(username='testuser')        self.token = Token.objects.create(user=self.user)        self.header = {'HTTP_AUTHORIZATION': 'Token {}'.format(self.token.key)}",
        "labels_text": "Create a test user and token for API call"
    },
    {
        "input_text": "summarize: def test_graphql_enabled(self):                url = reverse('graphql')        response = self.client.get(url)        self.assertHttpStatus(response, 404)",
        "labels_text": "The graphql URL should return a when GRAPHQLENABLEDFalse"
    },
    {
        "input_text": "summarize: def test_graphiql_interface(self):                url = reverse('graphql')        header = {            'HTTP_ACCEPT': 'text/html',        }        # Authenticated request        response = self.client.get(url, **header)        self.assertHttpStatus(response, 200)        # Non-authenticated request        self.client.logout()        response = self.client.get(url, **header)        with disable_warnings('django.request'):            self.assertHttpStatus(response, 302)",
        "labels_text": "Test rendering of the GraphiQL interactive web interface"
    },
    {
        "input_text": "summarize: def test_menu(self):                menu = registry['plugins']['menus'][0]        self.assertIsInstance(menu, PluginMenu)        self.assertEqual(menu.label, 'Dummy Plugin')",
        "labels_text": "Check menu registration"
    },
    {
        "input_text": "summarize: def test_menu_items(self):                self.assertIn('Dummy plugin', registry['plugins']['menu_items'])        menu_items = registry['plugins']['menu_items']['Dummy plugin']        self.assertEqual(len(menu_items), 2)        self.assertEqual(len(menu_items[0].buttons), 2)",
        "labels_text": "Check menuitems registration"
    },
    {
        "input_text": "summarize: def test_template_extensions(self):                from netbox.tests.dummy_plugin.template_content import GlobalContent, SiteContent        self.assertIn(GlobalContent, registry['plugins']['template_extensions'][None])        self.assertIn(SiteContent, registry['plugins']['template_extensions']['dcim.site'])",
        "labels_text": "Check that plugin TemplateExtensions are registered"
    },
    {
        "input_text": "summarize: def test_registered_columns(self):                from dcim.models import Site        from dcim.tables import SiteTable        table = SiteTable(Site.objects.all())        self.assertIn('foo', table.columns.names())",
        "labels_text": "Check that a plugin can register a custom column on a core model table"
    },
    {
        "input_text": "summarize: def test_user_preferences(self):                self.assertIn('dummy_plugin', registry['plugins']['preferences'])        user_preferences = registry['plugins']['preferences']['dummy_plugin']        self.assertEqual(type(user_preferences), dict)        self.assertEqual(list(user_preferences.keys()), ['pref1', 'pref2'])",
        "labels_text": "Check that plugin UserPreferences are registered"
    },
    {
        "input_text": "summarize: def test_middleware(self):                self.assertIn('netbox.tests.dummy_plugin.middleware.DummyMiddleware', settings.MIDDLEWARE)",
        "labels_text": "Check that plugin middleware is registered"
    },
    {
        "input_text": "summarize: def test_data_backends(self):                self.assertIn('dummy', registry['data_backends'])        self.assertIs(registry['data_backends']['dummy'], DummyBackend)",
        "labels_text": "Check registered data backends"
    },
    {
        "input_text": "summarize: def test_queues(self):                self.assertIn('netbox.tests.dummy_plugin.testing-low', settings.RQ_QUEUES)        self.assertIn('netbox.tests.dummy_plugin.testing-medium', settings.RQ_QUEUES)        self.assertIn('netbox.tests.dummy_plugin.testing-high', settings.RQ_QUEUES)",
        "labels_text": "Check that plugin queue are registered with the accurate name"
    },
    {
        "input_text": "summarize: def test_min_version(self):                with self.assertRaises(ImproperlyConfigured):            dummy_config.validate({}, '0.9')",
        "labels_text": "Check enforcement of minimum NetBox version"
    },
    {
        "input_text": "summarize: def test_max_version(self):                with self.assertRaises(ImproperlyConfigured):            dummy_config.validate({}, '10.0')",
        "labels_text": "Check enforcement of maximum NetBox version"
    },
    {
        "input_text": "summarize: def test_required_settings(self):                class DummyConfigWithRequiredSettings(dummy_config):            required_settings = ['foo']        # Validation should pass when all required settings are present        DummyConfigWithRequiredSettings.validate({'foo': True}, settings.RELEASE.version)        # Validation should fail when a required setting is missing        with self.assertRaises(ImproperlyConfigured):            DummyConfigWithRequiredSettings.validate({}, settings.RELEASE.version)",
        "labels_text": "Validate enforcement of required setting"
    },
    {
        "input_text": "summarize: def test_graphql(self):                from netbox.tests.dummy_plugin.graphql import DummyQuery        self.assertIn(DummyQuery, registry['plugins']['graphql_schemas'])        self.assertTrue(issubclass(Query, DummyQuery))",
        "labels_text": "Validate the registration and operation of pluginprovided GraphQL schema"
    },
    {
        "input_text": "summarize: def test_get_plugin_config(self):                plugin = 'netbox.tests.dummy_plugin'        self.assertEqual(get_plugin_config(plugin, 'foo'), 123)        self.assertEqual(get_plugin_config(plugin, 'bar'), None)        self.assertEqual(get_plugin_config(plugin, 'bar', default=456), 456)",
        "labels_text": "Validate that getpluginconfig return config parameter correctly"
    },
    {
        "input_text": "summarize: def test_remove_on_delete(self):                site = Site.objects.first()        site.delete()        content_type = ContentType.objects.get_for_model(Site)        self.assertFalse(            CachedValue.objects.filter(object_type=content_type, object_id=site.pk).exists()        )",
        "labels_text": "Test that any cached value for an object are automatically removed on delete"
    },
    {
        "input_text": "summarize: def test_clear_all(self):                sites = Site.objects.all()        search_backend.cache(sites)        self.assertTrue(            CachedValue.objects.exists()        )        search_backend.clear()        self.assertFalse(            CachedValue.objects.exists()        )",
        "labels_text": "Test that calling clear on the backend remove all cached entry"
    },
    {
        "input_text": "summarize: def test_search(self):                sites = Site.objects.all()        search_backend.cache(sites)        results = search_backend.search('site')        self.assertEqual(len(results), 3)        results = search_backend.search('first')        self.assertEqual(len(results), 1)        results = search_backend.search('xxxxx')        self.assertEqual(len(results), 0)",
        "labels_text": "Test various search"
    },
    {
        "input_text": "summarize: def handler_404(request, exception):        if settings.SENTRY_ENABLED:        from sentry_sdk import capture_message        capture_message(\"Page not found\", level=\"error\")    return page_not_found(request, exception)",
        "labels_text": "Wrap Djangos default handler to enable Sentry reporting"
    },
    {
        "input_text": "summarize: def get_queryset(self, request):                if self.queryset is None:            raise ImproperlyConfigured(                f\"{self.__class__.__name__} does not define a queryset. Set queryset on the class or \"                f\"override its get_queryset() method.\"            )        return self.queryset.all()",
        "labels_text": "Return the base queryset for the view By default this return selfquerysetall Args request The current request"
    },
    {
        "input_text": "summarize: def get_object(self, **kwargs):                return get_object_or_404(self.queryset, **kwargs)",
        "labels_text": "Return the object being viewed or modified The object is identified by an arbitrary set of keyword argument gleaned from the URL which are passed to getobjector Typically only a primary key is needed If no matching object is found return a response"
    },
    {
        "input_text": "summarize: def get_extra_context(self, request, instance):                return {}",
        "labels_text": "Return any additional context data to include when rendering the template Args request The current request instance The object being viewed"
    },
    {
        "input_text": "summarize: def get_queryset(self, request):                if self.queryset is None:            raise ImproperlyConfigured(                f\"{self.__class__.__name__} does not define a queryset. Set queryset on the class or \"                f\"override its get_queryset() method.\"            )        return self.queryset.all()",
        "labels_text": "Return the base queryset for the view By default this return selfquerysetall Args request The current request"
    },
    {
        "input_text": "summarize: def get_extra_context(self, request):                return {}",
        "labels_text": "Return any additional context data to include when rendering the template Args request The current request"
    },
    {
        "input_text": "summarize: def export_yaml(self):                yaml_data = [obj.to_yaml() for obj in self.queryset]        return '---\\n'.join(yaml_data)",
        "labels_text": "Export the queryset of object a concatenated YAML document"
    },
    {
        "input_text": "summarize: def prep_related_object_data(self, parent, data):                return data",
        "labels_text": "Hook to modify the data for related object before it passed to the related object form for example to assign a parent object"
    },
    {
        "input_text": "summarize: def save_object(self, object_form, request):                return object_form.save()",
        "labels_text": "Provide a hook to modify the object immediately before saving it eg to encrypt secret data Args objectform The model form instance request The current request"
    },
    {
        "input_text": "summarize: def get_form(self):                class BulkDeleteForm(ConfirmationForm):            pk = ModelMultipleChoiceField(queryset=self.queryset, widget=MultipleHiddenInput)        return BulkDeleteForm",
        "labels_text": "Provide a standard bulk delete form if none ha been specified for the view"
    },
    {
        "input_text": "summarize: def get_permitted_actions(self, user, model=None):                model = model or self.queryset.model        # Resolve required permissions for each action        permitted_actions = []        for action in self.actions:            required_permissions = [                get_permission_for_model(model, name) for name in self.actions.get(action, set())            ]            if not required_permissions or user.has_perms(required_permissions):                permitted_actions.append(action)        return permitted_actions",
        "labels_text": "Return a tuple of action for which the given user is permitted to do"
    },
    {
        "input_text": "summarize: def get_table(self, data, request, bulk_actions=True):                table = self.table(data, user=request.user)        if 'pk' in table.base_columns and bulk_actions:            table.columns.show('pk')        table.configure(request)        return table",
        "labels_text": "Return the djangotables Table instance to be used for rendering the object list Args data Queryset or iterable containing table data request The current request bulkactions Render checkboxes for object selection"
    },
    {
        "input_text": "summarize: def get_template_name(self):                if self.template_name is not None:            return self.template_name        model_opts = self.queryset.model._meta        return f'{model_opts.app_label}/{model_opts.model_name}.html'",
        "labels_text": "Return selftemplatename if defined Otherwise dynamically resolve the template name using the queryset model applabel and modelname"
    },
    {
        "input_text": "summarize: def get(self, request, **kwargs):                instance = self.get_object(**kwargs)        return render(request, self.get_template_name(), {            'object': instance,            'tab': self.tab,            **self.get_extra_context(request, instance),        })",
        "labels_text": "GET request handler args and kwargs are passed to identify the object being queried Args request The current request"
    },
    {
        "input_text": "summarize: def get_children(self, request, parent):                raise NotImplementedError(_('{class_name} must implement get_children()').format(            class_name=self.__class__.__name__        ))",
        "labels_text": "Return a QuerySet of child object Args request The current request parent The parent object"
    },
    {
        "input_text": "summarize: def prep_table_data(self, request, queryset, parent):                return queryset",
        "labels_text": "Provides a hook for subclassed view to modify data before initializing the table Args request The current request queryset The filtered queryset of child object parent The parent object"
    },
    {
        "input_text": "summarize: def get_object(self, **kwargs):                if not kwargs:            # We're creating a new object            return self.queryset.model()        return super().get_object(**kwargs)",
        "labels_text": "Return an object for editing If no keyword argument have been specified this will be a new instance"
    },
    {
        "input_text": "summarize: def alter_object(self, obj, request, url_args, url_kwargs):                return obj",
        "labels_text": "Provides a hook for view to modify an object before it is processed For example a parent object can be defined given some parameter from the request URL Args obj The object being edited request The current request urlargs URL path args urlkwargs URL path kwargs"
    },
    {
        "input_text": "summarize: def get_extra_addanother_params(self, request):                return {}",
        "labels_text": "Return a dictionary of extra parameter to use on the Add Another button"
    },
    {
        "input_text": "summarize: def _handle_protected_objects(self, obj, protected_objects, request, exc):                handle_protectederror(protected_objects, request, exc)        if request.htmx:            return HttpResponse(headers={                'HX-Redirect': obj.get_absolute_url(),            })        else:            return redirect(obj.get_absolute_url())",
        "labels_text": "Handle a ProtectedError or RestrictedError exception raised while attempt to resolve dependent object"
    },
    {
        "input_text": "summarize: def get_prerequisite_model(queryset):        if not queryset.exists():        for prereq in getattr(queryset.model, 'prerequisite_models', []):            model = apps.get_model(prereq)            if not model.objects.exists():                return model",
        "labels_text": "Return any prerequisite model that must be created prior to creating an instance of the current model"
    },
    {
        "input_text": "summarize: def create_userconfig(instance, created, raw=False, **kwargs):        if created and not raw:        config = get_config()        UserConfig(user=instance, data=config.DEFAULT_USER_PREFERENCES).save()",
        "labels_text": "Automatically create a new UserConfig when a new User is created Skip this if importing a user from a fixture"
    },
    {
        "input_text": "summarize: def clean_username(value):        value = NO_ASCII_REGEX.sub('', value)    value = NO_SPECIAL_REGEX.sub('', value)    value = value.replace(':', '')    return value",
        "labels_text": "Clean username removing any unsupported character"
    },
    {
        "input_text": "summarize: def list(self, request):                userconfig = self.get_queryset().first()        return Response(userconfig.data)",
        "labels_text": "Return the UserConfig for the currently authenticated User"
    },
    {
        "input_text": "summarize: def patch(self, request):                # TODO: How can we validate this data?        userconfig = self.get_queryset().first()        userconfig.data = deepmerge(userconfig.data, request.data)        userconfig.save()        return Response(userconfig.data)",
        "labels_text": "Update the UserConfig for the currently authenticated User"
    },
    {
        "input_text": "summarize: def create(self, validated_data):                password = validated_data.pop('password')        user = super().create(validated_data)        user.set_password(password)        user.save()        return user",
        "labels_text": "Extract the password from validated data and set it separately to ensure proper hash generation"
    },
    {
        "input_text": "summarize: def update(self, instance, validated_data):                password = validated_data.pop('password', None)        if password is not None:            instance.set_password(password)        return super().update(instance, validated_data)",
        "labels_text": "Ensure proper updated password hash generation"
    },
    {
        "input_text": "summarize: def list_constraints(self):                if type(self.constraints) is not list:            return [self.constraints]        return self.constraints",
        "labels_text": "Return all constraint set a a list even if only a single set is defined"
    },
    {
        "input_text": "summarize: def all(self):                return flatten_dict(self.data)",
        "labels_text": "Return a dictionary of all defined key and their value"
    },
    {
        "input_text": "summarize: def clear(self, path, commit=False):                d = self.data        keys = path.split('.')        for key in keys[:-1]:            if key not in d:                break            if type(d[key]) is dict:                d = d[key]        key = keys[-1]        d.pop(key, None)  # Avoid a KeyError on invalid keys        if commit:            self.save()",
        "labels_text": "Delete a configuration parameter specified by it dotted path The key and any child key will be deleted Example userconfigclearfoobarbaz Invalid key will be ignored silently param path Dotted path to the configuration key For example foobar deletes selfdatafoobar param commit If true the UserConfig instance will be saved once the new value ha been applied"
    },
    {
        "input_text": "summarize: def validate_client_ip(self, client_ip):                if not self.allowed_ips:            return True        for ip_network in self.allowed_ips:            if client_ip in IPNetwork(ip_network):                return True        return False",
        "labels_text": "Validate the API client IP address against the source IP restriction if any set on the token"
    },
    {
        "input_text": "summarize: def test_bulk_update_objects(self):                return",
        "labels_text": "Disabled test Theres no attribute we can set in bulk for Groups"
    },
    {
        "input_text": "summarize: def test_provision_token_invalid(self):                data = {            'username': 'nonexistentuser',            'password': 'abc123',        }        url = reverse('users-api:token_provision')        response = self.client.post(url, data, format='json', **self.header)        self.assertEqual(response.status_code, 403)",
        "labels_text": "Test the behavior of the token provisioning view when invalid credential are supplied"
    },
    {
        "input_text": "summarize: def get_serializer_for_model(model, prefix=''):        app_label, model_name = model._meta.label.split('.')    serializer_name = f'{app_label}.api.serializers.{prefix}{model_name}Serializer'    try:        return import_string(serializer_name)    except ImportError:        raise SerializerNotFound(            f\"Could not determine serializer for {app_label}.{model_name} with prefix '{prefix}'\"        )",
        "labels_text": "Return the appropriate REST API serializer for the given model"
    },
    {
        "input_text": "summarize: def get_graphql_type_for_model(model):        app_label, model_name = model._meta.label.split('.')    class_name = f'{app_label}.graphql.types.{model_name}Type'    try:        return import_string(class_name)    except ImportError:        raise GraphQLTypeNotFound(f\"Could not find GraphQL type for {app_label}.{model_name}\")",
        "labels_text": "Return the GraphQL type class for the given model"
    },
    {
        "input_text": "summarize: def is_api_request(request):        api_path = reverse('api-root')    return request.path_info.startswith(api_path) and request.content_type == HTTP_CONTENT_TYPE_JSON",
        "labels_text": "Return True of the request is being made via the REST API"
    },
    {
        "input_text": "summarize: def get_view_name(view):        if hasattr(view, 'queryset'):        # Derive the model name from the queryset.        name = title(view.queryset.model._meta.verbose_name)        if suffix := getattr(view, 'suffix', None):            name = f'{name} {suffix}'        return name    # Fall back to DRF's default behavior    return drf_get_view_name(view)",
        "labels_text": "Derive the view name from it associated model if it ha one Fall back to DRFs builtin getviewname This function is provided to DRF a it VIEWNAMEFUNCTION"
    },
    {
        "input_text": "summarize: def unpack_grouped_choices(choices):        unpacked_choices = []    for key, value in choices:        if isinstance(value, (list, tuple)):            # Entered an optgroup            for optgroup_key, optgroup_value in value:                unpacked_choices.append((optgroup_key, optgroup_value))        else:            unpacked_choices.append((key, value))    return unpacked_choices",
        "labels_text": "Unpack a grouped choice hierarchy into a flat list of twotuples For example choice Foo A B Bar C D becomes choice A B C D"
    },
    {
        "input_text": "summarize: def get_counters_for_model(model):        return registry['counter_fields'][model].items()",
        "labels_text": "Return field mapping for all counter registered to the given model"
    },
    {
        "input_text": "summarize: def update_counter(model, pk, counter_name, value):        model.objects.filter(pk=pk).update(        **{counter_name: F(counter_name) + value}    )",
        "labels_text": "Increment or decrement a counter field on an object identified by it model and primary key PK Positive value will increment negative value will decrement"
    },
    {
        "input_text": "summarize: def update_counts(model, field_name, related_query):        subquery = Subquery(        model.objects.filter(pk=OuterRef('pk')).annotate(_count=Count(related_query)).values('_count')    )    return model.objects.update(**{        field_name: subquery    })",
        "labels_text": "Perform a bulk update for the given model and counter field For example updatecountsDevice interfacecount interface will effectively set DeviceobjectsupdateinterfacecountCountinterfaces"
    },
    {
        "input_text": "summarize: def post_delete_receiver(sender, instance, origin, **kwargs):        for field_name, counter_name in get_counters_for_model(sender):        parent_model = sender._meta.get_field(field_name).related_model        parent_pk = getattr(instance, field_name, None)        # Decrement the parent's counter by one        if parent_pk is not None and not hasattr(instance, \"_previously_removed\"):            update_counter(parent_model, parent_pk, counter_name, -1)",
        "labels_text": "Update counter field on related object when a TrackingModelMixin subclass is deleted"
    },
    {
        "input_text": "summarize: def deepmerge(original, new):        merged = dict(original)    for key, val in new.items():        if key in original and isinstance(original[key], dict) and val and isinstance(val, dict):            merged[key] = deepmerge(original[key], val)        else:            merged[key] = val    return merged",
        "labels_text": "Deep merge two dictionary new into original and return a new dict"
    },
    {
        "input_text": "summarize: def flatten_dict(d, prefix='', separator='.'):        ret = {}    for k, v in d.items():        key = separator.join([prefix, k]) if prefix else k        if type(v) is dict:            ret.update(flatten_dict(v, prefix=key, separator=separator))        else:            ret[key] = v    return ret",
        "labels_text": "Flatten nested dictionary into a single level by joining key name with a separator param d The dictionary to be flattened param prefix Initial prefix if any param separator The character to use when concatenating key name"
    },
    {
        "input_text": "summarize: def shallow_compare_dict(source_dict, destination_dict, exclude=tuple()):        difference = {}    for key, value in destination_dict.items():        if key in exclude:            continue        if source_dict.get(key) != value:            difference[key] = value    return difference",
        "labels_text": "Return a new dictionary of the different key The value of destinationdict are returned Only the equality of the first layer of keysvalues is checked exclude is a list or tuple of key to be ignored"
    },
    {
        "input_text": "summarize: def array_to_ranges(array):        group = (        list(x) for _, x in groupby(sorted(array), lambda x, c=count(): next(c) - x)    )    return [        (g[0], g[-1])[:len(g)] for g in group    ]",
        "labels_text": "Convert an arbitrary array of integer to a list of consecutive value Nonconsecutive value are returned a singleitem tuples For example"
    },
    {
        "input_text": "summarize: def array_to_string(array):        ret = []    ranges = array_to_ranges(array)    for value in ranges:        if len(value) == 1:            ret.append(str(value[0]))        else:            ret.append(f'{value[0]}-{value[1]}')    return ', '.join(ret)",
        "labels_text": "Generate an efficient humanfriendly string from a set of integer Intended for use with ArrayField For example"
    },
    {
        "input_text": "summarize: def drange(start, end, step=decimal.Decimal(1)):        start, end, step = decimal.Decimal(start), decimal.Decimal(end), decimal.Decimal(step)    if start < end:        while start < end:            yield start            start += step    else:        while start > end:            yield start            start += step",
        "labels_text": "Decimalcompatible implementation of Pythons range"
    },
    {
        "input_text": "summarize: def check_ranges_overlap(ranges):        ranges.sort(key=lambda x: x.lower)    for i in range(1, len(ranges)):        prev_range = ranges[i - 1]        prev_upper = prev_range.upper if prev_range.upper_inc else prev_range.upper - 1        lower = ranges[i].lower if ranges[i].lower_inc else ranges[i].lower + 1        if prev_upper >= lower:            return True    return False",
        "labels_text": "Check for overlap in an iterable of NumericRanges"
    },
    {
        "input_text": "summarize: def ranges_to_string(ranges):        if not ranges:        return ''    output = []    for r in ranges:        lower = r.lower if r.lower_inc else r.lower + 1        upper = r.upper if r.upper_inc else r.upper - 1        output.append(f'{lower}-{upper}')    return ','.join(output)",
        "labels_text": "Generate a humanfriendly string from a set of range Intended for use with ArrayField For example"
    },
    {
        "input_text": "summarize: def string_to_ranges(value):        if not value:        return None    value.replace(' ', '')  # Remove whitespace    values = []    for dash_range in value.split(','):        if '-' not in dash_range:            return None        lower, upper = dash_range.split('-')        values.append(NumericRange(int(lower), int(upper), bounds='[]'))    return values",
        "labels_text": "Given a string in the format return an list of NumericRanges Intended for use with ArrayField For example NumericRange NumericRange"
    },
    {
        "input_text": "summarize: def local_now():        return localtime(timezone.now())",
        "labels_text": "Return the current date time in the system timezone"
    },
    {
        "input_text": "summarize: def datetime_from_timestamp(value):        # Work around UTC issue for Python < 3.11; see    # https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat    # TODO: Remove this once Python 3.10 is no longer supported    if type(value) is str and value.endswith('Z'):        value = f'{value[:-1]}+00:00'    return datetime.datetime.fromisoformat(value)",
        "labels_text": "Convert an ISO or RFC timestamp to a datetime object"
    },
    {
        "input_text": "summarize: def handle_rest_api_exception(request, *args, **kwargs):        type_, error, traceback = sys.exc_info()    data = {        'error': str(error),        'exception': type_.__name__,        'netbox_version': settings.RELEASE.full_version,        'python_version': platform.python_version(),    }    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)",
        "labels_text": "Handle exception and return a useful error message for REST API request"
    },
    {
        "input_text": "summarize: def pre_save(self, model_instance, add):                original_value = getattr(model_instance, self.target_field)        naturalized_value = self.naturalize_function(original_value, max_length=self.max_length)        setattr(model_instance, self.attname, naturalized_value)        return naturalized_value",
        "labels_text": "Generate a naturalized value from the target field"
    },
    {
        "input_text": "summarize: def clean_html(html, schemes):        return nh3.clean(        html,        tags=HTML_ALLOWED_TAGS,        attributes=HTML_ALLOWED_ATTRIBUTES,        url_schemes=set(schemes)    )",
        "labels_text": "Sanitizes HTML based on a whitelist of allowed tag and attribute Also take a list of allowed URI scheme"
    },
    {
        "input_text": "summarize: def foreground_color(bg_color, dark='000000', light='ffffff'):        THRESHOLD = 150    bg_color = bg_color.strip('#')    r, g, b = [int(bg_color[c:c + 2], 16) for c in (0, 2, 4)]    if r * 0.299 + g * 0.587 + b * 0.114 > THRESHOLD:        return dark    else:        return light",
        "labels_text": "Return the ideal foreground color dark or light for a given background color in hexadecimal RGB format param dark RBG color code for dark text param light RBG color code for light text"
    },
    {
        "input_text": "summarize: def htmx_partial(request):        return request.htmx and not request.htmx.boosted",
        "labels_text": "Determines whether to render partial versus complete HTML content in response to an HTMX request based on the target element"
    },
    {
        "input_text": "summarize: def render_jinja2(template_code, context):        environment = SandboxedEnvironment()    environment.filters.update(get_config().JINJA2_FILTERS)    return environment.from_string(source=template_code).render(**context)",
        "labels_text": "Render a Jinja template with the provided context Return the rendered content"
    },
    {
        "input_text": "summarize: def custom_deconstruct(field):        name, path, args, kwargs = _deconstruct(field)    # Remove any ignored attributes    for attr in EXEMPT_ATTRS:        kwargs.pop(attr, None)    # Ignore any field defaults which reference a ConfigItem    kwargs = {        k: v for k, v in kwargs.items() if not isinstance(v, ConfigItem)    }    return name, path, args, kwargs",
        "labels_text": "Imitate the behavior of the stock deconstruct method but ignore the field attribute listed above"
    },
    {
        "input_text": "summarize: def object_type_identifier(object_type):        return f'{object_type.app_label}.{object_type.model}'",
        "labels_text": "Return a raw ObjectType identifier string suitable for bulk importexport eg dcimsite"
    },
    {
        "input_text": "summarize: def object_type_name(object_type, include_app=True):        try:        meta = object_type.model_class()._meta        app_label = title(meta.app_config.verbose_name)        model_name = title(meta.verbose_name)        if include_app:            return f'{app_label} > {model_name}'        return model_name    except AttributeError:        # Model does not exist        return f'{object_type.app_label} > {object_type.model}'",
        "labels_text": "Return a humanfriendly ObjectType name eg DCIM Site"
    },
    {
        "input_text": "summarize: def naturalize(value, max_length, integer_places=8):        if not value:        return value    output = []    for segment in re.split(r'(\\d+)', value):        if segment.isdigit():            output.append(segment.rjust(integer_places, '0'))        elif segment:            output.append(segment)    ret = ''.join(output)    return ret[:max_length]",
        "labels_text": "Take an alphanumeric string and prepend all integer to integerplaces place to ensure the string are ordered naturally For example siterouter siterouter siterouter becomes siterouter siterouter siterouter param value The value to be naturalized param maxlength The maximum length of the returned string Characters beyond this length will be stripped param integerplaces The number of place to which each integer will be expanded Default"
    },
    {
        "input_text": "summarize: def get_permission_for_model(model, action):        # Resolve to the \"concrete\" model (for proxy models)    model = model._meta.concrete_model    return f'{model._meta.app_label}.{action}_{model._meta.model_name}'",
        "labels_text": "Resolve the named permission for a given model or instance and action eg view or add param model A model or instance param action View add change or delete string"
    },
    {
        "input_text": "summarize: def resolve_permission(name):        try:        app_label, codename = name.split('.')        action, model_name = codename.rsplit('_', 1)    except ValueError:        raise ValueError(            _(\"Invalid permission name: {name}. Must be in the format <app_label>.<action>_<model>\").format(name=name)        )    return app_label, action, model_name",
        "labels_text": "Given a permission name return the applabel action and modelname component For example dcimviewsite return dcim view site param name Permission name in the format applabelactionmodel"
    },
    {
        "input_text": "summarize: def resolve_permission_type(name):        from core.models import ObjectType    app_label, action, model_name = resolve_permission(name)    try:        object_type = ObjectType.objects.get_by_natural_key(app_label=app_label, model=model_name)    except ObjectType.DoesNotExist:        raise ValueError(_(\"Unknown app_label/model_name for {name}\").format(name=name))    return object_type, action",
        "labels_text": "Given a permission name return the relevant ObjectType and action For example dcimviewsite return Site view param name Permission name in the format applabelactionmodel"
    },
    {
        "input_text": "summarize: def count_related(model, field):        subquery = Subquery(        model.objects.filter(            **{field: OuterRef('pk')}        ).order_by().values(            field        ).annotate(            c=Count('*')        ).values('c')    )    return Coalesce(subquery, 0)",
        "labels_text": "Return a Subquery suitable for annotating a child object count"
    },
    {
        "input_text": "summarize: def dict_to_filter_params(d, prefix=''):        params = {}    for key, val in d.items():        k = prefix + key        if isinstance(val, dict):            params.update(dict_to_filter_params(val, k + '__'))        else:            params[k] = val    return params",
        "labels_text": "Translate a dictionary of attribute to a nested set of parameter suitable for QuerySet filtering For example name Foo rack facilityid R Becomes name Foo rackfacilityid R And can be employed a filter parameter Deviceobjectsfilterdicttofilterattrsdict"
    },
    {
        "input_text": "summarize: def dict_to_querydict(d, mutable=True):        qd = QueryDict(mutable=True)    for k, v in d.items():        item = MultiValueDict({k: v}) if isinstance(v, (list, tuple, set)) else {k: v}        qd.update(item)    if not mutable:        qd._mutable = False    return qd",
        "labels_text": "Create a QueryDict instance from a regular Python dictionary"
    },
    {
        "input_text": "summarize: def normalize_querydict(querydict):        return {        k: v if len(v) > 1 else v[0] for k, v in querydict.lists()    }",
        "labels_text": "Convert a QueryDict to a normal mutable dictionary preserving list value For example QueryDictfoobarbarbaz becomes foo bar baz This function is necessary because QueryDict doe not provide any builtin mechanism which preserve multiple value"
    },
    {
        "input_text": "summarize: def get_related_models(model, ordered=True):        related_models = [        (field.related_model, field.remote_field.name)        for field in model._meta.related_objects        if type(field) is ManyToOneRel    ]    if ordered:        return sorted(related_models, key=lambda x: x[0]._meta.verbose_name.lower())    return related_models",
        "labels_text": "Return a list of all model which have a ForeignKey to the given model and the name of the field For example getrelatedmodelsTenant will return all model which have a ForeignKey relationship to Tenant"
    },
    {
        "input_text": "summarize: def get_queue_for_model(model):        return get_config().QUEUE_MAPPINGS.get(model, RQ_QUEUE_DEFAULT)",
        "labels_text": "Return the configured queue name for job associated with the given model"
    },
    {
        "input_text": "summarize: def get_workers_for_queue(queue_name):        return Worker.count(get_connection(queue_name))",
        "labels_text": "Returns True if a worker process is currently servicing the specified queue"
    },
    {
        "input_text": "summarize: def get_rq_retry():        retry_max = get_config().RQ_RETRY_MAX    retry_interval = get_config().RQ_RETRY_INTERVAL    if retry_max:        return Retry(max=retry_max, interval=retry_interval)",
        "labels_text": "If RQRETRYMAX is defined and greater than zero instantiate and return a Retry object to be used when queuing a job Otherwise return None"
    },
    {
        "input_text": "summarize: def deserialize_object(model, fields, pk=None):        content_type = ContentType.objects.get_for_model(model)    if 'custom_fields' in fields:        fields['custom_field_data'] = fields.pop('custom_fields')    data = {        'model': '.'.join(content_type.natural_key()),        'pk': pk,        'fields': fields,    }    instance = list(serializers.deserialize('python', [data]))[0]    return instance",
        "labels_text": "Instantiate an object from the given model and field data Functions a the complement to serializeobject"
    },
    {
        "input_text": "summarize: def title(value):        return ' '.join([w[0].upper() + w[1:] for w in str(value).split()])",
        "labels_text": "Improved implementation of strtitle retains all existing uppercase letter"
    },
    {
        "input_text": "summarize: def trailing_slash(value):        return f'{value.strip(\"/\")}/' if value else ''",
        "labels_text": "Remove a leading slash if any and include a trailing slash except for empty string"
    },
    {
        "input_text": "summarize: def get_table_ordering(request, table):        # Check for an explicit ordering    if 'sort' in request.GET:        return request.GET['sort'] or None    # Check for a configured preference    if request.user.is_authenticated:        if preference := request.user.config.get(f'tables.{table.__name__}.ordering'):            return preference",
        "labels_text": "Given a request return the prescribed table ordering if any This may be necessary to determine prior to rendering the table itself"
    },
    {
        "input_text": "summarize: def linkify_phone(value):        if value is None:        return None    return f\"tel:{value.replace(' ', '')}\"",
        "labels_text": "Render a telephone number a a hyperlink"
    },
    {
        "input_text": "summarize: def register_table_column(column, name, *tables):        for table in tables:        reg = registry['tables'][table]        if name in reg:            raise ValueError(_(\"A column named {name} is already defined for table {table_name}\").format(                name=name, table_name=table.__name__            ))        reg[name] = column",
        "labels_text": "Register a custom column for use on one or more table Args column The column instance to register name The name of the table column table One or more table class"
    },
    {
        "input_text": "summarize: def set(self, name, value):                self._changed_fields[name] = value",
        "labels_text": "Mark an attribute a having been changed and record it original value"
    },
    {
        "input_text": "summarize: def get(self, name):                return self._changed_fields[name]",
        "labels_text": "Return the original value of a changed field Raises KeyError if name is not found"
    },
    {
        "input_text": "summarize: def clear(self, *names):                for name in names:            self._changed_fields.pop(name, None)        else:            self._changed_fields = {}",
        "labels_text": "Clear any field that were recorded a having been changed"
    },
    {
        "input_text": "summarize: def tracker(self):                if not hasattr(self._state, \"_tracker\"):            self._state._tracker = Tracker()        return self._state._tracker",
        "labels_text": "Return the Tracker instance for this instance first creating it if necessary"
    },
    {
        "input_text": "summarize: def validate_regex(value):        try:        re.compile(value)    except re.error:        raise ValidationError(_(\"{value} is not a valid regular expression.\").format(value=value))",
        "labels_text": "Checks that the value is a valid regular expression Dont confuse this with RegexValidator which us a regex to validate a value"
    },
    {
        "input_text": "summarize: def get_required_permission(self):                raise NotImplementedError(_(\"{self.__class__.__name__} must implement get_required_permission()\").format(            class_name=self.__class__.__name__        ))",
        "labels_text": "Return the specific permission necessary to perform the requested action on an object"
    },
    {
        "input_text": "summarize: def get_required_permission(self):                raise NotImplementedError(_(\"{class_name} must implement get_required_permission()\").format(            class_name=self.__class__.__name__        ))",
        "labels_text": "Return the specific permission necessary to perform the requested action on an object"
    },
    {
        "input_text": "summarize: def render(self, instance):                badge_value = self._get_badge_value(instance)        if self.badge and self.hide_if_empty and not badge_value:            return None        return {            'label': self.label,            'badge': badge_value,            'weight': self.weight,        }",
        "labels_text": "Return the attribute needed to render a tab in HTML"
    },
    {
        "input_text": "summarize: def _clean_json(self, data):                try:            data = json.loads(data)            # Accommodate for users entering single objects            if type(data) is not list:                data = [data]            return data        except json.decoder.JSONDecodeError as err:            raise forms.ValidationError({                self.data_field: f\"Invalid JSON data: {err}\"            })",
        "labels_text": "Clean JSONformatted data If only a single object is defined it will be encapsulated a a list"
    },
    {
        "input_text": "summarize: def get_field_value(form, field_name):        field = form.fields[field_name]    if form.is_bound and (data := form.data.get(field_name)):        if hasattr(field, 'valid_value') and field.valid_value(data):            return data    return form.get_initial_for_field(field, field_name)",
        "labels_text": "Return the current bound or initial value associated with a form field prior to calling clean for the form"
    },
    {
        "input_text": "summarize: def add_blank_choice(choices):        return ((None, '---------'),) + tuple(choices)",
        "labels_text": "Add a blank choice to the beginning of a choice list"
    },
    {
        "input_text": "summarize: def form_from_model(model, fields):        form_fields = fields_for_model(model, fields=fields)    for field in form_fields.values():        field.required = False    return type('FormFromModel', (forms.Form,), form_fields)",
        "labels_text": "Return a Form class with the specified field derived from a model This is useful when we need a form to be used for creating object but want to avoid the model validation eg for bulk createedit function All field are marked a not required"
    },
    {
        "input_text": "summarize: def restrict_form_fields(form, user, action='view'):        for field in form.fields.values():        if hasattr(field, 'queryset') and issubclass(field.queryset.__class__, RestrictedQuerySet):            field.queryset = field.queryset.restrict(user, action)",
        "labels_text": "Restrict all form field which reference a RestrictedQuerySet This ensures that user see only permitted object a available choice"
    },
    {
        "input_text": "summarize: def clean(self, value):                if self.null_option is not None and value == settings.FILTERS_NULL_CHOICE_VALUE:            return None        return super().clean(value)",
        "labels_text": "When null option is enabled and None is sent a part of a form to be submitted it is sent a the string null This will check for that condition and gracefully handle the conversion to a NoneType"
    },
    {
        "input_text": "summarize: def __deepcopy__(self, memo):                result = super().__deepcopy__(memo)        result.dynamic_params = {}        result.static_params = {}        return result",
        "labels_text": "Reset staticparams and dynamicparams when APISelect is deepcopied"
    },
    {
        "input_text": "summarize: def _process_query_params(self, query_params):                for key, value in query_params.items():            if isinstance(value, (List, Tuple)):                # If value is a list/tuple, iterate through each item.                for item in value:                    self._process_query_param(key, item)            else:                self._process_query_param(key, value)",
        "labels_text": "Process an entire queryparams dictionary and handle primitive or list value"
    },
    {
        "input_text": "summarize: def _add_dynamic_params(self):                key = 'data-dynamic-params'        if len(self.dynamic_params) > 0:            try:                update = [{'fieldName': f, 'queryParam': q} for (f, q) in self.dynamic_params.items()]                self._serialize_params(key, update)            except IndexError as error:                raise RuntimeError(                    _(\"Missing required value for dynamic query param: '{dynamic_params}'\").format(                        dynamic_params=self.dynamic_params                    )                ) from error",
        "labels_text": "Convert postprocessed dynamic query params to data structure expected by front end serialize the value to JSON and add it to the widget attribute"
    },
    {
        "input_text": "summarize: def add_query_params(self, query_params):                # Process query parameters. This populates `self.dynamic_params` and `self.static_params`.        self._process_query_params(query_params)        # Add processed dynamic parameters to widget attributes.        self._add_dynamic_params()        # Add processed static parameters to widget attributes.        self._add_static_params()",
        "labels_text": "Proccess add a dictionary of URL query parameter to the widget attribute"
    },
    {
        "input_text": "summarize: def add_query_param(self, key, value) -> None:                self.add_query_params({key: value})",
        "labels_text": "Process add a keyvalue pair of URL query parameter to the widget attribute"
    },
    {
        "input_text": "summarize: def collect_models():                models = defaultdict(dict)        for model, field_mappings in registry['counter_fields'].items():            for field_name, counter_name in field_mappings.items():                fk_field = model._meta.get_field(field_name)        # Interface.device                parent_model = fk_field.related_model               # Device                related_query_name = fk_field.related_query_name()  # 'interfaces'                models[parent_model][counter_name] = related_query_name        return models",
        "labels_text": "Query the registry to find all model which have one or more counter field Return a mapping of counter field to related query name for each model"
    },
    {
        "input_text": "summarize: def getfield(form, fieldname):        try:        return form[fieldname]    except KeyError:        return None",
        "labels_text": "Return the specified bound field of a Form"
    },
    {
        "input_text": "summarize: def widget_type(field):        if hasattr(field, 'widget'):        return field.widget.__class__.__name__.lower()    elif hasattr(field, 'field'):        return field.field.widget.__class__.__name__.lower()    else:        return None",
        "labels_text": "Return the widget type"
    },
    {
        "input_text": "summarize: def render_field(field, bulk_nullable=False, label=None):        return {        'field': field,        'label': label or field.label,        'bulk_nullable': bulk_nullable or getattr(field, '_nullable', False),    }",
        "labels_text": "Render a single form field from template"
    },
    {
        "input_text": "summarize: def render_custom_fields(form):        return {        'form': form,    }",
        "labels_text": "Render all custom field in a form"
    },
    {
        "input_text": "summarize: def render_form(form):        return {        'form': form,    }",
        "labels_text": "Render an entire form from template"
    },
    {
        "input_text": "summarize: def render_errors(form):        return {        \"form\": form    }",
        "labels_text": "Render form error if they exist"
    },
    {
        "input_text": "summarize: def viewname(model, action):        return get_viewname(model, action)",
        "labels_text": "Return the view name for the given model and action Does not perform any validation"
    },
    {
        "input_text": "summarize: def validated_viewname(model, action):        viewname = get_viewname(model, action)    # Validate the view name    try:        reverse(viewname)        return viewname    except NoReverseMatch:        return None",
        "labels_text": "Return the view name for the given model and action if valid or None if invalid"
    },
    {
        "input_text": "summarize: def humanize_megabytes(mb):        if not mb:        return \"\"    PB_SIZE = 1000000000    TB_SIZE = 1000000    GB_SIZE = 1000    if mb >= PB_SIZE:        return f\"{mb / PB_SIZE:.2f} PB\"    if mb >= TB_SIZE:        return f\"{mb / TB_SIZE:.2f} TB\"    if mb >= GB_SIZE:        return f\"{mb / GB_SIZE:.2f} GB\"    return f\"{mb} MB\"",
        "labels_text": "Express a number of megabyte in the most suitable unit eg gigabyte terabyte etc"
    },
    {
        "input_text": "summarize: def divide(x, y):        if x is None or y is None:        return None    return round(x / y)",
        "labels_text": "Return xy rounded"
    },
    {
        "input_text": "summarize: def percentage(x, y):        if x is None or y is None:        return None    return round(x / y * 100, 1)",
        "labels_text": "Return xy a a percentage"
    },
    {
        "input_text": "summarize: def as_range(n):        try:        int(n)    except TypeError:        return list()    return range(n)",
        "labels_text": "Return a range of n item"
    },
    {
        "input_text": "summarize: def meters_to_feet(n):        return float(n) * 3.28084",
        "labels_text": "Convert a length from meter to foot"
    },
    {
        "input_text": "summarize: def kg_to_pounds(n):        return float(n) * 2.204623",
        "labels_text": "Convert a weight from kilogram to pound"
    },
    {
        "input_text": "summarize: def startswith(text: str, starts: str) -> bool:        if isinstance(text, str):        return text.startswith(starts)    return False",
        "labels_text": "Template implementation of strstartswith"
    },
    {
        "input_text": "summarize: def get_key(value: Dict, arg: str) -> Any:        return value.get(arg, None)",
        "labels_text": "Template implementation of dictget for accessing dict value by key when the key is not able to be used in a template For example uicolormode dark"
    },
    {
        "input_text": "summarize: def get_item(value: object, attr: str) -> Any:        return value[attr]",
        "labels_text": "Template implementation of getitem for accessing the getitem method of a class from a template"
    },
    {
        "input_text": "summarize: def status_from_tag(tag: str = \"info\") -> str:        status_map = {        'warning': 'warning',        'success': 'success',        'error': 'danger',        'danger': 'danger',        'debug': 'info',        'info': 'info',    }    return status_map.get(tag.lower(), 'info')",
        "labels_text": "Determine Bootstrap theme statuslevel from Djangos Messageleveltag"
    },
    {
        "input_text": "summarize: def icon_from_status(status: str = \"info\") -> str:        icon_map = {        'warning': 'alert',        'success': 'check-circle',        'danger': 'alert',        'info': 'information',    }    return icon_map.get(status.lower(), 'information')",
        "labels_text": "Determine icon class name from Bootstrap theme statuslevel"
    },
    {
        "input_text": "summarize: def querystring(request, **kwargs):        querydict = request.GET.copy()    for k, v in kwargs.items():        if v is not None:            querydict[k] = str(v)        elif k in querydict:            querydict.pop(k)    querystring = querydict.urlencode(safe='/')    if querystring:        return '?' + querystring    else:        return ''",
        "labels_text": "Append or update the page number in a querystring"
    },
    {
        "input_text": "summarize: def nested_tree(obj):        if not obj:        return mark_safe('&mdash;')    nodes = obj.get_ancestors(include_self=True)    return mark_safe(        ' / '.join(            f'<a href=\"{node.get_absolute_url()}\">{escape(node)}</a>' for node in nodes        )    )",
        "labels_text": "Renders the entire hierarchy of a recursivelynested object such a Region or SiteGroup"
    },
    {
        "input_text": "summarize: def plugin_navbar(context):        return _get_registered_content(None, 'navbar', context)",
        "labels_text": "Render any navbar content embedded by plugins"
    },
    {
        "input_text": "summarize: def plugin_list_buttons(context, model):        return _get_registered_content(model, 'list_buttons', context)",
        "labels_text": "Render all list button registered by plugins"
    },
    {
        "input_text": "summarize: def plugin_buttons(context, obj):        return _get_registered_content(obj, 'buttons', context)",
        "labels_text": "Render all button registered by plugins"
    },
    {
        "input_text": "summarize: def plugin_alerts(context, obj):        return _get_registered_content(obj, 'alerts', context)",
        "labels_text": "Render all object alert registered by plugins"
    },
    {
        "input_text": "summarize: def plugin_left_page(context, obj):        return _get_registered_content(obj, 'left_page', context)",
        "labels_text": "Render all left page content registered by plugins"
    },
    {
        "input_text": "summarize: def plugin_right_page(context, obj):        return _get_registered_content(obj, 'right_page', context)",
        "labels_text": "Render all right page content registered by plugins"
    },
    {
        "input_text": "summarize: def plugin_full_width_page(context, obj):        return _get_registered_content(obj, 'full_width_page', context)",
        "labels_text": "Render all full width page content registered by plugins"
    },
    {
        "input_text": "summarize: def linkify(instance, attr=None):        if instance is None:        return ''    text = getattr(instance, attr) if attr is not None else str(instance)    try:        url = instance.get_absolute_url()        return mark_safe(f'<a href=\"{url}\">{escape(text)}</a>')    except (AttributeError, TypeError):        return escape(text)",
        "labels_text": "Render a hyperlink for an object with a getabsoluteurl method optionally specifying the name of an attribute to use for the link text If no attribute is given the object string representation will be used If the object ha no getabsoluteurl method return the text without a hyperlink element"
    },
    {
        "input_text": "summarize: def bettertitle(value):        return title(value)",
        "labels_text": "Alternative to the builtin title Ensures that the first letter of each word is uppercase but retains the original case of all others"
    },
    {
        "input_text": "summarize: def fgcolor(value, dark='000000', light='ffffff'):        value = value.lower().strip('#')    if not re.match('^[0-9a-f]{6}$', value):        return ''    return f'#{foreground_color(value, dark, light)}'",
        "labels_text": "Return black or white ffffff given an arbitrary background color in RRGGBB format The foreground color with the better contrast is returned Args value The background color dark The foreground color to use for light background light The foreground color to use for dark background"
    },
    {
        "input_text": "summarize: def meta(model, attr):        return getattr(model._meta, attr, '')",
        "labels_text": "Return the specified Meta attribute of a model This is needed because Django doe not permit template to access attribute which begin with an underscore eg meta Args model A Django model class or instance attr The attribute name"
    },
    {
        "input_text": "summarize: def placeholder(value):        if value not in ('', None):        return value    return mark_safe('<span class=\"text-muted\">&mdash;</span>')",
        "labels_text": "Render a muted placeholder if the value equates to False"
    },
    {
        "input_text": "summarize: def split(value, separator=','):        return value.split(separator)",
        "labels_text": "Wrapper for Pythons split string method Args value A string separator String on which the value will be split"
    },
    {
        "input_text": "summarize: def tzoffset(value):        return datetime.datetime.now(value).strftime('%z')",
        "labels_text": "Returns the hour offset of a given time zone using the current time"
    },
    {
        "input_text": "summarize: def content_type(model):        return ContentType.objects.get_for_model(model)",
        "labels_text": "Return the ContentType for the given object"
    },
    {
        "input_text": "summarize: def content_type_id(model):        content_type = ContentType.objects.get_for_model(model)    if content_type:        return content_type.pk    return None",
        "labels_text": "Return the ContentType ID for the given object"
    },
    {
        "input_text": "summarize: def render_json(value):        return json.dumps(value, ensure_ascii=False, indent=4, sort_keys=True)",
        "labels_text": "Render a dictionary a formatted JSON This filter is invoked a json datadictjson"
    },
    {
        "input_text": "summarize: def render_yaml(value):        return yaml.dump(json.loads(json.dumps(value)))",
        "labels_text": "Render a dictionary a formatted YAML This filter is invoked a yaml datadictyaml"
    },
    {
        "input_text": "summarize: def tag(value, viewname=None):        return {        'tag': value,        'viewname': viewname,    }",
        "labels_text": "Display a tag optionally linked to a filtered list of object Args value A Tag instance viewname If provided the tag will be a hyperlink to the specified view URL"
    },
    {
        "input_text": "summarize: def customfield_value(customfield, value):        if value:        if customfield.type == CustomFieldTypeChoices.TYPE_SELECT:            value = customfield.get_choice_label(value)        elif customfield.type == CustomFieldTypeChoices.TYPE_MULTISELECT:            value = [customfield.get_choice_label(v) for v in value]    return {        'customfield': customfield,        'value': value,    }",
        "labels_text": "Render a custom field value according to the field type Args customfield A CustomField instance value The custom field value applied to an object"
    },
    {
        "input_text": "summarize: def badge(value, bg_color=None, show_empty=False):        return {        'value': value,        'bg_color': bg_color or 'secondary',        'show_empty': show_empty,    }",
        "labels_text": "Display the specified number a a badge Args value The value to be displayed within the badge bgcolor Background color CSS name showempty If true display the badge even if value is None or zero"
    },
    {
        "input_text": "summarize: def checkmark(value, show_false=True, true='Yes', false='No'):        return {        'value': bool(value),        'show_false': show_false,        'true_label': true,        'false_label': false,    }",
        "labels_text": "Display either a green checkmark or red X to indicate a boolean value Args value True or False showfalse Show false value true Text label for true value false Text label for false value"
    },
    {
        "input_text": "summarize: def copy_content(target, prefix=None, color='primary', classes=None):        return {        'target': f'#{prefix or \"\"}{target}',        'color': f'btn-{color}',        'classes': classes or '',    }",
        "labels_text": "Display a copy button to copy the content of a field"
    },
    {
        "input_text": "summarize: def htmx_table(context, viewname, return_url=None, **kwargs):        url_params = dict_to_querydict(kwargs)    url_params['return_url'] = return_url or context['request'].path    return {        'viewname': viewname,        'url_params': url_params,    }",
        "labels_text": "Embed an object list table retrieved using HTMX Any extra keyword argument are passed a URL query parameter Args context The current request context viewname The name of the view to use for the HTMX request eg dcimsitelist returnurl The URL to pas a the returnurl If not provided the current request path will be used"
    },
    {
        "input_text": "summarize: def formaction(context):        if context.get('htmx_navigation', False):        return mark_safe('hx-push-url=\"true\" hx-post')    return 'formaction'",
        "labels_text": "Replace the formaction attribute on an HTML element with the appropriate HTMX attribute if HTMX navigation is enabled per the user preference"
    },
    {
        "input_text": "summarize: def setUp(self):                # Create the test user and assign permissions        self.user = User.objects.create_user(username='testuser')        self.add_permissions(*self.user_permissions)        self.token = Token.objects.create(user=self.user)        self.header = {'HTTP_AUTHORIZATION': f'Token {self.token.key}'}",
        "labels_text": "Create a user and token for API call"
    },
    {
        "input_text": "summarize: def test_get_object_without_permission(self):                        url = self._get_detail_url(self._get_queryset().first())            # Try GET without permission            with disable_warnings('django.request'):                self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_403_FORBIDDEN)",
        "labels_text": "GET a single object a an authenticated user without the required permission"
    },
    {
        "input_text": "summarize: def test_options_object(self):                        url = self._get_detail_url(self._get_queryset().first())            response = self.client.options(url, **self.header)            self.assertHttpStatus(response, status.HTTP_200_OK)",
        "labels_text": "Make an OPTIONS request for a single object"
    },
    {
        "input_text": "summarize: def test_list_objects_without_permission(self):                        url = self._get_list_url()            # Try GET without permission            with disable_warnings('django.request'):                self.assertHttpStatus(self.client.get(url, **self.header), status.HTTP_403_FORBIDDEN)",
        "labels_text": "GET a list of object a an authenticated user without the required permission"
    },
    {
        "input_text": "summarize: def test_options_objects(self):                        response = self.client.options(self._get_list_url(), **self.header)            self.assertHttpStatus(response, status.HTTP_200_OK)",
        "labels_text": "Make an OPTIONS request for a list endpoint"
    },
    {
        "input_text": "summarize: def test_create_object_without_permission(self):                        url = self._get_list_url()            # Try POST without permission            with disable_warnings('django.request'):                response = self.client.post(url, self.create_data[0], format='json', **self.header)                self.assertHttpStatus(response, status.HTTP_403_FORBIDDEN)",
        "labels_text": "POST a single object without permission"
    },
    {
        "input_text": "summarize: def test_update_object_without_permission(self):                        url = self._get_detail_url(self._get_queryset().first())            update_data = self.update_data or getattr(self, 'create_data')[0]            # Try PATCH without permission            with disable_warnings('django.request'):                response = self.client.patch(url, update_data, format='json', **self.header)                self.assertHttpStatus(response, status.HTTP_403_FORBIDDEN)",
        "labels_text": "PATCH a single object without permission"
    },
    {
        "input_text": "summarize: def test_delete_object_without_permission(self):                        url = self._get_detail_url(self._get_queryset().first())            # Try DELETE without permission            with disable_warnings('django.request'):                response = self.client.delete(url, **self.header)                self.assertHttpStatus(response, status.HTTP_403_FORBIDDEN)",
        "labels_text": "DELETE a single object without permission"
    },
    {
        "input_text": "summarize: def _get_graphql_base_name(self):                        base_name = self.model._meta.verbose_name.lower().replace(' ', '_')            return getattr(self, 'graphql_base_name', base_name)",
        "labels_text": "Return graphqlbasename if set Otherwise construct the base name for the query field from the model verbose name"
    },
    {
        "input_text": "summarize: def _build_query(self, name, **filters):                        if filters:                filter_string = ', '.join(f'{k}:{v}' for k, v in filters.items())                filter_string = f'({filter_string})'            else:                filter_string = ''            return self._build_query_with_filter(name, filter_string)",
        "labels_text": "Create a normal query unfiltered or with a string query ie sitename aaa"
    },
    {
        "input_text": "summarize: def add_permissions(self, *names):                for name in names:            object_type, action = resolve_permission_type(name)            obj_perm = ObjectPermission(name=name, actions=[action])            obj_perm.save()            obj_perm.users.add(self.user)            obj_perm.object_types.add(object_type)",
        "labels_text": "Assign a set of permission to the test user Accepts permission name in the form appactionmodel"
    },
    {
        "input_text": "summarize: def _get_queryset(self):                return self.model.objects.all()",
        "labels_text": "Return a base queryset suitable for use in test method"
    },
    {
        "input_text": "summarize: def prepare_instance(self, instance):                return instance",
        "labels_text": "Test case can override this method to perform any necessary manipulation of an instance prior to it evaluation against test data For example it can be used to decrypt a Secrets plaintext attribute"
    },
    {
        "input_text": "summarize: def get_m2m_filter_name(self, field):                related_model_name = field.related_model._meta.verbose_name        return related_model_name.lower().replace(' ', '_')",
        "labels_text": "Given a ManyToManyField determine the correct name for it corresponding Filter Individual test case may override this method to prescribe deviation for specific field"
    },
    {
        "input_text": "summarize: def test_id(self):                params = {'id': self.queryset.values_list('pk', flat=True)[:2]}        self.assertGreater(self.queryset.count(), 2)        self.assertEqual(self.filterset(params, self.queryset).qs.count(), 2)",
        "labels_text": "Test filtering for two PKs from a set of object"
    },
    {
        "input_text": "summarize: def create_test_virtualmachine(name):        cluster_type, _ = ClusterType.objects.get_or_create(name='Cluster Type 1', slug='cluster-type-1')    cluster, _ = Cluster.objects.get_or_create(name='Cluster 1', type=cluster_type)    virtual_machine = VirtualMachine.objects.create(name=name, cluster=cluster)    return virtual_machine",
        "labels_text": "Convenience method for creating a VirtualMachine"
    },
    {
        "input_text": "summarize: def create_test_user(username='testuser', permissions=None):        user = User.objects.create_user(username=username)    if permissions is None:        permissions = ()    for perm_name in permissions:        app, codename = perm_name.split('.')        perm = Permission.objects.get(content_type__app_label=app, codename=codename)        user.user_permissions.add(perm)    return user",
        "labels_text": "Create a User with the given permission"
    },
    {
        "input_text": "summarize: def create_tags(*names):        tags = [Tag(name=name, slug=slugify(name)) for name in names]    Tag.objects.bulk_create(tags)    return tags",
        "labels_text": "Create and return a Tag instance for each name given"
    },
    {
        "input_text": "summarize: def extract_form_failures(content):        FORM_ERROR_REGEX = r'<!-- FORM-ERROR (.*) -->'    return re.findall(FORM_ERROR_REGEX, str(content))",
        "labels_text": "Given raw HTML content from an HTTP response return a list of form error"
    },
    {
        "input_text": "summarize: def disable_warnings(logger_name):        logger = logging.getLogger(logger_name)    current_level = logger.level    logger.setLevel(logging.ERROR)    yield    logger.setLevel(current_level)",
        "labels_text": "Temporarily suppress expected warning message to keep the test output clean"
    },
    {
        "input_text": "summarize: def disable_logging(level=logging.CRITICAL):        logging.disable(level)    yield    logging.disable(logging.NOTSET)",
        "labels_text": "Temporarily suppress log message at or below the specified level default critical"
    },
    {
        "input_text": "summarize: def _get_base_url(self):                return '{}:{}_{{}}'.format(            self.model._meta.app_label,            self.model._meta.model_name        )",
        "labels_text": "Return the base format for a URL for the test model Override this to test for a model which belongs to a different app eg testing Interfaces within the virtualization app"
    },
    {
        "input_text": "summarize: def _get_url(self, action, instance=None):                url_format = self._get_base_url()        # If no instance was provided, assume we don't need a unique identifier        if instance is None:            return reverse(url_format.format(action))        return reverse(url_format.format(action), kwargs={'pk': instance.pk})",
        "labels_text": "Return the URL name for a specific action and optionally a specific instance"
    },
    {
        "input_text": "summarize: def update_virtualmachine_disk(instance, **kwargs):        vm = instance.virtual_machine    VirtualMachine.objects.filter(pk=vm.pk).update(        disk=vm.virtualdisks.aggregate(Sum('size'))['size__sum']    )",
        "labels_text": "When a VirtualDisk ha been modified update the aggregate disksize value of it VM"
    },
    {
        "input_text": "summarize: def get_serializer_class(self):                request = self.get_serializer_context()['request']        if self.brief or 'config_context' in request.query_params.get('exclude', []):            return serializers.VirtualMachineSerializer        return serializers.VirtualMachineWithConfigContextSerializer",
        "labels_text": "Select the specific serializer based on the request context If the brief query param equates to True return the NestedVirtualMachineSerializer If the exclude query param includes configcontext a a value return the VirtualMachineSerializer Else return the VirtualMachineWithConfigContextSerializer"
    },
    {
        "input_text": "summarize: def test_config_context_included_by_default_in_list_view(self):                virtualmachine = VirtualMachine.objects.first()        url = '{}?id={}'.format(reverse('virtualization-api:virtualmachine-list'), virtualmachine.pk)        self.add_permissions('virtualization.view_virtualmachine')        response = self.client.get(url, **self.header)        self.assertEqual(response.data['results'][0].get('config_context', {}).get('A'), 1)",
        "labels_text": "Check that config context data is included by default in the virtual machine list"
    },
    {
        "input_text": "summarize: def test_config_context_excluded(self):                url = reverse('virtualization-api:virtualmachine-list') + '?exclude=config_context'        self.add_permissions('virtualization.view_virtualmachine')        response = self.client.get(url, **self.header)        self.assertFalse('config_context' in response.data['results'][0])",
        "labels_text": "Check that config context data can be excluded by passing excludeconfigcontext"
    },
    {
        "input_text": "summarize: def test_unique_name_per_cluster_constraint(self):                data = {            'name': 'Virtual Machine 1',            'cluster': Cluster.objects.first().pk,        }        url = reverse('virtualization-api:virtualmachine-list')        self.add_permissions('virtualization.add_virtualmachine')        response = self.client.post(url, data, format='json', **self.header)        self.assertHttpStatus(response, status.HTTP_400_BAD_REQUEST)",
        "labels_text": "Check that creating a virtual machine with a duplicate name fails"
    },
    {
        "input_text": "summarize: def mediafile_image(image_path, maxwidth=None):        with open(syspath(image_path), \"rb\") as f:        data = f.read()    return mediafile.Image(data, type=mediafile.ImageType.front)",
        "labels_text": "Return a mediafileImage object for the path"
    },
    {
        "input_text": "summarize: def resize_image(log, imagepath, maxwidth, quality):        log.debug(        \"Resizing album art to {0} pixels wide and encoding at quality \\              level {1}\",        maxwidth,        quality,    )    imagepath = ArtResizer.shared.resize(        maxwidth, syspath(imagepath), quality=quality    )    return imagepath",
        "labels_text": "Returns path to an image resized to maxwidth and encoded with the specified quality level"
    },
    {
        "input_text": "summarize: def check_art_similarity(    log,    item,    imagepath,    compare_threshold,    artresizer=None,):        with NamedTemporaryFile(delete=True) as f:        art = extract(log, f.name, item)        if not art:            return True        if artresizer is None:            artresizer = ArtResizer.shared        return artresizer.compare(art, imagepath, compare_threshold)",
        "labels_text": "A boolean indicating if an image is similar to embedded item art If no embedded art exists always return True If the comparison fails for some reason the return value is None This must only be called if ArtResizersharedcancompare is True"
    },
    {
        "input_text": "summarize: def _open_state():        try:        with open(config[\"statefile\"].as_filename(), \"rb\") as f:            return pickle.load(f)    except Exception as exc:        # The `pickle` module can emit all sorts of exceptions during        # unpickling, including ImportError. We use a catch-all        # exception to avoid enumerating them all (the docs don't even have a        # full list!).        log.debug(\"state file could not be read: {0}\", exc)        return {}",
        "labels_text": "Reads the state file returning a dictionary"
    },
    {
        "input_text": "summarize: def _save_state(state):        try:        with open(config[\"statefile\"].as_filename(), \"wb\") as f:            pickle.dump(state, f)    except OSError as exc:        log.error(\"state file could not be written: {0}\", exc)",
        "labels_text": "Writes the state dictionary out to disk"
    },
    {
        "input_text": "summarize: def progress_element(toppath, path):        state = progress_read()    if toppath not in state:        return False    imported = state[toppath]    i = bisect_left(imported, path)    return i != len(imported) and imported[i] == path",
        "labels_text": "Return whether path ha been imported in toppath"
    },
    {
        "input_text": "summarize: def has_progress(toppath):        state = progress_read()    return toppath in state",
        "labels_text": "Return True if there exist path that have already been imported under toppath"
    },
    {
        "input_text": "summarize: def history_add(paths):        state = _open_state()    if HISTORY_KEY not in state:        state[HISTORY_KEY] = set()    state[HISTORY_KEY].add(tuple(paths))    _save_state(state)",
        "labels_text": "Indicate that the import of the album in path is completed and should not be repeated in incremental import"
    },
    {
        "input_text": "summarize: def history_get():        state = _open_state()    if HISTORY_KEY not in state:        return set()    return state[HISTORY_KEY]",
        "labels_text": "Get the set of completed path tuples in incremental import"
    },
    {
        "input_text": "summarize: def __init__(self, lib, loghandler, paths, query):                self.lib = lib        self.logger = self._setup_logging(loghandler)        self.paths = paths        self.query = query        self._is_resuming = {}        self._merged_items = set()        self._merged_dirs = set()        # Normalize the paths.        if self.paths:            self.paths = list(map(normpath, self.paths))",
        "labels_text": "Create a session lib is a Library object loghandler is a loggingHandler Either path or query is nonnull and indicates the source of file to be imported"
    },
    {
        "input_text": "summarize: def tag_log(self, status, paths):                self.logger.info(\"{0} {1}\", status, displayable_path(paths))",
        "labels_text": "Log a message about a given album to the importer log The status should reflect the reason the album couldnt be tagged"
    },
    {
        "input_text": "summarize: def already_imported(self, toppath, paths):                if self.is_resuming(toppath) and all(            [progress_element(toppath, p) for p in paths]        ):            return True        if self.config[\"incremental\"] and tuple(paths) in self.history_dirs:            return True        return False",
        "labels_text": "Returns true if the file belonging to this task have already been imported in a previous session"
    },
    {
        "input_text": "summarize: def already_merged(self, paths):                for path in paths:            if path not in self._merged_items and path not in self._merged_dirs:                return False        return True",
        "labels_text": "Returns true if all the path being imported were part of a merge during previous task"
    },
    {
        "input_text": "summarize: def mark_merged(self, paths):                self._merged_items.update(paths)        dirs = {            os.path.dirname(path) if os.path.isfile(syspath(path)) else path            for path in paths        }        self._merged_dirs.update(dirs)",
        "labels_text": "Mark path and directory a merged for future reimport task"
    },
    {
        "input_text": "summarize: def is_resuming(self, toppath):                return self._is_resuming.get(toppath, False)",
        "labels_text": "Return True if user want to resume import of this path You have to call askresume first to determine the return value"
    },
    {
        "input_text": "summarize: def ask_resume(self, toppath):                if self.want_resume and has_progress(toppath):            # Either accept immediately or prompt for input to decide.            if self.want_resume is True or self.should_resume(toppath):                log.warning(                    \"Resuming interrupted import of {0}\",                    util.displayable_path(toppath),                )                self._is_resuming[toppath] = True            else:                # Clear progress; we're starting from the top.                progress_reset(toppath)",
        "labels_text": "If import of toppath wa aborted in an earlier session ask user if they want to resume the import Determines the return value of isresumingtoppath"
    },
    {
        "input_text": "summarize: def __init__(self, toppath, paths, items):                self.toppath = toppath        self.paths = paths        self.items = items",
        "labels_text": "Create a task The primary field that define a task are toppath The userspecified base directory that contains the music for this task If the task ha no userspecified base for example when importing based on an L query this can be None This is used for tracking progress and history path A list of specific path where the music for this task came from These path can be directory when their entire content are being imported or file when the task comprises individual track This is used for progresshistory tracking and for displaying the task to the user item A list of Item object representing the music being imported These field should not change after initialization"
    },
    {
        "input_text": "summarize: def set_choice(self, choice):                # Not part of the task structure:        assert choice != action.APPLY  # Only used internally.        if choice in (            action.SKIP,            action.ASIS,            action.TRACKS,            action.ALBUMS,            action.RETAG,        ):            self.choice_flag = choice            self.match = None        else:            self.choice_flag = action.APPLY  # Implicit choice.            self.match = choice",
        "labels_text": "Given an AlbumMatch or TrackMatch object or an action constant indicates that an action ha been selected for this task"
    },
    {
        "input_text": "summarize: def save_progress(self):                if self.toppath:            progress_add(self.toppath, *self.paths)",
        "labels_text": "Updates the progress state to indicate that this album ha finished"
    },
    {
        "input_text": "summarize: def save_history(self):                if self.paths:            history_add(self.paths)",
        "labels_text": "Save the directory in the history for incremental import"
    },
    {
        "input_text": "summarize: def chosen_info(self):                if self.choice_flag in (action.ASIS, action.RETAG):            likelies, consensus = autotag.current_metadata(self.items)            return likelies        elif self.choice_flag is action.APPLY:            return self.match.info.copy()        assert False",
        "labels_text": "Return a dictionary of metadata about the current choice May only be called when the choice flag is ASIS or RETAG in which case the data come from the file current metadata or APPLY in which case the data come from the choice"
    },
    {
        "input_text": "summarize: def imported_items(self):                if self.choice_flag in (action.ASIS, action.RETAG):            return list(self.items)        elif self.choice_flag == action.APPLY:            return list(self.match.mapping.keys())        else:            assert False",
        "labels_text": "Return a list of Items that should be added to the library If the task applies an album match the method only return the matched item"
    },
    {
        "input_text": "summarize: def apply_metadata(self):                if config[\"import\"][\"from_scratch\"]:            for item in self.match.mapping:                item.clear()        autotag.apply_metadata(self.match.info, self.match.mapping)",
        "labels_text": "Copy metadata from match info to the item"
    },
    {
        "input_text": "summarize: def handle_created(self, session):                tasks = plugins.send(\"import_task_created\", session=session, task=self)        if not tasks:            tasks = [self]        else:            # The plugins gave us a list of lists of tasks. Flatten it.            tasks = [t for inner in tasks for t in inner]        return tasks",
        "labels_text": "Send the importtaskcreated event for this task Return a list of task that should continue through the pipeline By default this is a list containing only the task itself but plugins can replace the task with new one"
    },
    {
        "input_text": "summarize: def lookup_candidates(self):                artist, album, prop = autotag.tag_album(            self.items, search_ids=self.search_ids        )        self.cur_artist = artist        self.cur_album = album        self.candidates = prop.candidates        self.rec = prop.recommendation",
        "labels_text": "Retrieve and store candidate for this album Userspecified candidate IDs are stored in selfsearchids if present the initial lookup is restricted to only those IDs"
    },
    {
        "input_text": "summarize: def choose_match(self, session):                choice = session.choose_match(self)        self.set_choice(choice)        session.log_choice(self)",
        "labels_text": "Ask the session which match should apply and apply it"
    },
    {
        "input_text": "summarize: def reload(self):                for item in self.imported_items():            item.load()        self.album.load()",
        "labels_text": "Reload album and item from the database"
    },
    {
        "input_text": "summarize: def prune(self, filename):                if self.toppath and not os.path.exists(syspath(filename)):            util.prune_dirs(                os.path.dirname(filename),                self.toppath,                clutter=config[\"clutter\"].as_str_seq(),            )",
        "labels_text": "Prune any empty directory above the given file If this task ha no toppath or the file path provided is not within the toppath then this function ha no effect Similarly if the file still exists no pruning is performed so it safe to call when the file in question may not have been removed"
    },
    {
        "input_text": "summarize: def chosen_info(self):                assert self.choice_flag in (action.ASIS, action.RETAG, action.APPLY)        if self.choice_flag in (action.ASIS, action.RETAG):            return dict(self.item)        elif self.choice_flag is action.APPLY:            return self.match.info.copy()",
        "labels_text": "Return a dictionary of metadata about the current choice May only be called when the choice flag is ASIS or RETAG in which case the data come from the file current metadata or APPLY in which case the data come from the choice"
    },
    {
        "input_text": "summarize: def choose_match(self, session):                choice = session.choose_item(self)        self.set_choice(choice)        session.log_choice(self)",
        "labels_text": "Ask the session which match should apply and apply it"
    },
    {
        "input_text": "summarize: def set_fields(self, lib):                for field, view in config[\"import\"][\"set_fields\"].items():            value = view.get()            log.debug(                \"Set field {1}={2} for {0}\",                displayable_path(self.paths),                field,                value,            )            self.item.set_parse(field, format(self.item, value))        self.item.store()",
        "labels_text": "Sets the field given at CLI or configuration to the specified value for the singleton item"
    },
    {
        "input_text": "summarize: def is_archive(cls, path):                if not os.path.isfile(path):            return False        for path_test, _ in cls.handlers():            if path_test(os.fsdecode(path)):                return True        return False",
        "labels_text": "Returns true if the given path point to an archive that can be handled"
    },
    {
        "input_text": "summarize: def cleanup(self, **kwargs):                if self.extracted:            log.debug(                \"Removing extracted directory: {0}\",                displayable_path(self.toppath),            )            shutil.rmtree(syspath(self.toppath))",
        "labels_text": "Removes the temporary directory the archive wa extracted to"
    },
    {
        "input_text": "summarize: def __init__(self, toppath, session):                self.toppath = toppath        self.session = session        self.skipped = 0  # Skipped due to incremental/resume.        self.imported = 0  # \"Real\" tasks created.        self.is_archive = ArchiveImportTask.is_archive(syspath(toppath))",
        "labels_text": "Create a new task factory toppath is the userspecified path to search for music to import session is the ImportSession which control how task are read from the directory"
    },
    {
        "input_text": "summarize: def _create(self, task):                if task:            tasks = task.handle_created(self.session)            self.imported += len(tasks)            return tasks        return []",
        "labels_text": "Handle a new task to be emitted by the factory Emit the importtaskcreated event and increment the imported count if the task is not skipped Return the same task If task is None do nothing"
    },
    {
        "input_text": "summarize: def paths(self):                if not os.path.isdir(syspath(self.toppath)):            yield [self.toppath], [self.toppath]        elif self.session.config[\"flat\"]:            paths = []            for dirs, paths_in_dir in albums_in_dir(self.toppath):                paths += paths_in_dir            yield [self.toppath], paths        else:            for dirs, paths in albums_in_dir(self.toppath):                yield dirs, paths",
        "labels_text": "Walk selftoppath and yield dirs file pair where file are individual music file and dirs the set of containing directory where the music wa found This can either be a recursive search in the ordinary case a single track when toppath is a file a single directory in flat mode"
    },
    {
        "input_text": "summarize: def singleton(self, path):                if self.session.already_imported(self.toppath, [path]):            log.debug(                \"Skipping previously-imported path: {0}\", displayable_path(path)            )            self.skipped += 1            return None        item = self.read_item(path)        if item:            return SingletonImportTask(self.toppath, item)        else:            return None",
        "labels_text": "Return a SingletonImportTask for the music file"
    },
    {
        "input_text": "summarize: def sentinel(self, paths=None):                return SentinelImportTask(self.toppath, paths)",
        "labels_text": "Return a SentinelImportTask indicating the end of a toplevel directory import"
    },
    {
        "input_text": "summarize: def read_item(self, path):                try:            return library.Item.from_path(path)        except library.ReadError as exc:            if isinstance(exc.reason, mediafile.FileTypeError):                # Silently ignore non-music files.                pass            elif isinstance(exc.reason, mediafile.UnreadableFileError):                log.warning(\"unreadable file: {0}\", displayable_path(path))            else:                log.error(\"error reading {0}: {1}\", displayable_path(path), exc)",
        "labels_text": "Return an Item read from the path If an item cannot be read return None instead and log an error"
    },
    {
        "input_text": "summarize: def import_asis(session, task):        if task.skip:        return    log.info(\"{}\", displayable_path(task.paths))    task.set_choice(action.ASIS)    apply_choice(session, task)",
        "labels_text": "Select the actionASIS choice for all task This stage replaces the initiallookup and userquery stage when the importer is run without autotagging"
    },
    {
        "input_text": "summarize: def plugin_stage(session, func, task):        if task.skip:        return    func(session, task)    # Stage may modify DB, so re-load cached item data.    # FIXME Importer plugins should not modify the database but instead    # the albums and items attached to tasks.    task.reload()",
        "labels_text": "A coroutine pipeline stage that call the given function with each nonskipped import task These stage occur between applying metadata change and movingcopyingwriting file"
    },
    {
        "input_text": "summarize: def log_files(session, task):        if isinstance(task, SingletonImportTask):        log.info(\"Singleton: {0}\", displayable_path(task.item[\"path\"]))    elif task.items:        log.info(\"Album: {0}\", displayable_path(task.paths[0]))        for item in task.items:            log.info(\"  {0}\", displayable_path(item[\"path\"]))",
        "labels_text": "A coroutine pipeline stage to log each file to be imported"
    },
    {
        "input_text": "summarize: def is_subdir_of_any_in_list(path, dirs):        ancestors = ancestry(path)    return any(d in ancestors for d in dirs)",
        "labels_text": "Returns True if path o a subdirectory of any directory in dirs a list In other case return False"
    },
    {
        "input_text": "summarize: def __init__(self, nullable=False):                self.nullable = nullable",
        "labels_text": "Create a path type object nullable control whether the type may be missing ie None"
    },
    {
        "input_text": "summarize: def __init__(self, path, reason):                super().__init__(path, reason)        self.path = path        self.reason = reason",
        "labels_text": "Create an exception describing an operation on the file at path with the underlying chained exception reason"
    },
    {
        "input_text": "summarize: def __str__(self):                return f\"{util.displayable_path(self.path)}: {self.reason}\"",
        "labels_text": "Get a string representing the error Describe both the underlying reason and the file path in question"
    },
    {
        "input_text": "summarize: def _get(self, key):                if self.for_path and key in self.album_keys:            return self._get_formatted(self.album, key)        elif key in self.model_keys:            return self._get_formatted(self.model, key)        elif key in self.album_keys:            return self._get_formatted(self.album, key)        else:            raise KeyError(key)",
        "labels_text": "Get the value for a key either from the album or the item Raise a KeyError for invalid key"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                value = self._get(key)        # `artist` and `albumartist` fields fall back to one another.        # This is helpful in path formats when the album artist is unset        # on as-is imports.        try:            if key == \"artist\" and not value:                return self._get(\"albumartist\")            elif key == \"albumartist\" and not value:                return self._get(\"artist\")        except KeyError:            pass        return value",
        "labels_text": "Get the value for a key artist and albumartist are fallback value for each other when not set"
    },
    {
        "input_text": "summarize: def relation_join(cls) -> str:                return (            f\"LEFT JOIN {cls._relation._table} \"            f\"ON {cls._table}.album_id = {cls._relation._table}.id\"        )",
        "labels_text": "Return the FROM clause which includes related album We need to use a LEFT JOIN here otherwise item that are not part of an album eg singleton would be left out"
    },
    {
        "input_text": "summarize: def filepath(self) -> Path | None:                return Path(os.fsdecode(self.path)) if self.path else self.path",
        "labels_text": "The path to the item file a pathlibPath"
    },
    {
        "input_text": "summarize: def _cached_album(self):                if not self.__album and self._db:            self.__album = self._db.get_album(self)        elif self.__album:            self.__album.load()        return self.__album",
        "labels_text": "The Album object that this item belongs to if any or None if the item is a singleton or is not associated with a library The instance is cached and refreshed on access DO NOT MODIFY If you want a copy to modify use methgetalbum"
    },
    {
        "input_text": "summarize: def from_path(cls, path):                # Initiate with values that aren't read from files.        i = cls(album_id=None)        i.read(path)        i.mtime = i.current_mtime()  # Initial mtime.        return i",
        "labels_text": "Create a new item from the medium file at the specified path"
    },
    {
        "input_text": "summarize: def __setitem__(self, key, value):                # Encode unicode paths and read buffers.        if key == \"path\":            if isinstance(value, str):                value = bytestring_path(value)            elif isinstance(value, BLOB_TYPE):                value = bytes(value)        elif key == \"album_id\":            self._cached_album = None        changed = super()._setitem(key, value)        if changed and key in MediaFile.fields():            self.mtime = 0",
        "labels_text": "Set the item value for a standard field or a flexattr"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                try:            return super().__getitem__(key)        except KeyError:            if self._cached_album:                return self._cached_album[key]            raise",
        "labels_text": "Get the value for a field falling back to the album if necessary Raise a KeyError if the field is not available"
    },
    {
        "input_text": "summarize: def keys(self, computed=False, with_album=True):                keys = super().keys(computed=computed)        if with_album and self._cached_album:            keys = set(keys)            keys.update(self._cached_album.keys(computed=computed))            keys = list(keys)        return keys",
        "labels_text": "Get a list of available field name withalbum control whether the album field are included"
    },
    {
        "input_text": "summarize: def get(self, key, default=None, with_album=True):                try:            return self._get(key, default, raise_=with_album)        except KeyError:            if self._cached_album:                return self._cached_album.get(key, default)            return default",
        "labels_text": "Get the value for a given key or default if it doe not exist Set withalbum to false to skip album fallback"
    },
    {
        "input_text": "summarize: def update(self, values):                super().update(values)        if self.mtime == 0 and \"mtime\" in values:            self.mtime = values[\"mtime\"]",
        "labels_text": "Set all keyvalue pair in the mapping If mtime is specified it is not reset a it might otherwise be"
    },
    {
        "input_text": "summarize: def clear(self):                for key in self._media_tag_fields:            setattr(self, key, None)",
        "labels_text": "Set all keyvalue pair to None"
    },
    {
        "input_text": "summarize: def get_album(self):                if not self._db:            return None        return self._db.get_album(self)",
        "labels_text": "Get the Album object that this item belongs to if any or None if the item is a singleton or is not associated with a library"
    },
    {
        "input_text": "summarize: def try_write(self, *args, **kwargs):                try:            self.write(*args, **kwargs)            return True        except FileOperationError as exc:            log.error(\"{0}\", exc)            return False",
        "labels_text": "Call write but catch and log FileOperationError exception Return False an exception wa caught and True otherwise"
    },
    {
        "input_text": "summarize: def try_sync(self, write, move, with_album=True):                if write:            self.try_write()        if move:            # Check whether this file is inside the library directory.            if self._db and self._db.directory in util.ancestry(self.path):                log.debug(                    \"moving {0} to synchronize path\",                    util.displayable_path(self.path),                )                self.move(with_album=with_album)        self.store()",
        "labels_text": "Synchronize the item with the database and possibly update it tag on disk and it path by moving the file write indicates whether to write new tag into the file Similarly move control whether the path should be updated In the latter case file are only moved when they are inside their library directory if any Similar to calling methwrite methmove and methstore conditionally"
    },
    {
        "input_text": "summarize: def current_mtime(self):                return int(os.path.getmtime(syspath(self.path)))",
        "labels_text": "Return the current mtime of the file rounded to the nearest integer"
    },
    {
        "input_text": "summarize: def try_filesize(self):                try:            return os.path.getsize(syspath(self.path))        except (OSError, Exception) as exc:            log.warning(\"could not get filesize: {0}\", exc)            return 0",
        "labels_text": "Get the size of the underlying file in byte If the file is missing return and log a warning"
    },
    {
        "input_text": "summarize: def relation_join(cls) -> str:                return (            f\"LEFT JOIN {cls._relation._table} \"            f\"ON {cls._table}.id = {cls._relation._table}.album_id\"        )",
        "labels_text": "Return FROM clause which join on related album item Use LEFT join to select all album including those that do not have any item"
    },
    {
        "input_text": "summarize: def items(self):                return self._db.items(dbcore.MatchQuery(\"album_id\", self.id))",
        "labels_text": "Return an iterable over the item associated with this album This method conflict with methLibModelitems which is inherited from methbeetsdbcoreModelitems Since methAlbumitems predates these method and is likely to be used by plugins we keep this interface asis"
    },
    {
        "input_text": "summarize: def remove(self, delete=False, with_items=True):                super().remove()        # Send a 'album_removed' signal to plugins        plugins.send(\"album_removed\", album=self)        # Delete art file.        if delete:            artpath = self.artpath            if artpath:                util.remove(artpath)        # Remove (and possibly delete) the constituent items.        if with_items:            for item in self.items():                item.remove(delete, False)",
        "labels_text": "Remove this album and all it associated item from the library If delete then the item file are also deleted from disk along with any album art The directory containing the album are also removed recursively if empty Set withitems to False to avoid removing the album item"
    },
    {
        "input_text": "summarize: def move(self, operation=MoveOperation.MOVE, basedir=None, store=True):                basedir = basedir or self._db.directory        # Ensure new metadata is available to items for destination        # computation.        if store:            self.store()        # Move items.        items = list(self.items())        for item in items:            item.move(operation, basedir=basedir, with_album=False, store=store)        # Move art.        self.move_art(operation)        if store:            self.store()",
        "labels_text": "Move copy link or hardlink depending on operation all item to their destination Any album art move along with them basedir override the library base directory for the destination operation should be an instance of utilMoveOperation By default the album is stored to the database persisting any modification to it metadata If store is False however the album is not stored automatically and it will have to be manually stored after invoking this method"
    },
    {
        "input_text": "summarize: def item_dir(self):                item = self.items().get()        if not item:            raise ValueError(\"empty album for album id %d\" % self.id)        return os.path.dirname(item.path)",
        "labels_text": "Return the directory containing the album first item provided that such an item exists"
    },
    {
        "input_text": "summarize: def _albumtotal(self):                if self.disctotal == 1 or not beets.config[\"per_disc_numbering\"]:            return self.items()[0].tracktotal        counted = []        total = 0        for item in self.items():            if item.disc in counted:                continue            total += item.tracktotal            counted.append(item.disc)            if len(counted) == self.disctotal:                break        return total",
        "labels_text": "Return the total number of track on all disc on the album"
    },
    {
        "input_text": "summarize: def try_sync(self, write, move, inherit=True):                self.store(inherit=inherit)        for item in self.items():            item.try_sync(write, move)",
        "labels_text": "Synchronize the album and it item with the database Optionally also write any new tag into the file and update their path write indicates whether to write tag to the item file and move control whether file both audio and album art are moved"
    },
    {
        "input_text": "summarize: def parse_query_string(s, model_cls):        message = f\"Query is not unicode: {s!r}\"    assert isinstance(s, str), message    try:        parts = shlex.split(s)    except ValueError as exc:        raise dbcore.InvalidQueryError(s, exc)    return parse_query_parts(parts, model_cls)",
        "labels_text": "Given a beet query string return the Query and Sort they represent The string is split into component using shelllike syntax"
    },
    {
        "input_text": "summarize: def add(self, obj):                obj.add(self)        self._memotable = {}        return obj.id",
        "labels_text": "Add the classItem or classAlbum object to the library database Return the object new id"
    },
    {
        "input_text": "summarize: def get_default_album_sort():                return dbcore.sort_from_strings(            Album, beets.config[\"sort_album\"].as_str_seq()        )",
        "labels_text": "Get a classSort object for album from the config option"
    },
    {
        "input_text": "summarize: def get_default_item_sort():                return dbcore.sort_from_strings(            Item, beets.config[\"sort_item\"].as_str_seq()        )",
        "labels_text": "Get a classSort object for item from the config option"
    },
    {
        "input_text": "summarize: def albums(self, query=None, sort=None) -> Results[Album]:                return self._fetch(Album, query, sort or self.get_default_album_sort())",
        "labels_text": "Get classAlbum object matching the query"
    },
    {
        "input_text": "summarize: def items(self, query=None, sort=None) -> Results[Item]:                return self._fetch(Item, query, sort or self.get_default_item_sort())",
        "labels_text": "Get classItem object matching the query"
    },
    {
        "input_text": "summarize: def get_item(self, id):                return self._get(Item, id)",
        "labels_text": "Fetch a classItem by it ID Return None if no match is found"
    },
    {
        "input_text": "summarize: def get_album(self, item_or_id):                if isinstance(item_or_id, int):            album_id = item_or_id        else:            album_id = item_or_id.album_id        if album_id is None:            return None        return self._get(Album, album_id)",
        "labels_text": "Given an album ID or an item associated with an album return a classAlbum object for the album If no such album exists return None"
    },
    {
        "input_text": "summarize: def _int_arg(s):        return int(s.strip())",
        "labels_text": "Convert a string argument to an integer for use in a template function May raise a ValueError"
    },
    {
        "input_text": "summarize: def _func_names(cls) -> list[str]:                return [s for s in dir(cls) if s.startswith(cls._prefix)]",
        "labels_text": "Names of tmpl function in this class"
    },
    {
        "input_text": "summarize: def __init__(self, item=None, lib=None):                self.item = item        self.lib = lib",
        "labels_text": "Parametrize the function If item or lib is None then some function namely aunique will always evaluate to the empty string"
    },
    {
        "input_text": "summarize: def functions(self):                out = {}        for key in self._func_names:            out[key[len(self._prefix) :]] = getattr(self, key)        return out",
        "labels_text": "Return a dictionary containing the function defined in this object The key are function name a exposed in template and the value are Python function"
    },
    {
        "input_text": "summarize: def tmpl_lower(s):                return s.lower()",
        "labels_text": "Convert a string to lower case"
    },
    {
        "input_text": "summarize: def tmpl_upper(s):                return s.upper()",
        "labels_text": "Convert a string to upper case"
    },
    {
        "input_text": "summarize: def tmpl_capitalize(s):                return s.capitalize()",
        "labels_text": "Converts to a capitalized string"
    },
    {
        "input_text": "summarize: def tmpl_title(s):                return string.capwords(s)",
        "labels_text": "Convert a string to title case"
    },
    {
        "input_text": "summarize: def tmpl_left(s, chars):                return s[0 : _int_arg(chars)]",
        "labels_text": "Get the leftmost character of a string"
    },
    {
        "input_text": "summarize: def tmpl_right(s, chars):                return s[-_int_arg(chars) :]",
        "labels_text": "Get the rightmost character of a string"
    },
    {
        "input_text": "summarize: def tmpl_if(condition, trueval, falseval=\"\"):                try:            int_condition = _int_arg(condition)        except ValueError:            if condition.lower() == \"false\":                return falseval        else:            condition = int_condition        if condition:            return trueval        else:            return falseval",
        "labels_text": "If condition is nonempty and nonzero emit trueval otherwise emit falseval if provided"
    },
    {
        "input_text": "summarize: def tmpl_asciify(s):                return util.asciify_path(s, beets.config[\"path_sep_replace\"].as_str())",
        "labels_text": "Translate nonASCII character to their ASCII equivalent"
    },
    {
        "input_text": "summarize: def tmpl_time(s, fmt):                cur_fmt = beets.config[\"time_format\"].as_str()        return time.strftime(fmt, time.strptime(s, cur_fmt))",
        "labels_text": "Format a time value using strftime"
    },
    {
        "input_text": "summarize: def _tmpl_unique_memokey(self, name, keys, disam, item_id):                return (name, keys, disam, item_id)",
        "labels_text": "Get the memokey for the unique template named name for the specific parameter"
    },
    {
        "input_text": "summarize: def tmpl_first(s, count=1, skip=0, sep=\"; \", join_str=\"; \"):                skip = int(skip)        count = skip + int(count)        return join_str.join(s.split(sep)[skip:count])",
        "labels_text": "Get the item from x to y in a string separated by something and join then with something Args s the string count The number of item included skip The number of item skipped sep the separator Usually is default or joinstr the string which will join the item default"
    },
    {
        "input_text": "summarize: def tmpl_ifdef(self, field, trueval=\"\", falseval=\"\"):                if field in self.item:            return trueval if trueval else self.item.formatted().get(field)        else:            return falseval",
        "labels_text": "If field exists return trueval or the field default otherwise emit return falseval if provided Args field The name of the field trueval The string if the condition is true falseval The string if the condition is false Returns The string based on condition"
    },
    {
        "input_text": "summarize: def _log(        self,        level,        msg,        args,        exc_info=None,        extra=None,        stack_info=False,        **kwargs,    ):                m = self._LogMessage(msg, args, kwargs)        stacklevel = kwargs.pop(\"stacklevel\", 1)        stacklevel = {\"stacklevel\": stacklevel}        return super()._log(            level,            m,            (),            exc_info=exc_info,            extra=extra,            stack_info=stack_info,            **stacklevel,        )",
        "labels_text": "Log msgformatargs kwargs"
    },
    {
        "input_text": "summarize: def set_global_level(self, level):                self.default_level = level        self.setLevel(level)",
        "labels_text": "Set the level on the current thread the default value for all thread"
    },
    {
        "input_text": "summarize: def commands(self):                return ()",
        "labels_text": "Should return a list of beetsuiSubcommand object for command that should be added to beet CLI"
    },
    {
        "input_text": "summarize: def _set_stage_log_level(self, stages):                return [            self._set_log_level_and_params(logging.WARNING, stage)            for stage in stages        ]",
        "labels_text": "Adjust all the stage in stage to WARNING logging level"
    },
    {
        "input_text": "summarize: def get_early_import_stages(self):                return self._set_stage_log_level(self.early_import_stages)",
        "labels_text": "Return a list of function that should be called a importer pipeline stage early in the pipeline The callables are wrapped version of the function in selfearlyimportstages Wrapping provides some bookkeeping for the plugin specifically the logging level is adjusted to WARNING"
    },
    {
        "input_text": "summarize: def get_import_stages(self):                return self._set_stage_log_level(self.import_stages)",
        "labels_text": "Return a list of function that should be called a importer pipeline stage The callables are wrapped version of the function in selfimportstages Wrapping provides some bookkeeping for the plugin specifically the logging level is adjusted to WARNING"
    },
    {
        "input_text": "summarize: def queries(self):                return {}",
        "labels_text": "Return a dict mapping prefix to Query subclass"
    },
    {
        "input_text": "summarize: def track_distance(self, item, info):                return beets.autotag.hooks.Distance()",
        "labels_text": "Should return a Distance object to be added to the distance for every track comparison"
    },
    {
        "input_text": "summarize: def album_distance(self, items, album_info, mapping):                return beets.autotag.hooks.Distance()",
        "labels_text": "Should return a Distance object to be added to the distance for every albumlevel comparison"
    },
    {
        "input_text": "summarize: def candidates(self, items, artist, album, va_likely, extra_tags=None):                return ()",
        "labels_text": "Should return a sequence of AlbumInfo object that match the album whose item are provided"
    },
    {
        "input_text": "summarize: def item_candidates(self, item, artist, title):                return ()",
        "labels_text": "Should return a sequence of TrackInfo object that match the item provided"
    },
    {
        "input_text": "summarize: def album_for_id(self, album_id):                return None",
        "labels_text": "Return an AlbumInfo object or None if no matching release wa found"
    },
    {
        "input_text": "summarize: def track_for_id(self, track_id):                return None",
        "labels_text": "Return a TrackInfo object or None if no matching release wa found"
    },
    {
        "input_text": "summarize: def add_media_field(self, name, descriptor):                # Defer import to prevent circular dependency        from beets import library        mediafile.MediaFile.add_field(name, descriptor)        library.Item._media_fields.add(name)",
        "labels_text": "Add a field that is synchronized between medium file and item When a medium field is added itemwrite will set the name property of the item MediaFile to itemname and save the change Similarly itemread will set itemname to the value of the name property of the medium file descriptor must be an instance of mediafileMediaField"
    },
    {
        "input_text": "summarize: def register_listener(self, event, func):                wrapped_func = self._set_log_level_and_params(logging.WARNING, func)        cls = self.__class__        if cls.listeners is None or cls._raw_listeners is None:            cls._raw_listeners = defaultdict(list)            cls.listeners = defaultdict(list)        if func not in cls._raw_listeners[event]:            cls._raw_listeners[event].append(func)            cls.listeners[event].append(wrapped_func)",
        "labels_text": "Add a function a a listener for the specified event"
    },
    {
        "input_text": "summarize: def template_func(cls, name):                def helper(func):            if cls.template_funcs is None:                cls.template_funcs = {}            cls.template_funcs[name] = func            return func        return helper",
        "labels_text": "Decorator that register a path template function The function will be invoked a name from path format string"
    },
    {
        "input_text": "summarize: def template_field(cls, name):                def helper(func):            if cls.template_fields is None:                cls.template_fields = {}            cls.template_fields[name] = func            return func        return helper",
        "labels_text": "Decorator that register a path template field computation The value will be referenced a name from path format string The function must accept a single parameter the Item being formatted"
    },
    {
        "input_text": "summarize: def find_plugins():        if _instances:        # After the first call, use cached instances for performance reasons.        # See https://github.com/beetbox/beets/pull/3810        return list(_instances.values())    load_plugins()    plugins = []    for cls in _classes:        # Only instantiate each plugin class once.        if cls not in _instances:            _instances[cls] = cls()        plugins.append(_instances[cls])    return plugins",
        "labels_text": "Returns a list of BeetsPlugin subclass instance from all currently loaded beet plugins Loads the default plugin set first"
    },
    {
        "input_text": "summarize: def commands():        out = []    for plugin in find_plugins():        out += plugin.commands()    return out",
        "labels_text": "Returns a list of Subcommand object from all loaded plugins"
    },
    {
        "input_text": "summarize: def queries():        out = {}    for plugin in find_plugins():        out.update(plugin.queries())    return out",
        "labels_text": "Returns a dict mapping prefix string to Query subclass all loaded plugins"
    },
    {
        "input_text": "summarize: def track_distance(item, info):        from beets.autotag.hooks import Distance    dist = Distance()    for plugin in find_plugins():        dist.update(plugin.track_distance(item, info))    return dist",
        "labels_text": "Gets the track distance calculated by all loaded plugins Returns a Distance object"
    },
    {
        "input_text": "summarize: def album_distance(items, album_info, mapping):        from beets.autotag.hooks import Distance    dist = Distance()    for plugin in find_plugins():        dist.update(plugin.album_distance(items, album_info, mapping))    return dist",
        "labels_text": "Returns the album distance calculated by plugins"
    },
    {
        "input_text": "summarize: def candidates(items, artist, album, va_likely, extra_tags=None):        for plugin in find_plugins():        yield from plugin.candidates(            items, artist, album, va_likely, extra_tags        )",
        "labels_text": "Gets MusicBrainz candidate for an album from each plugin"
    },
    {
        "input_text": "summarize: def item_candidates(item, artist, title):        for plugin in find_plugins():        yield from plugin.item_candidates(item, artist, title)",
        "labels_text": "Gets MusicBrainz candidate for an item from the plugins"
    },
    {
        "input_text": "summarize: def album_for_id(album_id):        for plugin in find_plugins():        album = plugin.album_for_id(album_id)        if album:            yield album",
        "labels_text": "Get AlbumInfo object for a given ID string"
    },
    {
        "input_text": "summarize: def track_for_id(track_id):        for plugin in find_plugins():        track = plugin.track_for_id(track_id)        if track:            yield track",
        "labels_text": "Get TrackInfo object for a given ID string"
    },
    {
        "input_text": "summarize: def template_funcs():        funcs = {}    for plugin in find_plugins():        if plugin.template_funcs:            funcs.update(plugin.template_funcs)    return funcs",
        "labels_text": "Get all the template function declared by plugins a a dictionary"
    },
    {
        "input_text": "summarize: def early_import_stages():        stages = []    for plugin in find_plugins():        stages += plugin.get_early_import_stages()    return stages",
        "labels_text": "Get a list of early import stage function defined by plugins"
    },
    {
        "input_text": "summarize: def import_stages():        stages = []    for plugin in find_plugins():        stages += plugin.get_import_stages()    return stages",
        "labels_text": "Get a list of import stage function defined by plugins"
    },
    {
        "input_text": "summarize: def _check_conflicts_and_merge(plugin, plugin_funcs, funcs):        if plugin_funcs:        if not plugin_funcs.keys().isdisjoint(funcs.keys()):            conflicted_fields = \", \".join(plugin_funcs.keys() & funcs.keys())            raise PluginConflictException(                f\"Plugin {plugin.name} defines template functions for \"                f\"{conflicted_fields} that conflict with another plugin.\"            )        funcs.update(plugin_funcs)",
        "labels_text": "Check the provided template function for conflict and merge into funcs Raises a PluginConflictException if a plugin defines template function for field that another plugin ha already defined template function for"
    },
    {
        "input_text": "summarize: def item_field_getters():        funcs = {}    for plugin in find_plugins():        _check_conflicts_and_merge(plugin, plugin.template_fields, funcs)    return funcs",
        "labels_text": "Get a dictionary mapping field name to unary function that compute the field value"
    },
    {
        "input_text": "summarize: def album_field_getters():        funcs = {}    for plugin in find_plugins():        _check_conflicts_and_merge(plugin, plugin.album_template_fields, funcs)    return funcs",
        "labels_text": "As above for album field"
    },
    {
        "input_text": "summarize: def event_handlers():        all_handlers = defaultdict(list)    for plugin in find_plugins():        if plugin.listeners:            for event, handlers in plugin.listeners.items():                all_handlers[event] += handlers    return all_handlers",
        "labels_text": "Find all event handler from plugins a a dictionary mapping event name to sequence of callables"
    },
    {
        "input_text": "summarize: def send(event, **arguments):        log.debug(\"Sending event: {0}\", event)    results = []    for handler in event_handlers()[event]:        result = handler(**arguments)        if result is not None:            results.append(result)    return results",
        "labels_text": "Send an event to all assigned event listener event is the name of the event to send all other named argument are passed along to the handler Return a list of nonNone value returned from the handler"
    },
    {
        "input_text": "summarize: def feat_tokens(for_artist=True):        feat_words = [\"ft\", \"featuring\", \"feat\", \"feat.\", \"ft.\"]    if for_artist:        feat_words += [\"with\", \"vs\", \"and\", \"con\", \"&\"]    return r\"(?<=\\s)(?:{})(?=\\s)\".format(        \"|\".join(re.escape(x) for x in feat_words)    )",
        "labels_text": "Return a regular expression that match phrase like featuring that separate a main artist or a song title from secondary artist The forartist option determines whether the regex should be suitable for matching artist field the default or title field"
    },
    {
        "input_text": "summarize: def sanitize_choices(choices, choices_all):        seen = set()    others = [x for x in choices_all if x not in choices]    res = []    for s in choices:        if s not in seen:            if s in list(choices_all):                res.append(s)            elif s == \"*\":                res.extend(others)        seen.add(s)    return res",
        "labels_text": "Clean up a stringlist configuration attribute keep only choice element present in choicesall remove duplicate element expand wildcard while keeping original stringlist order"
    },
    {
        "input_text": "summarize: def notify_info_yielded(event):        def decorator(generator):        def decorated(*args, **kwargs):            for v in generator(*args, **kwargs):                send(event, info=v)                yield v        return decorated    return decorator",
        "labels_text": "Makes a generator send the event event every time it yield This decorator is supposed to decorate a generator but any function returning an iterable should work Each yielded value is passed to plugins using the info parameter of send"
    },
    {
        "input_text": "summarize: def get_distance(config, data_source, info):        dist = beets.autotag.Distance()    if info.data_source == data_source:        dist.add(\"source\", config[\"source_weight\"].as_number())    return dist",
        "labels_text": "Returns the datasource weight and the maximum source weight for album or individual track"
    },
    {
        "input_text": "summarize: def apply_item_changes(lib, item, move, pretend, write):        if pretend:        return    from beets import util    # Move the item if it's in the library.    if move and lib.directory in util.ancestry(item.path):        item.move(with_album=False)    if write:        item.try_write()    item.store()",
        "labels_text": "Store move and write the item according to the argument param lib beet library type lib beetslibraryLibrary param item Item whose change to apply type item beetslibraryItem param move Move the item if it in the library type move bool param pretend Return without moving writing or storing the item metadata type pretend bool param write Write the item metadata to it medium file type write bool"
    },
    {
        "input_text": "summarize: def _get_id(url_type, id_, id_regex):                log.debug(\"Extracting {} ID from '{}'\", url_type, id_)        match = re.search(id_regex[\"pattern\"].format(url_type), str(id_))        if match:            id_ = match.group(id_regex[\"match_group\"])            if id_:                return id_        return None",
        "labels_text": "Parse an ID from it URL if necessary param urltype Type of URL Either album or track type urltype str param id Albumtrack ID or URL type id str param idregex A dictionary containing a regular expression extracting an ID from an URL if it not an ID already in pattern and the number of the match group in matchgroup type idregex dict return Albumtrack ID rtype str"
    },
    {
        "input_text": "summarize: def candidates(self, items, artist, album, va_likely, extra_tags=None):                query_filters = {\"album\": album}        if not va_likely:            query_filters[\"artist\"] = artist        results = self._search_api(query_type=\"album\", filters=query_filters)        albums = [self.album_for_id(album_id=r[\"id\"]) for r in results]        return [a for a in albums if a is not None]",
        "labels_text": "Returns a list of AlbumInfo object for Search API result matching an album and artist if not various param item List of item comprised by an album to be matched type item listbeetslibraryItem param artist The artist of the album to be matched type artist str param album The name of the album to be matched type album str param valikely True if the album to be matched likely ha Various Artists type valikely bool return Candidate AlbumInfo object rtype listbeetsautotaghooksAlbumInfo"
    },
    {
        "input_text": "summarize: def item_candidates(self, item, artist, title):                tracks = self._search_api(            query_type=\"track\", keywords=title, filters={\"artist\": artist}        )        return [self.track_for_id(track_data=track) for track in tracks]",
        "labels_text": "Returns a list of TrackInfo object for Search API result matching title and artist param item Singleton item to be matched type item beetslibraryItem param artist The artist of the track to be matched type artist str param title The title of the track to be matched type title str return Candidate TrackInfo object rtype listbeetsautotaghooksTrackInfo"
    },
    {
        "input_text": "summarize: def _length(obj, album):        if album:        return sum(i.length for i in obj.items())    else:        return obj.length",
        "labels_text": "Get the duration of an item or album"
    },
    {
        "input_text": "summarize: def _take(iter, num):        out = []    for val in iter:        out.append(val)        num -= 1        if num <= 0:            break    return out",
        "labels_text": "Return a list containing the first num value in iter or fewer if the iterable end early"
    },
    {
        "input_text": "summarize: def _take_time(iter, secs, album):        out = []    total_time = 0.0    for obj in iter:        length = _length(obj, album)        if total_time + length <= secs:            out.append(obj)            total_time += length    return out",
        "labels_text": "Return a list containing the first value in iter which should be Item or Album object that add up to the given amount of time in second"
    },
    {
        "input_text": "summarize: def _insert(node, path, itemid):        if len(path) == 1:        # Last component. Insert file.        node.files[path[0]] = itemid    else:        # In a directory.        dirname = path[0]        rest = path[1:]        if dirname not in node.dirs:            node.dirs[dirname] = Node({}, {})        _insert(node.dirs[dirname], rest, itemid)",
        "labels_text": "Insert an item into a virtual filesystem node"
    },
    {
        "input_text": "summarize: def libtree(lib):        root = Node({}, {})    for item in lib.items():        dest = item.destination(fragment=True)        parts = util.components(dest)        _insert(root, parts, item.id)    return root",
        "labels_text": "Generates a filesystemlike directory tree for the file contained in lib Filesystem node are file dirs named tuples in which both component are dictionary The first map filename to Item id The second map directory name to child node tuples"
    },
    {
        "input_text": "summarize: def _weights(cls) -> Dict[str, float]:  # noqa: N805                weights_view = config[\"match\"][\"distance_weights\"]        weights = {}        for key in weights_view.keys():            weights[key] = weights_view[key].as_number()        return weights",
        "labels_text": "A dictionary from key to floatingpoint weight"
    },
    {
        "input_text": "summarize: def distance(self) -> float:                dist_max = self.max_distance        if dist_max:            return self.raw_distance / self.max_distance        return 0.0",
        "labels_text": "Return a weighted and normalized distance across all penalty"
    },
    {
        "input_text": "summarize: def max_distance(self) -> float:                dist_max = 0.0        for key, penalty in self._penalties.items():            dist_max += len(penalty) * self._weights[key]        return dist_max",
        "labels_text": "Return the maximum distance penalty normalization factor"
    },
    {
        "input_text": "summarize: def raw_distance(self) -> float:                dist_raw = 0.0        for key, penalty in self._penalties.items():            dist_raw += sum(penalty) * self._weights[key]        return dist_raw",
        "labels_text": "Return the raw denormalized distance"
    },
    {
        "input_text": "summarize: def __getitem__(self, key) -> float:                dist = sum(self._penalties[key]) * self._weights[key]        dist_max = self.max_distance        if dist_max:            return dist / dist_max        return 0.0",
        "labels_text": "Returns the weighted distance for a named penalty"
    },
    {
        "input_text": "summarize: def update(self, dist: \"Distance\"):                if not isinstance(dist, Distance):            raise ValueError(                \"`dist` must be a Distance object, not {}\".format(type(dist))            )        for key, penalties in dist._penalties.items():            self._penalties.setdefault(key, []).extend(penalties)",
        "labels_text": "Adds all the distance penalty from dist"
    },
    {
        "input_text": "summarize: def _eq(self, value1: Union[re.Pattern[str], Any], value2: Any) -> bool:                if isinstance(value1, re.Pattern):            value2 = cast(str, value2)            return bool(value1.match(value2))        return value1 == value2",
        "labels_text": "Returns True if value is equal to value value may be a compiled regular expression in which case it will be matched against value"
    },
    {
        "input_text": "summarize: def add(self, key: str, dist: float):                if not 0.0 <= dist <= 1.0:            raise ValueError(f\"`dist` must be between 0.0 and 1.0, not {dist}\")        self._penalties.setdefault(key, []).append(dist)",
        "labels_text": "Adds a distance penalty key must correspond with a configured weight setting dist must be a float between and and will be added to any existing distance penalty for the same key"
    },
    {
        "input_text": "summarize: def add_equality(        self,        key: str,        value: Any,        options: Union[List[Any], Tuple[Any, ...], Any],    ):                if not isinstance(options, (list, tuple)):            options = [options]        for opt in options:            if self._eq(opt, value):                dist = 0.0                break        else:            dist = 1.0        self.add(key, dist)",
        "labels_text": "Adds a distance penalty of if value doesnt match any of the value in option If an option is a compiled regular expression it will be considered equal if it match against value"
    },
    {
        "input_text": "summarize: def add_expr(self, key: str, expr: bool):                if expr:            self.add(key, 1.0)        else:            self.add(key, 0.0)",
        "labels_text": "Adds a distance penalty of if expr evaluates to True or"
    },
    {
        "input_text": "summarize: def add_number(self, key: str, number1: int, number2: int):                diff = abs(number1 - number2)        if diff:            for i in range(diff):                self.add(key, 1.0)        else:            self.add(key, 0.0)",
        "labels_text": "Adds a distance penalty of for each number of difference between number and number or when there is no difference Use this when there is no upper limit on the difference between the two number"
    },
    {
        "input_text": "summarize: def add_priority(        self,        key: str,        value: Any,        options: Union[List[Any], Tuple[Any, ...], Any],    ):                if not isinstance(options, (list, tuple)):            options = [options]        unit = 1.0 / (len(options) or 1)        for i, opt in enumerate(options):            if self._eq(opt, value):                dist = i * unit                break        else:            dist = 1.0        self.add(key, dist)",
        "labels_text": "Adds a distance penalty that corresponds to the position at which value appears in option A distance penalty of for the first option or if there is no matching option If an option is a compiled regular expression it will be considered equal if it match against value"
    },
    {
        "input_text": "summarize: def add_ratio(        self,        key: str,        number1: Union[int, float],        number2: Union[int, float],    ):                number = float(max(min(number1, number2), 0))        if number2:            dist = number / number2        else:            dist = 0.0        self.add(key, dist)",
        "labels_text": "Adds a distance penalty for number a a ratio of number number is bound at and number"
    },
    {
        "input_text": "summarize: def add_string(self, key: str, str1: Optional[str], str2: Optional[str]):                dist = string_dist(str1, str2)        self.add(key, dist)",
        "labels_text": "Adds a distance penalty based on the edit distance between str and str"
    },
    {
        "input_text": "summarize: def album_for_mbid(release_id: str) -> Optional[AlbumInfo]:        try:        album = mb.album_for_id(release_id)        if album:            plugins.send(\"albuminfo_received\", info=album)        return album    except mb.MusicBrainzAPIError as exc:        exc.log(log)        return None",
        "labels_text": "Get an AlbumInfo object for a MusicBrainz release ID Return None if the ID is not found"
    },
    {
        "input_text": "summarize: def track_for_mbid(recording_id: str) -> Optional[TrackInfo]:        try:        track = mb.track_for_id(recording_id)        if track:            plugins.send(\"trackinfo_received\", info=track)        return track    except mb.MusicBrainzAPIError as exc:        exc.log(log)        return None",
        "labels_text": "Get a TrackInfo object for a MusicBrainz recording ID Return None if the ID is not found"
    },
    {
        "input_text": "summarize: def albums_for_id(album_id: str) -> Iterable[AlbumInfo]:        a = album_for_mbid(album_id)    if a:        yield a    for a in plugins.album_for_id(album_id):        if a:            plugins.send(\"albuminfo_received\", info=a)            yield a",
        "labels_text": "Get a list of album for an ID"
    },
    {
        "input_text": "summarize: def tracks_for_id(track_id: str) -> Iterable[TrackInfo]:        t = track_for_mbid(track_id)    if t:        yield t    for t in plugins.track_for_id(track_id):        if t:            plugins.send(\"trackinfo_received\", info=t)            yield t",
        "labels_text": "Get a list of track for an ID"
    },
    {
        "input_text": "summarize: def item_candidates(item: Item, artist: str, title: str) -> Iterable[Tuple]:        # MusicBrainz candidates.    if config[\"musicbrainz\"][\"enabled\"] and artist and title:        yield from invoke_mb(mb.match_track, artist, title)    # Plugin candidates.    yield from plugins.item_candidates(item, artist, title)",
        "labels_text": "Search for item match item is the Item to be matched artist and title are string and either reflect the item or are specified by the user"
    },
    {
        "input_text": "summarize: def track_index_changed(item: Item, track_info: TrackInfo) -> bool:        return item.track not in (track_info.medium_index, track_info.index)",
        "labels_text": "Returns True if the item and track info index is different Tolerates per disc and per release numbering"
    },
    {
        "input_text": "summarize: def _sort_candidates(candidates: Iterable[AnyMatch]) -> Sequence[AnyMatch]:        return sorted(candidates, key=lambda match: match.distance)",
        "labels_text": "Sort candidate by distance"
    },
    {
        "input_text": "summarize: def _flatten_artist_credit(credit: List[Dict]) -> Tuple[str, str, str]:        artist_parts, artist_sort_parts, artist_credit_parts = _multi_artist_credit(        credit, include_join_phrase=True    )    return (        \"\".join(artist_parts),        \"\".join(artist_sort_parts),        \"\".join(artist_credit_parts),    )",
        "labels_text": "Given a list representing an artistcredit block flatten the data into a triple of joined artist name string canonical sort and credit"
    },
    {
        "input_text": "summarize: def _artist_ids(credit: List[Dict]) -> List[str]:        artist_ids: List[str] = []    for el in credit:        if isinstance(el, dict):            artist_ids.append(el[\"artist\"][\"id\"])    return artist_ids",
        "labels_text": "Given a list representing an artistcredit return a list of artist IDs"
    },
    {
        "input_text": "summarize: def _get_related_artist_names(relations, relation_type):        related_artists = []    for relation in relations:        if relation[\"type\"] == relation_type:            related_artists.append(relation[\"artist\"][\"name\"])    return \", \".join(related_artists)",
        "labels_text": "Given a list representing the artist relationship extract the name of the remixers and concatenate them"
    },
    {
        "input_text": "summarize: def _parse_id(s: str) -> Optional[str]:        # Find the first thing that looks like a UUID/MBID.    match = re.search(\"[a-f0-9]{8}(-[a-f0-9]{4}){3}-[a-f0-9]{12}\", s)    if match is not None:        return match.group() if match else None    return None",
        "labels_text": "Search for a MusicBrainz ID in the given string and return it If no ID can be found return None"
    },
    {
        "input_text": "summarize: def apply_album_metadata(album_info: AlbumInfo, album: Album):        _apply_metadata(album_info, album)",
        "labels_text": "Set the album metadata to match the AlbumInfo object"
    },
    {
        "input_text": "summarize: def get(  # type: ignore        self,        key: str,        default: Optional[str] = None,    ) -> str:                if default is None:            default = self.model._type(key).format(None)        return super().get(key, default)",
        "labels_text": "Similar to Mappinggetkey default but always format to str"
    },
    {
        "input_text": "summarize: def __init__(self, model_cls: \"Model\"):                # FIXME: Dict[str, SQLiteType]        self._data: Dict[str, Any] = {}        self.model_cls = model_cls        self._converted: Dict[str, Any] = {}",
        "labels_text": "Initialize the object empty"
    },
    {
        "input_text": "summarize: def init(self, data: Dict[str, Any]):                self._data = data",
        "labels_text": "Set the base data that should be lazily converted"
    },
    {
        "input_text": "summarize: def _convert(self, key: str, value: Any):                return self.model_cls._type(key).from_sql(value)",
        "labels_text": "Convert the attribute type according to the SQL type"
    },
    {
        "input_text": "summarize: def __setitem__(self, key: str, value: Any):                self._converted[key] = value",
        "labels_text": "Set an attribute value assume it already converted"
    },
    {
        "input_text": "summarize: def __getitem__(self, key: str) -> Any:                if key in self._converted:            return self._converted[key]        elif key in self._data:            value = self._convert(key, self._data[key])            self._converted[key] = value            return value",
        "labels_text": "Get an attribute value converting the type on demand if needed"
    },
    {
        "input_text": "summarize: def __delitem__(self, key: str):                if key in self._converted:            del self._converted[key]        if key in self._data:            del self._data[key]",
        "labels_text": "Delete both converted and base data"
    },
    {
        "input_text": "summarize: def keys(self) -> List[str]:                return list(self._converted.keys()) + list(self._data.keys())",
        "labels_text": "Get a list of available field name for this object"
    },
    {
        "input_text": "summarize: def copy(self) -> LazyConvertDict:                new = self.__class__(self.model_cls)        new._data = self._data.copy()        new._converted = self._converted.copy()        return new",
        "labels_text": "Create a copy of the object"
    },
    {
        "input_text": "summarize: def update(self, values: Mapping[str, Any]):                for key, value in values.items():            self[key] = value",
        "labels_text": "Assign all value in the given dict"
    },
    {
        "input_text": "summarize: def items(self) -> Iterable[Tuple[str, Any]]:                for key in self:            yield key, self[key]",
        "labels_text": "Iterate over key value pair that this object contains Computed field are not included"
    },
    {
        "input_text": "summarize: def get(self, key: str, default: Optional[Any] = None):                if key in self:            return self[key]        else:            return default",
        "labels_text": "Get the value for a given key or default if it doe not exist"
    },
    {
        "input_text": "summarize: def __contains__(self, key: Any) -> bool:                return key in self._converted or key in self._data",
        "labels_text": "Determine whether key is an attribute on this object"
    },
    {
        "input_text": "summarize: def __iter__(self) -> Iterator[str]:                # NOTE: It would be nice to use the following:        # yield from self._converted        # yield from self._data        # but that won't work since some code relies on modifying `self`        # during iteration.        return iter(self.keys())",
        "labels_text": "Iterate over the available field name excluding computed field"
    },
    {
        "input_text": "summarize: def _relation(cls) -> type[Model]:                return cls",
        "labels_text": "The model that this model is closely related to"
    },
    {
        "input_text": "summarize: def relation_join(cls) -> str:                return \"\"",
        "labels_text": "Return the join required to include the related table in the query This is intended to be used a a FROM clause in the SQL query"
    },
    {
        "input_text": "summarize: def other_db_fields(cls) -> set[str]:                return cls._relation._fields.keys() - cls.shared_db_fields",
        "labels_text": "Fields in the related table"
    },
    {
        "input_text": "summarize: def _getters(cls: Type[\"Model\"]):                # We could cache this if it becomes a performance problem to        # gather the getter mapping every time.        raise NotImplementedError()",
        "labels_text": "Return a mapping from field name to getter function"
    },
    {
        "input_text": "summarize: def _template_funcs(self) -> Mapping[str, Callable[[str], str]]:                # As above: we could consider caching this result.        raise NotImplementedError()",
        "labels_text": "Return a mapping from function name to texttransformer function"
    },
    {
        "input_text": "summarize: def __init__(self, db: Optional[Database] = None, **values):                self._db = db        self._dirty: set[str] = set()        self._values_fixed = LazyConvertDict(self)        self._values_flex = LazyConvertDict(self)        # Initial contents.        self.update(values)        self.clear_dirty()",
        "labels_text": "Create a new object with an optional Database association and initial field value"
    },
    {
        "input_text": "summarize: def _awaken(        cls: Type[AnyModel],        db: Optional[Database] = None,        fixed_values: Dict[str, Any] = {},        flex_values: Dict[str, Any] = {},    ) -> AnyModel:                obj = cls(db)        obj._values_fixed.init(fixed_values)        obj._values_flex.init(flex_values)        return obj",
        "labels_text": "Create an object with value drawn from the database This is a performance optimization the check involved with ordinary construction are bypassed"
    },
    {
        "input_text": "summarize: def clear_dirty(self):                self._dirty = set()        if self._db:            self._revision = self._db.revision",
        "labels_text": "Mark all field a clean ie not needing to be stored to the database Also update the revision"
    },
    {
        "input_text": "summarize: def _check_db(self, need_id: bool = True) -> Database:                if not self._db:            raise ValueError(\"{} has no database\".format(type(self).__name__))        if need_id and not self.id:            raise ValueError(\"{} has no id\".format(type(self).__name__))        return self._db",
        "labels_text": "Ensure that this object is associated with a database row it ha a reference to a database db and an id A ValueError exception is raised otherwise"
    },
    {
        "input_text": "summarize: def copy(self) -> \"Model\":                new = self.__class__()        new._db = self._db        new._values_fixed = self._values_fixed.copy()        new._values_flex = self._values_flex.copy()        new._dirty = self._dirty.copy()        return new",
        "labels_text": "Create a copy of the model object The field value and other state is duplicated but the new copy remains associated with the same database a the old object A simple copydeepcopy will not work because it would try to duplicate the SQLite connection"
    },
    {
        "input_text": "summarize: def _type(cls, key) -> types.Type:                return cls._fields.get(key) or cls._types.get(key) or types.DEFAULT",
        "labels_text": "Get the type of a field a Type instance If the field ha no explicit type it is given the base Type which doe no conversion"
    },
    {
        "input_text": "summarize: def __getitem__(self, key):                return self._get(key, raise_=True)",
        "labels_text": "Get the value for a field Raise a KeyError if the field is not available"
    },
    {
        "input_text": "summarize: def __setitem__(self, key, value):                self._setitem(key, value)",
        "labels_text": "Assign the value for a field"
    },
    {
        "input_text": "summarize: def keys(self, computed: bool = False):                base_keys = list(self._fields) + list(self._values_flex.keys())        if computed:            return base_keys + list(self._getters().keys())        else:            return base_keys",
        "labels_text": "Get a list of available field name for this object The computed parameter control whether computed pluginprovided field are included in the key list"
    },
    {
        "input_text": "summarize: def all_keys(cls):                return list(cls._fields) + list(cls._getters().keys())",
        "labels_text": "Get a list of available key for object of this type Includes fixed and computed field"
    },
    {
        "input_text": "summarize: def update(self, values):                for key, value in values.items():            self[key] = value",
        "labels_text": "Assign all value in the given dict"
    },
    {
        "input_text": "summarize: def items(self) -> Iterator[Tuple[str, Any]]:                for key in self:            yield key, self[key]",
        "labels_text": "Iterate over key value pair that this object contains Computed field are not included"
    },
    {
        "input_text": "summarize: def __contains__(self, key) -> bool:                return key in self.keys(computed=True)",
        "labels_text": "Determine whether key is an attribute on this object"
    },
    {
        "input_text": "summarize: def __iter__(self) -> Iterator[str]:                return iter(self.keys())",
        "labels_text": "Iterate over the available field name excluding computed field"
    },
    {
        "input_text": "summarize: def load(self):                db = self._check_db()        if not self._dirty and db.revision == self._revision:            # Exit early            return        stored_obj = db._get(type(self), self.id)        assert stored_obj is not None, f\"object {self.id} not in DB\"        self._values_fixed = LazyConvertDict(self)        self._values_flex = LazyConvertDict(self)        self.update(dict(stored_obj))        self.clear_dirty()",
        "labels_text": "Refresh the object metadata from the library database If checkrevision is true the database is only queried loaded when a transaction ha been committed since the item wa last loaded"
    },
    {
        "input_text": "summarize: def remove(self):                db = self._check_db()        with db.transaction() as tx:            tx.mutate(f\"DELETE FROM {self._table} WHERE id=?\", (self.id,))            tx.mutate(                f\"DELETE FROM {self._flex_table} WHERE entity_id=?\", (self.id,)            )",
        "labels_text": "Remove the object associated row from the database"
    },
    {
        "input_text": "summarize: def formatted(        self,        included_keys: str = _formatter.ALL_KEYS,        for_path: bool = False,    ):                return self._formatter(self, included_keys, for_path)",
        "labels_text": "Get a mapping containing all value on this object formatted a humanreadable unicode string"
    },
    {
        "input_text": "summarize: def evaluate_template(        self,        template: Union[str, functemplate.Template],        for_path: bool = False,    ) -> str:                # Perform substitution.        if isinstance(template, str):            t = functemplate.template(template)        else:            # Help out mypy            t = template        return t.substitute(            self.formatted(for_path=for_path), self._template_funcs()        )",
        "labels_text": "Evaluate a template a string or a Template object using the object field If forpath is true then no new path separator will be added to the template"
    },
    {
        "input_text": "summarize: def _parse(cls, key, string: str) -> Any:                if not isinstance(string, str):            raise TypeError(\"_parse() argument must be a string\")        return cls._type(key).parse(string)",
        "labels_text": "Parse a string a a value for the given key"
    },
    {
        "input_text": "summarize: def set_parse(self, key, string: str):                self[key] = self._parse(key, string)",
        "labels_text": "Set the object key to a value represented by a string"
    },
    {
        "input_text": "summarize: def field_query(        cls,        field,        pattern,        query_cls: Type[FieldQuery] = MatchQuery,    ) -> FieldQuery:                return query_cls(field, pattern, field in cls._fields)",
        "labels_text": "Get a FieldQuery for this model"
    },
    {
        "input_text": "summarize: def all_fields_query(        cls: Type[\"Model\"],        pats: Mapping,        query_cls: Type[FieldQuery] = MatchQuery,    ):                subqueries = [cls.field_query(k, v, query_cls) for k, v in pats.items()]        return AndQuery(subqueries)",
        "labels_text": "Get a query that match many field with different pattern pat should be a mapping from field name to pattern The resulting query is a conjunction and of perfield query for all of these fieldpattern pair"
    },
    {
        "input_text": "summarize: def __iter__(self) -> Iterator[AnyModel]:                if self.sort:            # Slow sort. Must build the full list first.            objects = self.sort.sort(list(self._get_objects()))            return iter(objects)        else:            # Objects are pre-sorted (i.e., by the database).            return self._get_objects()",
        "labels_text": "Construct and generate Model object for all matching object in sorted order"
    },
    {
        "input_text": "summarize: def _get_indexed_flex_attrs(self) -> Mapping:                flex_values: Dict[int, Dict[str, Any]] = {}        for row in self.flex_rows:            if row[\"entity_id\"] not in flex_values:                flex_values[row[\"entity_id\"]] = {}            flex_values[row[\"entity_id\"]][row[\"key\"]] = row[\"value\"]        return flex_values",
        "labels_text": "Index flexible attribute by the entity id they belong to"
    },
    {
        "input_text": "summarize: def _make_model(self, row, flex_values: Dict = {}) -> AnyModel:                cols = dict(row)        values = {k: v for (k, v) in cols.items() if not k[:4] == \"flex\"}        # Construct the Python object        obj = self.model_class._awaken(self.db, values, flex_values)        return obj",
        "labels_text": "Create a Model object for the given row"
    },
    {
        "input_text": "summarize: def __len__(self) -> int:                if not self._rows:            # Fully materialized. Just count the objects.            return len(self._objects)        elif self.query:            # A slow query. Fall back to testing every object.            count = 0            for obj in self:                count += 1            return count        else:            # A fast query. Just count the rows.            return self._row_count",
        "labels_text": "Get the number of matching object"
    },
    {
        "input_text": "summarize: def __nonzero__(self) -> bool:                return self.__bool__()",
        "labels_text": "Does this result contain any object"
    },
    {
        "input_text": "summarize: def __bool__(self) -> bool:                return bool(len(self))",
        "labels_text": "Does this result contain any object"
    },
    {
        "input_text": "summarize: def __getitem__(self, n):                if not self._rows and not self.sort:            # Fully materialized and already in order. Just look up the            # object.            return self._objects[n]        it = iter(self)        try:            for i in range(n):                next(it)            return next(it)        except StopIteration:            raise IndexError(f\"result index {n} out of range\")",
        "labels_text": "Get the nth item in this result set This is inefficient all item up to n are materialized and thrown away"
    },
    {
        "input_text": "summarize: def get(self) -> Optional[AnyModel]:                it = iter(self)        try:            return next(it)        except StopIteration:            return None",
        "labels_text": "Return the first matching object or None if no object match"
    },
    {
        "input_text": "summarize: def __enter__(self) -> \"Transaction\":                with self.db._tx_stack() as stack:            first = not stack            stack.append(self)        if first:            # Beginning a \"root\" transaction, which corresponds to an            # SQLite transaction.            self.db._db_lock.acquire()        return self",
        "labels_text": "Begin a transaction This transaction may be created while another is active in a different thread"
    },
    {
        "input_text": "summarize: def query(self, statement: str, subvals: Sequence = ()) -> List:                cursor = self.db._connection().execute(statement, subvals)        return cursor.fetchall()",
        "labels_text": "Execute an SQL statement with substitution value and return a list of row from the database"
    },
    {
        "input_text": "summarize: def script(self, statements: str):                # We don't know whether this mutates, but quite likely it does.        self._mutated = True        self.db._connection().executescript(statements)",
        "labels_text": "Execute a string containing multiple SQL statement"
    },
    {
        "input_text": "summarize: def _connection(self) -> Connection:                thread_id = threading.current_thread().ident        # Help the type checker: ident can only be None if the thread has not        # been started yet; but since this results from current_thread(), that        # can't happen        assert thread_id is not None        with self._shared_map_lock:            if thread_id in self._connections:                return self._connections[thread_id]            else:                conn = self._create_connection()                self._connections[thread_id] = conn                return conn",
        "labels_text": "Get a SQLite connection object to the underlying database One connection object is created per thread"
    },
    {
        "input_text": "summarize: def _close(self):                with self._shared_map_lock:            while self._connections:                _thread_id, conn = self._connections.popitem()                conn.close()",
        "labels_text": "Close the all connection to the underlying SQLite database from all thread This doe not render the database object unusable new connection can still be opened on demand"
    },
    {
        "input_text": "summarize: def _tx_stack(self) -> Generator[List, None, None]:                thread_id = threading.current_thread().ident        # Help the type checker: ident can only be None if the thread has not        # been started yet; but since this results from current_thread(), that        # can't happen        assert thread_id is not None        with self._shared_map_lock:            yield self._tx_stacks[thread_id]",
        "labels_text": "A context manager providing access to the current thread transaction stack The context manager synchronizes access to the stack map Transactions should never migrate across thread"
    },
    {
        "input_text": "summarize: def transaction(self) -> Transaction:                return Transaction(self)",
        "labels_text": "Get a classTransaction object for interacting directly with the underlying SQLite database"
    },
    {
        "input_text": "summarize: def load_extension(self, path: str):                if not self.supports_extensions:            raise ValueError(                \"this sqlite3 installation does not support extensions\"            )        self._extensions.append(path)        # Load the extension into every open connection.        for conn in self._connections.values():            conn.load_extension(path)",
        "labels_text": "Load an SQLite extension into all open connection"
    },
    {
        "input_text": "summarize: def _make_attribute_table(self, flex_table: str):                with self.transaction() as tx:            tx.script(                .format(                    flex_table                )            )",
        "labels_text": "Create a table and associated index for flexible attribute for the given entity if they dont exist"
    },
    {
        "input_text": "summarize: def _get(        self,        model_cls: Type[AnyModel],        id,    ) -> Optional[AnyModel]:                return self._fetch(model_cls, MatchQuery(\"id\", id)).get()",
        "labels_text": "Get a Model object by it id or None if the id doe not exist"
    },
    {
        "input_text": "summarize: def field_names(self) -> Set[str]:                return set()",
        "labels_text": "Return a set with field name that this query operates on"
    },
    {
        "input_text": "summarize: def clause(self) -> Tuple[Optional[str], Sequence[Any]]:                return None, ()",
        "labels_text": "Generate an SQLite expression implementing the query Return clause subvals where clause is a valid sqlite WHERE clause implementing the query and subvals is a list of item to be substituted for s in the clause The default implementation return None falling back to a slow query using match"
    },
    {
        "input_text": "summarize: def match(self, obj: Model):                ...",
        "labels_text": "Check whether this query match a given Model Can be used to perform query on arbitrary set of Model"
    },
    {
        "input_text": "summarize: def __hash__(self) -> int:                return hash(type(self))",
        "labels_text": "Minimalistic default implementation of a hash Given the implementation if eq above this is certainly correct"
    },
    {
        "input_text": "summarize: def field_names(self) -> Set[str]:                return {self.field_name}",
        "labels_text": "Return a set with field name that this query operates on"
    },
    {
        "input_text": "summarize: def value_match(cls, pattern: P, value: Any):                raise NotImplementedError()",
        "labels_text": "Determine whether the value match the pattern"
    },
    {
        "input_text": "summarize: def value_match(cls, pattern: P, value: Any):                return cls.string_match(pattern, util.as_string(value))",
        "labels_text": "Determine whether the value match the pattern The value may have any type"
    },
    {
        "input_text": "summarize: def string_match(        cls,        pattern: P,        value: str,    ) -> bool:                raise NotImplementedError()",
        "labels_text": "Determine whether the value match the pattern Both argument are string Subclasses implement this method"
    },
    {
        "input_text": "summarize: def _normalize(s: str) -> str:                return unicodedata.normalize(\"NFC\", s)",
        "labels_text": "Normalize a Unicode string representation used on both pattern and matched value"
    },
    {
        "input_text": "summarize: def _convert(self, s: str) -> Union[float, int, None]:                # This is really just a bit of fun premature optimization.        if not s:            return None        try:            return int(s)        except ValueError:            try:                return float(s)            except ValueError:                raise InvalidQueryArgumentValueError(s, \"an int or a float\")",
        "labels_text": "Convert a string to a numeric type float or int Return None if s is empty Raise an InvalidQueryError if the string cannot be converted"
    },
    {
        "input_text": "summarize: def field_names(self) -> Set[str]:                return reduce(or_, (sq.field_names for sq in self.subqueries))",
        "labels_text": "Return a set with field name that this query operates on"
    },
    {
        "input_text": "summarize: def __hash__(self) -> int:                return reduce(mul, map(hash, self.subqueries), 1)",
        "labels_text": "Since subqueries are mutable this object should not be hashable However and for convenience purpose it can be hashed"
    },
    {
        "input_text": "summarize: def field_names(self) -> Set[str]:                return set(self.fields)",
        "labels_text": "Return a set with field name that this query operates on"
    },
    {
        "input_text": "summarize: def field_names(self) -> Set[str]:                return self.subquery.field_names",
        "labels_text": "Return a set with field name that this query operates on"
    },
    {
        "input_text": "summarize: def _parse_periods(pattern: str) -> Tuple[Optional[Period], Optional[Period]]:        parts = pattern.split(\"..\", 1)    if len(parts) == 1:        instant = Period.parse(parts[0])        return (instant, instant)    else:        start = Period.parse(parts[0])        end = Period.parse(parts[1])        return (start, end)",
        "labels_text": "Parse a string containing two date separated by two dot Return a pair of Period object"
    },
    {
        "input_text": "summarize: def __init__(self, date: datetime, precision: str):                if precision not in Period.precisions:            raise ValueError(f\"Invalid precision {precision}\")        self.date = date        self.precision = precision",
        "labels_text": "Create a period with the given date a datetime object and precision a string one of year month day hour minute or second"
    },
    {
        "input_text": "summarize: def from_periods(        cls,        start: Optional[Period],        end: Optional[Period],    ) -> DateInterval:                end_date = end.open_right_endpoint() if end is not None else None        start_date = start.date if start is not None else None        return cls(start_date, end_date)",
        "labels_text": "Create an interval with two Periods a the endpoint"
    },
    {
        "input_text": "summarize: def _convert(self, s: str) -> Optional[float]:                if not s:            return None        try:            return util.raw_seconds_short(s)        except ValueError:            try:                return float(s)            except ValueError:                raise InvalidQueryArgumentValueError(                    s, \"a M:SS string or a float\"                )",
        "labels_text": "Convert a MSS or numeric string to a float Return None if s is empty Raise an InvalidQueryError if the string cannot be converted"
    },
    {
        "input_text": "summarize: def order_clause(self) -> Optional[str]:                return None",
        "labels_text": "Generates a SQL fragment to be used in a ORDER BY clause or None if no fragment is used ie this is a slow sort"
    },
    {
        "input_text": "summarize: def sort(self, items: List) -> List:                return sorted(items)",
        "labels_text": "Sort the list of object and return a list"
    },
    {
        "input_text": "summarize: def is_slow(self) -> bool:                return False",
        "labels_text": "Indicate whether this query is slow meaning that it cannot be executed in SQL and must be executed in Python"
    },
    {
        "input_text": "summarize: def order_clause(self) -> str:                order_strings = []        for sort in reversed(self.sorts):            clause = sort.order_clause()            if clause is None:                break            order_strings.append(clause)        order_strings.reverse()        return \", \".join(order_strings)",
        "labels_text": "Return the list SQL clause for those subsorts for which we can be at least partially fast A contiguous suffix of fast SQLcapable subsorts are executable in SQL The remaining even if they are fast independently must be executed slowly"
    },
    {
        "input_text": "summarize: def query_from_strings(    query_cls: Type[query.CollectionQuery],    model_cls: Type[Model],    prefixes: Dict,    query_parts: Collection[str],) -> query.Query:        subqueries = []    for part in query_parts:        subqueries.append(construct_query_part(model_cls, prefixes, part))    if not subqueries:  # No terms in query.        subqueries = [query.TrueQuery()]    return query_cls(subqueries)",
        "labels_text": "Creates a collection query of type querycls from a list of string in the format used by parsequerypart modelcls determines how query are constructed from string"
    },
    {
        "input_text": "summarize: def null(self) -> N:                # Note that this default implementation only makes sense for T = N.        # It would be better to implement `null()` only in subclasses, or        # have a field null_type similar to `model_type` and use that here.        return cast(N, self.model_type())",
        "labels_text": "The value to be exposed when the underlying value is None"
    },
    {
        "input_text": "summarize: def format(self, value: Union[N, T]) -> str:                if value is None:            value = self.null        # `self.null` might be `None`        if value is None:            return \"\"        elif isinstance(value, bytes):            return value.decode(\"utf-8\", \"ignore\")        else:            return str(value)",
        "labels_text": "Given a value of this type produce a Unicode string representing the value This is used in template evaluation"
    },
    {
        "input_text": "summarize: def parse(self, string: str) -> Union[T, N]:                try:            return self.model_type(string)        except ValueError:            return self.null",
        "labels_text": "Parse a possibly humanwritten string and return the indicated value of this type"
    },
    {
        "input_text": "summarize: def normalize(self, value: Any) -> Union[T, N]:                # TYPING ERROR        if value is None:            return self.null        else:            # TODO This should eventually be replaced by            # `self.model_type(value)`            return cast(T, value)",
        "labels_text": "Given a value that will be assigned into a field of this type normalize the value to have the appropriate type This base implementation only reinterprets None"
    },
    {
        "input_text": "summarize: def from_sql(        self,        sql_value: Union[None, int, float, str, bytes],    ) -> Union[T, N]:                if isinstance(sql_value, memoryview):            sql_value = bytes(sql_value).decode(\"utf-8\", \"ignore\")        if isinstance(sql_value, str):            return self.parse(sql_value)        else:            return self.normalize(sql_value)",
        "labels_text": "Receives the value stored in the SQL backend and return the value to be stored in the model For fixed field the type of value is determined by the column type affinity given in the sql property and the SQL to Python mapping of the database adapter For more information see httpswwwsqliteorgdatatypehtml httpsdocspythonorglibrarysqlitehtmlsqliteandpythontypes Flexible field have the type affinity TEXT This mean the sqlvalue is either a memoryview or a unicode object and the method must handle these in addition"
    },
    {
        "input_text": "summarize: def to_sql(self, model_value: Any) -> Union[None, int, float, str, bytes]:                return model_value",
        "labels_text": "Convert a value a stored in the model object to a value used by the database adapter"
    },
    {
        "input_text": "summarize: def control_stdin(input=None):        org = sys.stdin    sys.stdin = StringIO(input)    try:        yield sys.stdin    finally:        sys.stdin = org",
        "labels_text": "Sends input to stdin with controlstdinyes input yes"
    },
    {
        "input_text": "summarize: def capture_stdout():        org = sys.stdout    sys.stdout = capture = StringIO()    try:        yield sys.stdout    finally:        sys.stdout = org        print(capture.getvalue())",
        "labels_text": "Save stdout in a StringIO with capturestdout a output printspam outputgetvalue spam"
    },
    {
        "input_text": "summarize: def _convert_args(args):        for i, elem in enumerate(args):        if isinstance(elem, bytes):            args[i] = elem.decode(util.arg_encoding())    return args",
        "labels_text": "Convert args to bytestrings for Python and convert them to string on Python"
    },
    {
        "input_text": "summarize: def has_program(cmd, args=[\"--version\"]):        full_cmd = _convert_args([cmd] + args)    try:        with open(os.devnull, \"wb\") as devnull:            subprocess.check_call(                full_cmd, stderr=devnull, stdout=devnull, stdin=devnull            )    except OSError:        return False    except subprocess.CalledProcessError:        return False    else:        return True",
        "labels_text": "Returns True if cmd can be executed"
    },
    {
        "input_text": "summarize: def add_item(self, **values):                # When specifying a path, store it normalized (as beets does        # ordinarily).        if \"path\" in values:            values[\"path\"] = util.normpath(values[\"path\"])        item = self.create_item(**values)        item.add(self.lib)        # Ensure every item has a path.        if \"path\" not in values:            item[\"path\"] = item.destination()            item.store()        return item",
        "labels_text": "Add an item to the library and return it Creates the item by passing the parameter to createitem If path is not set in value it is set to itemdestination"
    },
    {
        "input_text": "summarize: def add_item_fixture(self, **values):                item = self.create_item(**values)        extension = item[\"format\"].lower()        item[\"path\"] = os.path.join(            _common.RSRC, util.bytestring_path(\"min.\" + extension)        )        item.add(self.lib)        item.move(operation=MoveOperation.COPY)        item.store()        return item",
        "labels_text": "Add an item with an actual audio file to the library"
    },
    {
        "input_text": "summarize: def run_command(self, *args, **kwargs):                sys.argv = [\"beet\"]  # avoid leakage from test suite args        lib = None        if hasattr(self, \"lib\"):            lib = self.lib        lib = kwargs.get(\"lib\", lib)        beets.ui._raw_main(_convert_args(list(args)), lib)",
        "labels_text": "Run a beet command with an arbitrary amount of argument The Library default to selflib but can be overridden with the keyword argument lib"
    },
    {
        "input_text": "summarize: def create_temp_dir(self, **kwargs):                temp_dir = mkdtemp(**kwargs)        self.temp_dir = util.bytestring_path(temp_dir)",
        "labels_text": "Create a temporary directory and assign it into selftempdir Call removetempdir later to delete it"
    },
    {
        "input_text": "summarize: def remove_temp_dir(self):                shutil.rmtree(syspath(self.temp_dir))",
        "labels_text": "Delete the temporary directory created by createtempdir"
    },
    {
        "input_text": "summarize: def assert_file_in_lib(self, *segments):                self.assertExists(os.path.join(self.libdir, *segments))",
        "labels_text": "Join the segment and assert that this path exists in the library directory"
    },
    {
        "input_text": "summarize: def assert_file_not_in_lib(self, *segments):                self.assertNotExists(os.path.join(self.libdir, *segments))",
        "labels_text": "Join the segment and assert that this path doe not exist in the library directory"
    },
    {
        "input_text": "summarize: def generate_album_info(album_id, track_values):        tracks = [generate_track_info(id, values) for id, values in track_values]    album = AlbumInfo(        album_id=\"album info\",        album=\"album info\",        artist=\"album info\",        artist_id=\"album info\",        tracks=tracks,    )    for field in ALBUM_INFO_FIELDS:        setattr(album, field, \"album info\")    return album",
        "labels_text": "Return AlbumInfo populated with mock data Sets the album info albumid field is set to the corresponding argument For each pair id value in trackvalues the TrackInfo from generatetrackinfo is added to the album info track field Most other field of the album and track info are set to album info and track info respectively"
    },
    {
        "input_text": "summarize: def generate_track_info(track_id=\"track info\", values={}):        track = TrackInfo(        title=\"track info\",        track_id=track_id,    )    for field in TRACK_INFO_FIELDS:        setattr(track, field, \"track info\")    for field, value in values.items():        setattr(track, field, value)    return track",
        "labels_text": "Return TrackInfo populated with mock data The trackid field is set to the corresponding argument All other string field are set to track info"
    },
    {
        "input_text": "summarize: def tearDownClass(cls) -> None:                for module in cls.modules:            clean_module_tempdir(module)",
        "labels_text": "Remove file created by the plugin"
    },
    {
        "input_text": "summarize: def assert_equal_path(self, a, b):                a_bytes, b_bytes = util.normpath(a), util.normpath(b)        assert a_bytes == b_bytes, f\"{a_bytes=} != {b_bytes=}\"",
        "labels_text": "Check that two path are equal"
    },
    {
        "input_text": "summarize: def _print_keys(query):        for row in query:        print_(\" \" * 2 + row[\"key\"])",
        "labels_text": "Given a SQLite query result print the key field of each returned row with indentation of space"
    },
    {
        "input_text": "summarize: def disambig_string(info):        if isinstance(info, hooks.AlbumInfo):        disambig = get_album_disambig_fields(info)    elif isinstance(info, hooks.TrackInfo):        disambig = get_singleton_disambig_fields(info)    else:        return \"\"    return \", \".join(disambig)",
        "labels_text": "Generate a string for an AlbumInfo or TrackInfo object that provides context that help disambiguate similarlooking album and track"
    },
    {
        "input_text": "summarize: def dist_colorize(string, dist):        if dist <= config[\"match\"][\"strong_rec_thresh\"].as_number():        string = ui.colorize(\"text_success\", string)    elif dist <= config[\"match\"][\"medium_rec_thresh\"].as_number():        string = ui.colorize(\"text_warning\", string)    else:        string = ui.colorize(\"text_error\", string)    return string",
        "labels_text": "Formats a string a a colorized similarity string according to a distance"
    },
    {
        "input_text": "summarize: def dist_string(dist):        string = \"{:.1f}%\".format(((1 - dist) * 100))    return dist_colorize(string, dist)",
        "labels_text": "Formats a distance a float a a colorized similarity percentage string"
    },
    {
        "input_text": "summarize: def show_change(cur_artist, cur_album, match):        change = AlbumChange(        cur_artist=cur_artist, cur_album=cur_album, match=match    )    # Print the match header.    change.show_match_header()    # Print the match details.    change.show_match_details()    # Print the match tracks.    change.show_match_tracks()",
        "labels_text": "Print out a representation of the change that will be made if an album tag are changed according to match which must be an AlbumMatch object"
    },
    {
        "input_text": "summarize: def show_item_change(item, match):        change = TrackChange(        cur_artist=item.artist, cur_title=item.title, match=match    )    # Print the match header.    change.show_match_header()    # Print the match details.    change.show_match_details()",
        "labels_text": "Print out the change that would occur by tagging item with the metadata from match a TrackMatch object"
    },
    {
        "input_text": "summarize: def manual_search(session, task):        artist = input_(\"Artist:\").strip()    name = input_(\"Album:\" if task.is_album else \"Track:\").strip()    if task.is_album:        _, _, prop = autotag.tag_album(task.items, artist, name)        return prop    else:        return autotag.tag_item(task.item, artist, name)",
        "labels_text": "Get a new Proposal using manual search criterion Input either an artist and album for full album or artist and track name for singleton for manual search"
    },
    {
        "input_text": "summarize: def manual_id(session, task):        prompt = \"Enter {} ID:\".format(\"release\" if task.is_album else \"recording\")    search_id = input_(prompt).strip()    if task.is_album:        _, _, prop = autotag.tag_album(task.items, search_ids=search_id.split())        return prop    else:        return autotag.tag_item(task.item, search_ids=search_id.split())",
        "labels_text": "Get a new Proposal using a manuallyentered ID Input an ID either for an album release or a track recording"
    },
    {
        "input_text": "summarize: def abort_action(session, task):        raise importer.ImportAbort()",
        "labels_text": "A prompt choice callback that abort the importer"
    },
    {
        "input_text": "summarize: def list_items(lib, query, album, fmt=\"\"):        if album:        for album in lib.albums(query):            ui.print_(format(album, fmt))    else:        for item in lib.items(query):            ui.print_(format(item, fmt))",
        "labels_text": "Print out item in lib matching query If album then search for album instead of single item"
    },
    {
        "input_text": "summarize: def print_and_modify(obj, mods, dels):        obj.update(mods)    for field in dels:        try:            del obj[field]        except KeyError:            pass    return ui.show_model_changes(obj)",
        "labels_text": "Print the modification to an item and return a bool indicating whether any change were made mod is a dictionary of field and value to update on the object dels is a sequence of field to delete"
    },
    {
        "input_text": "summarize: def modify_parse_args(args):        mods = {}    dels = []    query = []    for arg in args:        if arg.endswith(\"!\") and \"=\" not in arg and \":\" not in arg:            dels.append(arg[:-1])  # Strip trailing !.        elif \"=\" in arg and \":\" not in arg.split(\"=\", 1)[0]:            key, val = arg.split(\"=\", 1)            mods[key] = val        else:            query.append(arg)    return query, mods, dels",
        "labels_text": "Split the argument for the modify subcommand into query part assignment fieldvalue and deletion field Returns the result a a threetuple in that order"
    },
    {
        "input_text": "summarize: def config_edit():        path = config.user_config_path()    editor = util.editor_command()    try:        if not os.path.isfile(path):            open(path, \"w+\").close()        util.interactive_open([path], editor)    except OSError as exc:        message = f\"Could not edit configuration: {exc}\"        if not editor:            message += (                \". Please set the VISUAL (or EDITOR) environment variable\"            )        raise ui.UserError(message)",
        "labels_text": "Open a program to edit the user configuration An empty config file is created if no existing config file exists"
    },
    {
        "input_text": "summarize: def _in_encoding():        return _stream_encoding(sys.stdin)",
        "labels_text": "Get the encoding to use for inputting string from the console"
    },
    {
        "input_text": "summarize: def _out_encoding():        return _stream_encoding(sys.stdout)",
        "labels_text": "Get the encoding to use for outputting string to the console"
    },
    {
        "input_text": "summarize: def decargs(arglist):        return arglist",
        "labels_text": "Given a list of commandline argument bytestrings attempt to decode them to Unicode string when running under Python"
    },
    {
        "input_text": "summarize: def _bool_fallback(a, b):        if a is None:        assert isinstance(b, bool)        return b    else:        assert isinstance(a, bool)        return a",
        "labels_text": "Given a boolean or None return the original value or a fallback"
    },
    {
        "input_text": "summarize: def should_write(write_opt=None):        return _bool_fallback(write_opt, config[\"import\"][\"write\"].get(bool))",
        "labels_text": "Decide whether a command that update metadata should also write tag using the importer configuration a the default"
    },
    {
        "input_text": "summarize: def should_move(move_opt=None):        return _bool_fallback(        move_opt,        config[\"import\"][\"move\"].get(bool)        or config[\"import\"][\"copy\"].get(bool),    )",
        "labels_text": "Decide whether a command that update metadata should also move file when theyre inside the library using the importer configuration a the default Specifically command should move file after metadata update only when the importer is configured either to move or to copy file They should avoid moving file when the importer is configured not to touch any filename"
    },
    {
        "input_text": "summarize: def indent(count):        return \" \" * count",
        "labels_text": "Returns a string with count many space"
    },
    {
        "input_text": "summarize: def input_(prompt=None):        # raw_input incorrectly sends prompts to stderr, not stdout, so we    # use print_() explicitly to display prompts.    # https://bugs.python.org/issue1927    if prompt:        print_(prompt, end=\" \")    try:        resp = input()    except EOFError:        raise UserError(\"stdin stream ended while input required\")    return resp",
        "labels_text": "Like input but decodes the result to a Unicode string Raises a UserError if stdin is not available The prompt is sent to stdout rather than stderr A printed between the prompt and the input cursor"
    },
    {
        "input_text": "summarize: def input_yn(prompt, require=False):        # Start prompt with U+279C: Heavy Round-Tipped Rightwards Arrow    yesno = colorize(\"action\", \"\\u279C \") + colorize(        \"action_description\", \"Enter Y or N:\"    )    sel = input_options((\"y\", \"n\"), require, prompt, yesno)    return sel == \"y\"",
        "labels_text": "Prompts the user for a yes or no response The default is yes unless require is True in which case there is no default"
    },
    {
        "input_text": "summarize: def human_bytes(size):        powers = [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\", \"H\"]    unit = \"B\"    for power in powers:        if size < 1024:            return f\"{size:3.1f} {power}{unit}\"        size /= 1024.0        unit = \"iB\"    return \"big\"",
        "labels_text": "Formats size a number of byte in a humanreadable way"
    },
    {
        "input_text": "summarize: def human_seconds_short(interval):        interval = int(interval)    return \"%i:%02i\" % (interval // 60, interval % 60)",
        "labels_text": "Formats a number of second a a short humanreadable MSS string"
    },
    {
        "input_text": "summarize: def _colorize(color, text):        # Construct escape sequence to be put before the text by iterating    # over all \"ANSI codes\" in `color`.    escape = \"\"    for code in color:        escape = escape + COLOR_ESCAPE + \"%im\" % ANSI_CODES[code]    return escape + text + RESET_COLOR",
        "labels_text": "Returns a string that print the given text in the given color in a terminal that is ANSI coloraware The color must be a list of string from ANSICODES"
    },
    {
        "input_text": "summarize: def color_len(colored_text):        # Return the length of the uncolored string.    return len(uncolorize(colored_text))",
        "labels_text": "Measure the length of a string while excluding ANSI code from the measurement The standard lenmystring method also count ANSI code to the string length which is counterproductive when layouting a Terminal interface"
    },
    {
        "input_text": "summarize: def colordiff(a, b):        if config[\"ui\"][\"color\"]:        return _colordiff(a, b)    else:        return str(a), str(b)",
        "labels_text": "Colorize difference between two value if color is enabled Like colordiff but conditional"
    },
    {
        "input_text": "summarize: def get_path_formats(subview=None):        path_formats = []    subview = subview or config[\"paths\"]    for query, view in subview.items():        query = PF_KEY_QUERIES.get(query, query)  # Expand common queries.        path_formats.append((query, template(view.as_str())))    return path_formats",
        "labels_text": "Get the configuration path format a a list of querytemplate pair"
    },
    {
        "input_text": "summarize: def get_replacements():        replacements = []    for pattern, repl in config[\"replace\"].get(dict).items():        repl = repl or \"\"        try:            replacements.append((re.compile(pattern), repl))        except re.error:            raise UserError(                \"malformed regular expression in replace: {}\".format(pattern)            )    return replacements",
        "labels_text": "Confuse validation function that read regexstring pair"
    },
    {
        "input_text": "summarize: def add_album_option(self, flags=(\"-a\", \"--album\")):                album = optparse.Option(            *flags, action=\"store_true\", help=\"match albums instead of tracks\"        )        self.add_option(album)        self._album_flags = set(flags)",
        "labels_text": "Add a aalbum option to match album instead of track If used then the format option can autodetect whether were setting the format for item or album Sets the album property on the option extracted from the CLI"
    },
    {
        "input_text": "summarize: def add_path_option(self, flags=(\"-p\", \"--path\")):                path = optparse.Option(            *flags,            nargs=0,            action=\"callback\",            callback=self._set_format,            callback_kwargs={\"fmt\": \"$path\", \"store_true\": True},            help=\"print paths for matched items or albums\",        )        self.add_option(path)",
        "labels_text": "Add a ppath option to display the path instead of the default format By default this affect both item and album If addalbumoption is used then the target will be autodetected Sets the format property to path on the option extracted from the CLI"
    },
    {
        "input_text": "summarize: def add_format_option(self, flags=(\"-f\", \"--format\"), target=None):                kwargs = {}        if target:            if isinstance(target, str):                target = {\"item\": library.Item, \"album\": library.Album}[target]            kwargs[\"target\"] = target        opt = optparse.Option(            *flags,            action=\"callback\",            callback=self._set_format,            callback_kwargs=kwargs,            help=\"print with custom format\",        )        self.add_option(opt)",
        "labels_text": "Add fformat option to print some LibModel instance with a custom format target is optional and can be one of libraryItem item libraryAlbum and album Several behavior are available if target is given then the format is only applied to that LibModel if the album option is used then the target will be autodetected otherwise the format is applied to both item and album Sets the format property on the option extracted from the CLI"
    },
    {
        "input_text": "summarize: def add_all_common_options(self):                self.add_album_option()        self.add_path_option()        self.add_format_option()",
        "labels_text": "Add album path and format option"
    },
    {
        "input_text": "summarize: def __init__(self, name, parser=None, help=\"\", aliases=(), hide=False):                self.name = name        self.parser = parser or CommonOptionsParser()        self.aliases = aliases        self.help = help        self.hide = hide        self._root_parser = None",
        "labels_text": "Creates a new subcommand name is the primary way to invoke the subcommand alias are alternate name parser is an OptionParser responsible for parsing the subcommands option help is a short description of the command If no parser is given it default to a new empty CommonOptionsParser"
    },
    {
        "input_text": "summarize: def __init__(self, *args, **kwargs):                # A more helpful default usage.        if \"usage\" not in kwargs:            kwargs[                \"usage\"            ] =         kwargs[\"add_help_option\"] = False        # Super constructor.        super().__init__(*args, **kwargs)        # Our root parser needs to stop on the first unrecognized argument.        self.disable_interspersed_args()        self.subcommands = []",
        "labels_text": "Create a new subcommandaware option parser All of the option to OptionParserinit are supported in addition to subcommands a sequence of Subcommand object"
    },
    {
        "input_text": "summarize: def add_subcommand(self, *cmds):                for cmd in cmds:            cmd.root_parser = self            self.subcommands.append(cmd)",
        "labels_text": "Adds a Subcommand object to the parser list of command"
    },
    {
        "input_text": "summarize: def _subcommand_for_name(self, name):                for subcommand in self.subcommands:            if name == subcommand.name or name in subcommand.aliases:                return subcommand        return None",
        "labels_text": "Return the subcommand in selfsubcommands matching the given name The name may either be the name of a subcommand or an alias If no subcommand match return None"
    },
    {
        "input_text": "summarize: def parse_global_options(self, args):                options, subargs = self.parse_args(args)        # Force the help command        if options.help:            subargs = [\"help\"]        elif options.version:            subargs = [\"version\"]        return options, subargs",
        "labels_text": "Parse option up to the subcommand argument Returns a tuple of the option object and the remaining argument"
    },
    {
        "input_text": "summarize: def parse_subcommand(self, args):                # Help is default command        if not args:            args = [\"help\"]        cmdname = args.pop(0)        subcommand = self._subcommand_for_name(cmdname)        if not subcommand:            raise UserError(f\"unknown command '{cmdname}'\")        suboptions, subargs = subcommand.parse_args(args)        return subcommand, suboptions, subargs",
        "labels_text": "Given the args left unused by a parseglobaloptions return the invoked subcommand the subcommand option and the subcommand argument"
    },
    {
        "input_text": "summarize: def resize_url(url, maxwidth, quality=0):        params = {        \"url\": url.replace(\"http://\", \"\"),        \"w\": maxwidth,    }    if quality > 0:        params[\"q\"] = quality    return \"{}?{}\".format(PROXY_URL, urlencode(params))",
        "labels_text": "Return a proxied image URL that resizes the original image to maxwidth preserving aspect ratio"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version()",
        "labels_text": "Initialize a wrapper around PIL for local image operation If PIL is not available raise an Exception"
    },
    {
        "input_text": "summarize: def __init__(self):                # Check if a local backend is available, and store an instance of the        # backend class. Otherwise, fallback to the web proxy.        for backend_cls in BACKEND_CLASSES:            try:                self.local_method = backend_cls()                log.debug(f\"artresizer: method is {self.local_method.NAME}\")                break            except LocalBackendNotAvailableError:                continue        else:            log.debug(\"artresizer: method is WEBPROXY\")            self.local_method = None",
        "labels_text": "Create a resizer object with an inferred method"
    },
    {
        "input_text": "summarize: def resize(        self, maxwidth, path_in, path_out=None, quality=0, max_filesize=0    ):                if self.local:            return self.local_method.resize(                maxwidth,                path_in,                path_out,                quality=quality,                max_filesize=max_filesize,            )        else:            # Handled by `proxy_url` already.            return path_in",
        "labels_text": "Manipulate an image file according to the method returning a new path For PIL or IMAGEMAGIC method resizes the image to a temporary file and encodes with the specified quality level For WEBPROXY return pathin unmodified"
    },
    {
        "input_text": "summarize: def deinterlace(self, path_in, path_out=None):                if self.local:            return self.local_method.deinterlace(path_in, path_out)        else:            # FIXME: Should probably issue a warning?            return path_in",
        "labels_text": "Deinterlace an image Only available locally"
    },
    {
        "input_text": "summarize: def proxy_url(self, maxwidth, url, quality=0):                if self.local:            # Going to be handled by `resize()`.            return url        else:            return resize_url(url, maxwidth, quality)",
        "labels_text": "Modifies an image URL according the method returning a new URL For WEBPROXY a URL on the proxy server is returned Otherwise the URL is returned unmodified"
    },
    {
        "input_text": "summarize: def local(self):                return self.local_method is not None",
        "labels_text": "A boolean indicating whether the resizing method is performed locally ie PIL or ImageMagick"
    },
    {
        "input_text": "summarize: def get_size(self, path_in):                if self.local:            return self.local_method.get_size(path_in)        else:            # FIXME: Should probably issue a warning?            return path_in",
        "labels_text": "Return the size of an image file a an int couple width height in pixel Only available locally"
    },
    {
        "input_text": "summarize: def get_format(self, path_in):                if self.local:            return self.local_method.get_format(path_in)        else:            # FIXME: Should probably issue a warning?            return None",
        "labels_text": "Returns the format of the image a a string Only available locally"
    },
    {
        "input_text": "summarize: def can_compare(self):                if self.local:            return self.local_method.can_compare        else:            return False",
        "labels_text": "A boolean indicating whether image comparison is available"
    },
    {
        "input_text": "summarize: def compare(self, im1, im2, compare_threshold):                if self.local:            return self.local_method.compare(im1, im2, compare_threshold)        else:            # FIXME: Should probably issue a warning?            return None",
        "labels_text": "Return a boolean indicating whether two image are similar Only available locally"
    },
    {
        "input_text": "summarize: def can_write_metadata(self):                if self.local:            return self.local_method.can_write_metadata        else:            return False",
        "labels_text": "A boolean indicating whether writing image metadata is supported"
    },
    {
        "input_text": "summarize: def write_metadata(self, file, metadata):                if self.local:            self.local_method.write_metadata(file, metadata)        else:            # FIXME: Should probably issue a warning?            pass",
        "labels_text": "Write keyvalue metadata to the image file Only available locally Currently expects the image to be a PNG file"
    },
    {
        "input_text": "summarize: def waitables(self):                return (), (), ()",
        "labels_text": "Return waitable object to pas to select Should return three iterables for input readiness output readiness and exceptional condition ie the three list passed to select"
    },
    {
        "input_text": "summarize: def fire(self):                pass",
        "labels_text": "Called when an associated file descriptor becomes ready ie is returned from a select call"
    },
    {
        "input_text": "summarize: def __init__(self, host, port):                self._closed = False        self.host = host        self.port = port        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)        self.sock.bind((host, port))        self.sock.listen(5)",
        "labels_text": "Create a listening socket on the given hostname and port"
    },
    {
        "input_text": "summarize: def accept(self):                if self._closed:            raise SocketClosedError()        return AcceptEvent(self)",
        "labels_text": "An event that wait for a connection on the listening socket When a connection is made the event return a Connection object"
    },
    {
        "input_text": "summarize: def close(self):                self._closed = True        self.sock.close()",
        "labels_text": "Immediately close the listening socket Not an event"
    },
    {
        "input_text": "summarize: def close(self):                self._closed = True        self.sock.close()",
        "labels_text": "Close the connection"
    },
    {
        "input_text": "summarize: def recv(self, size):                if self._closed:            raise SocketClosedError()        if self._buf:            # We already have data read previously.            out = self._buf[:size]            self._buf = self._buf[size:]            return ValueEvent(out)        else:            return ReceiveEvent(self, size)",
        "labels_text": "Read at most size byte of data from the socket"
    },
    {
        "input_text": "summarize: def send(self, data):                if self._closed:            raise SocketClosedError()        return SendEvent(self, data)",
        "labels_text": "Sends data on the socket returning the number of byte successfully sent"
    },
    {
        "input_text": "summarize: def sendall(self, data):                if self._closed:            raise SocketClosedError()        return SendEvent(self, data, True)",
        "labels_text": "Send all of data on the socket"
    },
    {
        "input_text": "summarize: def readline(self, terminator=b\"\\n\", bufsize=1024):                if self._closed:            raise SocketClosedError()        while True:            if terminator in self._buf:                line, self._buf = self._buf.split(terminator, 1)                line += terminator                yield ReturnEvent(line)                break            data = yield ReceiveEvent(self, bufsize)            if data:                self._buf += data            else:                line = self._buf                self._buf = b\"\"                yield ReturnEvent(line)                break",
        "labels_text": "Reads a line delimited by terminator from the socket"
    },
    {
        "input_text": "summarize: def null():        return ValueEvent(None)",
        "labels_text": "Event yield to the scheduler without doing anything special"
    },
    {
        "input_text": "summarize: def spawn(coro):        if not isinstance(coro, types.GeneratorType):        raise ValueError(\"%s is not a coroutine\" % coro)    return SpawnEvent(coro)",
        "labels_text": "Event add another coroutine to the scheduler Both the parent and child coroutines run concurrently"
    },
    {
        "input_text": "summarize: def call(coro):        if not isinstance(coro, types.GeneratorType):        raise ValueError(\"%s is not a coroutine\" % coro)    return DelegationEvent(coro)",
        "labels_text": "Event delegate to another coroutine The current coroutine is resumed once the subcoroutine finish If the subcoroutine return a value using end then this event return that value"
    },
    {
        "input_text": "summarize: def end(value=None):        return ReturnEvent(value)",
        "labels_text": "Event end the coroutine and return a value to it delegator"
    },
    {
        "input_text": "summarize: def read(fd, bufsize=None):        if bufsize is None:        # Read all.        def reader():            buf = []            while True:                data = yield read(fd, 1024)                if not data:                    break                buf.append(data)            yield ReturnEvent(\"\".join(buf))        return DelegationEvent(reader())    else:        return ReadEvent(fd, bufsize)",
        "labels_text": "Event read from a file descriptor asynchronously"
    },
    {
        "input_text": "summarize: def write(fd, data):        return WriteEvent(fd, data)",
        "labels_text": "Event write to a file descriptor asynchronously"
    },
    {
        "input_text": "summarize: def connect(host, port):        addr = (host, port)    sock = socket.create_connection(addr)    return ValueEvent(Connection(sock, addr))",
        "labels_text": "Event connect to a network address and return a Connection object for communicating on the socket"
    },
    {
        "input_text": "summarize: def sleep(duration):        return SleepEvent(duration)",
        "labels_text": "Event suspend the thread for duration second"
    },
    {
        "input_text": "summarize: def join(coro):        return JoinEvent(coro)",
        "labels_text": "Suspend the thread until another previously spawned thread completes"
    },
    {
        "input_text": "summarize: def kill(coro):        return KillEvent(coro)",
        "labels_text": "Halt the execution of a different spawned thread"
    },
    {
        "input_text": "summarize: def server(host, port, func):        def handler(conn):        try:            yield func(conn)        finally:            conn.close()    listener = Listener(host, port)    try:        while True:            conn = yield listener.accept()            yield spawn(handler(conn))    except KeyboardInterrupt:        pass    finally:        listener.close()",
        "labels_text": "A coroutine that run a network server Host and port specify the listening address func should be a coroutine that take a single parameter a Connection object The coroutine is invoked for every incoming connection on the listening socket"
    },
    {
        "input_text": "summarize: def ex_rvalue(name):        return ast.Name(name, ast.Load())",
        "labels_text": "A variable store expression"
    },
    {
        "input_text": "summarize: def ex_literal(val):        return ast.Constant(val)",
        "labels_text": "An int float long bool string or None literal with the given value"
    },
    {
        "input_text": "summarize: def ex_call(func, args):        if isinstance(func, str):        func = ex_rvalue(func)    args = list(args)    for i in range(len(args)):        if not isinstance(args[i], ast.expr):            args[i] = ex_literal(args[i])    return ast.Call(func, args, [])",
        "labels_text": "A functioncall expression with only positional parameter The function may be an expression or the name of a function Each argument may be an expression or a value to be used a a literal"
    },
    {
        "input_text": "summarize: def evaluate(self, env):                if self.ident in env.values:            # Substitute for a value.            return env.values[self.ident]        else:            # Keep original text.            return self.original",
        "labels_text": "Evaluate the symbol in the environment returning a Unicode string"
    },
    {
        "input_text": "summarize: def translate(self):                ident = self.ident        expr = ex_rvalue(VARIABLE_PREFIX + ident)        return [expr], {ident}, set()",
        "labels_text": "Compile the variable lookup"
    },
    {
        "input_text": "summarize: def evaluate(self, env):                if self.ident in env.functions:            arg_vals = [expr.evaluate(env) for expr in self.args]            try:                out = env.functions[self.ident](*arg_vals)            except Exception as exc:                # Function raised exception! Maybe inlining the name of                # the exception will help debug.                return \"<%s>\" % str(exc)            return str(out)        else:            return self.original",
        "labels_text": "Evaluate the function call in the environment returning a Unicode string"
    },
    {
        "input_text": "summarize: def evaluate(self, env):                out = []        for part in self.parts:            if isinstance(part, str):                out.append(part)            else:                out.append(part.evaluate(env))        return \"\".join(map(str, out))",
        "labels_text": "Evaluate the entire expression in the environment returning a Unicode string"
    },
    {
        "input_text": "summarize: def translate(self):                expressions = []        varnames = set()        funcnames = set()        for part in self.parts:            if isinstance(part, str):                expressions.append(ex_literal(part))            else:                e, v, f = part.translate()                expressions.extend(e)                varnames.update(v)                funcnames.update(f)        return expressions, varnames, funcnames",
        "labels_text": "Compile the expression to a list of Python AST expression a set of variable name used and a set of function name"
    },
    {
        "input_text": "summarize: def __init__(self, string, in_argument=False):                self.string = string        self.in_argument = in_argument        self.pos = 0        self.parts = []",
        "labels_text": "Create a new parser param inarguments boolean that indicates the parser is to be used for parsing function argument ie considering comma ARGSEP a special character"
    },
    {
        "input_text": "summarize: def _parse_ident(self):                remainder = self.string[self.pos :]        ident = re.match(r\"\\w*\", remainder).group(0)        self.pos += len(ident)        return ident",
        "labels_text": "Parse an identifier and return it possibly an empty string Updates po"
    },
    {
        "input_text": "summarize: def _parse(template):        parser = Parser(template)    parser.parse_expression()    parts = parser.parts    remainder = parser.string[parser.pos :]    if remainder:        parts.append(remainder)    return Expression(parts)",
        "labels_text": "Parse a toplevel template string Expression Any extraneous text is considered literal text"
    },
    {
        "input_text": "summarize: def interpret(self, values={}, functions={}):                return self.expr.evaluate(Environment(values, functions))",
        "labels_text": "Like substitute but force the interpreter rather than the compiled version to be used The interpreter includes exceptionhandling code for missing variable and buggy template function but is much slower"
    },
    {
        "input_text": "summarize: def substitute(self, values={}, functions={}):                try:            res = self.compiled(values, functions)        except Exception:  # Handle any exceptions thrown by compiled version.            res = self.interpret(values, functions)        return res",
        "labels_text": "Evaluate the template given the value and function"
    },
    {
        "input_text": "summarize: def __init__(self, path):                self.path = path        self.extm3u = False        self.media_list = []",
        "labels_text": "path is the absolute path to the playlist file The playlist file type mu or mu is determined by the ending being mu and the file path contained in the list being utf encoded Since the list is passed from the outside this is currently out of control of this class"
    },
    {
        "input_text": "summarize: def set_contents(self, media_list, extm3u=True):                self.media_list = media_list        self.extm3u = extm3u",
        "labels_text": "Sets selfmedialist to a list of medium file path Also set additional flag changing the final mufiles format medialist is a list of path to medium file that should be added to the playlist relative or absolute path thats the responsibility of the caller By default the extmu flag is set to ensure a saveoperation writes an muextended playlist comment EXTMU at the top of the file"
    },
    {
        "input_text": "summarize: def acquire(self):                with self.mutex:            assert not self.poisoned            assert self.nthreads >= 0            self.nthreads += 1",
        "labels_text": "Indicate that a thread will start putting into this queue Should not be called after the queue is already poisoned"
    },
    {
        "input_text": "summarize: def multiple(messages):        return MultiMessage(messages)",
        "labels_text": "Yield multiplemessage from a pipeline stage to send multiple value to the next pipeline stage"
    },
    {
        "input_text": "summarize: def stage(func):        def coro(*args):        task = None        while True:            task = yield task            task = func(*(args + (task,)))    return coro",
        "labels_text": "Decorate a function to become a simple stage stage def addn i return i n pipe Pipeline iter add listpipepull"
    },
    {
        "input_text": "summarize: def mutator_stage(func):        def coro(*args):        task = None        while True:            task = yield task            func(*(args + (task,)))    return coro",
        "labels_text": "Decorate a function that manipulates item in a coroutine to become a simple stage mutatorstage def setkeykey item itemkey True pipe Pipeline iterx False a False setkeyx listpipepull x True a False x True"
    },
    {
        "input_text": "summarize: def _allmsgs(obj):        if isinstance(obj, MultiMessage):        return obj.messages    elif obj == BUBBLE:        return []    else:        return [obj]",
        "labels_text": "Returns a list of all the message encapsulated in obj If obj is a MultiMessage return it enclosed message If obj is BUBBLE return an empty list Otherwise return a list containing obj"
    },
    {
        "input_text": "summarize: def abort(self):                with self.abort_lock:            self.abort_flag = True            # Ensure that we are not blocking on a queue read or write.            if hasattr(self, \"in_queue\"):                _invalidate_queue(self.in_queue, POISON)            if hasattr(self, \"out_queue\"):                _invalidate_queue(self.out_queue, POISON)",
        "labels_text": "Shut down the thread at the next chance possible"
    },
    {
        "input_text": "summarize: def abort_all(self, exc_info):                self.exc_info = exc_info        for thread in self.all_threads:            thread.abort()",
        "labels_text": "Abort all other thread in the system for an exception"
    },
    {
        "input_text": "summarize: def __init__(self, stages):                if len(stages) < 2:            raise ValueError(\"pipeline must have at least two stages\")        self.stages = []        for stage in stages:            if isinstance(stage, (list, tuple)):                self.stages.append(stage)            else:                # Default to one thread per stage.                self.stages.append((stage,))",
        "labels_text": "Makes a new pipeline from a list of coroutines There must be at least two stage"
    },
    {
        "input_text": "summarize: def run_sequential(self):                list(self.pull())",
        "labels_text": "Run the pipeline sequentially in the current thread The stage are run one after the other Only the first coroutine in each stage is used"
    },
    {
        "input_text": "summarize: def _gerund(self):                if \" \" in self.verb:            return self.verb        gerund = self.verb[:-1] if self.verb.endswith(\"e\") else self.verb        gerund += \"ing\"        return gerund",
        "labels_text": "Generate a likely gerund form of the English verb"
    },
    {
        "input_text": "summarize: def _reasonstr(self):                if isinstance(self.reason, str):            return self.reason        elif isinstance(self.reason, bytes):            return self.reason.decode(\"utf-8\", \"ignore\")        elif hasattr(self.reason, \"strerror\"):  # i.e., EnvironmentError            return self.reason.strerror        else:            return '\"{}\"'.format(str(self.reason))",
        "labels_text": "Get the reason a a string"
    },
    {
        "input_text": "summarize: def get_message(self):                raise NotImplementedError",
        "labels_text": "Create the humanreadable description of the error sans introduction"
    },
    {
        "input_text": "summarize: def log(self, logger):                if self.tb:            logger.debug(self.tb)        logger.error(\"{0}: {1}\", self.error_kind, self.args[0])",
        "labels_text": "Log to the provided logger a humanreadable message a an error and a verbose traceback a a debug message"
    },
    {
        "input_text": "summarize: def normpath(path: bytes) -> bytes:        path = syspath(path, prefix=False)    path = os.path.normpath(os.path.abspath(os.path.expanduser(path)))    return bytestring_path(path)",
        "labels_text": "Provide the canonical form of the path suitable for storing in the database"
    },
    {
        "input_text": "summarize: def ancestry(path: bytes) -> List[str]:        out = []    last_path = None    while path:        path = os.path.dirname(path)        if path == last_path:            break        last_path = path        if path:            # don't yield ''            out.insert(0, path)    return out",
        "labels_text": "Return a list consisting of path parent directory it grandparent and so on For instance ancestrybabc a ab The argument should not be the result of a call to syspath"
    },
    {
        "input_text": "summarize: def path_as_posix(path: bytes) -> bytes:        return path.replace(b\"\\\\\", b\"/\")",
        "labels_text": "Return the string representation of the path with forward slash"
    },
    {
        "input_text": "summarize: def mkdirall(path: bytes):        for ancestor in ancestry(path):        if not os.path.isdir(syspath(ancestor)):            try:                os.mkdir(syspath(ancestor))            except OSError as exc:                raise FilesystemError(                    exc, \"create\", (ancestor,), traceback.format_exc()                )",
        "labels_text": "Make all the enclosing directory of path like mkdir p on the parent"
    },
    {
        "input_text": "summarize: def fnmatch_all(names: Sequence[bytes], patterns: Sequence[bytes]) -> bool:        for name in names:        matches = False        for pattern in patterns:            matches = fnmatch.fnmatch(name, pattern)            if matches:                break        if not matches:            return False    return True",
        "labels_text": "Determine whether all string in name match at least one of the pattern which should be shell glob expression"
    },
    {
        "input_text": "summarize: def components(path: AnyStr) -> MutableSequence[AnyStr]:        comps = []    ances = ancestry(path)    for anc in ances:        comp = os.path.basename(anc)        if comp:            comps.append(comp)        else:  # root            comps.append(anc)    last = os.path.basename(path)    if last:        comps.append(last)    return comps",
        "labels_text": "Return a list of the path component in path For instance componentsbabc a b c The argument should not be the result of a call to syspath"
    },
    {
        "input_text": "summarize: def arg_encoding() -> str:        return sys.getfilesystemencoding()",
        "labels_text": "Get the encoding for commandline argument and other OS localesensitive string"
    },
    {
        "input_text": "summarize: def _fsencoding() -> str:        encoding = sys.getfilesystemencoding() or sys.getdefaultencoding()    if encoding == \"mbcs\":        # On Windows, a broken encoding known to Python as \"MBCS\" is        # used for the filesystem. However, we only use the Unicode API        # for Windows paths, so the encoding is actually immaterial so        # we can avoid dealing with this nastiness. We arbitrarily        # choose UTF-8.        encoding = \"utf-8\"    return encoding",
        "labels_text": "Get the system filesystem encoding On Windows this is always UTF not MBCS"
    },
    {
        "input_text": "summarize: def samefile(p1: bytes, p2: bytes) -> bool:        if p1 == p2:        return True    return shutil._samefile(syspath(p1), syspath(p2))",
        "labels_text": "Safer equality for path"
    },
    {
        "input_text": "summarize: def remove(path: Optional[bytes], soft: bool = True):        path = syspath(path)    if not path or (soft and not os.path.exists(path)):        return    try:        os.remove(path)    except OSError as exc:        raise FilesystemError(exc, \"delete\", (path,), traceback.format_exc())",
        "labels_text": "Remove the file If soft then no error will be raised if the file doe not exist"
    },
    {
        "input_text": "summarize: def copy(path: bytes, dest: bytes, replace: bool = False):        if samefile(path, dest):        return    path = syspath(path)    dest = syspath(dest)    if not replace and os.path.exists(dest):        raise FilesystemError(\"file exists\", \"copy\", (path, dest))    try:        shutil.copyfile(path, dest)    except OSError as exc:        raise FilesystemError(exc, \"copy\", (path, dest), traceback.format_exc())",
        "labels_text": "Copy a plain file Permissions are not copied If dest already exists raise a FilesystemError unless replace is True Has no effect if path is the same a dest Paths are translated to system path before the syscall"
    },
    {
        "input_text": "summarize: def sanitize_path(    path: str,    replacements: Optional[Sequence[Sequence[Union[Pattern, str]]]] = None,) -> str:        replacements = replacements or CHAR_REPLACE    comps = components(path)    if not comps:        return \"\"    for i, comp in enumerate(comps):        for regex, repl in replacements:            comp = regex.sub(repl, comp)        comps[i] = comp    return os.path.join(*comps)",
        "labels_text": "Takes a path a a Unicode string and make sure that it is legal Returns a new path Only work with fragment wont work reliably on Windows when a path begin with a drive letter Path separator including altsep should already be cleaned from the path component If replacement is specified it is used instead of the default set of replacement it must be a list of compiled regex replacement string pair"
    },
    {
        "input_text": "summarize: def truncate_path(path: AnyStr, length: int = MAX_FILENAME_LENGTH) -> AnyStr:        comps = components(path)    out = [c[:length] for c in comps]    base, ext = os.path.splitext(comps[-1])    if ext:        # Last component has an extension.        base = base[: length - len(ext)]        out[-1] = base + ext    return os.path.join(*out)",
        "labels_text": "Given a bytestring path or a Unicode path fragment truncate the component to a legal length In the last component the extension is preserved"
    },
    {
        "input_text": "summarize: def str2bool(value: str) -> bool:        return value.lower() in (\"yes\", \"1\", \"true\", \"t\", \"y\")",
        "labels_text": "Returns a boolean reflecting a humanentered string"
    },
    {
        "input_text": "summarize: def as_string(value: Any) -> str:        if value is None:        return \"\"    elif isinstance(value, memoryview):        return bytes(value).decode(\"utf-8\", \"ignore\")    elif isinstance(value, bytes):        return value.decode(\"utf-8\", \"ignore\")    else:        return str(value)",
        "labels_text": "Convert a value to a Unicode object for matching with a query None becomes the empty string Bytestrings are silently decoded"
    },
    {
        "input_text": "summarize: def plurality(objs: Sequence[T]) -> T:        c = Counter(objs)    if not c:        raise ValueError(\"sequence must be non-empty\")    return c.most_common(1)[0]",
        "labels_text": "Given a sequence of hashble object return the object that is most common in the set and the it number of appearance The sequence must contain at least one object"
    },
    {
        "input_text": "summarize: def convert_command_args(args: List[bytes]) -> List[str]:        assert isinstance(args, list)    def convert(arg) -> str:        if isinstance(arg, bytes):            return os.fsdecode(arg)        return arg    return [convert(a) for a in args]",
        "labels_text": "Convert command argument which may either be byte or str object to uniformly surrogateescaped string"
    },
    {
        "input_text": "summarize: def max_filename_length(path: AnyStr, limit=MAX_FILENAME_LENGTH) -> int:        if hasattr(os, \"statvfs\"):        try:            res = os.statvfs(path)        except OSError:            return limit        return min(res[9], limit)    else:        return limit",
        "labels_text": "Attempt to determine the maximum filename length for the filesystem containing path If the value is greater than limit then limit is used instead to prevent error when a filesystem misreports it capacity If it cannot be determined eg on Windows return limit"
    },
    {
        "input_text": "summarize: def open_anything() -> str:        sys_name = platform.system()    if sys_name == \"Darwin\":        base_cmd = \"open\"    elif sys_name == \"Windows\":        base_cmd = \"start\"    else:  # Assume Unix        base_cmd = \"xdg-open\"    return base_cmd",
        "labels_text": "Return the system command that dispatch execution to the correct program"
    },
    {
        "input_text": "summarize: def editor_command() -> str:        return (        os.environ.get(\"VISUAL\") or os.environ.get(\"EDITOR\") or open_anything()    )",
        "labels_text": "Get a command for opening a text file First try environment variable VISUAL followed by EDITOR As last resort fall back to openanything the platformspecific tool for opening file in general"
    },
    {
        "input_text": "summarize: def interactive_open(targets: Sequence[str], command: str):        assert command    # Split the command string into its arguments.    try:        args = shlex.split(command)    except ValueError:  # Malformed shell tokens.        args = [command]    args.insert(0, args[0])  # for argv[0]    args += targets    return os.execlp(*args)",
        "labels_text": "Open the file in target by execing a new command given a a Unicode string The new program take over and Python execution end this doe not fork a subprocess Can raise OSError"
    },
    {
        "input_text": "summarize: def raw_seconds_short(string: str) -> float:        match = re.match(r\"^(\\d+):([0-5]\\d)$\", string)    if not match:        raise ValueError(\"String not in M:SS format\")    minutes, seconds = map(int, match.groups())    return float(minutes * 60 + seconds)",
        "labels_text": "Formats a humanreadable MSS string a a float number of second Raises ValueError if the conversion cannot take place due to string not being in the right format"
    },
    {
        "input_text": "summarize: def par_map(transform: Callable, items: Iterable):        pool = ThreadPool()    pool.map(transform, items)    pool.close()    pool.join()",
        "labels_text": "Apply the function transform to all the element in the iterable item like maptransform item but with no return value The parallelism us thread not process so this is only useful for IObound transforms"
    },
    {
        "input_text": "summarize: def get_module_tempdir(module: str) -> Path:        module = module.replace(\"beets.\", \"\").replace(\".\", \"_\")    return Path(tempfile.gettempdir()) / \"beets\" / module",
        "labels_text": "Return the temporary directory for the given module The directory is created within the tmpbeetsmodule directory on Linux or the equivalent temporary directory on other system Dots in the module name are replaced by underscore"
    },
    {
        "input_text": "summarize: def clean_module_tempdir(module: str) -> None:        tempdir = get_module_tempdir(module)    shutil.rmtree(tempdir, ignore_errors=True)    with suppress(OSError):        # remove parent (/tmp/beets) directory if it is empty        tempdir.parent.rmdir()",
        "labels_text": "Clean the temporary directory for the given module"
    },
    {
        "input_text": "summarize: def get_temp_filename(    module: str,    prefix: str = \"\",    path: PathLike | None = None,    suffix: str = \"\",) -> bytes:        if not suffix and path:        suffix = Path(os.fsdecode(path)).suffix    tempdir = get_module_tempdir(module)    tempdir.mkdir(parents=True, exist_ok=True)    _, filename = tempfile.mkstemp(dir=tempdir, prefix=prefix, suffix=suffix)    return bytestring_path(filename)",
        "labels_text": "Return temporary filename for the given module and prefix The filename start with the given prefix If suffix is given it is used a the file extension If path is given we use the same suffix"
    },
    {
        "input_text": "summarize: def call(args):        try:        return util.command_output(args).stdout    except subprocess.CalledProcessError as e:        raise ABSubmitError(            \"{} exited with status {}\".format(args[0], e.returncode)        )",
        "labels_text": "Execute the command and return it output Raise a AnalysisABSubmitError on failure"
    },
    {
        "input_text": "summarize: def import_task_files(self, session, task):                self._fetch_info(task.imported_items(), False, True)",
        "labels_text": "Function is called upon beet import"
    },
    {
        "input_text": "summarize: def _generate_urls(base_url, mbid):        for level in LEVELS:        yield base_url + mbid + level",
        "labels_text": "Generates AcousticBrainz end point url for given mbid"
    },
    {
        "input_text": "summarize: def rewriter(field, simple_rules, advanced_rules):        def fieldfunc(item):        value = item._values_fixed[field]        for pattern, replacement in simple_rules:            if pattern.match(value.lower()):                # Rewrite activated.                return replacement        for query, replacement in advanced_rules:            if query.match(item):                # Rewrite activated.                return replacement        # Not activated; return original value.        return value    return fieldfunc",
        "labels_text": "Template field function factory Create a template field function that rewrite the given field with the given rewriting rule simplerules must be a list of pattern replacement pair advancedrules must be a list of query replacement pair"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.album_template_fields[\"atypes\"] = self._atypes        self.config.add(            {                \"types\": [                    (\"ep\", \"EP\"),                    (\"single\", \"Single\"),                    (\"soundtrack\", \"OST\"),                    (\"live\", \"Live\"),                    (\"compilation\", \"Anthology\"),                    (\"remix\", \"Remix\"),                ],                \"ignore_va\": [\"compilation\"],                \"bracket\": \"[]\",            }        )",
        "labels_text": "Init AlbumTypesPlugin"
    },
    {
        "input_text": "summarize: def from_app(cls) -> Self:                return cls(current_app.config[\"lib\"], request.args)",
        "labels_text": "Initialise the document using the global app and request"
    },
    {
        "input_text": "summarize: def error(status, title, detail):                document = {            \"errors\": [{\"status\": status, \"title\": title, \"detail\": detail}]        }        return make_response(document, status)",
        "labels_text": "Make a response for an error following the JSONAPI spec Args status An HTTP status code string eg Not Found title A short humanreadable summary of the problem detail A humanreadable explanation specific to this occurrence of the problem"
    },
    {
        "input_text": "summarize: def get_attribute_converter(cls, beets_attr: str) -> Type[SQLiteType]:                try:            # Look for field in list of Album fields            # and get python type of database type.            # See beets.library.Album and beets.dbcore.types            return cls.model_cls._fields[beets_attr].model_type        except KeyError:            # Fall back to string (NOTE: probably not good)            return str",
        "labels_text": "Work out what data type an attribute should be for beet Args beetsattr The name of the beet attribute eg title"
    },
    {
        "input_text": "summarize: def single_resource_document(self, resource_object):                document = {\"data\": resource_object}        include_str = self.args.get(\"include\", None)        if include_str:            # [document[\"data\"]] is because arg needs to be list            document[\"included\"] = self.get_included(                [document[\"data\"]], include_str            )        return document",
        "labels_text": "Build document for a specific requested resource Args resourceobject A dictionary in the form of a JSONAPI resource object"
    },
    {
        "input_text": "summarize: def get_collection(self, query=None, sort=None):                return self.lib.items(query, sort)",
        "labels_text": "Get Item object from the library Args query A beet Query object or a beet query string sort A beet Sort object"
    },
    {
        "input_text": "summarize: def get_attribute_converter(cls, beets_attr: str) -> Type[SQLiteType]:                # filesize is a special field (read from disk not db?)        if beets_attr == \"filesize\":            return int        return super().get_attribute_converter(beets_attr)",
        "labels_text": "Work out what data type an attribute should be for beet Args beetsattr The name of the beet attribute eg title"
    },
    {
        "input_text": "summarize: def single_resource(self, track_id):                track = self.lib.get_item(track_id)        if not track:            return self.error(                \"404 Not Found\",                \"No track with the requested id.\",                \"There is no track with an id of {} in the library.\".format(                    track_id                ),            )        return self.single_resource_document(            self.get_resource_object(self.lib, track)        )",
        "labels_text": "Get track from the library and build a document Args trackid The beet id of the track integer"
    },
    {
        "input_text": "summarize: def get_collection(self, query=None, sort=None):                return self.lib.albums(query, sort)",
        "labels_text": "Get Album object from the library Args query A beet Query object or a beet query string sort A beet Sort object"
    },
    {
        "input_text": "summarize: def single_resource(self, album_id):                album = self.lib.get_album(album_id)        if not album:            return self.error(                \"404 Not Found\",                \"No album with the requested id.\",                \"There is no album with an id of {} in the library.\".format(                    album_id                ),            )        return self.single_resource_document(            self.get_resource_object(self.lib, album)        )",
        "labels_text": "Get album from the library and build a document Args albumid The beet id of the album integer"
    },
    {
        "input_text": "summarize: def get_collection(self, query=None, sort=None):                # Gets only tracks with matching artist information        tracks = self.lib.items(query, sort)        collection = []        for track in tracks:            # Do not add duplicates            if track.artist not in collection:                collection.append(track.artist)        return collection",
        "labels_text": "Get a list of artist name from the library Args query A beet Query object or a beet query string sort A beet Sort object"
    },
    {
        "input_text": "summarize: def single_resource(self, artist_id):                artist_resource = self.get_resource_object(self.lib, artist_id)        if not artist_resource:            return self.error(                \"404 Not Found\",                \"No artist with the requested id.\",                \"There is no artist with an id of {} in the library.\".format(                    artist_id                ),            )        return self.single_resource_document(artist_resource)",
        "labels_text": "Get info for the requested artist and build a document Args artistid A string which is the artist name"
    },
    {
        "input_text": "summarize: def safe_filename(fn):        # Rule out any directories.    if os.path.basename(fn) != fn:        return False    # In single names, rule out Unix directory traversal names.    if fn in (\".\", \"..\"):        return False    return True",
        "labels_text": "Check whether a string is a simple nonpath filename For example footxt is safe because it is a plain filename But foobartxt and footxt and are all nonsafe because they can traverse to other directory other than the current one"
    },
    {
        "input_text": "summarize: def single_resource(self, image_id):                image_resource = self.get_resource_object(self.lib, image_id)        if not image_resource:            return self.error(                \"404 Not Found\",                \"No image with the requested id.\",                \"There is no image with an id of {} in the library.\".format(                    image_id                ),            )        return self.single_resource_document(image_resource)",
        "labels_text": "Get info for the requested image and build a document Args imageid A string in the form parenttypeparentidimgfilename"
    },
    {
        "input_text": "summarize: def server_info():        return {\"data\": {\"type\": \"server\", \"id\": \"0\", \"attributes\": SERVER_INFO}}",
        "labels_text": "Respond with info about the server"
    },
    {
        "input_text": "summarize: def all_tracks():        return TrackDocument.from_app().all_resources()",
        "labels_text": "Respond with a list of all track and related information"
    },
    {
        "input_text": "summarize: def single_track(track_id):        return TrackDocument.from_app().single_resource(track_id)",
        "labels_text": "Respond with info about the specified track Args trackid The id of the track provided in the URL integer"
    },
    {
        "input_text": "summarize: def all_albums():        return AlbumDocument.from_app().all_resources()",
        "labels_text": "Respond with a list of all album and related information"
    },
    {
        "input_text": "summarize: def single_album(album_id):        return AlbumDocument.from_app().single_resource(album_id)",
        "labels_text": "Respond with info about the specified album Args albumid The id of the album provided in the URL integer"
    },
    {
        "input_text": "summarize: def all_artists():        return ArtistDocument.from_app().all_resources()",
        "labels_text": "Respond with a list of all artist and related information"
    },
    {
        "input_text": "summarize: def single_artist(artist_id):        return ArtistDocument.from_app().single_resource(artist_id)",
        "labels_text": "Respond with info about the specified artist Args artistid The id of the artist provided in the URL A string which is the artist name"
    },
    {
        "input_text": "summarize: def single_image(image_id):        return ImageDocument.from_app().single_resource(image_id)",
        "labels_text": "Respond with info about the specified image Args imageid The id of the image provided in the URL A string in the form parenttypeparentidimgfilename"
    },
    {
        "input_text": "summarize: def image_file(image_id):        img_path = ImageDocument.get_image_path(current_app.config[\"lib\"], image_id)    if not img_path:        return AURADocument.error(            \"404 Not Found\",            \"No image with the requested id.\",            \"There is no image with an id of {} in the library\".format(                image_id            ),        )    return send_file(img_path)",
        "labels_text": "Supply an image file for the specified image Args imageid The id of the image provided in the URL A string in the form parenttypeparentidimgfilename"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()",
        "labels_text": "Add configuration option for the AURA plugin"
    },
    {
        "input_text": "summarize: def string_match(cls, pattern, val):                # smartcase        if pattern.islower():            val = val.lower()        pattern = unidecode(pattern)        val = unidecode(val)        return pattern in val",
        "labels_text": "Convert both pattern and string to plain ASCII before matching If pattern is all lower case also convert string to lower case so match is also case insensitive"
    },
    {
        "input_text": "summarize: def col_clause(self):                clause = f\"unidecode({self.field})\"        if self.pattern.islower():            clause = f\"lower({clause})\"        return rf\"{clause} LIKE ? ESCAPE '\\'\", [f\"%{unidecode(self.pattern)}%\"]",
        "labels_text": "Compare ascii version of the pattern"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.config.add(            {                \"prefix\": \"#\",            }        )",
        "labels_text": "Default prefix for selecting bareASCII matching is"
    },
    {
        "input_text": "summarize: def queries(self):                prefix = self.config[\"prefix\"].as_str()        return {prefix: BareascQuery}",
        "labels_text": "Register bareASCII matching"
    },
    {
        "input_text": "summarize: def commands(self):                cmd = ui.Subcommand(            \"bareasc\", help=\"unidecode version of beet list command\"        )        cmd.parser.usage += (            \"\\n\" \"Example: %prog -f '$album: $title' artist:beatles\"        )        cmd.parser.add_all_common_options()        cmd.func = self.unidecode_list        return [cmd]",
        "labels_text": "Add bareasc command a unidecode version of list"
    },
    {
        "input_text": "summarize: def unidecode_list(self, lib, opts, args):                query = decargs(args)        album = opts.album        # Copied from commands.py - list_items        if album:            for album in lib.albums(query):                bare = unidecode(str(album))                print_(bare)        else:            for item in lib.items(query):                bare = unidecode(str(item))                print_(bare)",
        "labels_text": "Emulate normal list command but with unidecode output"
    },
    {
        "input_text": "summarize: def __init__(self, c_key, c_secret, auth_key=None, auth_secret=None):                self.api = OAuth1Session(            client_key=c_key,            client_secret=c_secret,            resource_owner_key=auth_key,            resource_owner_secret=auth_secret,            callback_uri=\"oob\",        )        self.api.headers = {\"User-Agent\": USER_AGENT}",
        "labels_text": "Initiate the client with OAuth information For the initial authentication with the backend authkey and authsecret can be None Use getauthorizeurl and getaccesstoken to obtain them for subsequent us of the API param ckey OAuth client key param csecret OAuth client secret param authkey OAuth resource owner key param authsecret OAuth resource owner secret"
    },
    {
        "input_text": "summarize: def get_authorize_url(self):                self.api.fetch_request_token(            self._make_url(\"/identity/1/oauth/request-token\")        )        return self.api.authorization_url(            self._make_url(\"/identity/1/oauth/authorize\")        )",
        "labels_text": "Generate the URL for the user to authorize the application Retrieves a request token from the Beatport API and return the corresponding authorization URL on their end that the user ha to visit This is the first step of the initial authorization process with the API Once the user ha visited the URL call pymethodgetaccesstoken with the displayed data to complete the process return Authorization URL for the user to visit rtype unicode"
    },
    {
        "input_text": "summarize: def get_access_token(self, auth_data):                self.api.parse_authorization_response(            \"https://beets.io/auth?\" + auth_data        )        access_data = self.api.fetch_access_token(            self._make_url(\"/identity/1/oauth/access-token\")        )        return access_data[\"oauth_token\"], access_data[\"oauth_token_secret\"]",
        "labels_text": "Obtain the final access token and secret for the API param authdata URLencoded authorization data a displayed at the authorization url obtained via pymethgetauthorizeurl after signing in type authdata unicode return OAuth resource owner key and secret rtype unicode unicode tuple"
    },
    {
        "input_text": "summarize: def get_release(self, beatport_id):                response = self._get(\"/catalog/3/releases\", id=beatport_id)        if response:            release = BeatportRelease(response[0])            release.tracks = self.get_release_tracks(beatport_id)            return release        return None",
        "labels_text": "Get information about a single release param beatportid Beatport ID of the release return The matching release rtype pyclassBeatportRelease"
    },
    {
        "input_text": "summarize: def get_release_tracks(self, beatport_id):                response = self._get(            \"/catalog/3/tracks\", releaseId=beatport_id, perPage=100        )        return [BeatportTrack(t) for t in response]",
        "labels_text": "Get all track for a given release param beatportid Beatport ID of the release return Tracks in the matching release rtype list of pyclassBeatportTrack"
    },
    {
        "input_text": "summarize: def get_track(self, beatport_id):                response = self._get(\"/catalog/3/tracks\", id=beatport_id)        return BeatportTrack(response[0])",
        "labels_text": "Get information about a single track param beatportid Beatport ID of the track return The matching track rtype pyclassBeatportTrack"
    },
    {
        "input_text": "summarize: def _make_url(self, endpoint):                if not endpoint.startswith(\"/\"):            endpoint = \"/\" + endpoint        return self._api_base + endpoint",
        "labels_text": "Get complete URL for a given API endpoint"
    },
    {
        "input_text": "summarize: def _get(self, endpoint, **kwargs):                try:            response = self.api.get(self._make_url(endpoint), params=kwargs)        except Exception as e:            raise BeatportAPIError(                \"Error connecting to Beatport API: {}\".format(e)            )        if not response:            raise BeatportAPIError(                \"Error {0.status_code} for '{0.request.path_url}\".format(                    response                )            )        return response.json()[\"results\"]",
        "labels_text": "Perform a GET request on a given API endpoint Automatically extract result data from the response and convert HTTP exception into pyclassBeatportAPIError object"
    },
    {
        "input_text": "summarize: def _tokenfile(self):                return self.config[\"tokenfile\"].get(confuse.Filename(in_app_dir=True))",
        "labels_text": "Get the path to the JSON file for storing the OAuth token"
    },
    {
        "input_text": "summarize: def album_distance(self, items, album_info, mapping):                return get_distance(            data_source=self.data_source, info=album_info, config=self.config        )",
        "labels_text": "Returns the Beatport source weight and the maximum source weight for album"
    },
    {
        "input_text": "summarize: def track_distance(self, item, track_info):                return get_distance(            data_source=self.data_source, info=track_info, config=self.config        )",
        "labels_text": "Returns the Beatport source weight and the maximum source weight for individual track"
    },
    {
        "input_text": "summarize: def candidates(self, items, artist, release, va_likely, extra_tags=None):                if va_likely:            query = release        else:            query = f\"{artist} {release}\"        try:            return self._get_releases(query)        except BeatportAPIError as e:            self._log.debug(\"API Error: {0} (query: {1})\", e, query)            return []",
        "labels_text": "Returns a list of AlbumInfo object for beatport search result matching release and artist if not various"
    },
    {
        "input_text": "summarize: def item_candidates(self, item, artist, title):                query = f\"{artist} {title}\"        try:            return self._get_tracks(query)        except BeatportAPIError as e:            self._log.debug(\"API Error: {0} (query: {1})\", e, query)            return []",
        "labels_text": "Returns a list of TrackInfo object for beatport search result matching title and artist"
    },
    {
        "input_text": "summarize: def album_for_id(self, release_id):                self._log.debug(\"Searching for release {0}\", release_id)        release_id = self._get_id(\"album\", release_id, self.id_regex)        if release_id is None:            self._log.debug(\"Not a valid Beatport release ID.\")            return None        release = self.client.get_release(release_id)        if release:            return self._get_album_info(release)        return None",
        "labels_text": "Fetches a release by it Beatport ID and return an AlbumInfo object or None if the query is not a valid ID or release is not found"
    },
    {
        "input_text": "summarize: def _get_artist(self, artists):                return MetadataSourcePlugin.get_artist(            artists=artists, id_key=0, name_key=1        )",
        "labels_text": "Returns an artist string all artist and an artistid the main artist for a list of Beatport release or track artist"
    },
    {
        "input_text": "summarize: def _get_tracks(self, query):                bp_tracks = self.client.search(query, release_type=\"track\")        tracks = [self._get_track_info(x) for x in bp_tracks]        return tracks",
        "labels_text": "Returns a list of TrackInfo object for a Beatport query"
    },
    {
        "input_text": "summarize: def func(self, lib, opts, args):                move = ui.should_move(opts.move)        pretend = opts.pretend        write = ui.should_write(opts.write)        query = ui.decargs(args)        self.singletons(lib, query, move, pretend, write)        self.albums(lib, query, move, pretend, write)",
        "labels_text": "Command handler for the bpsync function"
    },
    {
        "input_text": "summarize: def pairwise(iterable):    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"    a, b = tee(iterable)    next(b, None)    return zip(a, b)",
        "labels_text": "s s s s s"
    },
    {
        "input_text": "summarize: def complete_year_spans(spans):        spans.sort(key=lambda x: x[\"from\"])    for x, y in pairwise(spans):        if \"to\" not in x:            x[\"to\"] = y[\"from\"] - 1    if spans and \"to\" not in spans[-1]:        spans[-1][\"to\"] = datetime.now().year",
        "labels_text": "Set the to value of span if empty and sort them chronologically"
    },
    {
        "input_text": "summarize: def build_year_spans(year_spans_str):        spans = []    for elem in year_spans_str:        spans.append(span_from_str(elem))    complete_year_spans(spans)    return spans",
        "labels_text": "Build a chronologically ordered list of span dict from unordered span stringlist"
    },
    {
        "input_text": "summarize: def format_span(fmt, yearfrom, yearto, fromnchars, tonchars):        args = str(yearfrom)[-fromnchars:]    if tonchars:        args = (str(yearfrom)[-fromnchars:], str(yearto)[-tonchars:])    return fmt % args",
        "labels_text": "Return a span string representation"
    },
    {
        "input_text": "summarize: def extract_modes(spans):        rangelen = sorted([x[\"to\"] - x[\"from\"] + 1 for x in spans])    deflen = sorted(rangelen, key=rangelen.count)[-1]    reprs = [str2fmt(x[\"str\"]) for x in spans]    deffmt = sorted(reprs, key=reprs.count)[-1]    return deflen, deffmt",
        "labels_text": "Extract the most common span length and representation format"
    },
    {
        "input_text": "summarize: def find_bucket_year(self, year):                for ys in self.year_spans:            if ys[\"from\"] <= int(year) <= ys[\"to\"]:                if \"str\" in ys:                    return ys[\"str\"]                else:                    return format_span(                        self.ys_repr_mode[\"fmt\"],                        ys[\"from\"],                        ys[\"to\"],                        self.ys_repr_mode[\"fromnchars\"],                        self.ys_repr_mode[\"tonchars\"],                    )        return year",
        "labels_text": "Return bucket that match given year or return the year if no matching bucket"
    },
    {
        "input_text": "summarize: def find_bucket_alpha(self, s):                for i, span in enumerate(self.alpha_spans):            if span.match(s):                return self.config[\"bucket_alpha\"].get()[i]        return s[0].upper()",
        "labels_text": "Return alpharange bucket that match given string or return the string initial if no matching bucket"
    },
    {
        "input_text": "summarize: def prefix(it, count):        for i, v in enumerate(it):        if i >= count:            break        yield v",
        "labels_text": "Truncate an iterable to at most count item"
    },
    {
        "input_text": "summarize: def _all_releases(items):        # Count the number of \"hits\" for each release.    relcounts = defaultdict(int)    for item in items:        if item.path not in _matches:            continue        _, release_ids = _matches[item.path]        for release_id in release_ids:            relcounts[release_id] += 1    for release_id, count in relcounts.items():        if float(count) / len(items) > COMMON_REL_THRESH:            yield release_id",
        "labels_text": "Given an iterable of Items determines according to Acoustid which release the item have in common Generates release IDs"
    },
    {
        "input_text": "summarize: def fingerprint_task(log, task, session):        items = task.items if task.is_album else [task.item]    for item in items:        acoustid_match(log, item.path)",
        "labels_text": "Fingerprint each item in the task for later use during the autotagging candidate search"
    },
    {
        "input_text": "summarize: def apply_acoustid_metadata(task, session):        for item in task.imported_items():        if item.path in _fingerprints:            item.acoustid_fingerprint = _fingerprints[item.path]        if item.path in _acoustids:            item.acoustid_id = _acoustids[item.path]",
        "labels_text": "Apply Acoustid metadata fingerprint and ID to the task item"
    },
    {
        "input_text": "summarize: def replace_ext(path, ext):        ext_dot = b\".\" + ext    return os.path.splitext(path)[0] + ext_dot",
        "labels_text": "Return the path with it extension replaced by ext The new extension must not contain a leading dot"
    },
    {
        "input_text": "summarize: def _parallel_convert(        self,        dest,        keep_new,        path_formats,        fmt,        pretend,        link,        hardlink,        threads,        items,    ):                convert = [            self.convert_item(                dest, keep_new, path_formats, fmt, pretend, link, hardlink            )            for _ in range(threads)        ]        pipe = util.pipeline.Pipeline([iter(items), convert])        pipe.run_parallel()",
        "labels_text": "Run the convertitem function for every item on a many thread a defined in thread"
    },
    {
        "input_text": "summarize: def commands(self):                deezer_update_cmd = ui.Subcommand(            \"deezerupdate\", help=f\"Update {self.data_source} rank\"        )        def func(lib, opts, args):            items = lib.items(ui.decargs(args))            self.deezerupdate(items, ui.should_write())        deezer_update_cmd.func = func        return [deezer_update_cmd]",
        "labels_text": "Add beet UI command to interact with Deezer"
    },
    {
        "input_text": "summarize: def _construct_search_query(filters=None, keywords=\"\"):                query_components = [            keywords,            \" \".join(f'{k}:\"{v}\"' for k, v in filters.items()),        ]        query = \" \".join([q for q in query_components if q])        if not isinstance(query, str):            query = query.decode(\"utf8\")        return unidecode.unidecode(query)",
        "labels_text": "Construct a query string with the specified filter and keywords to be provided to the Deezer Search API httpsdevelopersdeezercomapisearch param filter Optional Field filter to apply type filter dict param keywords Optional Query keywords to use type keywords str return Query string to be provided to the Search API rtype str"
    },
    {
        "input_text": "summarize: def reset_auth(self):                os.remove(self._tokenfile())        self.setup()",
        "labels_text": "Delete token file redo the auth step"
    },
    {
        "input_text": "summarize: def _tokenfile(self):                return self.config[\"tokenfile\"].get(confuse.Filename(in_app_dir=True))",
        "labels_text": "Get the path to the JSON file for storing the OAuth token"
    },
    {
        "input_text": "summarize: def album_distance(self, items, album_info, mapping):                return get_distance(            data_source=\"Discogs\", info=album_info, config=self.config        )",
        "labels_text": "Returns the album distance"
    },
    {
        "input_text": "summarize: def track_distance(self, item, track_info):                return get_distance(            data_source=\"Discogs\", info=track_info, config=self.config        )",
        "labels_text": "Returns the track distance"
    },
    {
        "input_text": "summarize: def select_cover_art(self, result):                if result.data.get(\"images\") and len(result.data.get(\"images\")) > 0:            # The first image in this list appears to be the one displayed first            # on the release page - even if it is not flagged as `type: \"primary\"` - and            # so it is the best candidate for the cover art.            return result.data.get(\"images\")[0].get(\"uri\")        return None",
        "labels_text": "Returns the best candidate image if any from a Discogs Release object"
    },
    {
        "input_text": "summarize: def get_track_length(self, duration):                try:            length = time.strptime(duration, \"%M:%S\")        except ValueError:            return None        return length.tm_min * 60 + length.tm_sec",
        "labels_text": "Returns the track length in second for a discogs duration"
    },
    {
        "input_text": "summarize: def _merge(self, objs):                kind = Item if all(isinstance(o, Item) for o in objs) else Album        if kind is Item:            objs = self._merge_items(objs)        else:            objs = self._merge_albums(objs)        return objs",
        "labels_text": "Merge duplicate item See mergeitems and mergealbums for the relevant strategy"
    },
    {
        "input_text": "summarize: def _duplicates(self, objs, keys, full, strict, tiebreak, merge):                offset = 0 if full else 1        for k, objs in self._group_by(objs, keys, strict).items():            if len(objs) > 1:                objs = self._order(objs, tiebreak)                if merge:                    objs = self._merge(objs)                yield (k, len(objs) - offset, objs[offset:])",
        "labels_text": "Generate triple of key duplicate count and constituent object"
    },
    {
        "input_text": "summarize: def edit(filename, log):        cmd = shlex.split(util.editor_command())    cmd.append(filename)    log.debug(\"invoking editor command: {!r}\", cmd)    try:        subprocess.call(cmd)    except OSError as exc:        raise ui.UserError(            \"could not run editor command {!r}: {}\".format(cmd[0], exc)        )",
        "labels_text": "Open filename in a text editor"
    },
    {
        "input_text": "summarize: def dump(arg):        return yaml.safe_dump_all(        arg,        allow_unicode=True,        default_flow_style=False,    )",
        "labels_text": "Dump a sequence of dictionary a YAML for editing"
    },
    {
        "input_text": "summarize: def _safe_value(obj, key, value):        typ = obj._type(key)    return isinstance(typ, SAFE_TYPES) and isinstance(value, typ.model_type)",
        "labels_text": "Check whether the value is safe to represent in YAML and trust a returned from parsed YAML This ensures that value do not change their type when the user edits their YAML representation"
    },
    {
        "input_text": "summarize: def apply_(obj, data):        for key, value in data.items():        if _safe_value(obj, key, value):            # A safe value *stayed* represented as a safe type. Assign it            # directly.            obj[key] = value        else:            # Either the field was stringified originally or the user changed            # it from a safe type to an unsafe one. Parse it as a string.            obj.set_parse(key, str(value))",
        "labels_text": "Set the field of a dbcoreModel object according to a dictionary This is the opposite of flatten The data dictionary should have string a value"
    },
    {
        "input_text": "summarize: def _get_fields(self, album, extra):                # Start with the configured base fields.        if album:            fields = self.config[\"albumfields\"].as_str_seq()        else:            fields = self.config[\"itemfields\"].as_str_seq()        # Add the requested extra fields.        if extra:            fields += extra        # Ensure we always have the `id` field for identification.        fields.append(\"id\")        return set(fields)",
        "labels_text": "Get the set of field to edit"
    },
    {
        "input_text": "summarize: def edit(self, album, objs, fields):                # Present the YAML to the user and let them change it.        success = self.edit_objects(objs, fields)        # Save the new data.        if success:            self.save_changes(objs)",
        "labels_text": "The core editor function album A flag indicating whether were editing Items or Albums objs The Items or Albums to edit field The set of field name to edit or None to edit everything"
    },
    {
        "input_text": "summarize: def save_changes(self, objs):                # Save to the database and possibly write tags.        for ob in objs:            if ob._dirty:                self._log.debug(\"saving changes to {}\", ob)                ob.try_sync(ui.should_write(), ui.should_move())",
        "labels_text": "Save a list of updated Model object to the database"
    },
    {
        "input_text": "summarize: def before_choose_candidate_listener(self, session, task):                choices = [PromptChoice(\"d\", \"eDit\", self.importer_edit)]        if task.candidates:            choices.append(                PromptChoice(                    \"c\", \"edit Candidates\", self.importer_edit_candidate                )            )        return choices",
        "labels_text": "Append an Edit choice and an edit Candidates choice if there are candidate to the interactive importer prompt"
    },
    {
        "input_text": "summarize: def importer_edit_candidate(self, session, task):                # Prompt the user for a candidate.        sel = ui.input_options([], numrange=(1, len(task.candidates)))        # Force applying the candidate on the items.        task.match = task.candidates[sel - 1]        task.apply_metadata()        return self.importer_edit(session, task)",
        "labels_text": "Callback for invoking the functionality during an interactive import session on a candidate The candidate metadata is applied to the original item"
    },
    {
        "input_text": "summarize: def _confirm(objs, album):        noun = \"album\" if album else \"file\"    prompt = \"Modify artwork for {} {}{} (Y/n)?\".format(        len(objs), noun, \"s\" if len(objs) > 1 else \"\"    )    # Show all the items or albums.    for obj in objs:        print_(format(obj))    # Confirm with user.    return ui.input_yn(prompt)",
        "labels_text": "Show the list of affected object item or album and confirm that the user want to modify their artwork album is a Boolean indicating whether these are album a opposed to item"
    },
    {
        "input_text": "summarize: def process_album(self, album):                if self.config[\"auto\"] and ui.should_write():            max_width = self.config[\"maxwidth\"].get(int)            art.embed_album(                self._log,                album,                max_width,                True,                self.config[\"compare_threshold\"].get(int),                self.config[\"ifempty\"].get(bool),            )            self.remove_artfile(album)",
        "labels_text": "Automatically embed art after art ha been set"
    },
    {
        "input_text": "summarize: def remove_artfile(self, album):                if self.config[\"remove_art_file\"] and album.artpath:            if os.path.isfile(syspath(album.artpath)):                self._log.debug(\"Removing album art file for {0}\", album)                os.remove(syspath(album.artpath))                album.artpath = None                album.store()",
        "labels_text": "Possibly delete the album art file for an album if the appropriate configuration option is enabled"
    },
    {
        "input_text": "summarize: def password_data(username, password):        return {        \"username\": username,        \"password\": hashlib.sha1(password.encode(\"utf-8\")).hexdigest(),        \"passwordMd5\": hashlib.md5(password.encode(\"utf-8\")).hexdigest(),    }",
        "labels_text": "Returns a dict with username and it encoded password param username Emby username param password Emby password type username str type password str return Dictionary with username and encoded password rtype dict"
    },
    {
        "input_text": "summarize: def create_headers(user_id, token=None):        headers = {}    authorization = (        'MediaBrowser UserId=\"{user_id}\", '        'Client=\"other\", '        'Device=\"beets\", '        'DeviceId=\"beets\", '        'Version=\"0.0.0\"'    ).format(user_id=user_id)    headers[\"x-emby-authorization\"] = authorization    if token:        headers[\"x-mediabrowser-token\"] = token    return headers",
        "labels_text": "Return header dict that is needed to talk to the Emby API param userid Emby user ID param token Authentication token for Emby type userid str type token str return Headers for request rtype dict"
    },
    {
        "input_text": "summarize: def get_token(host, port, headers, auth_data):        url = api_url(host, port, \"/Users/AuthenticateByName\")    r = requests.post(        url,        headers=headers,        data=auth_data,        timeout=10,    )    return r.json().get(\"AccessToken\")",
        "labels_text": "Return token for a user param host Emby host param port Emby port param header Headers for request param authdata Username and encoded password for authentication type host str type port int type header dict type authdata dict return Access Token rtype str"
    },
    {
        "input_text": "summarize: def get_user(host, port, username):        url = api_url(host, port, \"/Users/Public\")    r = requests.get(url, timeout=10)    user = [i for i in r.json() if i[\"Name\"] == username]    return user",
        "labels_text": "Return user dict from server or None if there is no user param host Emby host param port Emby port username Username type host str type port int type username str return Matched Users rtype list"
    },
    {
        "input_text": "summarize: def listen_for_db_change(self, lib, model):                self.register_listener(\"cli_exit\", self.update)",
        "labels_text": "Listens for beet db change and register the update for the end"
    },
    {
        "input_text": "summarize: def request(self, *args, **kwargs):                return _logged_get(self._log, *args, **kwargs)",
        "labels_text": "Like requestsget but us the logger selflog See also loggedget"
    },
    {
        "input_text": "summarize: def available(cls, log, config):                return True",
        "labels_text": "Return whether or not all dependency are met and the art source is in fact usable"
    },
    {
        "input_text": "summarize: def get(self, album, plugin, paths):                if album.asin:            for index in self.INDICES:                yield self._candidate(                    url=self.URL % (album.asin, index),                    match=Candidate.MATCH_EXACT,                )",
        "labels_text": "Generate URLs using Amazon ID ASIN string"
    },
    {
        "input_text": "summarize: def filename_priority(filename, cover_names):                return [idx for (idx, x) in enumerate(cover_names) if x in filename]",
        "labels_text": "Sort order for image name Return index of cover name found in the image filename This mean that image with lowernumbered and more keywords will have higher priority"
    },
    {
        "input_text": "summarize: def assign_art(self, session, task):                if task in self.art_candidates:            candidate = self.art_candidates.pop(task)            self._set_art(task.album, candidate, not self.src_removed)            if self.src_removed:                task.prune(candidate.path)",
        "labels_text": "Place the discovered art in the filesystem"
    },
    {
        "input_text": "summarize: def file_filter(self, full_path):                import_config = dict(config[\"import\"])        full_path = bytestring_path(full_path)        if \"singletons\" not in import_config or not import_config[\"singletons\"]:            # Album            return self.path_album_regex.match(full_path) is not None        else:            # Singleton            return self.path_singleton_regex.match(full_path) is not None",
        "labels_text": "Checks if the configured regular expression allow the import of the file given in fullpath"
    },
    {
        "input_text": "summarize: def equal(seq):        return len(set(seq)) <= 1",
        "labels_text": "Determine whether a sequence hold identical element"
    },
    {
        "input_text": "summarize: def equal_fields(matchdict, field):        return equal(m[field] for m in matchdict.values())",
        "labels_text": "Do all item in matchdict whose value are dictionary have the same value for field If they do the field is probably not the title"
    },
    {
        "input_text": "summarize: def all_matches(names, pattern):        matches = {}    for item, name in names.items():        m = re.match(pattern, name, re.IGNORECASE)        if m and m.groupdict():            # Only yield a match when the regex applies *and* has            # capture groups. Otherwise, no information can be extracted            # from the filename.            matches[item] = m.groupdict()        else:            return None    return matches",
        "labels_text": "If all the filename in the itemfilename mapping match the pattern return a dictionary mapping the item to dictionary giving the value for each named subpattern in the match Otherwise return None"
    },
    {
        "input_text": "summarize: def bad_title(title):        for pat in BAD_TITLE_PATTERNS:        if re.match(pat, title, re.IGNORECASE):            return True    return False",
        "labels_text": "Determine whether a given title is bad empty or otherwise meaningless and in need of replacement"
    },
    {
        "input_text": "summarize: def split_on_feat(artist):        # split on the first \"feat\".    regex = re.compile(plugins.feat_tokens(), re.IGNORECASE)    parts = [s.strip() for s in regex.split(artist, 1)]    if len(parts) == 1:        return parts[0], None    else:        return tuple(parts)",
        "labels_text": "Given an artist string split the main artist from any artist on the righthand side of a string like feat Return the main artist which is always a string and the featuring artist which may be a string or None if none is present"
    },
    {
        "input_text": "summarize: def contains_feat(title):        return bool(re.search(plugins.feat_tokens(), title, flags=re.IGNORECASE))",
        "labels_text": "Determine whether the title contains a featured marker"
    },
    {
        "input_text": "summarize: def imported(self, session, task):                drop_feat = self.config[\"drop\"].get(bool)        for item in task.imported_items():            self.ft_in_title(item, drop_feat)            item.store()",
        "labels_text": "Import hook for moving featuring artist automatically"
    },
    {
        "input_text": "summarize: def __init__(self, coding):                self._coding = coding",
        "labels_text": "Creates a new coding formatter with the provided coding"
    },
    {
        "input_text": "summarize: def convert_field(self, value, conversion):                converted = super().convert_field(value, conversion)        if isinstance(converted, bytes):            return converted.decode(self._coding)        return converted",
        "labels_text": "Converts the provided value given a conversion type This method decodes the converted value using the formatters coding"
    },
    {
        "input_text": "summarize: def summary(task):        if task.is_album:        return f\"{task.cur_artist} - {task.cur_album}\"    else:        return f\"{task.item.artist} - {task.item.title}\"",
        "labels_text": "Given an ImportTask produce a short string identifying the object"
    },
    {
        "input_text": "summarize: def do_i_hate_this(cls, task, action_patterns):                if action_patterns:            for query_string in action_patterns:                query, _ = parse_query_string(                    query_string,                    Album if task.is_album else Item,                )                if any(query.match(item) for item in task.imported_items()):                    return True        return False",
        "labels_text": "Process group of pattern warn or skip and return True if task is hated and not whitelisted"
    },
    {
        "input_text": "summarize: def write_file_mtime(self, path, mtime):                stat = os.stat(util.syspath(path))        os.utime(util.syspath(path), (stat.st_atime, mtime))",
        "labels_text": "Write the given mtime to the destination path"
    },
    {
        "input_text": "summarize: def write_item_mtime(self, item, mtime):                # The file's mtime on disk must be in sync with the item's mtime        self.write_file_mtime(util.syspath(item.path), mtime)        item.mtime = mtime",
        "labels_text": "Write the given mtime to an item mtime field and to the mtime of the item file"
    },
    {
        "input_text": "summarize: def record_import_mtime(self, item, source, destination):                mtime = os.stat(util.syspath(source)).st_mtime        self.item_mtime[destination] = mtime        self._log.debug(            \"Recorded mtime {0} for item '{1}' imported from \" \"'{2}'\",            mtime,            util.displayable_path(destination),            util.displayable_path(source),        )",
        "labels_text": "Record the file mtime of an item path before it import"
    },
    {
        "input_text": "summarize: def update_after_write_time(self, item, path):                if item.added:            if self.config[\"preserve_write_mtimes\"].get(bool):                self.write_item_mtime(item, item.added)            self._log.debug(                \"Write of item '{0}', selected item.added={1}\",                util.displayable_path(item.path),                item.added,            )",
        "labels_text": "Update the mtime of the item file with the itemadded value after each write of the item if preservewritemtimes is enabled"
    },
    {
        "input_text": "summarize: def _build_m3u_session_filename(basename):        date = datetime.datetime.now().strftime(\"%Y%m%d_%Hh%M\")    basename = re.sub(r\"(\\.m3u|\\.M3U)\", \"\", basename)    path = normpath(        os.path.join(            config[\"importfeeds\"][\"dir\"].as_filename(), f\"{basename}_{date}.m3u\"        )    )    return path",
        "labels_text": "Builds unique mu filename by putting current date between given basename and file ending"
    },
    {
        "input_text": "summarize: def _build_m3u_filename(basename):        basename = re.sub(r\"[\\s,/\\\\'\\\"]\", \"_\", basename)    date = datetime.datetime.now().strftime(\"%Y%m%d_%Hh%M\")    path = normpath(        os.path.join(            config[\"importfeeds\"][\"dir\"].as_filename(),            date + \"_\" + basename + \".m3u\",        )    )    return path",
        "labels_text": "Builds unique mu filename by appending given basename to current date"
    },
    {
        "input_text": "summarize: def _write_m3u(m3u_path, items_paths):        mkdirall(m3u_path)    with open(syspath(m3u_path), \"ab\") as f:        for path in items_paths:            f.write(path + b\"\\n\")",
        "labels_text": "Append relative path to item into mu file"
    },
    {
        "input_text": "summarize: def print_data_keys(data, item=None):        path = displayable_path(item.path) if item else None    formatted = []    for key, value in data.items():        formatted.append(key)    if len(formatted) == 0:        return    line_format = \"{0}{{0}}\".format(\" \" * 4)    if path:        ui.print_(displayable_path(path))    for field in sorted(formatted):        ui.print_(line_format.format(field))",
        "labels_text": "Print only the key field name for an item"
    },
    {
        "input_text": "summarize: def _compile_func(body):        body = \"def {}():\\n    {}\".format(FUNC_NAME, body.replace(\"\\n\", \"\\n    \"))    code = compile(body, \"inline\", \"exec\")    env = {}    eval(code, env)    return env[FUNC_NAME]",
        "labels_text": "Given Python code for a function body return a compiled callable that invokes that code"
    },
    {
        "input_text": "summarize: def ipfs_added_albums(self, rlib, tmpname):                tmplib = library.Library(tmpname)        for album in rlib.albums():            try:                if album.ipfs:                    self.create_new_album(album, tmplib)            except AttributeError:                pass        return tmplib",
        "labels_text": "Returns a new library with only albumsitems added to ipfs"
    },
    {
        "input_text": "summarize: def update_kodi(host, port, user, password):        url = f\"http://{host}:{port}/jsonrpc\"        headers = {\"Content-Type\": \"application/json\"}    # Create the payload. Id seems to be mandatory.    payload = {\"jsonrpc\": \"2.0\", \"method\": \"AudioLibrary.Scan\", \"id\": 1}    r = requests.post(        url,        auth=(user, password),        json=payload,        headers=headers,        timeout=10,    )    return r",
        "labels_text": "Sends request to the Kodi api to start a library refresh"
    },
    {
        "input_text": "summarize: def listen_for_db_change(self, lib, model):                self.register_listener(\"cli_exit\", self.update)",
        "labels_text": "Listens for beet db change and register the update"
    },
    {
        "input_text": "summarize: def get_top_tracks_by_page(        self, period=pylast.PERIOD_OVERALL, limit=None, page=1, cacheable=True    ):                params = self._get_params()        params[\"period\"] = period        params[\"page\"] = page        if limit:            params[\"limit\"] = limit        return self._get_things(            \"getTopTracks\", \"track\", pylast.Track, params, cacheable        )",
        "labels_text": "Returns the top track played by a user in a tuple with the total number of page of result period The period of time Possible value o PERIODOVERALL o PERIODDAYS o PERIODMONTH o PERIODMONTHS o PERIODMONTHS o PERIODMONTHS"
    },
    {
        "input_text": "summarize: def commands(self):                return [lslimit_cmd]",
        "labels_text": "Expose lslimit subcommand"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.token = self.config[\"token\"].get()        self.username = self.config[\"username\"].get()        self.AUTH_HEADER = {\"Authorization\": f\"Token {self.token}\"}        config[\"listenbrainz\"][\"token\"].redact = True",
        "labels_text": "Initialize the plugin"
    },
    {
        "input_text": "summarize: def commands(self):                lbupdate_cmd = ui.Subcommand(            \"lbimport\", help=f\"Import {self.data_source} history\"        )        def func(lib, opts, args):            self._lbupdate(lib, self._log)        lbupdate_cmd.func = func        return [lbupdate_cmd]",
        "labels_text": "Add beet UI command to interact with ListenBrainz"
    },
    {
        "input_text": "summarize: def _make_request(self, url, params=None):                try:            response = requests.get(                url=url,                headers=self.AUTH_HEADER,                timeout=10,                params=params,            )            response.raise_for_status()            return response.json()        except requests.exceptions.RequestException as e:            self._log.debug(f\"Invalid Search Error: {e}\")            return None",
        "labels_text": "Makes a request to the ListenBrainz API"
    },
    {
        "input_text": "summarize: def get_mb_recording_id(self, track):                resp = musicbrainzngs.search_recordings(            query=track[\"track_metadata\"].get(\"track_name\"),            release=track[\"track_metadata\"].get(\"release_name\"),            strict=True,        )        if resp.get(\"recording-count\") == \"1\":            return resp.get(\"recording-list\")[0].get(\"id\")        else:            return None",
        "labels_text": "Returns the MusicBrainz recording ID for a track"
    },
    {
        "input_text": "summarize: def get_playlists_createdfor(self, username):                url = f\"{self.ROOT}/user/{username}/playlists/createdfor\"        return self._make_request(url)",
        "labels_text": "Returns a list of playlist created by a user"
    },
    {
        "input_text": "summarize: def get_playlist(self, identifier):                url = f\"{self.ROOT}/playlist/{identifier}\"        return self._make_request(url)",
        "labels_text": "Returns a playlist"
    },
    {
        "input_text": "summarize: def get_tracks_from_playlist(self, playlist):                tracks = []        for track in playlist.get(\"playlist\").get(\"track\"):            tracks.append(                {                    \"artist\": track.get(\"creator\"),                    \"identifier\": track.get(\"identifier\").split(\"/\")[-1],                    \"title\": track.get(\"title\"),                }            )        return self.get_track_info(tracks)",
        "labels_text": "This function return a list of track in the playlist"
    },
    {
        "input_text": "summarize: def get_weekly_playlist(self, index):                playlists = self.get_listenbrainz_playlists()        playlist = self.get_playlist(playlists[index].get(\"identifier\"))        self._log.info(f\"Getting {playlist.get('playlist').get('title')}\")        return self.get_tracks_from_playlist(playlist)",
        "labels_text": "Returns a list of weekly playlist based on the index"
    },
    {
        "input_text": "summarize: def get_weekly_exploration(self):                return self.get_weekly_playlist(0)",
        "labels_text": "Returns a list of weekly exploration"
    },
    {
        "input_text": "summarize: def get_weekly_jams(self):                return self.get_weekly_playlist(1)",
        "labels_text": "Returns a list of weekly jam"
    },
    {
        "input_text": "summarize: def get_last_weekly_exploration(self):                return self.get_weekly_playlist(3)",
        "labels_text": "Returns a list of weekly exploration"
    },
    {
        "input_text": "summarize: def get_last_weekly_jams(self):                return self.get_weekly_playlist(3)",
        "labels_text": "Returns a list of weekly jam"
    },
    {
        "input_text": "summarize: def unescape(text):        if isinstance(text, bytes):        text = text.decode(\"utf-8\", \"ignore\")    out = text.replace(\"&nbsp;\", \" \")    def replchar(m):        num = m.group(1)        return unichar(int(num))    out = re.sub(\"&#(\\\\d+);\", replchar, out)    return out",
        "labels_text": "Resolve xxx HTML entity and some others"
    },
    {
        "input_text": "summarize: def slug(text):        return re.sub(r\"\\W+\", \"-\", unidecode(text).lower().strip()).strip(\"-\")",
        "labels_text": "Make a URLsafe humanreadable version of the given text This will do the following decode unicode character into ASCII shift everything to lowercase strip whitespace replace other nonword character with dash strip extra dash This somewhat duplicate the funcGoogleslugify function but slugify is not a generic a this one which can be reused elsewhere"
    },
    {
        "input_text": "summarize: def _encode(s):                if isinstance(s, str):            for char, repl in URL_CHARACTERS.items():                s = s.replace(char, repl)            s = s.encode(\"utf-8\", \"ignore\")        return urllib.parse.quote(s)",
        "labels_text": "Encode the string for inclusion in a URL"
    },
    {
        "input_text": "summarize: def _search(self, artist, title):                search_url = self.base_url + \"/search\"        data = {\"q\": title + \" \" + artist.lower()}        try:            response = requests.get(                search_url,                params=data,                headers=self.headers,                timeout=10,            )        except requests.RequestException as exc:            self._log.debug(\"Genius API request failed: {0}\", exc)            return None        try:            return response.json()        except ValueError:            return None",
        "labels_text": "Searches the genius api for a given artist and title httpsdocsgeniuscomsearchh return json response"
    },
    {
        "input_text": "summarize: def remove_credits(text):        textlines = text.split(\"\\n\")    credits = None    for i in (0, -1):        if textlines and \"lyrics\" in textlines[i].lower():            credits = textlines.pop(i)    if credits:        text = \"\\n\".join(textlines)    return text",
        "labels_text": "Remove firstlast line of text if it contains the word lyric eg Lyrics by songsdatabasecom"
    },
    {
        "input_text": "summarize: def writerest(self, directory):                if self.rest is not None and self.artist is not None:            path = os.path.join(                directory, \"artists\", slug(self.artist) + \".rst\"            )            with open(path, \"wb\") as output:                output.write(self.rest.encode(\"utf-8\"))",
        "labels_text": "Write selfrest to a ReST file"
    },
    {
        "input_text": "summarize: def imported(self, session, task):                if self.config[\"auto\"]:            for item in task.imported_items():                self.fetch_item_lyrics(                    session.lib, item, False, self.config[\"force\"]                )",
        "labels_text": "Import hook for fetching lyric automatically"
    },
    {
        "input_text": "summarize: def get_lyrics(self, artist, title, album=None, length=None):                for backend in self.backends:            lyrics = backend.fetch(artist, title, album=album, length=length)            if lyrics:                self._log.debug(                    \"got lyrics from backend: {0}\", backend.__class__.__name__                )                return _scrape_strip_cruft(lyrics, True)",
        "labels_text": "Fetch lyric trying each source in turn Return a string or None if no lyric were found"
    },
    {
        "input_text": "summarize: def submit_albums(collection_id, release_ids):        for i in range(0, len(release_ids), SUBMISSION_CHUNK_SIZE):        chunk = release_ids[i : i + SUBMISSION_CHUNK_SIZE]        mb_call(musicbrainzngs.add_releases_to_collection, collection_id, chunk)",
        "labels_text": "Add all of the release IDs to the indicated collection Multiple request are made if there are many release IDs to submit"
    },
    {
        "input_text": "summarize: def imported(self, session, task):                if task.is_album:            self.update_album_list(session.lib, [task.album])",
        "labels_text": "Add each imported album to the collection"
    },
    {
        "input_text": "summarize: def commands(self):                mbsubmit_cmd = ui.Subcommand(            \"mbsubmit\", help=\"Submit Tracks to MusicBrainz\"        )        def func(lib, opts, args):            items = lib.items(ui.decargs(args))            self._mbsubmit(items)        mbsubmit_cmd.func = func        return [mbsubmit_cmd]",
        "labels_text": "Add beet UI command for mbsubmit"
    },
    {
        "input_text": "summarize: def _mbsubmit(self, items):                for i in sorted(items, key=lambda i: i.track):            print_data(None, i, self.config[\"format\"].as_str())",
        "labels_text": "Print track information to be submitted to MusicBrainz"
    },
    {
        "input_text": "summarize: def func(self, lib, opts, args):                move = ui.should_move(opts.move)        pretend = opts.pretend        write = ui.should_write(opts.write)        query = ui.decargs(args)        self.singletons(lib, query, move, pretend, write)        self.albums(lib, query, move, pretend, write)",
        "labels_text": "Command handler for the mbsync function"
    },
    {
        "input_text": "summarize: def _missing_count(album):        return (album.albumtotal or 0) - len(album.items())",
        "labels_text": "Return number of missing item in album"
    },
    {
        "input_text": "summarize: def is_url(path):        if isinstance(path, bytes):  # if it's bytes, then it's a path        return False    return path.split(\"://\", 1)[0] in [\"http\", \"https\"]",
        "labels_text": "Try to determine if the path is an URL"
    },
    {
        "input_text": "summarize: def disconnect(self):                self.client.close()        self.client.disconnect()",
        "labels_text": "Disconnect from the MPD"
    },
    {
        "input_text": "summarize: def status(self):                return self.get(\"status\")",
        "labels_text": "Return the current status of the MPD"
    },
    {
        "input_text": "summarize: def events(self):                return self.get(\"idle\")",
        "labels_text": "Return list of event This may block a long time while waiting for an answer from MPD"
    },
    {
        "input_text": "summarize: def rating(self, play_count, skip_count, rating, skipped):                if skipped:            rolling = rating - rating / 2.0        else:            rolling = rating + (1.0 - rating) / 2.0        stable = (play_count + 1.0) / (play_count + skip_count + 2.0)        return self.rating_mix * stable + (1.0 - self.rating_mix) * rolling",
        "labels_text": "Calculate a new rating for a song based on play count skip count old rating and the fact if it wa skipped or not"
    },
    {
        "input_text": "summarize: def get_item(self, path):                query = library.PathQuery(\"path\", path)        item = self.lib.items(query).get()        if item:            return item        else:            self._log.info(\"item not found: {0}\", displayable_path(path))",
        "labels_text": "Return the beet item related to path"
    },
    {
        "input_text": "summarize: def update_item(self, item, attribute, value=None, increment=None):                if item is None:            return        if increment is not None:            item.load()            value = type(increment)(item.get(attribute, 0)) + increment        if value is not None:            item[attribute] = value            item.store()            self._log.debug(                \"updated: {0} = {1} [{2}]\",                attribute,                item[attribute],                displayable_path(item.path),            )",
        "labels_text": "Update the beet item Set attribute to value or increment the value of attribute If the increment argument is used the value is cast to the corresponding type"
    },
    {
        "input_text": "summarize: def update_rating(self, item, skipped):                if item is None:            return        item.load()        rating = self.rating(            int(item.get(\"play_count\", 0)),            int(item.get(\"skip_count\", 0)),            float(item.get(\"rating\", 0.5)),            skipped,        )        self.update_item(item, \"rating\", rating)",
        "labels_text": "Update the rating for a beet item The item can either be a beet Item or None If the item is None nothing change"
    },
    {
        "input_text": "summarize: def handle_song_change(self, song):                diff = abs(song[\"remaining\"] - (time.time() - song[\"started\"]))        skipped = diff >= self.time_threshold        if skipped:            self.handle_skipped(song)        else:            self.handle_played(song)        if self.do_rating:            self.update_rating(song[\"beets_item\"], skipped)        return skipped",
        "labels_text": "Determine if a song wa skipped or not and update it attribute To this end the difference between the song supposed end time and the current time is calculated If it greater than a threshold the song is considered skipped Returns whether the change wa manual skipped previous song or not"
    },
    {
        "input_text": "summarize: def handle_played(self, song):                self.update_item(song[\"beets_item\"], \"play_count\", increment=1)        self._log.info(\"played {0}\", displayable_path(song[\"path\"]))",
        "labels_text": "Updates the play count of a song"
    },
    {
        "input_text": "summarize: def handle_skipped(self, song):                self.update_item(song[\"beets_item\"], \"skip_count\", increment=1)        self._log.info(\"skipped {0}\", displayable_path(song[\"path\"]))",
        "labels_text": "Updates the skip count of a song"
    },
    {
        "input_text": "summarize: def work_parent_id(mb_workid):        work_date = None    while True:        new_mb_workid, work_date = direct_parent_id(mb_workid, work_date)        if not new_mb_workid:            return mb_workid, work_date        mb_workid = new_mb_workid    return mb_workid, work_date",
        "labels_text": "Find the parent work id and composition date of a work given it id"
    },
    {
        "input_text": "summarize: def find_parentwork_info(mb_workid):        parent_id, work_date = work_parent_id(mb_workid)    work_info = musicbrainzngs.get_work_by_id(        parent_id, includes=[\"artist-rels\"]    )    return work_info, work_date",
        "labels_text": "Get the MusicBrainz information dict about a parent work including the artist relation and the composition date for a work parent work"
    },
    {
        "input_text": "summarize: def imported(self, session, task):                force_parent = self.config[\"force\"].get(bool)        for item in task.imported_items():            self.find_work(item, force_parent)            item.store()",
        "labels_text": "Import hook for fetching parent work automatically"
    },
    {
        "input_text": "summarize: def convert_perm(perm):        if isinstance(perm, int):        perm = str(perm)    return int(perm, 8)",
        "labels_text": "Convert a string to an integer interpreting the text a octal Or if perm is an integer reinterpret it a an octal number that ha been misinterpreted a decimal"
    },
    {
        "input_text": "summarize: def check_permissions(path, permission):        return oct(stat.S_IMODE(os.stat(syspath(path)).st_mode)) == oct(permission)",
        "labels_text": "Check whether the file permission equal the given vector Return a boolean"
    },
    {
        "input_text": "summarize: def assert_permissions(path, permission, log):        if not check_permissions(path, permission):        log.warning(\"could not set permissions on {}\", displayable_path(path))        log.debug(            \"set permissions to {}, but permissions are now {}\",            permission,            os.stat(syspath(path)).st_mode & 0o777,        )",
        "labels_text": "Check whether the file permission are a expected otherwise log a warning message Return a boolean indicating the match like checkpermissions"
    },
    {
        "input_text": "summarize: def dirs_in_library(library, item):        return [        ancestor for ancestor in ancestry(item) if ancestor.startswith(library)    ][1:]",
        "labels_text": "Creates a list of ancestor directory in the beet library path"
    },
    {
        "input_text": "summarize: def fix(self, lib, item=None, album=None):                files = []        dirs = set()        if item:            files.append(item.path)            dirs.update(dirs_in_library(lib.directory, item.path))        elif album:            for album_item in album.items():                files.append(album_item.path)                dirs.update(dirs_in_library(lib.directory, album_item.path))        self.set_permissions(files=files, dirs=dirs)",
        "labels_text": "Fix the permission for an imported Item or Album"
    },
    {
        "input_text": "summarize: def fix_art(self, album):                if album.artpath:            self.set_permissions(files=[album.artpath])",
        "labels_text": "Fix the permission for Album art file"
    },
    {
        "input_text": "summarize: def _playlist_or_paths(self, paths):                if config[\"play\"][\"raw\"]:            return paths        else:            return [self._create_tmp_playlist(paths)]",
        "labels_text": "Return either the raw path of item or a playlist of the item"
    },
    {
        "input_text": "summarize: def _create_tmp_playlist(self, paths_list):                utf8_bom = config[\"play\"][\"bom\"].get(bool)        filename = get_temp_filename(__name__, suffix=\".m3u\")        with open(filename, \"wb\") as m3u:            if utf8_bom:                m3u.write(b\"\\xEF\\xBB\\xBF\")            for item in paths_list:                m3u.write(item + b\"\\n\")        return filename",
        "labels_text": "Create a temporary mu file Return the filename"
    },
    {
        "input_text": "summarize: def before_choose_candidate_listener(self, session, task):                return [PromptChoice(\"y\", \"plaY\", self.importer_play)]",
        "labels_text": "Append a Play choice to the interactive importer prompt"
    },
    {
        "input_text": "summarize: def importer_play(self, session, task):                selection = task.items        paths = [item.path for item in selection]        open_args = self._playlist_or_paths(paths)        command_str = self._command_str()        if not self._exceeds_threshold(selection, command_str, open_args):            play(                command_str,                selection,                paths,                open_args,                self._log,                keep_open=True,            )",
        "labels_text": "Get item from current import task and send to play function"
    },
    {
        "input_text": "summarize: def append_token(url, token):        if token:        url += \"?\" + urlencode({\"X-Plex-Token\": token})    return url",
        "labels_text": "Appends the Plex Home token to the api call if required"
    },
    {
        "input_text": "summarize: def listen_for_db_change(self, lib, model):                self.register_listener(\"cli_exit\", self.update)",
        "labels_text": "Listens for beet db change and register the update for the end"
    },
    {
        "input_text": "summarize: def random_func(lib, opts, args):        # Fetch all the objects matching the query into a list.    query = decargs(args)    if opts.album:        objs = list(lib.albums(query))    else:        objs = list(lib.items(query))    # Print a random subset.    objs = random_objs(        objs, opts.album, opts.number, opts.time, opts.equal_chance    )    for obj in objs:        print_(format(obj))",
        "labels_text": "Select some random item or album and print the result"
    },
    {
        "input_text": "summarize: def db_to_lufs(db: float) -> float:        return db - 107",
        "labels_text": "Convert db to LUFS According to httpswikihydrogenaudioindexphptitle ReplayGainspecificationReferencelevel"
    },
    {
        "input_text": "summarize: def lufs_to_db(db: float) -> float:        return db + 107",
        "labels_text": "Convert LUFS to db According to httpswikihydrogenaudioindexphptitle ReplayGainspecificationReferencelevel"
    },
    {
        "input_text": "summarize: def _store_track_gain(self, item: Item, track_gain: Gain):                item.rg_track_gain = track_gain.gain        item.rg_track_peak = track_gain.peak        item.store()        self._log.debug(            \"applied track gain {0} LU, peak {1} of FS\",            item.rg_track_gain,            item.rg_track_peak,        )",
        "labels_text": "Store track gain for a single item in the database"
    },
    {
        "input_text": "summarize: def _store_album_gain(self, item: Item, album_gain: Gain):                item.rg_album_gain = album_gain.gain        item.rg_album_peak = album_gain.peak        item.store()        self._log.debug(            \"applied album gain {0} LU, peak {1} of FS\",            item.rg_album_gain,            item.rg_album_peak,        )",
        "labels_text": "Store album gain for a single item in the database The caller need to ensure that selfalbumgain is not None"
    },
    {
        "input_text": "summarize: def store(self, write: bool):                if self.album is not None:            self._store_album(write)        else:            self._store_track(write)",
        "labels_text": "Store computed gain for the item of this task in the database"
    },
    {
        "input_text": "summarize: def _store_album_gain(self, item: Item, album_gain: Gain):                item.r128_album_gain = album_gain.gain        item.store()        self._log.debug(\"applied r128 album gain {0} LU\", item.r128_album_gain)",
        "labels_text": "The caller need to ensure that selfalbumgain is not None"
    },
    {
        "input_text": "summarize: def __init__(self, config: ConfigView, log: Logger):                self._log = log",
        "labels_text": "Initialize the backend with the configuration view for the plugin"
    },
    {
        "input_text": "summarize: def compute_track_gain(self, task: AnyRgTask) -> AnyRgTask:                raise NotImplementedError()",
        "labels_text": "Computes the track gain for the track belonging to task and set the trackgains attribute on the task Returns task"
    },
    {
        "input_text": "summarize: def compute_album_gain(self, task: AnyRgTask) -> AnyRgTask:                raise NotImplementedError()",
        "labels_text": "Computes the album gain for the album belonging to task and set the albumgain attribute on the task Returns task"
    },
    {
        "input_text": "summarize: def compute_track_gain(self, task: AnyRgTask) -> AnyRgTask:                task.track_gains = [            self._analyse_item(                item,                task.target_level,                task.peak_method,                count_blocks=False,            )[                0            ]  # take only the gain, discarding number of gating blocks            for item in task.items        ]        return task",
        "labels_text": "Computes the track gain for the track belonging to task and set the trackgains attribute on the task Returns task"
    },
    {
        "input_text": "summarize: def _construct_cmd(        self, item: Item, peak_method: Optional[PeakMethod]    ) -> List[Union[str, bytes]]:                return [            self._ffmpeg_path,            \"-nostats\",            \"-hide_banner\",            \"-i\",            item.path,            \"-map\",            \"a:0\",            \"-filter\",            \"ebur128=peak={}\".format(                \"none\" if peak_method is None else peak_method.name            ),            \"-f\",            \"null\",            \"-\",        ]",
        "labels_text": "Construct the shell command to analyse item"
    },
    {
        "input_text": "summarize: def compute_track_gain(self, task: AnyRgTask) -> AnyRgTask:                supported_items = list(filter(self.format_supported, task.items))        output = self.compute_gain(supported_items, task.target_level, False)        task.track_gains = output        return task",
        "labels_text": "Computes the track gain for the track belonging to task and set the trackgains attribute on the task Returns task"
    },
    {
        "input_text": "summarize: def format_supported(self, item: Item) -> bool:                if \"mp3gain\" in self.command and item.format != \"MP3\":            return False        elif \"aacgain\" in self.command and item.format not in (\"MP3\", \"AAC\"):            return False        return True",
        "labels_text": "Checks whether the given item is supported by the selected tool"
    },
    {
        "input_text": "summarize: def _import_audiotools(self):                try:            import audiotools            import audiotools.replaygain        except ImportError:            raise FatalReplayGainError(                \"Failed to load audiotools: audiotools not found\"            )        self._mod_audiotools = audiotools        self._mod_replaygain = audiotools.replaygain",
        "labels_text": "Check whether it possible to import the necessary module There is no check on the file format at runtime raise excReplayGainError if the module cannot be imported"
    },
    {
        "input_text": "summarize: def open_audio_file(self, item: Item):                try:            audiofile = self._mod_audiotools.open(                os.fsdecode(syspath(item.path))            )        except OSError:            raise ReplayGainError(f\"File {item.path} was not found\")        except self._mod_audiotools.UnsupportedFile:            raise ReplayGainError(f\"Unsupported file type {item.format}\")        return audiofile",
        "labels_text": "Open the file to read the PCM stream from the using itempath return the audiofile instance rtype classaudiotoolsAudioFile raise excReplayGainError if the file is not found or the file format is not supported"
    },
    {
        "input_text": "summarize: def init_replaygain(self, audiofile, item: Item):                try:            rg = self._mod_replaygain.ReplayGain(audiofile.sample_rate())        except ValueError:            raise ReplayGainError(f\"Unsupported sample rate {item.samplerate}\")            return        return rg",
        "labels_text": "Return an initialized classaudiotoolsreplaygainReplayGain instance which requires the sample rate of the song on which the ReplayGain value will be computed The item is passed in case the sample rate is invalid to log the stored item sample rate return initialized replagain object rtype classaudiotoolsreplaygainReplayGain raise excReplayGainError if the sample rate is invalid"
    },
    {
        "input_text": "summarize: def compute_track_gain(self, task: AnyRgTask) -> AnyRgTask:                gains = [            self._compute_track_gain(i, task.target_level) for i in task.items        ]        task.track_gains = gains        return task",
        "labels_text": "Computes the track gain for the track belonging to task and set the trackgains attribute on the task Returns task"
    },
    {
        "input_text": "summarize: def _with_target_level(self, gain: float, target_level: float):                return gain + (target_level - 89)",
        "labels_text": "Return gain relative to targetlevel Assumes gain is relative to db"
    },
    {
        "input_text": "summarize: def should_use_r128(self, item: Item) -> bool:                return item.format in self.r128_whitelist",
        "labels_text": "Checks the plugin setting to decide whether the calculation should be done using the EBU R standard and use R tag instead"
    },
    {
        "input_text": "summarize: def open_pool(self, threads: int):                if self.pool is None and self.backend_instance.do_parallel:            self.pool = ThreadPool(threads)            self.exc_queue: queue.Queue = queue.Queue()            signal.signal(signal.SIGINT, self._interrupt)            self.exc_watcher = ExceptionWatcher(                self.exc_queue,  # threads push exceptions here                self.terminate_pool,  # abort once an exception occurs            )            self.exc_watcher.start()",
        "labels_text": "Open a ThreadPool instance in selfpool"
    },
    {
        "input_text": "summarize: def terminate_pool(self):                if self.pool is not None:            self.pool.terminate()            self.pool.join()            # Terminating the processes leaves the ExceptionWatcher's queues            # in an unknown state, so don't wait for it.            # self.exc_watcher.join()            self.pool = None",
        "labels_text": "Forcibly terminate the ThreadPool instance in selfpool Sends SIGTERM to all process"
    },
    {
        "input_text": "summarize: def close_pool(self):                if self.pool is not None:            self.pool.close()            self.pool.join()            self.exc_watcher.join()            self.pool = None",
        "labels_text": "Regularly close the ThreadPool instance in selfpool"
    },
    {
        "input_text": "summarize: def import_begin(self, session: ImportSession):                threads = cast(int, self.config[\"threads\"].get(int))        if (            self.config[\"parallel_on_import\"]            and self.config[\"auto\"]            and threads        ):            self.open_pool(threads)",
        "labels_text": "Handle importbegin event open pool"
    },
    {
        "input_text": "summarize: def import_end(self, paths):                self.close_pool()",
        "labels_text": "Handle import event close pool"
    },
    {
        "input_text": "summarize: def imported(self, session: ImportSession, task: ImportTask):                if self.config[\"auto\"]:            if task.is_album:                self.handle_album(task.album, False, self.force_on_import)            else:                # Should be a SingletonImportTask                assert hasattr(task, \"item\")                self.handle_track(task.item, False, self.force_on_import)",
        "labels_text": "Add replay gain info to item or album of task"
    },
    {
        "input_text": "summarize: def rewriter(field, rules):        def fieldfunc(item):        value = item._values_fixed[field]        for pattern, replacement in rules:            if pattern.match(value.lower()):                # Rewrite activated.                return replacement        # Not activated; return original value.        return value    return fieldfunc",
        "labels_text": "Create a template field function that rewrite the given field with the given rewriting rule rule must be a list of pattern replacement pair"
    },
    {
        "input_text": "summarize: def _mutagen_classes():                classes = []        for modname, clsname in _MUTAGEN_FORMATS.items():            mod = __import__(f\"mutagen.{modname}\", fromlist=[clsname])            classes.append(getattr(mod, clsname))        return classes",
        "labels_text": "Get a list of file type class from the Mutagen module"
    },
    {
        "input_text": "summarize: def import_task_files(self, session, task):                for item in task.imported_items():            self._log.debug(                \"auto-scrubbing {0}\", util.displayable_path(item.path)            )            self._scrub_item(item, ui.should_write())",
        "labels_text": "Automatically scrub imported file"
    },
    {
        "input_text": "summarize: def listen_for_db_change(self, lib, model):                self.register_listener(\"cli_exit\", self.update)",
        "labels_text": "Listens for beet db change and register the update"
    },
    {
        "input_text": "summarize: def update(self, lib):                self._log.info(\"Requesting a Sonos library update...\")        device = soco.discovery.any_soco()        if device:            device.music_library.start_library_update()        else:            self._log.warning(\"Could not find a Sonos device.\")            return        self._log.info(\"Sonos update triggered\")",
        "labels_text": "When the client exists try to send refresh request to a Sonos controller"
    },
    {
        "input_text": "summarize: def setup(self):                try:            with open(self.tokenfile) as f:                token_data = json.load(f)        except OSError:            self._authenticate()        else:            self.access_token = token_data[\"access_token\"]",
        "labels_text": "Retrieve previously saved OAuth token or generate a new one"
    },
    {
        "input_text": "summarize: def _construct_search_query(filters=None, keywords=\"\"):                query_components = [            keywords,            \" \".join(\":\".join((k, v)) for k, v in filters.items()),        ]        query = \" \".join([q for q in query_components if q])        if not isinstance(query, str):            query = query.decode(\"utf8\")        return unidecode.unidecode(query)",
        "labels_text": "Construct a query string with the specified filter and keywords to be provided to the Spotify Search API httpsdeveloperspotifycomdocumentationwebapireferencesearchsearchwritingaqueryguidelines param filter Optional Field filter to apply type filter dict param keywords Optional Query keywords to use type keywords str return Query string to be provided to the Search API rtype str"
    },
    {
        "input_text": "summarize: def track_audio_features(self, track_id=None):                try:            return self._handle_response(                requests.get, self.audio_features_url + track_id            )        except SpotifyAPIError as e:            self._log.debug(\"Spotify API error: {}\", e)            return None",
        "labels_text": "Fetch track audio feature by it Spotify ID"
    },
    {
        "input_text": "summarize: def __create_token():                password = config[\"subsonic\"][\"pass\"].as_str()        # Pick the random sequence and salt the password        r = string.ascii_letters + string.digits        salt = \"\".join([random.choice(r) for _ in range(6)])        salted_password = password + salt        token = hashlib.md5(salted_password.encode(\"utf-8\")).hexdigest()        # Put together the payload of the request to the server and the URL        return salt, token",
        "labels_text": "Create salt and token from given password return The generated salt and hashed token"
    },
    {
        "input_text": "summarize: def tmpl_substitute(self, text):                if text:            for pattern, replacement in self.substitute_rules:                if pattern.match(text.lower()):                    return replacement            return text        else:            return \"\"",
        "labels_text": "Do the actual replacing"
    },
    {
        "input_text": "summarize: def __init__(self):                super().__init__()        self.substitute_rules = []        self.template_funcs[\"substitute\"] = self.tmpl_substitute        for key, view in self.config.items():            value = view.as_str()            pattern = re.compile(key.lower())            self.substitute_rules.append((pattern, value))",
        "labels_text": "Initialize the substitute plugin Get the configuration register template function and create list of substitute rule"
    },
    {
        "input_text": "summarize: def unthe(self, text, pattern):                if text:            r = re.compile(pattern, flags=re.IGNORECASE)            try:                t = r.findall(text)[0]            except IndexError:                return text            else:                r = re.sub(r, \"\", text).strip()                if self.config[\"strip\"]:                    return r                else:                    fmt = self.config[\"format\"].as_str()                    return fmt.format(r, t.strip()).strip()        else:            return \"\"",
        "labels_text": "Moves pattern in the path format string or strip it text text to handle pattern regexp pattern case ignore is already on strip if True pattern will be removed"
    },
    {
        "input_text": "summarize: def thumbnail_file_name(self, path):                uri = self.get_uri(path)        hash = md5(uri.encode(\"utf-8\")).hexdigest()        return bytestring_path(f\"{hash}.png\")",
        "labels_text": "Compute the thumbnail file name See httpsstandardsfreedesktoporgthumbnailspeclatestxhtml"
    },
    {
        "input_text": "summarize: def add_tags(self, album, image_path):                mtime = os.stat(syspath(album.artpath)).st_mtime        metadata = {            \"Thumb::URI\": self.get_uri(album.artpath),            \"Thumb::MTime\": str(mtime),        }        try:            ArtResizer.shared.write_metadata(image_path, metadata)        except Exception:            self._log.exception(                \"could not write metadata to {0}\", displayable_path(image_path)            )",
        "labels_text": "Write required metadata to the thumbnail See httpsstandardsfreedesktoporgthumbnailspeclatestxhtml"
    },
    {
        "input_text": "summarize: def copy_c_string(c_string):        # This is a pretty dumb way to get a string copy, but it seems to    # work. A more surefire way would be to allocate a ctypes buffer and copy    # the data with `memcpy` or somesuch.    s = ctypes.cast(c_string, ctypes.c_char_p).value    return b\"\" + s",
        "labels_text": "Copy a ctypesPOINTERctypescchar value into a new Python string and return it The old memory is then safe to free"
    },
    {
        "input_text": "summarize: def _match_progs(value, progs):        if not progs:        return True    for prog in progs:        if prog.search(str(value)):            return True    return False",
        "labels_text": "Check if value a string is matching any of the compiled regexes in the progs list"
    },
    {
        "input_text": "summarize: def _get_state(self):                # gst's get_state function returns a 3-tuple; we just want the        # status flag in position 1.        return self.player.get_state(Gst.CLOCK_TIME_NONE)[1]",
        "labels_text": "Returns the current state flag of the playbin"
    },
    {
        "input_text": "summarize: def _set_volume(self, volume):                # And the volume for the playbin.        self._volume = volume        self.player.set_property(\"volume\", volume)",
        "labels_text": "Set the volume level to a value in the range"
    },
    {
        "input_text": "summarize: def _get_volume(self):                return self._volume",
        "labels_text": "Get the volume a a float in the range"
    },
    {
        "input_text": "summarize: def play_file(self, path):                self.player.set_state(Gst.State.NULL)        if isinstance(path, str):            path = path.encode(\"utf-8\")        uri = \"file://\" + urllib.parse.quote(path)        self.player.set_property(\"uri\", uri)        self.player.set_state(Gst.State.PLAYING)        self.playing = True",
        "labels_text": "Immediately begin playing the audio file at the given path"
    },
    {
        "input_text": "summarize: def play(self):                if self._get_state() == Gst.State.PAUSED:            self.player.set_state(Gst.State.PLAYING)            self.playing = True",
        "labels_text": "If paused resume playback"
    },
    {
        "input_text": "summarize: def pause(self):                self.player.set_state(Gst.State.PAUSED)",
        "labels_text": "Pause playback"
    },
    {
        "input_text": "summarize: def stop(self):                self.player.set_state(Gst.State.NULL)        self.playing = False        self.cached_time = None",
        "labels_text": "Halt playback"
    },
    {
        "input_text": "summarize: def run(self):                # If we don't use the MainLoop, messages are never sent.        def start():            loop = GLib.MainLoop()            loop.run()        _thread.start_new_thread(start, ())",
        "labels_text": "Start a new thread for the player Call this function before trying to play any music with playfile or play"
    },
    {
        "input_text": "summarize: def seek(self, position):                cur_pos, cur_len = self.time()        if position > cur_len:            self.stop()            return        fmt = Gst.Format(Gst.Format.TIME)        ns = position * 10**9  # convert to nanoseconds        self.player.seek_simple(fmt, Gst.SeekFlags.FLUSH, ns)        # save new cached time        self.cached_time = (position, cur_len)",
        "labels_text": "Seeks to position in second"
    },
    {
        "input_text": "summarize: def block(self):                while self.playing:            time.sleep(1)",
        "labels_text": "Block until playing finish"
    },
    {
        "input_text": "summarize: def play_simple(paths):        p = GstPlayer()    p.run()    for path in paths:        p.play_file(path)        p.block()",
        "labels_text": "Play the file in path in a straightforward way without using the player callback function"
    },
    {
        "input_text": "summarize: def play_complicated(paths):        my_paths = copy.copy(paths)    def next_song():        my_paths.pop(0)        p.play_file(my_paths[0])    p = GstPlayer(next_song)    p.run()    p.play_file(my_paths[0])    while my_paths:        time.sleep(1)",
        "labels_text": "Play the file in the path one after the other by using the callback function to advance to the next song"
    },
    {
        "input_text": "summarize: def response(self):                return self.template.substitute(            {                \"resp\": RESP_ERR,                \"code\": self.code,                \"index\": self.index,                \"cmd_name\": self.cmd_name,                \"message\": self.message,            }        )",
        "labels_text": "Returns a string to be used a the response code for the erring command"
    },
    {
        "input_text": "summarize: def make_bpd_error(s_code, s_message):        class NewBPDError(BPDError):        code = s_code        message = s_message        cmd_name = \"\"        index = 0        def __init__(self):            pass    return NewBPDError",
        "labels_text": "Create a BPDError subclass for a static code and message"
    },
    {
        "input_text": "summarize: def cast_arg(t, val):        if t == \"intbool\":        return cast_arg(bool, cast_arg(int, val))    else:        try:            return t(val)        except ValueError:            raise ArgumentTypeError()",
        "labels_text": "Attempts to call t on val raising a ArgumentTypeError on ValueError If t is the special string intbool attempt to cast first to an int and then to a bool ie True False"
    },
    {
        "input_text": "summarize: def connect(self, conn):                self.connections.add(conn)",
        "labels_text": "A new client ha connected"
    },
    {
        "input_text": "summarize: def disconnect(self, conn):                self.connections.remove(conn)",
        "labels_text": "Client ha disconnected clean up residual state"
    },
    {
        "input_text": "summarize: def run(self):                self.startup_time = time.time()        def start():            yield bluelet.spawn(                bluelet.server(                    self.ctrl_host,                    self.ctrl_port,                    ControlConnection.handler(self),                )            )            yield bluelet.server(                self.host, self.port, MPDConnection.handler(self)            )        bluelet.run(start())",
        "labels_text": "Block and start listening for connection from client An interrupt C close the server"
    },
    {
        "input_text": "summarize: def dispatch_events(self):                # We need a copy of `self.connections` here since clients might        # disconnect once we try and send to them, changing `self.connections`.        for conn in list(self.connections):            yield bluelet.spawn(conn.send_notifications())",
        "labels_text": "If any client have idle event ready send them"
    },
    {
        "input_text": "summarize: def _ctrl_send(self, message):                if not self.ctrl_sock:            self.ctrl_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)            self.ctrl_sock.connect((self.ctrl_host, self.ctrl_port))        self.ctrl_sock.sendall((message + \"\\n\").encode(\"utf-8\"))",
        "labels_text": "Send some data over the control socket If it our first time open the socket The message should be a string without a terminal newline"
    },
    {
        "input_text": "summarize: def _send_event(self, event):                for conn in self.connections:            conn.notify(event)",
        "labels_text": "Notify subscribed connection of an event"
    },
    {
        "input_text": "summarize: def _item_info(self, item):                raise NotImplementedError",
        "labels_text": "An abstract method that should response line containing a single song metadata"
    },
    {
        "input_text": "summarize: def _item_id(self, item):                raise NotImplementedError",
        "labels_text": "An abstract method returning the integer id for an item"
    },
    {
        "input_text": "summarize: def _id_to_index(self, track_id):                track_id = cast_arg(int, track_id)        for index, track in enumerate(self.playlist):            if self._item_id(track) == track_id:                return index        # Loop finished with no track found.        raise ArgumentNotFoundError()",
        "labels_text": "Searches the playlist for a song with the given id and return it index in the playlist"
    },
    {
        "input_text": "summarize: def _random_idx(self):                if len(self.playlist) < 2:            return len(self.playlist) - 1        new_index = self.random_obj.randint(0, len(self.playlist) - 1)        while new_index == self.current_index:            new_index = self.random_obj.randint(0, len(self.playlist) - 1)        return new_index",
        "labels_text": "Returns a random index different from the current one If there are no song in the playlist it return If there is only one song in the playlist it return"
    },
    {
        "input_text": "summarize: def _succ_idx(self):                if self.repeat and self.single:            return self.current_index        if self.random:            return self._random_idx()        return self.current_index + 1",
        "labels_text": "Returns the index for the next song to play It also considers random single and repeat flag No boundary are checked"
    },
    {
        "input_text": "summarize: def _prev_idx(self):                if self.repeat and self.single:            return self.current_index        if self.random:            return self._random_idx()        return self.current_index - 1",
        "labels_text": "Returns the index for the previous song to play It also considers random and repeat flag No boundary are checked"
    },
    {
        "input_text": "summarize: def cmd_ping(self, conn):                pass",
        "labels_text": "Succeeds"
    },
    {
        "input_text": "summarize: def cmd_kill(self, conn):                sys.exit(0)",
        "labels_text": "Exits the server process"
    },
    {
        "input_text": "summarize: def cmd_close(self, conn):                raise BPDClose()",
        "labels_text": "Closes the connection"
    },
    {
        "input_text": "summarize: def cmd_password(self, conn, password):                if password == self.password:            conn.authenticated = True        else:            conn.authenticated = False            raise BPDError(ERROR_PASSWORD, \"incorrect password\")",
        "labels_text": "Attempts password authentication"
    },
    {
        "input_text": "summarize: def cmd_commands(self, conn):                if self.password and not conn.authenticated:            # Not authenticated. Show limited list of commands.            for cmd in SAFE_COMMANDS:                yield \"command: \" + cmd        else:            # Authenticated. Show all commands.            for func in dir(self):                if func.startswith(\"cmd_\"):                    yield \"command: \" + func[4:]",
        "labels_text": "Lists the command available to the user"
    },
    {
        "input_text": "summarize: def cmd_notcommands(self, conn):                if self.password and not conn.authenticated:            # Not authenticated. Show privileged commands.            for func in dir(self):                if func.startswith(\"cmd_\"):                    cmd = func[4:]                    if cmd not in SAFE_COMMANDS:                        yield \"command: \" + cmd        else:            # Authenticated. No commands are unavailable.            pass",
        "labels_text": "Lists all unavailable command"
    },
    {
        "input_text": "summarize: def cmd_clearerror(self, conn):                self.error = None",
        "labels_text": "Removes the persistent error state of the server This error is set when a problem arises not in response to a command for instance when playing a file"
    },
    {
        "input_text": "summarize: def cmd_random(self, conn, state):                self.random = cast_arg(\"intbool\", state)        self._send_event(\"options\")",
        "labels_text": "Set or unset random shuffle mode"
    },
    {
        "input_text": "summarize: def cmd_repeat(self, conn, state):                self.repeat = cast_arg(\"intbool\", state)        self._send_event(\"options\")",
        "labels_text": "Set or unset repeat mode"
    },
    {
        "input_text": "summarize: def cmd_consume(self, conn, state):                self.consume = cast_arg(\"intbool\", state)        self._send_event(\"options\")",
        "labels_text": "Set or unset consume mode"
    },
    {
        "input_text": "summarize: def cmd_single(self, conn, state):                # TODO support oneshot in addition to 0 and 1 [MPD 0.20]        self.single = cast_arg(\"intbool\", state)        self._send_event(\"options\")",
        "labels_text": "Set or unset single mode"
    },
    {
        "input_text": "summarize: def cmd_setvol(self, conn, vol):                vol = cast_arg(int, vol)        if vol < VOLUME_MIN or vol > VOLUME_MAX:            raise BPDError(ERROR_ARG, \"volume out of range\")        self.volume = vol        self._send_event(\"mixer\")",
        "labels_text": "Set the player volume level"
    },
    {
        "input_text": "summarize: def cmd_volume(self, conn, vol_delta):                vol_delta = cast_arg(int, vol_delta)        return self.cmd_setvol(conn, self.volume + vol_delta)",
        "labels_text": "Deprecated command to change the volume by a relative amount"
    },
    {
        "input_text": "summarize: def cmd_crossfade(self, conn, crossfade):                crossfade = cast_arg(int, crossfade)        if crossfade < 0:            raise BPDError(ERROR_ARG, \"crossfade time must be nonnegative\")        self._log.warning(\"crossfade is not implemented in bpd\")        self.crossfade = crossfade        self._send_event(\"options\")",
        "labels_text": "Set the number of second of crossfading"
    },
    {
        "input_text": "summarize: def cmd_mixrampdb(self, conn, db):                db = cast_arg(float, db)        if db > 0:            raise BPDError(ERROR_ARG, \"mixrampdb time must be negative\")        self._log.warning(\"mixramp is not implemented in bpd\")        self.mixrampdb = db        self._send_event(\"options\")",
        "labels_text": "Set the mixramp normalised max volume in dB"
    },
    {
        "input_text": "summarize: def cmd_mixrampdelay(self, conn, delay):                delay = cast_arg(float, delay)        if delay < 0:            raise BPDError(ERROR_ARG, \"mixrampdelay time must be nonnegative\")        self._log.warning(\"mixramp is not implemented in bpd\")        self.mixrampdelay = delay        self._send_event(\"options\")",
        "labels_text": "Set the mixramp delay in second"
    },
    {
        "input_text": "summarize: def cmd_replay_gain_mode(self, conn, mode):                if mode not in [\"off\", \"track\", \"album\", \"auto\"]:            raise BPDError(ERROR_ARG, \"Unrecognised replay gain mode\")        self._log.warning(\"replay gain is not implemented in bpd\")        self.replay_gain_mode = mode        self._send_event(\"options\")",
        "labels_text": "Set the replay gain mode"
    },
    {
        "input_text": "summarize: def cmd_replay_gain_status(self, conn):                yield \"replay_gain_mode: \" + str(self.replay_gain_mode)",
        "labels_text": "Get the replaygain mode"
    },
    {
        "input_text": "summarize: def cmd_clear(self, conn):                self.playlist = []        self.playlist_version += 1        self.cmd_stop(conn)        self._send_event(\"playlist\")",
        "labels_text": "Clear the playlist"
    },
    {
        "input_text": "summarize: def cmd_delete(self, conn, index):                index = cast_arg(int, index)        try:            del self.playlist[index]        except IndexError:            raise ArgumentIndexError()        self.playlist_version += 1        if self.current_index == index:  # Deleted playing song.            self.cmd_stop(conn)        elif index < self.current_index:  # Deleted before playing.            # Shift playing index down.            self.current_index -= 1        self._send_event(\"playlist\")",
        "labels_text": "Remove the song at index from the playlist"
    },
    {
        "input_text": "summarize: def cmd_urlhandlers(self, conn):                pass",
        "labels_text": "Indicates supported URL scheme None by default"
    },
    {
        "input_text": "summarize: def cmd_playlistinfo(self, conn, index=None):                if index is None:            for track in self.playlist:                yield self._item_info(track)        else:            indices = self._parse_range(index, accept_single_number=True)            try:                tracks = [self.playlist[i] for i in indices]            except IndexError:                raise ArgumentIndexError()            for track in tracks:                yield self._item_info(track)",
        "labels_text": "Gives metadata information about the entire playlist or a single track given by it index"
    },
    {
        "input_text": "summarize: def cmd_plchanges(self, conn, version):                return self.cmd_playlistinfo(conn)",
        "labels_text": "Sends playlist change since the given version This is a fake implementation that ignores the version and just return the entire playlist rather like version This seems to satisfy many client"
    },
    {
        "input_text": "summarize: def cmd_plchangesposid(self, conn, version):                for idx, track in enumerate(self.playlist):            yield \"cpos: \" + str(idx)            yield \"Id: \" + str(track.id)",
        "labels_text": "Like plchanges but only sends position and id Also a dummy implementation"
    },
    {
        "input_text": "summarize: def cmd_currentsong(self, conn):                if self.current_index != -1:  # -1 means stopped.            track = self.playlist[self.current_index]            yield self._item_info(track)",
        "labels_text": "Sends information about the currentlyplaying song"
    },
    {
        "input_text": "summarize: def cmd_previous(self, conn):                old_index = self.current_index        self.current_index = self._prev_idx()        if self.consume:            self.playlist.pop(old_index)        if self.current_index < 0:            if self.repeat:                self.current_index = len(self.playlist) - 1            else:                self.current_index = 0        return self.cmd_play(conn)",
        "labels_text": "Step back to the last song"
    },
    {
        "input_text": "summarize: def cmd_pause(self, conn, state=None):                if state is None:            self.paused = not self.paused  # Toggle.        else:            self.paused = cast_arg(\"intbool\", state)        self._send_event(\"player\")",
        "labels_text": "Set the pause state playback"
    },
    {
        "input_text": "summarize: def cmd_stop(self, conn):                self.current_index = -1        self.paused = False        self._send_event(\"player\")",
        "labels_text": "Stop playback"
    },
    {
        "input_text": "summarize: def cmd_seek(self, conn, index, pos):                index = cast_arg(int, index)        if index < 0 or index >= len(self.playlist):            raise ArgumentIndexError()        self.current_index = index        self._send_event(\"player\")",
        "labels_text": "Seek to a specified point in a specified song"
    },
    {
        "input_text": "summarize: def cmd_crash_TypeError(self, conn):  # noqa: N802                \"a\" + 2",
        "labels_text": "Deliberately trigger a TypeError for testing purpose We want to test that the server properly responds with ERRORSYSTEM without crashing and that this is not treated a ERRORARG since it is caused by a programming error not a protocol error"
    },
    {
        "input_text": "summarize: def __init__(self, server, sock):                self.server = server        self.sock = sock        self.address = \"{}:{}\".format(*sock.sock.getpeername())",
        "labels_text": "Create a new connection for the accepted socket client"
    },
    {
        "input_text": "summarize: def debug(self, message, kind=\" \"):                self.server._log.debug(\"{}[{}]: {}\", kind, self.address, message)",
        "labels_text": "Log a debug message about this connection"
    },
    {
        "input_text": "summarize: def send(self, lines):                if isinstance(lines, str):            lines = [lines]        out = NEWLINE.join(lines) + NEWLINE        for l in out.split(NEWLINE)[:-1]:            self.debug(l, kind=\">\")        if isinstance(out, str):            out = out.encode(\"utf-8\")        return self.sock.sendall(out)",
        "labels_text": "Send line which which is either a single string or an iterable consisting of string to the client A newline is added after every string Returns a Bluelet event that sends the data"
    },
    {
        "input_text": "summarize: def __init__(self, server, sock):                super().__init__(server, sock)        self.authenticated = False        self.notifications = set()        self.idle_subscriptions = set()",
        "labels_text": "Create a new connection for the accepted socket client"
    },
    {
        "input_text": "summarize: def do_command(self, command):                try:            yield bluelet.call(command.run(self))        except BPDError as e:            # Send the error.            yield self.send(e.response())        else:            # Send success code.            yield self.send(RESP_OK)",
        "labels_text": "A coroutine that run the given command and sends an appropriate response"
    },
    {
        "input_text": "summarize: def disconnect(self):                self.server.disconnect(self)        self.debug(\"disconnected\", kind=\"*\")",
        "labels_text": "The connection ha closed for any reason"
    },
    {
        "input_text": "summarize: def notify(self, event):                self.notifications.add(event)",
        "labels_text": "Queue up an event for sending to this client"
    },
    {
        "input_text": "summarize: def send_notifications(self, force_close_idle=False):                pending = self.notifications.intersection(self.idle_subscriptions)        try:            for event in pending:                yield self.send(f\"changed: {event}\")            if pending or force_close_idle:                self.idle_subscriptions = set()                self.notifications = self.notifications.difference(pending)                yield self.send(RESP_OK)        except bluelet.SocketClosedError:            self.disconnect()",
        "labels_text": "Send the client any queued event now"
    },
    {
        "input_text": "summarize: def __init__(self, server, sock):                super().__init__(server, sock)",
        "labels_text": "Create a new connection for the accepted socket client"
    },
    {
        "input_text": "summarize: def ctrl_play_finished(self):                yield bluelet.call(self.server.dispatch_events())",
        "labels_text": "Callback from the player signalling a song finished playing"
    },
    {
        "input_text": "summarize: def ctrl_profile(self):                from guppy import hpy        heap = hpy().heap()        yield self.send(heap)",
        "labels_text": "Memory profiling for debugging"
    },
    {
        "input_text": "summarize: def ctrl_nickname(self, oldlabel, newlabel):                for c in self.server.connections:            if c.address == oldlabel:                c.address = newlabel                break        else:            yield self.send(f\"ERROR: no such client: {oldlabel}\")",
        "labels_text": "Rename a client in the log message"
    },
    {
        "input_text": "summarize: def __init__(self, sequence=None, verbose=False):                if sequence:            for item in sequence:                self.append(item)        self.verbose = verbose",
        "labels_text": "Create a new CommandList from the given sequence of Commands If verbose this is a verbose command list"
    },
    {
        "input_text": "summarize: def run(self, conn):                for i, command in enumerate(self):            try:                yield bluelet.call(command.run(conn))            except BPDError as e:                # If the command failed, stop executing.                e.index = i  # Give the error the correct index.                raise e            # Otherwise, possibly send the output delimiter if we're in a            # verbose (\"OK\") command list.            if self.verbose:                yield conn.send(RESP_CLIST_VERBOSE)",
        "labels_text": "Coroutine executing all the command in this list"
    },
    {
        "input_text": "summarize: def play_finished(self):                self.cmd_next(None)        self._ctrl_send(\"play_finished\")",
        "labels_text": "A callback invoked every time our player finish a track"
    },
    {
        "input_text": "summarize: def _parse_range(self, items, accept_single_number=False):                try:            start, stop = str(items).split(\":\", 1)        except ValueError:            if accept_single_number:                return [cast_arg(int, items)]            raise BPDError(ERROR_ARG, \"bad range syntax\")        start = cast_arg(int, start)        stop = cast_arg(int, stop)        return range(start, stop)",
        "labels_text": "Convert a range of position to a list of item info MPD specifies range a STARTSTOP endpoint excluded for some command Sometimes a single number can be provided instead"
    },
    {
        "input_text": "summarize: def cmd_update(self, conn, path=\"/\"):                # Path is ignored. Also, the real MPD does this asynchronously;        # this is done inline.        self._log.debug(\"Building directory tree...\")        self.tree = vfs.libtree(self.lib)        self._log.debug(\"Finished building directory tree.\")        self.updated_time = time.time()        self._send_event(\"update\")        self._send_event(\"database\")",
        "labels_text": "Updates the catalog to reflect the current database state"
    },
    {
        "input_text": "summarize: def _resolve_path(self, path):                components = path.split(\"/\")        node = self.tree        for component in components:            if not component:                continue            if isinstance(node, int):                # We're trying to descend into a file node.                raise ArgumentNotFoundError()            if component in node.files:                node = node.files[component]            elif component in node.dirs:                node = node.dirs[component]            else:                raise ArgumentNotFoundError()        return node",
        "labels_text": "Returns a VFS node or an item ID located at the path given If the path doe not exist raise a"
    },
    {
        "input_text": "summarize: def _path_join(self, p1, p2):                out = p1 + \"/\" + p2        return out.replace(\"//\", \"/\").replace(\"//\", \"/\")",
        "labels_text": "Smashes together two BPD path"
    },
    {
        "input_text": "summarize: def cmd_listall(self, conn, path=\"/\"):                return self._listall(path, self._resolve_path(path), False)",
        "labels_text": "Send the path all item in the directory recursively"
    },
    {
        "input_text": "summarize: def cmd_listallinfo(self, conn, path=\"/\"):                return self._listall(path, self._resolve_path(path), True)",
        "labels_text": "Send info on all the item in the directory recursively"
    },
    {
        "input_text": "summarize: def _all_items(self, node):                if isinstance(node, int):            # Could be more efficient if we built up all the IDs and            # then issued a single SELECT.            yield self.lib.get_item(node)        else:            # Recurse into a directory.            for name, itemid in sorted(node.files.items()):                # \"yield from\"                yield from self._all_items(itemid)            for name, subdir in sorted(node.dirs.items()):                yield from self._all_items(subdir)",
        "labels_text": "Generator yielding all item under a VFS node"
    },
    {
        "input_text": "summarize: def _add(self, path, send_id=False):                for item in self._all_items(self._resolve_path(path)):            self.playlist.append(item)            if send_id:                yield \"Id: \" + str(item.id)        self.playlist_version += 1        self._send_event(\"playlist\")",
        "labels_text": "Adds a track or directory to the playlist specified by the path If sendid write each item id to the client"
    },
    {
        "input_text": "summarize: def cmd_add(self, conn, path):                return self._add(path, False)",
        "labels_text": "Adds a track or directory to the playlist specified by a path"
    },
    {
        "input_text": "summarize: def cmd_addid(self, conn, path):                return self._add(path, True)",
        "labels_text": "Same a cmdadd but sends an id back to the client"
    },
    {
        "input_text": "summarize: def cmd_decoders(self, conn):                decoders = self.player.get_decoders()        for name, (mimes, exts) in decoders.items():            yield f\"plugin: {name}\"            for ext in exts:                yield f\"suffix: {ext}\"            for mime in mimes:                yield f\"mime_type: {mime}\"",
        "labels_text": "Send list of supported decoder and format"
    },
    {
        "input_text": "summarize: def cmd_tagtypes(self, conn):                for tag in self.tagtype_map:            yield \"tagtype: \" + tag",
        "labels_text": "Returns a list of the metadata tag field available for searching"
    },
    {
        "input_text": "summarize: def _tagtype_lookup(self, tag):                for test_tag, key in self.tagtype_map.items():            # Match case-insensitively.            if test_tag.lower() == tag.lower():                return test_tag, key        raise BPDError(ERROR_UNKNOWN, \"no such tagtype\")",
        "labels_text": "Uses tagtypemap to look up the beet column name for an MPD tagtype or throw an appropriate exception Returns both the canonical name of the MPD tagtype and the beet column name"
    },
    {
        "input_text": "summarize: def cmd_search(self, conn, *kv):                query = self._metadata_query(            dbcore.query.SubstringQuery, dbcore.query.AnyFieldQuery, kv        )        for item in self.lib.items(query):            yield self._item_info(item)",
        "labels_text": "Perform a substring match for item"
    },
    {
        "input_text": "summarize: def cmd_find(self, conn, *kv):                query = self._metadata_query(dbcore.query.MatchQuery, None, kv)        for item in self.lib.items(query):            yield self._item_info(item)",
        "labels_text": "Perform an exact match for item"
    },
    {
        "input_text": "summarize: def cmd_count(self, conn, tag, value):                _, key = self._tagtype_lookup(tag)        songs = 0        playtime = 0.0        for item in self.lib.items(dbcore.query.MatchQuery(key, value)):            songs += 1            playtime += item.length        yield \"songs: \" + str(songs)        yield \"playtime: \" + str(int(playtime))",
        "labels_text": "Returns the number and total time of song matching the tagvalue query"
    },
    {
        "input_text": "summarize: def cmd_outputs(self, conn):                yield (            \"outputid: 0\",            \"outputname: gstreamer\",            \"outputenabled: 1\",        )",
        "labels_text": "List the available output"
    },
    {
        "input_text": "summarize: def cmd_seek(self, conn, index, pos):                index = cast_arg(int, index)        pos = cast_arg(float, pos)        super().cmd_seek(conn, index, pos)        self.player.seek(pos)",
        "labels_text": "Seeks to the specified position in the specified song"
    },
    {
        "input_text": "summarize: def start_bpd(self, lib, host, port, password, volume, ctrl_port):                try:            server = Server(lib, host, port, password, ctrl_port, self._log)            server.cmd_setvol(None, volume)            server.run()        except NoGstreamerError:            self._log.error(\"Gstreamer Python bindings not found.\")            self._log.error(                'Install \"gstreamer1.0\" and \"python-gi\"'                \"or similar package to use BPD.\"            )",
        "labels_text": "Starts a BPD server"
    },
    {
        "input_text": "summarize: def deduplicate(seq):        seen = set()    return [x for x in seq if x not in seen and not seen.add(x)]",
        "labels_text": "Remove duplicate from sequence while preserving order"
    },
    {
        "input_text": "summarize: def flatten_tree(elem, path, branches):        if not path:        path = []    if isinstance(elem, dict):        for k, v in elem.items():            flatten_tree(v, path + [k], branches)    elif isinstance(elem, list):        for sub in elem:            flatten_tree(sub, path, branches)    else:        branches.append(path + [str(elem)])",
        "labels_text": "Flatten nested listsdictionaries into list of string branch"
    },
    {
        "input_text": "summarize: def find_parents(candidate, branches):        for branch in branches:        try:            idx = branch.index(candidate.lower())            return list(reversed(branch[: idx + 1]))        except ValueError:            continue    return [candidate]",
        "labels_text": "Find parent genre of a given genre ordered from the closest to the further parent"
    },
    {
        "input_text": "summarize: def sources(self):                source = self.config[\"source\"].as_choice((\"track\", \"album\", \"artist\"))        if source == \"track\":            return \"track\", \"album\", \"artist\"        elif source == \"album\":            return \"album\", \"artist\"        elif source == \"artist\":            return (\"artist\",)",
        "labels_text": "A tuple of allowed genre source May contain track album or artist"
    },
    {
        "input_text": "summarize: def _get_depth(self, tag):                depth = None        for key, value in enumerate(self.c14n_branches):            if tag in value:                depth = value.index(tag)                break        return depth",
        "labels_text": "Find the depth of a tag in the genre tree"
    },
    {
        "input_text": "summarize: def _sort_by_depth(self, tags):                depth_tag_pairs = [(self._get_depth(t), t) for t in tags]        depth_tag_pairs = [e for e in depth_tag_pairs if e[0] is not None]        depth_tag_pairs.sort(reverse=True)        return [p[1] for p in depth_tag_pairs]",
        "labels_text": "Given a list of tag sort the tag by their depth in the genre tree"
    },
    {
        "input_text": "summarize: def fetch_genre(self, lastfm_obj):                min_weight = self.config[\"min_weight\"].get(int)        return self._resolve_genres(self._tags_for(lastfm_obj, min_weight))",
        "labels_text": "Return the genre for a pylast entity or None if no suitable genre can be found Ex Electronic House Dance"
    },
    {
        "input_text": "summarize: def _is_allowed(self, genre):                if genre is None:            return False        if not self.whitelist or genre in self.whitelist:            return True        return False",
        "labels_text": "Determine whether the genre is present in the whitelist returning a boolean"
    },
    {
        "input_text": "summarize: def fetch_album_genre(self, obj):                return self._last_lookup(            \"album\", LASTFM.get_album, obj.albumartist, obj.album        )",
        "labels_text": "Return the album genre for this Item or Album"
    },
    {
        "input_text": "summarize: def fetch_album_artist_genre(self, obj):                return self._last_lookup(\"artist\", LASTFM.get_artist, obj.albumartist)",
        "labels_text": "Return the album artist genre for this Item or Album"
    },
    {
        "input_text": "summarize: def fetch_artist_genre(self, item):                return self._last_lookup(\"artist\", LASTFM.get_artist, item.artist)",
        "labels_text": "Returns the track artist genre for this Item"
    },
    {
        "input_text": "summarize: def fetch_track_genre(self, obj):                return self._last_lookup(            \"track\", LASTFM.get_track, obj.artist, obj.title        )",
        "labels_text": "Returns the track genre for this Item"
    },
    {
        "input_text": "summarize: def load_meta_sources():        meta_sources = {}    for module_path, class_name in SOURCES.items():        module = import_module(METASYNC_MODULE + \".\" + module_path)        meta_sources[class_name.lower()] = getattr(module, class_name)    return meta_sources",
        "labels_text": "Returns a dictionary of all the MetaSources Eg itunes Itunes with isinstanceItunes MetaSource true"
    },
    {
        "input_text": "summarize: def load_item_types():        item_types = {}    for meta_source in META_SOURCES.values():        item_types.update(meta_source.item_types)    return item_types",
        "labels_text": "Returns a dictionary containing the itemtypes of all the MetaSources"
    },
    {
        "input_text": "summarize: def json_generator(items, root, expand=False):        yield '{\"%s\":[' % root    first = True    for item in items:        if first:            first = False        else:            yield \",\"        yield json.dumps(_rep(item, expand=expand))    yield \"]}\"",
        "labels_text": "Generator that dump list of beet Items or Albums a JSON param root root key for JSON param item list of classItem or classAlbum to dump param expand If true every classAlbum contains it item in the json representation return generator that yield string"
    },
    {
        "input_text": "summarize: def is_expand():        return flask.request.args.get(\"expand\") is not None",
        "labels_text": "Returns whether the current request is for an expanded response"
    },
    {
        "input_text": "summarize: def is_delete():        return flask.request.args.get(\"delete\") is not None",
        "labels_text": "Returns whether the current delete request should remove the selected file"
    },
    {
        "input_text": "summarize: def get_method():        return flask.request.method",
        "labels_text": "Returns the HTTP method of the current request"
    },
    {
        "input_text": "summarize: def resource_list(name):        def make_responder(list_all):        def responder():            return app.response_class(                json_generator(list_all(), root=name, expand=is_expand()),                mimetype=\"application/json\",            )        responder.__name__ = f\"all_{name}\"        return responder    return make_responder",
        "labels_text": "Decorates a function to handle RESTful HTTP request for a list of resource"
    },
    {
        "input_text": "summarize: def _get_unique_table_field_values(model, field, sort_field):        if field not in model.all_keys() or sort_field not in model.all_keys():        raise KeyError    with g.lib.transaction() as tx:        rows = tx.query(            'SELECT DISTINCT \"{}\" FROM \"{}\" ORDER BY \"{}\"'.format(                field, model._table, sort_field            )        )    return [row[0] for row in rows]",
        "labels_text": "retrieve all unique value belonging to a key from a model"
    },
    {
        "input_text": "summarize: def validate_new_version(    ctx: click.Context, param: click.Argument, value: Version) -> Version:        with PYPROJECT.open(\"rb\") as f:        current = parse(tomli.load(f)[\"tool\"][\"poetry\"][\"version\"])    if not value > current:        msg = f\"version must be newer than {current}\"        raise click.BadParameter(msg)    return value",
        "labels_text": "Validate the version is newer than the current one"
    },
    {
        "input_text": "summarize: def bump_version(new: Version) -> None:        for path, perform_update in FILENAME_AND_UPDATE_TEXT:        with path.open(\"r+\") as f:            contents = f.read()            f.seek(0)            f.write(perform_update(contents, new))            f.truncate()",
        "labels_text": "Update the version number in specified file"
    },
    {
        "input_text": "summarize: def bump(version: Version) -> None:        bump_version(version)",
        "labels_text": "Bump the version in project file"
    },
    {
        "input_text": "summarize: def changelog():        print(changelog_as_markdown())",
        "labels_text": "Get the most recent version changelog a Markdown"
    },
    {
        "input_text": "summarize: def __init__(self):                self.version = (7, 0, 0)        self.legacy = False        self.convert_cmd = [\"magick\"]        self.identify_cmd = [\"magick\", \"identify\"]        self.compare_cmd = [\"magick\", \"compare\"]",
        "labels_text": "Init a dummy backend class for mocked ImageMagick test"
    },
    {
        "input_text": "summarize: def __init__(self):                pass",
        "labels_text": "Init a dummy backend class for mocked PIL test"
    },
    {
        "input_text": "summarize: def test_pil_file_resize(self):                self._test_img_resize(PILBackend())",
        "labels_text": "Test PIL resize function is lowering file size"
    },
    {
        "input_text": "summarize: def test_im_file_resize(self):                self._test_img_resize(IMBackend())",
        "labels_text": "Test IM resize function is lowering file size"
    },
    {
        "input_text": "summarize: def test_pil_file_deinterlace(self):                path = PILBackend().deinterlace(self.IMG_225x225)        from PIL import Image        with Image.open(path) as img:            assert \"progression\" not in img.info",
        "labels_text": "Test PIL deinterlace function Check if the PILBackenddeinterlace function return image that are nonprogressive"
    },
    {
        "input_text": "summarize: def test_im_file_deinterlace(self):                im = IMBackend()        path = im.deinterlace(self.IMG_225x225)        cmd = im.identify_cmd + [            \"-format\",            \"%[interlace]\",            syspath(path, prefix=False),        ]        out = command_output(cmd).stdout        assert out == b\"None\"",
        "labels_text": "Test ImageMagick deinterlace function Check if the IMBackenddeinterlace function return image that are nonprogressive"
    },
    {
        "input_text": "summarize: def _clear_weights():        Distance.__dict__[\"_weights\"].cache = {}",
        "labels_text": "Hack around the lazy descriptor used to cache weight for Distance calculation"
    },
    {
        "input_text": "summarize: def test_format_fixed_field_integer_normalized(self):                model = ModelFixture1()        model.field_one = 142.432        value = model.formatted().get(\"field_one\")        assert value == \"142\"        model.field_one = 142.863        value = model.formatted().get(\"field_one\")        assert value == \"143\"",
        "labels_text": "The normalize method of the Integer class round float"
    },
    {
        "input_text": "summarize: def match_album_mock(*args, **kwargs):        track_info = TrackInfo(        title=\"new title\",        track_id=\"trackid\",        index=0,    )    album_info = AlbumInfo(        artist=\"artist\",        album=\"album\",        tracks=[track_info],        album_id=\"albumid\",        artist_id=\"artistid\",        flex=\"flex\",    )    return iter([album_info])",
        "labels_text": "Create an AlbumInfo object for testing"
    },
    {
        "input_text": "summarize: def _normalize_path(self, path):                path = path.decode(\"utf-8\")        norm_form = \"NFD\" if sys.platform == \"darwin\" else \"NFC\"        path = unicodedata.normalize(norm_form, path)        return path.encode(\"utf-8\")",
        "labels_text": "Normalize a path Unicode combining form according to the platform"
    },
    {
        "input_text": "summarize: def test_playlist_write_empty(self):                tempdir = bytestring_path(mkdtemp())        the_playlist_file = path.join(tempdir, b\"playlist.m3u8\")        m3ufile = M3UFile(the_playlist_file)        with pytest.raises(EmptyPlaylistError):            m3ufile.write()        rmtree(tempdir)",
        "labels_text": "Test whether saving an empty playlist file raise an error"
    },
    {
        "input_text": "summarize: def test_playlist_load_ascii(self):                the_playlist_file = path.join(RSRC, b\"playlist.m3u\")        m3ufile = M3UFile(the_playlist_file)        m3ufile.load()        assert m3ufile.media_list[0] == bytestring_path(            \"/This/is/a/path/to_a_file.mp3\"        )",
        "labels_text": "Test loading ascii path from a playlist file"
    },
    {
        "input_text": "summarize: def test_playlist_load_unicode(self):                the_playlist_file = path.join(RSRC, b\"playlist.m3u8\")        m3ufile = M3UFile(the_playlist_file)        m3ufile.load()        assert m3ufile.media_list[0] == bytestring_path(            \"/This/is/\u00e5/path/to_a_file.mp3\"        )",
        "labels_text": "Test loading unicode path from a playlist file"
    },
    {
        "input_text": "summarize: def test_playlist_load_extm3u(self):                the_playlist_file = path.join(RSRC, b\"playlist.m3u\")        m3ufile = M3UFile(the_playlist_file)        m3ufile.load()        assert m3ufile.extm3u",
        "labels_text": "Test loading a playlist with an EXTMU header"
    },
    {
        "input_text": "summarize: def test_playlist_load_non_extm3u(self):                the_playlist_file = path.join(RSRC, b\"playlist_non_ext.m3u\")        m3ufile = M3UFile(the_playlist_file)        m3ufile.load()        assert not m3ufile.extm3u",
        "labels_text": "Test loading a playlist without an EXTMU header"
    },
    {
        "input_text": "summarize: def test_get_prefixes_keyed(self):                q0 = \"-title:qux\"        q1 = \"^title:qux\"        results0 = self.lib.items(q0)        results1 = self.lib.items(q1)        self.assert_items_matched(results0, [\"foo bar\", \"beets 4 eva\"])        self.assert_items_matched(results1, [\"foo bar\", \"beets 4 eva\"])",
        "labels_text": "Test both negation prefix on a keyed query"
    },
    {
        "input_text": "summarize: def test_get_prefixes_unkeyed(self):                q0 = \"-qux\"        q1 = \"^qux\"        results0 = self.lib.items(q0)        results1 = self.lib.items(q1)        self.assert_items_matched(results0, [\"foo bar\", \"beets 4 eva\"])        self.assert_items_matched(results1, [\"foo bar\", \"beets 4 eva\"])",
        "labels_text": "Test both negation prefix on an unkeyed query"
    },
    {
        "input_text": "summarize: def test_negation_interaction(self):                query, sort = beets.library.parse_query_string(            \"-bar+\", beets.library.Item        )        assert len(query.subqueries) == 1        assert isinstance(query.subqueries[0], dbcore.query.TrueQuery)        assert isinstance(sort, dbcore.query.SlowFieldSort)        assert sort.field == \"-bar\"",
        "labels_text": "Test the handling of negation and sorting together If a string end with a sorting suffix it take precedence over the NotQuery parsing"
    },
    {
        "input_text": "summarize: def _normexpr(expr):        textbuf = []    for part in expr.parts:        if isinstance(part, str):            textbuf.append(part)        else:            if textbuf:                text = \"\".join(textbuf)                if text:                    yield text                    textbuf = []            yield part    if textbuf:        text = \"\".join(textbuf)        if text:            yield text",
        "labels_text": "Normalize an Expression object part collapsing multiple adjacent text block and removing empty text block Generates a sequence of part"
    },
    {
        "input_text": "summarize: def _normparse(text):        return _normexpr(functemplate._parse(text))",
        "labels_text": "Parse a template and then normalize the resulting Expression"
    },
    {
        "input_text": "summarize: def _assert_symbol(self, obj, ident):                assert isinstance(obj, functemplate.Symbol), f\"not a Symbol: {obj}\"        assert obj.ident == ident, f\"wrong identifier: {obj.ident} vs. {ident}\"",
        "labels_text": "Assert that an object is a Symbol with the given identifier"
    },
    {
        "input_text": "summarize: def _assert_call(self, obj, ident, numargs):                assert isinstance(obj, functemplate.Call), f\"not a Call: {obj}\"        assert obj.ident == ident, f\"wrong identifier: {obj.ident} vs. {ident}\"        assert (            len(obj.args) == numargs        ), f\"wrong argument count in {obj.ident}: {len(obj.args)} vs. {numargs}\"",
        "labels_text": "Assert that an object is a Call with the given identifier and argument count"
    },
    {
        "input_text": "summarize: def test_non_metadata_field_unchanged(self):                # An item that starts out \"clean\".        item = self.add_item_fixture()        item.read()        # ... but with a mismatched bitrate.        item.bitrate = 123        item.store()        output = self.write_cmd()        assert output == \"\"",
        "labels_text": "Changing a nontag field like bitrate and writing should have no effect"
    },
    {
        "input_text": "summarize: def safe_open_w(path):        mkdir_p(os.path.dirname(path))    return open(path, \"w\")",
        "labels_text": "Open path for writing creating any parent directory a needed"
    },
    {
        "input_text": "summarize: def test_renames_types(self):                self._set_config(            types=[(\"ep\", \"EP\"), (\"remix\", \"Remix\")], ignore_va=[], bracket=\"()\"        )        album = self._create_album(album_types=[\"ep\", \"remix\"])        subject = AlbumTypesPlugin()        result = subject._atypes(album)        assert \"(EP)(Remix)\" == result        return",
        "labels_text": "Tests if the plugin correctly renames the specified type"
    },
    {
        "input_text": "summarize: def test_returns_only_specified_types(self):                self._set_config(            types=[(\"ep\", \"EP\"), (\"soundtrack\", \"\")], ignore_va=[], bracket=\"()\"        )        album = self._create_album(album_types=[\"ep\", \"remix\", \"soundtrack\"])        subject = AlbumTypesPlugin()        result = subject._atypes(album)        assert \"(EP)\" == result",
        "labels_text": "Tests if the plugin return only nonblank type given in config"
    },
    {
        "input_text": "summarize: def test_respects_type_order(self):                self._set_config(            types=[(\"remix\", \"Remix\"), (\"ep\", \"EP\")], ignore_va=[], bracket=\"()\"        )        album = self._create_album(album_types=[\"ep\", \"remix\"])        subject = AlbumTypesPlugin()        result = subject._atypes(album)        assert \"(Remix)(EP)\" == result        return",
        "labels_text": "Tests if the type are returned in the same order a config"
    },
    {
        "input_text": "summarize: def test_ignores_va(self):                self._set_config(            types=[(\"ep\", \"EP\"), (\"soundtrack\", \"OST\")],            ignore_va=[\"ep\"],            bracket=\"()\",        )        album = self._create_album(            album_types=[\"ep\", \"soundtrack\"], artist_id=VARIOUS_ARTISTS_ID        )        subject = AlbumTypesPlugin()        result = subject._atypes(album)        assert \"(OST)\" == result",
        "labels_text": "Tests if the specified type is ignored for VA album"
    },
    {
        "input_text": "summarize: def test_respects_defaults(self):                album = self._create_album(            album_types=[                \"ep\",                \"single\",                \"soundtrack\",                \"live\",                \"compilation\",                \"remix\",            ],            artist_id=VARIOUS_ARTISTS_ID,        )        subject = AlbumTypesPlugin()        result = subject._atypes(album)        assert \"[EP][Single][OST][Live][Remix]\" == result",
        "labels_text": "Tests if the plugin us the default value if config not given"
    },
    {
        "input_text": "summarize: def _require_backend(self):                if not ArtResizer.shared.local:            self.skipTest(\"ArtResizer has no local imaging backend available\")",
        "labels_text": "Skip the test if the art resizer doesnt have ImageMagick or PIL so comparison and measurement are unavailable"
    },
    {
        "input_text": "summarize: def _other_album_and_item(helper):        item = helper.add_item_fixture(        album=\"Other Album\",        title=\"Other Title\",        artist=\"Other Artist\",        albumartist=\"Other Album Artist\",    )    helper.lib.add_album([item])",
        "labels_text": "Add another item and album to prove that filtering work"
    },
    {
        "input_text": "summarize: def get_response_data(self, client: Client, item):                def get(            endpoint: str, params: Dict[str, str]        ) -> Optional[Dict[str, Any]]:                        response = client.get(                endpoint,                query_string={\"include\": \"tracks,artists,albums\", **params},            )            assert response.status_code == HTTPStatus.OK            return response.json        return get",
        "labels_text": "Return a callback accepting endpoint and params parameter"
    },
    {
        "input_text": "summarize: def test_bareasc_list_output(self):                with capture_stdout() as output:            self.run_command(\"bareasc\", \"with accents\")        assert \"Antonin Dvorak\" in output.getvalue()",
        "labels_text": "BareASCII version of list command check output"
    },
    {
        "input_text": "summarize: def test_bareasc_format_output(self):                with capture_stdout() as output:            self.run_command(                \"bareasc\", \"with accents\", \"-f\", \"$artist:: $title\"            )        assert \"Antonin Dvorak:: with accents\\n\" == output.getvalue()",
        "labels_text": "BareASCII version of list f command check output"
    },
    {
        "input_text": "summarize: def test_sub_genre_empty_fallback(self):                self.response_tracks[0][\"subGenres\"] = []        tracks = [beatport.BeatportTrack(t) for t in self.response_tracks]        self.test_tracks[0][\"subGenres\"] = []        assert tracks[0].genre == self.test_tracks[0][\"genres\"][0][\"name\"]",
        "labels_text": "No subgenre is provided Test if fallback to genre work"
    },
    {
        "input_text": "summarize: def test_genre_empty(self):                self.response_tracks[0][\"genres\"] = []        tracks = [beatport.BeatportTrack(t) for t in self.response_tracks]        self.test_tracks[0][\"genres\"] = []        assert tracks[0].genre == self.test_tracks[0][\"subGenres\"][0][\"name\"]",
        "labels_text": "No genre is provided Test if subgenre is applied"
    },
    {
        "input_text": "summarize: def test_year_single_year(self):                self._setup_config(bucket_year=[\"1950s\", \"1970s\"])        assert self.plugin._tmpl_bucket(\"1959\") == \"1950s\"        assert self.plugin._tmpl_bucket(\"1969\") == \"1950s\"",
        "labels_text": "If a single year is given range start from this year and stop at the year preceding the one of next bucket"
    },
    {
        "input_text": "summarize: def test_year_single_year_last_folder(self):                self._setup_config(bucket_year=[\"1950\", \"1970\"])        assert self.plugin._tmpl_bucket(\"2014\") == \"1970\"        assert self.plugin._tmpl_bucket(\"2025\") == \"2025\"",
        "labels_text": "If a single year is given for the last bucket extend it to current year"
    },
    {
        "input_text": "summarize: def test_year_two_years(self):                self._setup_config(bucket_year=[\"1950-59\", \"1960-1969\"])        assert self.plugin._tmpl_bucket(\"1959\") == \"1950-59\"        assert self.plugin._tmpl_bucket(\"1969\") == \"1960-1969\"",
        "labels_text": "Buckets can be named with the fromto syntax"
    },
    {
        "input_text": "summarize: def test_year_multiple_years(self):                self._setup_config(bucket_year=[\"1950,51,52,53\"])        assert self.plugin._tmpl_bucket(\"1953\") == \"1950,51,52,53\"        assert self.plugin._tmpl_bucket(\"1974\") == \"1974\"",
        "labels_text": "Buckets can be named by listing all the year"
    },
    {
        "input_text": "summarize: def test_year_out_of_range(self):                self._setup_config(bucket_year=[\"1950-59\", \"1960-69\"])        assert self.plugin._tmpl_bucket(\"1974\") == \"1974\"        self._setup_config(bucket_year=[])        assert self.plugin._tmpl_bucket(\"1974\") == \"1974\"",
        "labels_text": "If no range match return the year"
    },
    {
        "input_text": "summarize: def test_alpha_all_chars(self):                self._setup_config(bucket_alpha=[\"ABCD\", \"FGH\", \"IJKL\"])        assert self.plugin._tmpl_bucket(\"garry\") == \"FGH\"",
        "labels_text": "Alphabet bucket can be named by listing all their char"
    },
    {
        "input_text": "summarize: def test_alpha_first_last_chars(self):                self._setup_config(bucket_alpha=[\"0->9\", \"A->D\", \"F-H\", \"I->Z\"])        assert self.plugin._tmpl_bucket(\"garry\") == \"F-H\"        assert self.plugin._tmpl_bucket(\"2pac\") == \"0->9\"",
        "labels_text": "Alphabet bucket can be named by listing the fromto syntax"
    },
    {
        "input_text": "summarize: def test_alpha_out_of_range(self):                self._setup_config(bucket_alpha=[\"ABCD\", \"FGH\", \"IJKL\"])        assert self.plugin._tmpl_bucket(\"errol\") == \"E\"        self._setup_config(bucket_alpha=[])        assert self.plugin._tmpl_bucket(\"errol\") == \"E\"",
        "labels_text": "If no range match return the initial"
    },
    {
        "input_text": "summarize: def test_alpha_regex(self):                self._setup_config(            bucket_alpha=[\"foo\", \"bar\"],            bucket_alpha_regex={\"foo\": \"^[a-d]\", \"bar\": \"^[e-z]\"},        )        assert self.plugin._tmpl_bucket(\"alpha\") == \"foo\"        assert self.plugin._tmpl_bucket(\"delta\") == \"foo\"        assert self.plugin._tmpl_bucket(\"zeta\") == \"bar\"        assert self.plugin._tmpl_bucket(\"Alpha\") == \"A\"",
        "labels_text": "Check regex is used"
    },
    {
        "input_text": "summarize: def test_bad_alpha_range_def(self):                with pytest.raises(ui.UserError):            self._setup_config(bucket_alpha=[\"$%\"])",
        "labels_text": "If bad alpha range definition a UserError is raised"
    },
    {
        "input_text": "summarize: def test_bad_year_range_def_no4digits(self):                with pytest.raises(ui.UserError):            self._setup_config(bucket_year=[\"62-64\"])",
        "labels_text": "If bad year range definition a UserError is raised Range origin must be expressed on digit"
    },
    {
        "input_text": "summarize: def test_bad_year_range_def_nodigits(self):                with pytest.raises(ui.UserError):            self._setup_config(bucket_year=[\"nodigits\"])",
        "labels_text": "If bad year range definition a UserError is raised At least the range origin must be declared"
    },
    {
        "input_text": "summarize: def tagged_copy_cmd(self, tag):                if re.search(\"[^a-zA-Z0-9]\", tag):            raise ValueError(                \"tag '{}' must only contain letters and digits\".format(tag)            )        # A Python script that copies the file and appends a tag.        stub = os.path.join(_common.RSRC, b\"convert_stub.py\").decode(\"utf-8\")        return \"{} {} $source $dest {}\".format(            shell_quote(sys.executable), shell_quote(stub), tag        )",
        "labels_text": "Return a conversion command that copy file and appends tag to the copy"
    },
    {
        "input_text": "summarize: def assertFileTag(self, path, tag):  # noqa                display_tag = tag        tag = tag.encode(\"utf-8\")        self.assertIsFile(path)        with open(path, \"rb\") as f:            f.seek(-len(display_tag), os.SEEK_END)            assert (                f.read() == tag            ), f\"{displayable_path(path)} is not tagged with {display_tag}\"",
        "labels_text": "Assert that the path is a file and the file content end with tag"
    },
    {
        "input_text": "summarize: def assertNoFileTag(self, path, tag):  # noqa                display_tag = tag        tag = tag.encode(\"utf-8\")        self.assertIsFile(path)        with open(path, \"rb\") as f:            f.seek(-len(tag), os.SEEK_END)            assert (                f.read() != tag            ), f\"{displayable_path(path)} is unexpectedly tagged with {display_tag}\"",
        "labels_text": "Assert that the path is a file and the file content doe not end with tag"
    },
    {
        "input_text": "summarize: def run_convert_path(self, path, *args):                # The path is currently a filesystem bytestring. Convert it to        # an argument bytestring.        path = path.decode(util._fsencoding()).encode(util.arg_encoding())        args = args + (b\"path:\" + path,)        return self.run_command(\"convert\", *args)",
        "labels_text": "Run the convert command on a given path"
    },
    {
        "input_text": "summarize: def run_convert(self, *args):                return self.run_convert_path(self.item.path, *args)",
        "labels_text": "Run the convert command on selfitem"
    },
    {
        "input_text": "summarize: def _make_release_from_positions(self, positions):                tracks = [            self._make_track(\"TITLE%s\" % i, position)            for (i, position) in enumerate(positions, start=1)        ]        return self._make_release(tracks)",
        "labels_text": "Return a Bag that mimic a discogsclientRelease with a tracklist where track have the specified position"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_without_sides(self):                release = self._make_release_from_positions([\"1\", \"2\", \"3\"])        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 1        assert len(d.tracks) == 3",
        "labels_text": "Test standard Discogs position without side"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_with_sides(self):                release = self._make_release_from_positions([\"A1\", \"A2\", \"B1\", \"B2\"])        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 1  # 2 sides = 1 LP        assert len(d.tracks) == 4",
        "labels_text": "Test standard Discogs position with side"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_multiple_lp(self):                release = self._make_release_from_positions([\"A1\", \"A2\", \"B1\", \"C1\"])        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 2  # 3 sides = 1 LP + 1 LP        assert len(d.tracks) == 4",
        "labels_text": "Test standard Discogs position multiple LP"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_multiple_cd(self):                release = self._make_release_from_positions(            [\"1-1\", \"1-2\", \"2-1\", \"3-1\"]        )        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 3        assert len(d.tracks) == 4",
        "labels_text": "Test standard Discogs position multiple CDs"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_non_standard(self):                release = self._make_release_from_positions([\"I\", \"II\", \"III\", \"IV\"])        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 1        assert len(d.tracks) == 4",
        "labels_text": "Test non standard Discogs position"
    },
    {
        "input_text": "summarize: def test_parse_tracklist_subtracks_extra_material(self):                release = self._make_release_from_positions([\"1\", \"2\", \"Video 1\"])        d = DiscogsPlugin().get_album_info(release)        assert d.mediums == 2        assert len(d.tracks) == 3",
        "labels_text": "Test standard Discogs position extra material"
    },
    {
        "input_text": "summarize: def test_parse_release_without_required_fields(self):                release = Bag(data={}, refresh=lambda *args: None)        with capture_log() as logs:            d = DiscogsPlugin().get_album_info(release)        assert d is None        assert \"Release does not contain the required fields\" in logs[0]",
        "labels_text": "Test parsing of a release that doe not have the required field"
    },
    {
        "input_text": "summarize: def test_default_genre_style_settings(self):                release = self._make_release_from_positions([\"1\", \"2\"])        d = DiscogsPlugin().get_album_info(release)        assert d.genre == \"GENRE1, GENRE2\"        assert d.style == \"STYLE1, STYLE2\"",
        "labels_text": "Test genre default setting genre to genre style to style"
    },
    {
        "input_text": "summarize: def test_append_style_to_genre(self):                config[\"discogs\"][\"append_style_genre\"] = True        release = self._make_release_from_positions([\"1\", \"2\"])        d = DiscogsPlugin().get_album_info(release)        assert d.genre == \"GENRE1, GENRE2, STYLE1, STYLE2\"        assert d.style == \"STYLE1, STYLE2\"",
        "labels_text": "Test appending style to genre if config enabled"
    },
    {
        "input_text": "summarize: def test_append_style_to_genre_no_style(self):                config[\"discogs\"][\"append_style_genre\"] = True        release = self._make_release_from_positions([\"1\", \"2\"])        release.data[\"styles\"] = []        d = DiscogsPlugin().get_album_info(release)        assert d.genre == \"GENRE1, GENRE2\"        assert d.style is None",
        "labels_text": "Test nothing appended to genre if style is empty"
    },
    {
        "input_text": "summarize: def __init__(self, contents=None, replacements=None):                self.contents = contents        self.replacements = replacements        self.action = self.overwrite_contents        if replacements:            self.action = self.replace_contents",
        "labels_text": "selfcontents and selfreplacements are initialized here in order to keep the rest of the function of this class with the same signature a EditPlugingeteditor making mocking easier content string with the content of the file to be used for overwritecontents replacement dict with the inplace replacement to be used for replacecontents in the form previous string new string TODO check if it can be solved more elegantly with a decorator"
    },
    {
        "input_text": "summarize: def overwrite_contents(self, filename, log):                if self.contents:            with codecs.open(filename, \"w\", encoding=\"utf-8\") as f:                f.write(self.contents)",
        "labels_text": "Modify filename replacing it content with selfcontents If selfcontents is empty the file remains unchanged"
    },
    {
        "input_text": "summarize: def replace_contents(self, filename, log):                with codecs.open(filename, \"r\", encoding=\"utf-8\") as f:            contents = f.read()        for old, new_ in self.replacements.items():            contents = contents.replace(old, new_)        with codecs.open(filename, \"w\", encoding=\"utf-8\") as f:            f.write(contents)",
        "labels_text": "Modify filename reading it content and replacing the string specified in selfreplacements"
    },
    {
        "input_text": "summarize: def assertItemFieldsModified(  # noqa        self, library_items, items, fields=[], allowed=[\"path\"]    ):                for lib_item, item in zip(library_items, items):            diff_fields = [                field                for field in lib_item._fields                if lib_item[field] != item[field]            ]            assert set(diff_fields).difference(allowed) == set(fields)",
        "labels_text": "Assert that item in the library libitems have different value on the specified field and only on those field compared to item An empty field list result in asserting that no modification have been performed allowed is a list of field change that are ignored they may or may not have changed the assertion doesnt care"
    },
    {
        "input_text": "summarize: def run_mocked_interpreter(self, modify_file_args={}, stdin=[]):                m = ModifyFileMocker(**modify_file_args)        with patch(\"beetsplug.edit.edit\", side_effect=m.action):            with control_stdin(\"\\n\".join(stdin)):                self.importer.run()",
        "labels_text": "Run the edit command during an import session with mocked stdin and yaml writing"
    },
    {
        "input_text": "summarize: def run_mocked_command(self, modify_file_args={}, stdin=[], args=[]):                m = ModifyFileMocker(**modify_file_args)        with patch(\"beetsplug.edit.edit\", side_effect=m.action):            with control_stdin(\"\\n\".join(stdin)):                self.run_command(\"edit\", *args)",
        "labels_text": "Run the edit command with mocked stdin and yaml writing and passing args to runcommand"
    },
    {
        "input_text": "summarize: def test_noedit(self, mock_write):                # Do not edit anything.        self.run_mocked_command(            {\"contents\": None},            # No stdin.            [],        )        self.assertCounts(            mock_write, write_call_count=0, title_starts_with=\"t\\u00eftle\"        )        self.assertItemFieldsModified(self.album.items(), self.items_orig, [])",
        "labels_text": "Do not edit anything"
    },
    {
        "input_text": "summarize: def test_malformed_yaml(self, mock_write):                # Edit the yaml file to an invalid file.        self.run_mocked_command(            {\"contents\": \"!MALFORMED\"},            # Edit again to fix? No.            [\"n\"],        )        self.assertCounts(            mock_write, write_call_count=0, title_starts_with=\"t\\u00eftle\"        )",
        "labels_text": "Edit the yaml file incorrectly resulting in a malformed yaml document"
    },
    {
        "input_text": "summarize: def test_invalid_yaml(self, mock_write):                # Edit the yaml file to an invalid but parseable file.        self.run_mocked_command(            {\"contents\": \"wellformed: yes, but invalid\"},            # No stdin.            [],        )        self.assertCounts(            mock_write, write_call_count=0, title_starts_with=\"t\\u00eftle\"        )",
        "labels_text": "Edit the yaml file incorrectly resulting in a wellformed but invalid yaml document"
    },
    {
        "input_text": "summarize: def _popen(self, status=0, stdout=\"\", stderr=\"\"):                popen = MagicMock(returncode=status)        popen.communicate.return_value = stdout, stderr        return popen",
        "labels_text": "Create a mock Popen object"
    },
    {
        "input_text": "summarize: def test_import_default(self):                self._run({}, 3, self.all_tracks)",
        "labels_text": "The default configuration should import everything"
    },
    {
        "input_text": "summarize: def test_singleton_config(self):                self._run({\"singleton_path\": \".*other_album.*\"}, 3, self.all_tracks)",
        "labels_text": "Check that singleton configuration is ignored for album import"
    },
    {
        "input_text": "summarize: def test_album_config(self):                self._run({\"album_path\": \".*other_album.*\"}, 0, self.all_tracks)",
        "labels_text": "Check that album configuration is ignored for singleton import"
    },
    {
        "input_text": "summarize: def setUp(self):                ftintitle.FtInTitlePlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def preserve_plugin_listeners():        if not ImportAddedPlugin.listeners:        ImportAddedPlugin.listeners = _listeners",
        "labels_text": "Preserve the initial plugin listener a they would otherwise be deleted after the first setup tear down cycle"
    },
    {
        "input_text": "summarize: def find_media_file(self, item):                for m in self.import_media:            if m.title.replace(\"Tag\", \"Applied\") == item.title:                return m        raise AssertionError(            \"No MediaFile found for Item \" + displayable_path(item.path)        )",
        "labels_text": "Find the preimport MediaFile for an Item"
    },
    {
        "input_text": "summarize: def assertEqualTimes(self, first, second, msg=None):  # noqa                assert first == pytest.approx(second, rel=1e-4), msg",
        "labels_text": "For comparing file modification time at a sufficient precision"
    },
    {
        "input_text": "summarize: def test_default(self):                self._setup_config()        assert self.plugin._resolve_genres([\"delta blues\"]) == \"Delta Blues\"",
        "labels_text": "Fetch genre with whitelist and cn deactivated"
    },
    {
        "input_text": "summarize: def test_c14n_only(self):                self._setup_config(canonical=True, count=99)        assert self.plugin._resolve_genres([\"delta blues\"]) == \"Blues\"        assert self.plugin._resolve_genres([\"iota blues\"]) == \"Iota Blues\"",
        "labels_text": "Default cn tree funnel up to most common genre except for wrong genre that stay unchanged"
    },
    {
        "input_text": "summarize: def test_whitelist_only(self):                self._setup_config(whitelist=True)        assert self.plugin._resolve_genres([\"iota blues\"]) == \"\"",
        "labels_text": "Default whitelist reject wrong non existing genre"
    },
    {
        "input_text": "summarize: def test_whitelist_c14n(self):                self._setup_config(canonical=True, whitelist=True, count=99)        assert (            self.plugin._resolve_genres([\"delta blues\"]) == \"Delta Blues, Blues\"        )",
        "labels_text": "Default whitelist and cn both activated result in all parent genre being selected from specific to common"
    },
    {
        "input_text": "summarize: def test_whitelist_custom(self):                self._setup_config(whitelist={\"blues\", \"rock\", \"jazz\"}, count=2)        assert self.plugin._resolve_genres([\"pop\", \"blues\"]) == \"Blues\"        self._setup_config(canonical=\"\", whitelist={\"rock\"})        assert self.plugin._resolve_genres([\"delta blues\"]) == \"\"",
        "labels_text": "Keep only genre that are in the whitelist"
    },
    {
        "input_text": "summarize: def test_count(self):                self._setup_config(whitelist={\"blues\", \"rock\", \"jazz\"}, count=2)        assert (            self.plugin._resolve_genres([\"jazz\", \"pop\", \"rock\", \"blues\"])            == \"Jazz, Rock\"        )",
        "labels_text": "Keep the n first genre a we expect them to be sorted from more to less popular"
    },
    {
        "input_text": "summarize: def test_count_c14n(self):                self._setup_config(            whitelist={\"blues\", \"rock\", \"jazz\"}, canonical=True, count=2        )        # thanks to c14n, 'blues' superseeds 'country blues' and takes the        # second slot        assert (            self.plugin._resolve_genres(                [\"jazz\", \"pop\", \"country blues\", \"rock\"]            )            == \"Jazz, Blues\"        )",
        "labels_text": "Keep the n first genre after having applied cn when necessary"
    },
    {
        "input_text": "summarize: def test_c14n_whitelist(self):                self._setup_config(canonical=True, whitelist={\"rock\"})        assert self.plugin._resolve_genres([\"delta blues\"]) == \"\"",
        "labels_text": "Genres first pas through cn and are then filtered"
    },
    {
        "input_text": "summarize: def test_empty_string_enables_canonical(self):                self._setup_config(canonical=\"\", count=99)        assert self.plugin._resolve_genres([\"delta blues\"]) == \"Blues\"",
        "labels_text": "For backwards compatibility setting the canonical option to the empty string enables it using the default tree"
    },
    {
        "input_text": "summarize: def test_empty_string_enables_whitelist(self):                self._setup_config(whitelist=\"\")        assert self.plugin._resolve_genres([\"iota blues\"]) == \"\"",
        "labels_text": "Again for backwards compatibility setting the whitelist option to the empty string enables the default set of genre"
    },
    {
        "input_text": "summarize: def test_prefer_specific_loads_tree(self):                self._setup_config(prefer_specific=True, canonical=False)        assert self.plugin.c14n_branches != []",
        "labels_text": "When preferspecific is enabled but canonical is not the tree still ha to be loaded"
    },
    {
        "input_text": "summarize: def test_prefer_specific_without_canonical(self):                self._setup_config(prefer_specific=True, canonical=False, count=4)        assert (            self.plugin._resolve_genres([\"math rock\", \"post-rock\"])            == \"Post-Rock, Math Rock\"        )",
        "labels_text": "Preferspecific work without canonical"
    },
    {
        "input_text": "summarize: def test_no_duplicate(self):                self._setup_config(count=99)        assert self.plugin._resolve_genres([\"blues\", \"blues\"]) == \"Blues\"",
        "labels_text": "Remove duplicated genre"
    },
    {
        "input_text": "summarize: def test_no_limit(self):                result = self.run_with_output(\"lslimit\")        assert result.count(\"\\n\") == self.num_test_items",
        "labels_text": "Returns all when there is no limit or filter"
    },
    {
        "input_text": "summarize: def test_lslimit_head(self):                result = self.run_with_output(\"lslimit\", \"--head\", str(self.num_limit))        assert result.count(\"\\n\") == self.num_limit",
        "labels_text": "Returns the expected number with lslimit head"
    },
    {
        "input_text": "summarize: def test_lslimit_tail(self):                result = self.run_with_output(\"lslimit\", \"--tail\", str(self.num_limit))        assert result.count(\"\\n\") == self.num_limit",
        "labels_text": "Returns the expected number with lslimit tail"
    },
    {
        "input_text": "summarize: def test_lslimit_head_invariant(self):                result = self.run_with_output(            \"lslimit\", \"--head\", str(self.num_limit), self.track_tail_range        )        assert result.count(\"\\n\") == self.num_limit",
        "labels_text": "Returns the expected number with lslimit head and a filter"
    },
    {
        "input_text": "summarize: def test_lslimit_tail_invariant(self):                result = self.run_with_output(            \"lslimit\", \"--tail\", str(self.num_limit), self.track_head_range        )        assert result.count(\"\\n\") == self.num_limit",
        "labels_text": "Returns the expected number with lslimit tail and a filter"
    },
    {
        "input_text": "summarize: def test_prefix(self):                result = self.lib.items(self.num_limit_prefix)        assert len(result) == self.num_limit",
        "labels_text": "Returns the expected number with the query prefix"
    },
    {
        "input_text": "summarize: def test_prefix_when_correctly_ordered(self):                correct_order = self.track_tail_range + \" \" + self.num_limit_prefix        result = self.lib.items(correct_order)        assert len(result) == self.num_limit",
        "labels_text": "Returns the expected number with the query prefix and filter when the prefix portion correctly appears last"
    },
    {
        "input_text": "summarize: def test_prefix_when_incorrectly_ordred(self):                incorrect_order = self.num_limit_prefix + \" \" + self.track_tail_range        result = self.lib.items(incorrect_order)        assert len(result) == 0",
        "labels_text": "Returns no result with the query prefix and filter when the prefix portion incorrectly appears first"
    },
    {
        "input_text": "summarize: def setUp(self):                lyrics.LyricsPlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def setUp(self):                try:            __import__(\"bs4\")        except ImportError:            self.skipTest(\"Beautiful Soup 4 not available\")",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_backend_sources_ok(self):                # Don't test any sources marked as skipped.        sources = [s for s in self.DEFAULT_SOURCES if not s.get(\"skip\", False)]        for s in sources:            with self.subTest(s[\"backend\"].__name__):                backend = s[\"backend\"](self.plugin.config, self.plugin._log)                res = backend.fetch(s[\"artist\"], s[\"title\"])                self.assertLyricsContentOk(s[\"title\"], res)",
        "labels_text": "Test default backends with song known to exist in respective database"
    },
    {
        "input_text": "summarize: def setUp(self):                LyricsGoogleBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_mocked_source_ok(self):                url = self.source[\"url\"] + self.source[\"path\"]        res = lyrics.scrape_lyrics_from_html(raw_backend.fetch_url(url))        assert google.is_lyrics(res), url        self.assertLyricsContentOk(self.source[\"title\"], res, url)",
        "labels_text": "Test that lyric of the mocked page are correctly scraped"
    },
    {
        "input_text": "summarize: def test_is_page_candidate_exact_match(self):                from bs4 import BeautifulSoup, SoupStrainer        s = self.source        url = str(s[\"url\"] + s[\"path\"])        html = raw_backend.fetch_url(url)        soup = BeautifulSoup(            html, \"html.parser\", parse_only=SoupStrainer(\"title\")        )        assert google.is_page_candidate(            url, soup.title.string, s[\"title\"], s[\"artist\"]        ), url",
        "labels_text": "Test matching html page title with song info when song info are present in the title"
    },
    {
        "input_text": "summarize: def test_is_page_candidate_special_chars(self):                # https://github.com/beetbox/beets/issues/1673        s = self.source        url = s[\"url\"] + s[\"path\"]        url_title = \"foo\"        google.is_page_candidate(url, url_title, s[\"title\"], \"Sunn O)))\")",
        "labels_text": "Ensure that ispagecandidate doesnt crash when the artist and such contain special regular expression character"
    },
    {
        "input_text": "summarize: def setUp(self):                try:            __import__(\"bs4\")        except ImportError:            self.skipTest(\"Beautiful Soup 4 not available\")",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def setUp(self):                GeniusBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_no_lyrics_div(self):                # https://github.com/beetbox/beets/issues/3535        # expected return value None        url = \"https://genius.com/sample\"        mock = MockFetchUrl()        assert genius._scrape_lyrics_from_html(mock(url)) is None",
        "labels_text": "Ensure we dont crash when the scraping the html for a genius page doesnt contain div classlyricsdiv"
    },
    {
        "input_text": "summarize: def test_good_lyrics(self):                url = \"https://genius.com/Ttng-chinchilla-lyrics\"        mock = MockFetchUrl()        lyrics = genius._scrape_lyrics_from_html(mock(url))        assert lyrics is not None        assert lyrics.count(\"\\n\") == 28",
        "labels_text": "Ensure we are able to scrape a page with lyric"
    },
    {
        "input_text": "summarize: def test_good_lyrics_multiple_divs(self):                url = \"https://genius.com/2pac-all-eyez-on-me-lyrics\"        mock = MockFetchUrl()        lyrics = genius._scrape_lyrics_from_html(mock(url))        assert lyrics is not None        assert lyrics.count(\"\\n\") == 133",
        "labels_text": "Ensure we are able to scrape a page with lyric"
    },
    {
        "input_text": "summarize: def setUp(self):                GeniusBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def setUp(self):                try:            __import__(\"bs4\")        except ImportError:            self.skipTest(\"Beautiful Soup 4 not available\")",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def setUp(self):                TekstowoBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()        tekstowo.config = self.plugin.config",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_good_lyrics(self):                url = \"https://www.tekstowo.pl/piosenka,24kgoldn,city_of_angels_1.html\"        mock = MockFetchUrl()        assert (            tekstowo.extract_lyrics(mock(url), \"24kGoldn\", \"City of Angels\")            is not None        )",
        "labels_text": "Ensure we are able to scrape a page with lyric"
    },
    {
        "input_text": "summarize: def setUp(self):                TekstowoBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_no_results(self):                url = (            \"https://www.tekstowo.pl/szukaj,wykonawca,\"            \"agfdgja,tytul,agfdgafg.html\"        )        mock = MockFetchUrl()        assert tekstowo.parse_search_results(mock(url)) is None",
        "labels_text": "Ensure we are able to scrape a page with no search result"
    },
    {
        "input_text": "summarize: def setUp(self):                TekstowoBaseTest.setUp(self)        self.plugin = lyrics.LyricsPlugin()        tekstowo.config = self.plugin.config",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def test_normal(self):                lyrics = tekstowo.fetch(\"Boy in Space\", \"u n eye\")        self.assertLyricsContentOk(\"u n eye\", lyrics)",
        "labels_text": "Ensure we can fetch a song lyric in the ordinary case"
    },
    {
        "input_text": "summarize: def test_no_matching_results(self):                # https://github.com/beetbox/beets/issues/4406        # expected return value None        lyrics = tekstowo.fetch(\"Kelly Bailey\", \"Black Mesa Inbound\")        assert lyrics is None",
        "labels_text": "Ensure we fetch nothing if there are search result returned but no match"
    },
    {
        "input_text": "summarize: def setUp(self):                super().setUp()        self.patcher = patch(            \"musicbrainzngs.get_work_by_id\", side_effect=mock_workid_response        )        self.patcher.start()",
        "labels_text": "Set up configuration"
    },
    {
        "input_text": "summarize: def readline(self, terminator=b\"\\n\", bufsize=1024):                while True:            if terminator in self.buf:                line, self.buf = self.buf.split(terminator, 1)                line += terminator                return line            self.sock.settimeout(1)            data = self.sock.recv(bufsize)            if data:                self.buf += data            else:                line = self.buf                self.buf = b\"\"                return line",
        "labels_text": "Reads a line of data from the socket"
    },
    {
        "input_text": "summarize: def _assert_failed(self, response, code, pos=None):                if pos is not None:            previous_commands = response[0:pos]            self._assert_ok(*previous_commands)            response = response[pos]        assert not response.ok        if pos is not None:            assert pos == response.err_data[1]        if code is not None:            assert code == response.err_data[0]",
        "labels_text": "Check that a command failed with a specific error code If this is a list of response first check all preceding command were OK"
    },
    {
        "input_text": "summarize: def add_response_update_plex(self):                body = \"\"        status = 200        content_type = \"text/html\"        responses.add(            responses.GET,            \"http://localhost:32400/library/sections/2/refresh\",            body=body,            status=status,            content_type=content_type,        )",
        "labels_text": "Create response for mocking the updateplex function"
    },
    {
        "input_text": "summarize: def test_backend(self):                try:            # Check if required plugins can be loaded by instantiating a            # GStreamerBackend (via its .__init__).            config[\"replaygain\"][\"targetlevel\"] = 89            GStreamerBackend(config[\"replaygain\"], None)        except FatalGstreamerPluginReplayGainError as e:            # Skip the test if plugins could not be loaded.            self.skipTest(str(e))",
        "labels_text": "Check whether the backend actually ha all required functionality"
    },
    {
        "input_text": "summarize: def test_backend(self):                pass",
        "labels_text": "Check whether the backend actually ha all required functionality"
    },
    {
        "input_text": "summarize: def test_backend(self):                pass",
        "labels_text": "Check whether the backend actually ha all required functionality"
    },
    {
        "input_text": "summarize: def _params(url):        return parse_qs(urlparse(url).query)",
        "labels_text": "Get the query parameter from a URL"
    },
    {
        "input_text": "summarize: def __init__(self, mode, show_failures):                self.mode = mode        self.show_failures = show_failures        self.verbose = 1",
        "labels_text": "Constructs ArgumentsMock"
    },
    {
        "input_text": "summarize: def _params(url):        return parse_qs(urlparse(url).query)",
        "labels_text": "Get the query parameter from a URL"
    },
    {
        "input_text": "summarize: def setUp(self):                super().setUp()        config[\"subsonic\"][\"user\"] = \"admin\"        config[\"subsonic\"][\"pass\"] = \"admin\"        config[\"subsonic\"][\"url\"] = \"http://localhost:4040\"        responses.add(            responses.GET,            \"http://localhost:4040/rest/ping.view\",            status=200,            body=self.PING_BODY,        )        self.subsonicupdate = subsonicupdate.SubsonicUpdate()",
        "labels_text": "Sets up config and plugin for test"
    },
    {
        "input_text": "summarize: def test_start_scan(self):                responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=200,            body=self.SUCCESS_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests success path based on best case scenario"
    },
    {
        "input_text": "summarize: def test_start_scan_failed_bad_credentials(self):                responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=200,            body=self.FAILED_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests failed path based on bad credential"
    },
    {
        "input_text": "summarize: def test_start_scan_failed_not_found(self):                responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=404,            body=self.ERROR_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests failed path based on resource not found"
    },
    {
        "input_text": "summarize: def test_start_scan_failed_unreachable(self):                self.subsonicupdate.start_scan()",
        "labels_text": "Tests failed path based on service not available"
    },
    {
        "input_text": "summarize: def test_url_with_context_path(self):                config[\"subsonic\"][\"url\"] = \"http://localhost:4040/contextPath/\"        responses.add(            responses.GET,            \"http://localhost:4040/contextPath/rest/startScan\",            status=200,            body=self.SUCCESS_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests success for included with contextPath"
    },
    {
        "input_text": "summarize: def test_url_with_trailing_forward_slash_url(self):                config[\"subsonic\"][\"url\"] = \"http://localhost:4040/\"        responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=200,            body=self.SUCCESS_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests success path based on trailing forward slash"
    },
    {
        "input_text": "summarize: def test_url_with_missing_port(self):                config[\"subsonic\"][\"url\"] = \"http://localhost/airsonic\"        responses.add(            responses.GET,            \"http://localhost/airsonic/rest/startScan\",            status=200,            body=self.SUCCESS_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests failed path based on missing port"
    },
    {
        "input_text": "summarize: def test_url_with_missing_schema(self):                config[\"subsonic\"][\"url\"] = \"localhost:4040/airsonic\"        responses.add(            responses.GET,            \"http://localhost:4040/rest/startScan\",            status=200,            body=self.SUCCESS_BODY,        )        self.subsonicupdate.start_scan()",
        "labels_text": "Tests failed path based on missing schema"
    },
    {
        "input_text": "summarize: def test_get_item_empty_query(self):                response = self.client.get(\"/item/query/\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"items\"]) == 3",
        "labels_text": "testing item query empty"
    },
    {
        "input_text": "summarize: def test_get_simple_item_query(self):                response = self.client.get(\"/item/query/another\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"title\"] == \"another title\"",
        "labels_text": "testing item query another"
    },
    {
        "input_text": "summarize: def test_query_item_string(self):                response = self.client.get(\"/item/query/testattr%3aABC\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"title\"] == \"and a third\"",
        "labels_text": "testing item query testattrABC"
    },
    {
        "input_text": "summarize: def test_query_item_regex(self):                response = self.client.get(\"/item/query/testattr%3a%3a[A-C]%2b\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"title\"] == \"and a third\"",
        "labels_text": "testing item query testattrAC"
    },
    {
        "input_text": "summarize: def test_query_item_path(self):        #                         response = self.client.get(            \"/item/query/path:\" + self.path_prefix + \"\\\\somewhere\\\\a\"        )        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"title\"] == \"another title\"",
        "labels_text": "Note path query are special the query item must match the path from the root all the way to a directory so this match item"
    },
    {
        "input_text": "summarize: def test_query_album_string(self):                response = self.client.get(\"/album/query/albumtest%3axy\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"album\"] == \"album\"",
        "labels_text": "testing query albumtestxy"
    },
    {
        "input_text": "summarize: def test_query_album_artpath_regex(self):                response = self.client.get(\"/album/query/artpath%3a%3aart_\")        res_json = json.loads(response.data.decode(\"utf-8\"))        assert response.status_code == 200        assert len(res_json[\"results\"]) == 1        assert res_json[\"results\"][0][\"album\"] == \"other album\"",
        "labels_text": "testing query artpathart"
    },
    {
        "input_text": "summarize: def test_delete_item_all_fails(self):                web.app.config[\"READONLY\"] = False        # Delete all items        response = self.client.delete(\"/item/\")        assert response.status_code == 405",
        "labels_text": "DELETE is not supported for list all"
    },
    {
        "input_text": "summarize: def test_delete_album_all_fails(self):                web.app.config[\"READONLY\"] = False        # Delete all albums        response = self.client.delete(\"/album/\")        assert response.status_code == 405",
        "labels_text": "DELETE is not supported for list all"
    },
    {
        "input_text": "summarize: def convert(in_file, out_file, tag):        if not isinstance(tag, bytes):        tag = tag.encode(\"utf-8\")    with open(out_file, \"wb\") as out_f:        with open(in_file, \"rb\") as in_f:            out_f.write(in_f.read())        out_f.write(tag)",
        "labels_text": "Copy infile to outfile and append the string tag"
    },
    {
        "input_text": "summarize: def commands(self):                # Import locally to reduce impact on initialization time.        from .cli.conda_argparse import find_builtin_commands, generate_parser        from .cli.find_commands import find_commands        # return value meant to be written to stdout        # Hidden commands to provide metadata to shells.        return \"\\n\".join(            sorted(                find_builtin_commands(generate_parser()) + tuple(find_commands(True))            )        )",
        "labels_text": "Returns a list of possible subcommands that are valid immediately following conda at the command line This method is generally only used by tabcompletion"
    },
    {
        "input_text": "summarize: def _build_activator_cls(shell):        shell_etc = shell.split(\"+\")    activator, formatters = shell_etc[0], shell_etc[1:]    bases = [activator_map[activator]]    for f in formatters:        bases.append(formatter_map[f])    cls = type(\"Activator\", tuple(reversed(bases)), {})    return cls",
        "labels_text": "Dynamically construct the activator class Detect the base activator and any number of formatters appended using to the base name For example posixjson a in conda shellposixjson activate would use the PosixActivator base class and add the JSONFormatMixin"
    },
    {
        "input_text": "summarize: def __init__(        self, prefix, channels, subdirs=(), specs_to_add=(), specs_to_remove=()    ):                solver_backend = context.plugin_manager.get_cached_solver_backend()        self._internal = solver_backend(            prefix, channels, subdirs, specs_to_add, specs_to_remove        )",
        "labels_text": "Beta Args prefix str The conda prefix environment location for which the classSolver is being instantiated channel SequenceclassChannel A prioritized list of channel to use for the solution subdirs Sequencestr A prioritized list of subdirs to use for the solution specstoadd setclassMatchSpec The set of package spec to add to the prefix specstoremove setclassMatchSpec The set of package spec to remove from the prefix"
    },
    {
        "input_text": "summarize: def solve_final_state(        self,        update_modifier=NULL,        deps_modifier=NULL,        prune=NULL,        ignore_pinned=NULL,        force_remove=NULL,    ):                return self._internal.solve_final_state(            update_modifier, deps_modifier, prune, ignore_pinned, force_remove        )",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Gives the final solved state of the environment Args depsmodifier DepsModifier An optional flag indicating special solver handling for dependency The default solver behavior is to be a conservative a possible with dependency update in the case the dependency already exists in the environment while still ensuring all dependency are satisfied Options include NODEPS ONLYDEPS UPDATEDEPS UPDATEDEPSONLYDEPS FREEZEINSTALLED prune bool If True the solution will not contain package that were previously brought into the environment a dependency but are no longer required a dependency and are not userrequested ignorepinned bool If True the solution will ignore pinned package configuration for the prefix forceremove bool Forces removal of a package without removing package that depend on it Returns tuplePackageRef In sorted dependency order from root to leaf the package reference for the solved state of the environment"
    },
    {
        "input_text": "summarize: def solve_for_diff(        self,        update_modifier=NULL,        deps_modifier=NULL,        prune=NULL,        ignore_pinned=NULL,        force_remove=NULL,        force_reinstall=False,    ):                return self._internal.solve_for_diff(            update_modifier,            deps_modifier,            prune,            ignore_pinned,            force_remove,            force_reinstall,        )",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Gives the package reference to remove from an environment followed by the package reference to add to an environment Args depsmodifier DepsModifier See methsolvefinalstate prune bool See methsolvefinalstate ignorepinned bool See methsolvefinalstate forceremove bool See methsolvefinalstate forcereinstall bool For requested specstoadd that are already satisfied in the environment instructs the solver to remove the package and spec from the environment and then add it backpossibly with the exact package instance modified depending on the spec exactness Returns tuplePackageRef tuplePackageRef A twotuple of PackageRef sequence The first is the group of package to remove from the environment in sorted dependency order from leaf to root The second is the group of package to add to the environment in sorted dependency order from root to leaf"
    },
    {
        "input_text": "summarize: def solve_for_transaction(        self,        update_modifier=NULL,        deps_modifier=NULL,        prune=NULL,        ignore_pinned=NULL,        force_remove=NULL,        force_reinstall=False,    ):                return self._internal.solve_for_transaction(            update_modifier,            deps_modifier,            prune,            ignore_pinned,            force_remove,            force_reinstall,        )",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Gives an UnlinkLinkTransaction instance that can be used to execute the solution on an environment Args depsmodifier DepsModifier See methsolvefinalstate prune bool See methsolvefinalstate ignorepinned bool See methsolvefinalstate forceremove bool See methsolvefinalstate forcereinstall bool See methsolvefordiff Returns UnlinkLinkTransaction"
    },
    {
        "input_text": "summarize: def __init__(self, channel):                channel = Channel(channel)        assert channel.subdir        self._internal = _SubdirData(channel)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Args channel str or Channel The target subdir for the instance Must either be a url that includes a subdir or a objChannel that includes a subdir eg httpsrepoanacondacompkgsmainlinux Channelhttpsrepoanacondacompkgsmainlinux Channelcondaforgeosx"
    },
    {
        "input_text": "summarize: def query(self, package_ref_or_match_spec):                return tuple(self._internal.query(package_ref_or_match_spec))",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Run a query against this specific instance of repodata Args packagereformatchspec PackageRef or MatchSpec or str Either an exact objPackageRef to match against or a objMatchSpec query object A objstr will be turned into a objMatchSpec automatically Returns tuplePackageRecord"
    },
    {
        "input_text": "summarize: def query_all(package_ref_or_match_spec, channels=None, subdirs=None):                return tuple(            _SubdirData.query_all(package_ref_or_match_spec, channels, subdirs)        )",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Run a query against all repodata instance in channelsubdir matrix Args packagereformatchspec PackageRef or MatchSpec or str Either an exact objPackageRef to match against or a objMatchSpec query object A objstr will be turned into a objMatchSpec automatically channel IterableChannel or str or None An iterable of url for channel or objChannel object If None will fall back to contextchannels subdirs Iterablestr or None If None will fall back to contextsubdirs Returns tuplePackageRecord"
    },
    {
        "input_text": "summarize: def iter_records(self):                return self._internal.iter_records()",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Returns IterablePackageRecord A generator over all record contained in the repodatajson instance Warning this is a generator that is exhausted on first use"
    },
    {
        "input_text": "summarize: def reload(self):                self._internal = self._internal.reload()        return self",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Update the instance with new information Backing information ie repodatajson is lazily downloadedloaded on first use by the other method of this class You should only use this method if you are sure you have outdated data Returns SubdirData"
    },
    {
        "input_text": "summarize: def __init__(self, pkgs_dir):                self._internal = _PackageCacheData(pkgs_dir)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Args pkgsdir str"
    },
    {
        "input_text": "summarize: def get(self, package_ref, default=NULL):                return self._internal.get(package_ref, default)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Args packageref PackageRef A objPackageRef instance representing the key for the objPackageCacheRecord being sought default The default value to return if the record doe not exist If not specified and no record exists excKeyError is raised Returns PackageCacheRecord"
    },
    {
        "input_text": "summarize: def query(self, package_ref_or_match_spec):                return tuple(self._internal.query(package_ref_or_match_spec))",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Run a query against this specific package cache instance Args packagereformatchspec PackageRef or MatchSpec or str Either an exact objPackageRef to match against or a objMatchSpec query object A objstr will be turned into a objMatchSpec automatically Returns tuplePackageCacheRecord"
    },
    {
        "input_text": "summarize: def query_all(package_ref_or_match_spec, pkgs_dirs=None):                return tuple(_PackageCacheData.query_all(package_ref_or_match_spec, pkgs_dirs))",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Run a query against all package cache Args packagereformatchspec PackageRef or MatchSpec or str Either an exact objPackageRef to match against or a objMatchSpec query object A objstr will be turned into a objMatchSpec automatically pkgsdirs Iterablestr or None If None will fall back to contextpkgsdirs Returns tuplePackageCacheRecord"
    },
    {
        "input_text": "summarize: def iter_records(self):                return self._internal.iter_records()",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Returns IterablePackageCacheRecord A generator over all record contained in the package cache instance Warning this is a generator that is exhausted on first use"
    },
    {
        "input_text": "summarize: def is_writable(self):                return self._internal.is_writable",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Indicates if the package cache location is writable or readonly Returns bool"
    },
    {
        "input_text": "summarize: def first_writable(pkgs_dirs=None):                return PackageCacheData(_PackageCacheData.first_writable(pkgs_dirs).pkgs_dir)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Get an instance object for the first writable package cache Args pkgsdirs Iterablestr If None will fall back to contextpkgsdirs Returns PackageCacheData An instance for the first writable package cache"
    },
    {
        "input_text": "summarize: def reload(self):                self._internal = self._internal.reload()        return self",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Update the instance with new information Backing information ie content of the pkgsdir is lazily loaded on first use by the other method of this class You should only use this method if you are sure you have outdated data Returns PackageCacheData"
    },
    {
        "input_text": "summarize: def __init__(self, prefix_path):                self._internal = _PrefixData(prefix_path)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Args prefixpath str"
    },
    {
        "input_text": "summarize: def get(self, package_ref, default=NULL):                return self._internal.get(package_ref.name, default)",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Args packageref PackageRef A objPackageRef instance representing the key for the objPrefixRecord being sought default The default value to return if the record doe not exist If not specified and no record exists excKeyError is raised Returns PrefixRecord"
    },
    {
        "input_text": "summarize: def query(self, package_ref_or_match_spec):                return tuple(self._internal.query(package_ref_or_match_spec))",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Run a query against this specific prefix instance Args packagereformatchspec PackageRef or MatchSpec or str Either an exact objPackageRef to match against or a objMatchSpec query object A objstr will be turned into a objMatchSpec automatically Returns tuplePrefixRecord"
    },
    {
        "input_text": "summarize: def iter_records(self):                return self._internal.iter_records()",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Returns IterablePrefixRecord A generator over all record contained in the prefix Warning this is a generator that is exhausted on first use"
    },
    {
        "input_text": "summarize: def is_writable(self):                return self._internal.is_writable",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Indicates if the prefix is writable or readonly Returns bool or None True if the prefix is writable False if readonly None if the prefix doe not exist a a conda environment"
    },
    {
        "input_text": "summarize: def reload(self):                self._internal = self._internal.reload()        return self",
        "labels_text": "Beta While in beta expect both major and minor change across minor release Update the instance with new information Backing information ie content of the condameta directory is lazily loaded on first use by the other method of this class You should only use this method if you are sure you have outdated data Returns PrefixData"
    },
    {
        "input_text": "summarize: def __init__(self: Self, version: str) -> None:                self._version = version        # Try to parse the version string as a simple tuple[int, ...] to avoid        # packaging.version import and costlier version comparisons.        self._version_tuple = self._get_version_tuple(version)        self._version_object = None",
        "labels_text": "Factory to create a deprecation handle for the specified version param version The version to compare against when checking deprecation status"
    },
    {
        "input_text": "summarize: def _get_version_tuple(version: str) -> tuple[int, ...] | None:                try:            return tuple(int(part) for part in version.strip().split(\".\")) or None        except (AttributeError, ValueError):            return None",
        "labels_text": "Return version a nonempty tuple of ints if possible else None param version Version string to parse"
    },
    {
        "input_text": "summarize: def module(        self: Self,        deprecate_in: str,        remove_in: str,        *,        addendum: str | None = None,        stack: int = 0,    ) -> None:                self.topic(            deprecate_in=deprecate_in,            remove_in=remove_in,            topic=self._get_module(stack)[1],            addendum=addendum,            stack=2 + stack,        )",
        "labels_text": "Deprecation function for module param deprecatein Version in which code will be marked a deprecated param removein Version in which code is expected to be removed param addendum Optional additional messaging Useful to indicate what to do instead param stack Optional stacklevel increment"
    },
    {
        "input_text": "summarize: def linked_data(prefix, ignore_channels=False):        from .core.prefix_data import PrefixData    from .models.dist import Dist    pd = PrefixData(prefix)    return {        Dist(prefix_record): prefix_record        for prefix_record in pd._prefix_records.values()    }",
        "labels_text": "Return a dictionary of the linked package in prefix"
    },
    {
        "input_text": "summarize: def linked(prefix, ignore_channels=False):        from .models.enums import PackageType    conda_package_types = PackageType.conda_package_types()    ld = linked_data(prefix, ignore_channels=ignore_channels).items()    return {        dist        for dist, prefix_rec in ld        if prefix_rec.package_type in conda_package_types    }",
        "labels_text": "Return the Dists of linked package in prefix"
    },
    {
        "input_text": "summarize: def is_linked(prefix, dist):        # FIXME Functions that begin with `is_` should return True/False    from .core.prefix_data import PrefixData    pd = PrefixData(prefix)    prefix_record = pd.get(dist.name, None)    if prefix_record is None:        return None    elif MatchSpec(dist).match(prefix_record):        return prefix_record    else:        return None",
        "labels_text": "Return the install metadata for a linked package in a prefix or None if the package is not linked in the prefix"
    },
    {
        "input_text": "summarize: def _parse_old_format_specs_string(specs_string):                specs = []        for spec in specs_string.split(\",\"):            # If the spec starts with a version qualifier, then it actually belongs to the            # previous spec. But don't try to join if there was no previous spec.            if version_relation_re.match(spec) and specs:                specs[-1] = \",\".join([specs[-1], spec])            else:                specs.append(spec)        return specs",
        "labels_text": "Parse specification string that use conda syntax Examples param pythonjupyter matplotlib"
    },
    {
        "input_text": "summarize: def get_state(self, rev=-1):                states = self.construct_states()        if not states:            return set()        times, pkgs = zip(*states)        return pkgs[rev]",
        "labels_text": "Return the state ie the set of distribution for a given revision Defaults to latest which is the same a the current state when the log file is uptodate Returns a list of diststrs"
    },
    {
        "input_text": "summarize: def conda_installed_files(prefix, exclude_self_build=False):        res = set()    for meta in PrefixData(prefix).iter_records():        if exclude_self_build and \"file_hash\" in meta:            continue        res.update(set(meta.get(\"files\", ())))    return res",
        "labels_text": "Return the set of file which have been installed using conda into a given prefix"
    },
    {
        "input_text": "summarize: def untracked(prefix, exclude_self_build=False):        conda_files = conda_installed_files(prefix, exclude_self_build)    return {        path        for path in walk_prefix(prefix) - conda_files        if not (            path.endswith(\"~\")            or on_mac            and path.endswith(\".DS_Store\")            or path.endswith(\".pyc\")            and path[:-1] in conda_files        )    }",
        "labels_text": "Return the set of all untracked file for a given prefix"
    },
    {
        "input_text": "summarize: def touch_nonadmin(prefix):        if on_win and exists(join(context.root_prefix, \".nonadmin\")):        if not isdir(prefix):            os.makedirs(prefix)        with open_utf8(join(prefix, \".nonadmin\"), \"w\") as fo:            fo.write(\"\")",
        "labels_text": "Creates PREFIXnonadmin if sysprefixnonadmin exists on Windows"
    },
    {
        "input_text": "summarize: def execute_plan(old_plan, index=None, verbose=False):  # pragma: no cover        plan = _update_old_plan(old_plan)    execute_instructions(plan, index, verbose)",
        "labels_text": "Deprecated This should condainstructionsexecuteinstructions instead"
    },
    {
        "input_text": "summarize: def _update_old_plan(old_plan):  # pragma: no cover        plan = []    for line in old_plan:        if line.startswith(\"#\"):            continue        if \" \" not in line:            from .exceptions import ArgumentError            raise ArgumentError(f\"The instruction {line!r} takes at least one argument\")        instruction, arg = line.split(\" \", 1)        plan.append((instruction, arg))    return plan",
        "labels_text": "Update an old plan object to work with condainstructionsexecuteinstructions"
    },
    {
        "input_text": "summarize: def exactness_and_number_of_deps(resolve_obj, ms):        if ms.strictness == 3:        prec = resolve_obj.find_matches(ms)        value = 3        if prec:            for dep in prec[0].depends:                value += MatchSpec(dep).strictness    else:        value = ms.strictness    return value",
        "labels_text": "Sorting key to emphasize package that have more strict requirement More strict mean the reduced index can be reduced more so we want to consider these more constrained deps earlier in reducing the index"
    },
    {
        "input_text": "summarize: def _broader(self, ms, specs_by_name):                if not specs_by_name:            return False        return ms.strictness < specs_by_name[0].strictness",
        "labels_text": "Prevent introduction of matchspecs that broaden our selection of choice"
    },
    {
        "input_text": "summarize: def human_bytes(n):        if n < 1024:        return \"%d B\" % n    k = n / 1024    if k < 1024:        return \"%d KB\" % round(k)    m = k / 1024    if m < 1024:        return f\"{m:.1f} MB\"    g = m / 1024    return f\"{g:.2f} GB\"",
        "labels_text": "Return the number of byte n in more human readable form Examples humanbytes B humanbytes KB humanbytes MB humanbytes GB"
    },
    {
        "input_text": "summarize: def sys_prefix_unfollowed():        try:        frame = next(iter(sys._current_frames().values()))        while frame.f_back:            frame = frame.f_back        code = frame.f_code        filename = code.co_filename        unfollowed = dirname(dirname(filename))    except Exception:        return sys.prefix    return unfollowed",
        "labels_text": "Since conda is installed into nonroot environment a a symlink only and because sysprefix follows symlinks this function can be used to get the unfollowed sysprefix This value is usually the same a the prefix of the environment into which conda ha been symlinked An example of when this is necessary is when conda look for external subcommands in findcommandspy"
    },
    {
        "input_text": "summarize: def quote_for_shell(*arguments):        # [backport] Support passing in a list of strings or args of string.    if len(arguments) == 1 and isiterable(arguments[0]):        arguments = arguments[0]    return _args_join(arguments)",
        "labels_text": "Properly quote argument for command line passing For POSIX us shlexjoin for Windows us a custom implementation to properly escape metacharacters param argument Arguments to quote type argument list of str return Quoted argument rtype str"
    },
    {
        "input_text": "summarize: def _args_join(args):                def quote(s):            # derived from shlex.quote            if not s:                return '\"\"'            # if any unsafe chars are present we must quote            if not _RE_UNSAFE.search(s):                return s            # double escape (\" -> \"\")            s = _RE_DBL.sub(r\"\\1\\1\", s)            # quote entire string            return f'\"{s}\"'        return \" \".join(quote(arg) for arg in args)",
        "labels_text": "Return a shellescaped string from args"
    },
    {
        "input_text": "summarize: def _args_join(args):                        from shlex import quote            return \" \".join(quote(arg) for arg in args)",
        "labels_text": "Return a shellescaped string from args"
    },
    {
        "input_text": "summarize: def ensure_dir_exists(func):        @wraps(func)    def wrapper(*args, **kwargs):        result = func(*args, **kwargs)        if isinstance(result, Path):            try:                result.mkdir(parents=True, exist_ok=True)            except OSError as exc:                raise CondaError(                    \"Error encountered while attempting to create cache directory.\"                    f\"\\n  Directory: {result}\"                    f\"\\n  Exception: {exc}\"                )        return result    return wrapper",
        "labels_text": "Ensures that the directory exists for function returning a Path object containing a directory"
    },
    {
        "input_text": "summarize: def first(seq, key=bool, default=None, apply=lambda x: x):        return next((apply(x) for x in seq if key(x)), default() if callable(default) else default)",
        "labels_text": "Give the first value that satisfies the key test Args seq iterable key callable test for each element of iterable default returned when all element fail test apply callable applied to element before return but not to default value Returns first element in seq that pass key mutated with optional apply Examples first False None first False None is None True first False None defaultohai ohai import re m firstrematchregex abc for regex in b a mgroup bc The optional key argument specifies a oneargument predicate function like that used for filter The key argument if supplied must be in keyword form For example first keylambda x x"
    },
    {
        "input_text": "summarize: def clear_memoized_methods(obj, *method_names):        for key in list(getattr(obj, '_memoized_results', {}).keys()):        # key[0] is the method name        if key[0] in method_names:            del obj._memoized_results[key]    property_dict = obj._cache_    for prop in method_names:        inner_attname = '__%s' % prop        if inner_attname in property_dict:            del property_dict[inner_attname]",
        "labels_text": "Clear the memoized method or memoizedproperty result for the given method name from the given object v def inc v return v class Fooobject memoizemethod def fooself return inc memoizedproperty def gself return inc f Foo ffoo ffoo clearmemoizedmethodsf foo ffoo ffoo fg fg ffoo ffoo fg fg clearmemoizedmethodsf g noproblemifundefined fg ffoo fg ffoo"
    },
    {
        "input_text": "summarize: def memoizedproperty(func):        inner_attname = '__%s' % func.__name__    def new_fget(self):        if not hasattr(self, '_cache_'):            self._cache_ = {}        cache = self._cache_        if inner_attname not in cache:            cache[inner_attname] = func(self)        return cache[inner_attname]    return property(new_fget)",
        "labels_text": "Decorator to cause a method to cache it result in self for each combination of input and return the cached result on subsequent call Does not support named argument or arg value that are not hashable class Foo object x memoizedproperty def fooself selfx printupdating and returning formatselfx return selfx foo Foo foo Foo foofoo updating and returning foofoo foofoo updating and returning foofoo"
    },
    {
        "input_text": "summarize: def validate(self, instance, val):                # note here calling, but not assigning; could lead to unexpected behavior        if isinstance(val, self._type) and (self._validation is None or self._validation(val)):            return val        elif val is NULL and not self.required:            return val        elif val is None and self.nullable:            return val        else:            raise ValidationError(getattr(self, 'name', 'undefined name'), val)",
        "labels_text": "Returns True if val is valid Raises ValidationError"
    },
    {
        "input_text": "summarize: def dals(string):        return dedent(string).lstrip()",
        "labels_text": "dedent and leftstrip"
    },
    {
        "input_text": "summarize: def numberify(value):        if isinstance(value, bool):        return int(value)    if isinstance(value, NUMBER_TYPES):        return value    candidate = _REGEX.convert_number(value)    if candidate is not NO_MATCH:        return candidate    raise TypeCoercionError(value, f\"Cannot convert {value} to a number.\")",
        "labels_text": "Examples numberifyx for x in o False True numberifyx for x in j j j j"
    },
    {
        "input_text": "summarize: def plugin_manager(self) -> CondaPluginManager:                from ..plugins.manager import get_plugin_manager        return get_plugin_manager()",
        "labels_text": "This is the preferred way of accessing the PluginManager object for this application and is located here to avoid problem with cyclical import elsewhere in the code"
    },
    {
        "input_text": "summarize: def plugins(self) -> PluginConfig:                self.plugin_manager.load_settings()        return PluginConfig(self.raw_data)",
        "labels_text": "Preferred way of accessing setting introduced by the setting plugin hook"
    },
    {
        "input_text": "summarize: def croot(self):                if self._croot:            return abspath(expanduser(self._croot))        elif self.bld_path:            return abspath(expanduser(self.bld_path))        elif \"root-dir\" in self.conda_build:            return abspath(expanduser(self.conda_build[\"root-dir\"]))        elif self.root_writable:            return join(self.root_prefix, \"conda-bld\")        else:            return expand(\"~/conda-bld\")",
        "labels_text": "This is where source cache and work folder live"
    },
    {
        "input_text": "summarize: def fetch_threads(self) -> int | None:                if self._fetch_threads == 0 and self._default_threads == 0:            return 5        return self._fetch_threads or self.default_threads",
        "labels_text": "If both are not overriden return experimentallydetermined value of"
    },
    {
        "input_text": "summarize: def av_data_dir(self):                # TODO (AV): Find ways to make this user configurable?        return join(self.conda_prefix, \"etc\", \"conda\")",
        "labels_text": "Where critical artifact verification data eg various public key can be found"
    },
    {
        "input_text": "summarize: def signing_metadata_url_base(self):                if self._signing_metadata_url_base:            return self._signing_metadata_url_base        else:            return None",
        "labels_text": "Base URL for artifact verification signing metadata rootjson keymgrjson"
    },
    {
        "input_text": "summarize: def trace(self) -> bool:                return self.verbosity >= 4",
        "labels_text": "Alias for contextverbosity"
    },
    {
        "input_text": "summarize: def debug(self) -> bool:                return self.verbosity >= 3",
        "labels_text": "Alias for contextverbosity"
    },
    {
        "input_text": "summarize: def info(self) -> bool:                return self.verbosity >= 2",
        "labels_text": "Alias for contextverbosity"
    },
    {
        "input_text": "summarize: def verbose(self) -> bool:                return self.verbosity >= 1",
        "labels_text": "Alias for contextverbosity"
    },
    {
        "input_text": "summarize: def verbosity(self) -> int:                #                   0 \u2192 logging.WARNING, standard output        #           -v    = 1 \u2192 logging.WARNING, detailed output        #           -vv   = 2 \u2192 logging.INFO        # --debug = -vvv  = 3 \u2192 logging.DEBUG        # --trace = -vvvv = 4 \u2192 conda.gateways.logging.TRACE        if self._trace:            return 4        elif self._debug:            return 3        else:            return self._verbosity",
        "labels_text": "Verbosity level For cleaner and readable code it is preferable to use the following alias property contexttrace contextdebug contextinfo contextverbose contextloglevel"
    },
    {
        "input_text": "summarize: def log_level(self) -> int:                if 4 < self.verbosity:            return logging.NOTSET  # 0        elif 3 < self.verbosity <= 4:            return TRACE  # 5        elif 2 < self.verbosity <= 3:            return logging.DEBUG  # 10        elif 1 < self.verbosity <= 2:            return logging.INFO  # 20        else:            return logging.WARNING",
        "labels_text": "Map contextverbosity to logging level"
    },
    {
        "input_text": "summarize: def _override(self, key, value):                old = getattr(self, key)        setattr(self, key, value)        try:            yield        finally:            setattr(self, key, old)",
        "labels_text": "TODO This might be broken in some way Unsure what happens if the old value is a property and get set to a new value Or if the new value override the validation logic on the underlying ParameterLoader instance Investigate and implement in a safer way"
    },
    {
        "input_text": "summarize: def reporters(self) -> tuple[Mapping[str, str]]:                if not self._reporters:            return (                {                    \"backend\": \"json\" if self.json else \"console\",                    \"output\": \"stdout\",                    \"verbosity\": self.verbosity,                    \"quiet\": self.quiet,                },            )        return self._reporters",
        "labels_text": "Determine the value of reporter based on other setting and the selfreporters value itself"
    },
    {
        "input_text": "summarize: def locate_prefix_by_name(name, envs_dirs=None):        assert name    if name in (ROOT_ENV_NAME, \"root\"):        return context.root_prefix    if envs_dirs is None:        envs_dirs = context.envs_dirs    for envs_dir in envs_dirs:        if not isdir(envs_dir):            continue        prefix = join(envs_dir, name)        if isdir(prefix):            return abspath(prefix)    from ..exceptions import EnvironmentNameNotFound    raise EnvironmentNameNotFound(name)",
        "labels_text": "Find the location of a prefix given a conda env name If the location doe not exist an error is raised"
    },
    {
        "input_text": "summarize: def add_plugin_setting(name: str, parameter: Parameter, aliases: tuple[str, ...] = ()):        PluginConfig.parameter_names = PluginConfig.parameter_names + (name,)    loader = ParameterLoader(parameter, aliases=aliases)    name = loader._set_name(name)    setattr(PluginConfig, name, loader)",
        "labels_text": "Adds a setting to the classPluginConfig class"
    },
    {
        "input_text": "summarize: def remove_all_plugin_settings() -> None:        for name in PluginConfig.parameter_names:        try:            delattr(PluginConfig, name)        except AttributeError:            continue    PluginConfig.parameter_names = tuple()",
        "labels_text": "Removes all attached setting from the classPluginConfig class"
    },
    {
        "input_text": "summarize: def is_active_prefix(prefix: str) -> bool:        if context.active_prefix is None:        return False    return (        paths_equal(prefix, context.active_prefix)        # normcasing our prefix check for Windows, for case insensitivity        or normcase(prefix) == normcase(env_name(context.active_prefix))    )",
        "labels_text": "Determines whether the args we pas in are pointing to the active prefix Can be used a validation step to make sure operation are not being performed on the active prefix"
    },
    {
        "input_text": "summarize: def validate_prefix(prefix):        if isdir(prefix):        if not isfile(join(prefix, \"conda-meta\", \"history\")):            raise DirectoryNotACondaEnvironmentError(prefix)    else:        raise EnvironmentLocationNotFound(prefix)    return prefix",
        "labels_text": "Verifies the prefix is a valid conda environment raise EnvironmentLocationNotFound Nonexistent path or not a directory raise DirectoryNotACondaEnvironmentError Directory is not a conda environment return Valid prefix rtype str"
    },
    {
        "input_text": "summarize: def _get_subactions(self):                return sorted(self._choices_actions, key=lambda action: action.dest)",
        "labels_text": "Sort action for subcommands to appear alphabetically in help blurb"
    },
    {
        "input_text": "summarize: def add_parser_help(p: ArgumentParser) -> None:        p.add_argument(        \"-h\",        \"--help\",        action=_HelpAction,        help=\"Show this help message and exit.\",    )",
        "labels_text": "So we can use consistent capitalization and period in the help You must use the addhelpFalse argument to ArgumentParser or addparser to use this Add this first to be consistent with the default argparse output"
    },
    {
        "input_text": "summarize: def add_parser_solver(p: ArgumentParser) -> None:        from ..base.context import context    from ..common.constants import NULL    group = p.add_mutually_exclusive_group()    group.add_argument(        \"--solver\",        dest=\"solver\",        choices=context.plugin_manager.get_solvers(),        help=\"Choose which solver backend to use.\",        default=NULL,    )",
        "labels_text": "Add a commandline flag for alternative solver backends See contextsolver for more info"
    },
    {
        "input_text": "summarize: def validate_prefix_exists(prefix: str | Path) -> None:        prefix = Path(prefix)    if not prefix.exists():        raise CondaEnvException(\"The environment you have specified does not exist.\")",
        "labels_text": "Validate that we are receiving at least one valid value for name or prefix"
    },
    {
        "input_text": "summarize: def dump_record(prec: PackageRecord) -> dict[str, Any]:        return {k: v for k, v in prec.dump().items() if k not in IGNORE_FIELDS}",
        "labels_text": "Returns a dictionary of keyvalue pair from prec Keys included in IGNOREFIELDS are not returned param prec A PackageRecord object return A dictionary of element dumped from prec"
    },
    {
        "input_text": "summarize: def execute(args: Namespace, parser: ArgumentParser) -> int:        from ..exceptions import CondaError    from ..notices import core as notices    try:        channel_notice_set = notices.retrieve_notices()    except OSError as exc:        raise CondaError(f\"Unable to retrieve notices: {exc}\")    notices.display_notices(channel_notice_set)    return 0",
        "labels_text": "Command that retrieves channel notification cache them and display them"
    },
    {
        "input_text": "summarize: def remove(prefix, files):        dst_dirs = set()    for f in files:        dst = join(prefix, f)        dst_dirs.add(dirname(dst))        os.unlink(dst)    for path in sorted(dst_dirs, key=len, reverse=True):        try:            os.rmdir(path)        except OSError:  # directory might not be empty            pass",
        "labels_text": "Remove file for a given prefix"
    },
    {
        "input_text": "summarize: def which_package(path):        from ..common.path import paths_equal    from ..core.prefix_data import PrefixData    path = abspath(path)    prefix = which_prefix(path)    if prefix is None:        from ..exceptions import CondaVerificationError        raise CondaVerificationError(f\"could not determine conda prefix from: {path}\")    for prec in PrefixData(prefix).iter_records():        if any(paths_equal(join(prefix, f), path) for f in prec[\"files\"] or ()):            yield prec",
        "labels_text": "Return the package containing the path Provided the path of a presumably conda installed file iterate over the conda package the file came from Usually the iteration yield only one package"
    },
    {
        "input_text": "summarize: def which_prefix(path):        prefix = abspath(path)    while True:        if isdir(join(prefix, \"conda-meta\")):            # we found the it, so let's return it            return prefix        if prefix == dirname(prefix):            # we cannot chop off any more directories, so we didn't find it            return None        prefix = dirname(prefix)",
        "labels_text": "Return the prefix for the provided path Provided the path of a presumably conda installed file return the environment prefix in which the file in located"
    },
    {
        "input_text": "summarize: def encode_for_env_var(value) -> str:        if isinstance(value, str):        return value    elif isinstance(value, bytes):        return value.decode()    return str(value)",
        "labels_text": "Environment name and value need to be string"
    },
    {
        "input_text": "summarize: def __init__(self, name, value, key_flag, value_flags, validation=None):                self._name = name        self.value = value        self.key_flag = key_flag        self.value_flags = value_flags        self._validation = validation",
        "labels_text": "Represents a Parameter that ha been loaded with configuration value Args name str name of the loaded parameter value LoadedParameter or primitive the value of the loaded parameter keyflag ParameterFlag or None priority flag for the parameter itself valueflags Any or None priority flag for the parameter value validation callable Given a parameter value a input return a boolean indicating validity or alternately return a string describing an invalid value"
    },
    {
        "input_text": "summarize: def merge(self, matches):                raise NotImplementedError()",
        "labels_text": "Recursively merges match into one LoadedParameter Args match ListLoadedParameter list of match of this parameter Returns LoadedParameter"
    },
    {
        "input_text": "summarize: def __init__(        self, name, element_type, value, key_flag, value_flags, validation=None    ):                self._type = element_type        self._element_type = element_type        super().__init__(name, value, key_flag, value_flags, validation)",
        "labels_text": "Args elementtype type or tupletype Typevalidation of parameter value value primitive value primitive python value"
    },
    {
        "input_text": "summarize: def __init__(        self, name, value, element_type, key_flag, value_flags, validation=None    ):                self._element_type = element_type        super().__init__(name, value, key_flag, value_flags, validation)",
        "labels_text": "Args value Mapping Map of string key to LoadedParameter value elementtype Parameter The Parameter type that is held in value valueflags Mapping Map of priority value flag"
    },
    {
        "input_text": "summarize: def __init__(        self, name, value, element_type, key_flag, value_flags, validation=None    ):                self._element_type = element_type        super().__init__(name, value, key_flag, value_flags, validation)",
        "labels_text": "Args value Sequence Sequence of LoadedParameter value elementtype Parameter The Parameter type that is held in the sequence valueflags Sequence Sequence of priority valueflags"
    },
    {
        "input_text": "summarize: def __init__(        self, name, value, element_type, key_flag, value_flags, validation=None    ):                self._element_type = element_type        super().__init__(name, value, key_flag, value_flags, validation)",
        "labels_text": "Args value Sequence Object with LoadedParameter field elementtype object The Parameter type that is held in the sequence valueflags Sequence Sequence of priority valueflags"
    },
    {
        "input_text": "summarize: def __init__(self, default, validation=None):                self._default = default        self._validation = validation",
        "labels_text": "The Parameter class represents an unloaded configuration parameter holding type default and validation information until the parameter is loaded with a configuration Args default Any the typed python representation default value given if the Parameter is not found in a Configuration validation callable Given a parameter value a input return a boolean indicating validity or alternately return a string describing an invalid value"
    },
    {
        "input_text": "summarize: def default(self):                wrapped_default = DefaultValueRawParameter(\"default\", \"default\", self._default)        return self.load(\"default\", wrapped_default)",
        "labels_text": "Returns a DefaultValueRawParameter that wrap the actual default value"
    },
    {
        "input_text": "summarize: def get_all_matches(self, name, names, instance):                matches = []        multikey_exceptions = []        for filepath, raw_parameters in instance.raw_data.items():            match, error = ParameterLoader.raw_parameters_from_single_source(                name, names, raw_parameters            )            if match is not None:                matches.append(match)            if error:                multikey_exceptions.append(error)        return matches, multikey_exceptions",
        "labels_text": "Finds all match of a Parameter in a Configuration instance Args name str canonical name of the parameter to search for name tuplestr alternative alias of the parameter instance Configuration instance of the configuration to search within Returns ListRawParameter match of the parameter found in the configuration"
    },
    {
        "input_text": "summarize: def load(self, name, match):                raise NotImplementedError()",
        "labels_text": "Loads a Parameter with the value in a RawParameter Args name str name of the parameter to pas through match RawParameter the value of the RawParameter match Returns a LoadedParameter"
    },
    {
        "input_text": "summarize: def __init__(self, default, element_type=None, validation=None):                self._type = type(default) if element_type is None else element_type        self._element_type = self._type        super().__init__(default, validation)",
        "labels_text": "Args default primitive value default value if the Parameter is not found elementtype type or tupletype Typevalidation of parameter value If None typedefault is used"
    },
    {
        "input_text": "summarize: def __init__(self, element_type, default=frozendict(), validation=None):                self._element_type = element_type        default = default and frozendict(default) or frozendict()        super().__init__(default, validation=validation)",
        "labels_text": "Args elementtype Parameter The Parameter type held in the MapParameter default Mapping The parameter default value If None will be an empty dict"
    },
    {
        "input_text": "summarize: def __init__(self, element_type, default=(), validation=None, string_delimiter=\",\"):                self._element_type = element_type        self.string_delimiter = string_delimiter        super().__init__(default, validation)",
        "labels_text": "Args elementtype Parameter The Parameter type that is held in the sequence default Sequence default value empty tuple if not given stringdelimiter str separation string used to parse string into sequence"
    },
    {
        "input_text": "summarize: def __init__(self, element_type, default=ConfigurationObject(), validation=None):                self._element_type = element_type        super().__init__(default, validation)",
        "labels_text": "Args elementtype object The object type with parameter field held in ObjectParameter default Sequence default value empty tuple if not given"
    },
    {
        "input_text": "summarize: def __init__(self, parameter_type, aliases=(), expandvars=False):                self._name = None        self._names = None        self.type = parameter_type        self.aliases = aliases        self._expandvars = expandvars",
        "labels_text": "Args parametertype Parameter the type of Parameter that is stored in the loader alias tuplestr alternative alias for the Parameter expandvars bool whether or not to recursively expand environmental variable"
    },
    {
        "input_text": "summarize: def refresh(self):                if self.enabled and not self.json and IS_INTERACTIVE:            self.pbar.refresh()",
        "labels_text": "Force refresh ie once ha been reached"
    },
    {
        "input_text": "summarize: def _tqdm(*args, **kwargs):                from tqdm.auto import tqdm        return tqdm(*args, **kwargs)",
        "labels_text": "Deferred import so it doesnt hit the conda activate path"
    },
    {
        "input_text": "summarize: def groupby_to_dict(keyfunc, sequence):        result = collections.defaultdict(list)    for key, group in itertools.groupby(sequence, keyfunc):        result[key].extend(group)    return dict(result)",
        "labels_text": "A toolzstyle groupby implementation Returns a dictionary of key group instead of iterators"
    },
    {
        "input_text": "summarize: def unique(sequence: Sequence[Any]) -> Generator[Any, None, None]:        seen: set[Any] = set()    yield from (        # seen.add always returns None so we will always return element        seen.add(element) or element        for element in sequence        # only pass along novel elements        if element not in seen    )",
        "labels_text": "A toolz inspired unique implementation Returns a generator of unique element in the sequence"
    },
    {
        "input_text": "summarize: def ITE(self, c, t, f, polarity=None, name=None):                return self._eval(self._clauses.ITE, (c, t, f), (), polarity, name)",
        "labels_text": "If c Then t Else f In this function if any of c t or f are True and False the resulting expression is resolved"
    },
    {
        "input_text": "summarize: def yaml_safe_load(string):        return _yaml_safe().load(string)",
        "labels_text": "Examples yamlsafeloadkey value key value"
    },
    {
        "input_text": "summarize: def yaml_round_trip_dump(object, stream=None):        ostream = stream or StringIO()    _yaml_round_trip().dump(object, ostream)    if not stream:        return ostream.getvalue()",
        "labels_text": "Dump object to string or stream"
    },
    {
        "input_text": "summarize: def yaml_safe_dump(object, stream=None):        ostream = stream or StringIO()    _yaml_safe().dump(object, ostream)    if not stream:        return ostream.getvalue()",
        "labels_text": "Dump object to string or stream"
    },
    {
        "input_text": "summarize: def get_signal_name(signum):        return next(        (            k            for k, v in signal.__dict__.items()            if v == signum and k.startswith(\"SIG\") and not k.startswith(\"SIG_\")        ),        None,    )",
        "labels_text": "Examples from signal import SIGINT getsignalnameSIGINT SIGINT"
    },
    {
        "input_text": "summarize: def pop_key(data):        items = sorted(data.items(), key=lambda item: (len(item[1]), item[0]))    key = items[0][0]    data.pop(key)    for dep in data.values():        dep.discard(key)    return key",
        "labels_text": "Pop an item from the graph that ha the fewest dependency in the case of a tie The winner will be sorted alphabetically"
    },
    {
        "input_text": "summarize: def _safe_toposort(data):        # Special case empty input.    if len(data) == 0:        return    t = _toposort(data)    while True:        try:            value = next(t)            yield value        except ValueError as err:            log.debug(err.args[0])            if not data:                return  # pragma: nocover            yield pop_key(data)            t = _toposort(data)            continue        except StopIteration:            return",
        "labels_text": "Dependencies are expressed a a dictionary whose key are item and whose value are a set of dependent item Output is a list of set in topological order The first set consists of item with no dependency each subsequent set consists of item that depend upon item in the preceding set"
    },
    {
        "input_text": "summarize: def as_dict(self) -> dict:                return self._asdict()",
        "labels_text": "Provide a public interface for namedtuples asdict"
    },
    {
        "input_text": "summarize: def replace(self, **kwargs) -> \"Url\":                return self._replace(**kwargs)",
        "labels_text": "Provide a public interface for namedtuples replace"
    },
    {
        "input_text": "summarize: def url_to_s3_info(url):        parsed_url = urlparse(url)    assert parsed_url.scheme == \"s3\", f\"You can only use s3: urls (not {url!r})\"    bucket, key = parsed_url.hostname, parsed_url.path    return bucket, key",
        "labels_text": "Convert an s url to a tuple of bucket and key Examples urltosinfosbucketnamebuckethereisthekey bucketnamebucket hereisthekey"
    },
    {
        "input_text": "summarize: def is_url(url):        if not url:        return False    try:        return urlparse(url).scheme != \"\"    except ValueError:        return False",
        "labels_text": "Examples isurlNone False isurlssomebucket True"
    },
    {
        "input_text": "summarize: def is_ipv4_address(string_ip):        try:        socket.inet_aton(string_ip)    except OSError:        return False    return string_ip.count(\".\") == 3",
        "labels_text": "Examples isipvaddressip for ip in True True True isipvaddressip for ip in False False False False"
    },
    {
        "input_text": "summarize: def is_ipv6_address(string_ip):        try:        socket.inet_pton(socket.AF_INET6, string_ip)    except OSError:        return False    return True",
        "labels_text": "Examples isipvaddressip for ip in dba True True True isipvaddressip for ip in False False"
    },
    {
        "input_text": "summarize: def is_ip_address(string_ip):        return is_ipv4_address(string_ip) or is_ipv6_address(string_ip)",
        "labels_text": "Examples isipaddress True isipaddress True isipaddresswwwgooglecom False"
    },
    {
        "input_text": "summarize: def strip_scheme(url):        return url.split(\"://\", 1)[-1]",
        "labels_text": "Examples stripschemehttpswwwcondaio wwwcondaio stripschemessomebucketplusapathext somebucketplusapathext"
    },
    {
        "input_text": "summarize: def split_anaconda_token(url):        _token_match = re.search(r\"/t/([a-zA-Z0-9-]*)\", url)    token = _token_match.groups()[0] if _token_match else None    cleaned_url = url.replace(\"/t/\" + token, \"\", 1) if token is not None else url    return cleaned_url.rstrip(\"/\"), token",
        "labels_text": "Examples splitanacondatokenhttpsttkpath uhttpspath utk splitanacondatokenhttpstpath uhttpspath u splitanacondatokenhttpssomedomainapittkpath uhttpssomedomainapipath utk splitanacondatokenhttpscondattkpath uhttpscondapath utk splitanacondatokenhttpspath uhttpspath None splitanacondatokenhttpscondattk uhttpsconda utk"
    },
    {
        "input_text": "summarize: def split_platform(known_subdirs, url):        _platform_match = _split_platform_re(known_subdirs).search(url)    platform = _platform_match.groups()[0] if _platform_match else None    cleaned_url = url.replace(\"/\" + platform, \"\", 1) if platform is not None else url    return cleaned_url.rstrip(\"/\"), platform",
        "labels_text": "Examples from condabaseconstants import KNOWNSUBDIRS splitplatformKNOWNSUBDIRS httpsttklinuxppclepath uhttpsttkpath ulinuxppcle"
    },
    {
        "input_text": "summarize: def split_scheme_auth_token(url):        if not url:        return None, None, None, None    cleaned_url, token = split_anaconda_token(url)    url_parts = urlparse(cleaned_url)    remainder_url = Url(        hostname=url_parts.hostname,        port=url_parts.port,        path=url_parts.path,        query=url_parts.query,    )    return str(remainder_url), url_parts.scheme, url_parts.auth, token",
        "labels_text": "Examples splitschemeauthtokenhttpsupcondaiotxmorepath condaiomorepath http up x splitschemeauthtokenNone None None None None"
    },
    {
        "input_text": "summarize: def add_username_and_password(url: str, username: str, password: str) -> str:        url = urlparse(url)    url_with_auth = url.replace(username=username, password=quote(password, safe=\"\"))    return str(url_with_auth)",
        "labels_text": "Inserts username and password into provided url addusernameandpasswordhttpsanacondaorg TestUser Password httpsTestUserPasswordanacondaorg"
    },
    {
        "input_text": "summarize: def maybe_add_auth(url: str, auth: str, force=False) -> str:        if not auth:        return url    url_parts = urlparse(url)    if url_parts.username and url_parts.password and not force:        return url    auth_parts = auth.split(\":\")    if len(auth_parts) > 1:        url_parts = url_parts.replace(username=auth_parts[0], password=auth_parts[1])    return str(url_parts)",
        "labels_text": "Add auth if the url doesnt currently have it By default doe not replace auth if it already exists Setting force to True override this behavior Examples maybeaddauthhttpswwwcondaio userpasswd httpsuserpasswdwwwcondaio maybeaddauthhttpswwwcondaio httpswwwcondaio"
    },
    {
        "input_text": "summarize: def remove_auth(url: str) -> str:        url = urlparse(url)    url_no_auth = url.replace(username=\"\", password=\"\")    return str(url_no_auth)",
        "labels_text": "Remove embedded authentication from URL codeblock pycon removeauthhttpsuserpasswordanacondacom httpsanacondacom"
    },
    {
        "input_text": "summarize: def get_clause_count(self):                return len(self._clause_list)",
        "labels_text": "Return number of stored clause"
    },
    {
        "input_text": "summarize: def save_state(self):                return len(self._clause_list)",
        "labels_text": "Get state information to be able to revert temporary addition of supplementary clause ClauseList state is simply the number of clause"
    },
    {
        "input_text": "summarize: def restore_state(self, saved_state):                len_clauses = saved_state        self._clause_list[len_clauses:] = []",
        "labels_text": "Restore state saved via savestate Removes clause that were added after the state ha been saved"
    },
    {
        "input_text": "summarize: def as_list(self):                return self._clause_list",
        "labels_text": "Return clause a a list of tuples of ints"
    },
    {
        "input_text": "summarize: def as_array(self):                clause_array = array(\"i\")        for c in self._clause_list:            clause_array.extend(c)            clause_array.append(0)        return clause_array",
        "labels_text": "Return clause a a flat int array each clause being terminated by"
    },
    {
        "input_text": "summarize: def get_clause_count(self):                return self._clause_array.count(0)",
        "labels_text": "Return number of stored clause This is an On operation since we dont store the number of clause explicitly due to performance reason Python interpreter overhead in selfappend"
    },
    {
        "input_text": "summarize: def save_state(self):                return len(self._clause_array)",
        "labels_text": "Get state information to be able to revert temporary addition of supplementary clause ClauseArray state is the length of the int array NOT number of clause"
    },
    {
        "input_text": "summarize: def restore_state(self, saved_state):                len_clause_array = saved_state        self._clause_array[len_clause_array:] = array(\"i\")",
        "labels_text": "Restore state saved via savestate Removes clause that were added after the state ha been saved"
    },
    {
        "input_text": "summarize: def as_list(self):                clause = []        for v in self._clause_array:            if v == 0:                yield tuple(clause)                clause.clear()            else:                clause.append(v)",
        "labels_text": "Return clause a a list of tuples of ints"
    },
    {
        "input_text": "summarize: def as_array(self):                return self._clause_array",
        "labels_text": "Return clause a a flat int array each clause being terminated by"
    },
    {
        "input_text": "summarize: def setup(self, m, **kwargs):                raise NotImplementedError()",
        "labels_text": "Create a solver instance add the clause to it and return it"
    },
    {
        "input_text": "summarize: def invoke(self, solver):                raise NotImplementedError()",
        "labels_text": "Start the actual SAT solving and return the calculated solution"
    },
    {
        "input_text": "summarize: def process_solution(self, sat_solution):                raise NotImplementedError()",
        "labels_text": "Process the solution returned by selfinvoke Returns a list of satisfied variable or None if no solution is found"
    },
    {
        "input_text": "summarize: def win_path_to_unix(    paths: PathType | PathsType | None,    prefix: PathType | None = None,    *,    cygdrive: bool = False,) -> str | tuple[str, ...] | None:        return _path_to(paths, prefix=prefix, cygdrive=cygdrive, to_unix=True)",
        "labels_text": "Convert Windows path to Unix path note Produces unexpected result when run on Unix Args path The path to convert prefix The Windows pathstyle prefix directory to use for the conversion If not provided no check for prefix path will be made cygdrive Whether to use the Cygwinstyle drive prefix"
    },
    {
        "input_text": "summarize: def unix_path_to_win(    paths: PathType | PathsType | None,    prefix: PathType | None = None,    *,    cygdrive: bool = False,) -> str | tuple[str, ...] | None:        return _path_to(paths, prefix, cygdrive=cygdrive, to_unix=False)",
        "labels_text": "Convert Unix path to Windows path note Produces unexpected result when run on Unix Args path The path to convert prefix The Windows pathstyle prefix directory to use for the conversion If not provided no check for prefix path will be made cygdrive Unused Present to keep the signature consistent with winpathtounix"
    },
    {
        "input_text": "summarize: def paths_equal(path1, path2):        if on_win:        return normcase(abspath(path1)) == normcase(abspath(path2))    else:        return abspath(path1) == abspath(path2)",
        "labels_text": "Examples pathsequalabc abcd True"
    },
    {
        "input_text": "summarize: def ensure_pad(name, pad=\"_\"):        if not name or name[0] == name[-1] == pad:        return name    else:        return f\"{pad}{name}{pad}\"",
        "labels_text": "Examples ensurepadconda conda ensurepadconda conda ensurepad"
    },
    {
        "input_text": "summarize: def is_private_env_name(env_name):        return env_name and env_name[0] == env_name[-1] == \"_\"",
        "labels_text": "Examples isprivateenvnameconda False isprivateenvnameconda True"
    },
    {
        "input_text": "summarize: def is_private_env_path(env_path):        if env_path is not None:        envs_directory, env_name = split(env_path)        if basename(envs_directory) != \"envs\":            return False        return is_private_env_name(env_name)    return False",
        "labels_text": "Examples isprivateenvpathsomepathtoenvsconda True isprivateenvpathnotanenvsdirconda False"
    },
    {
        "input_text": "summarize: def strip_pkg_extension(path: str):        # NOTE: not using CONDA_TARBALL_EXTENSION_V1 or CONDA_TARBALL_EXTENSION_V2 to comply with    #       import rules and to avoid a global lookup.    for extension in KNOWN_EXTENSIONS:        if path.endswith(extension):            return path[: -len(extension)], extension    return path, None",
        "labels_text": "Examples strippkgextensionpathlicensepytarbz pathlicensepy tarbz strippkgextensionpathlicensepyconda pathlicensepy conda strippkgextensionpathlicensepy pathlicensepy None"
    },
    {
        "input_text": "summarize: def is_package_file(path):        # NOTE: not using CONDA_TARBALL_EXTENSION_V1 or CONDA_TARBALL_EXTENSION_V2 to comply with    #       import rules and to avoid a global lookup.    return path[-6:] == \".conda\" or path[-8:] == \".tar.bz2\"",
        "labels_text": "Examples ispackagefilepathlicensepytarbz True ispackagefilepathlicensepyconda True ispackagefilepathlicensepy False"
    },
    {
        "input_text": "summarize: def _check_files(self):                for fname in self.MANDATORY_FILES:            if self._metadata_dir_full_path:                fpath = join(self._metadata_dir_full_path, fname)                if not isfile(fpath):                    raise OSError(ENOENT, strerror(ENOENT), fpath)",
        "labels_text": "Check the existence of mandatory file for a given distribution"
    },
    {
        "input_text": "summarize: def _check_path_data(self, path, checksum, size):                if checksum:            assert checksum.startswith(\"sha256=\"), (                self._metadata_dir_full_path,                path,                checksum,            )            checksum = checksum[7:]        else:            checksum = None        size = int(size) if size else None        return path, checksum, size",
        "labels_text": "Normalizes record data content and format"
    },
    {
        "input_text": "summarize: def _read_metadata(cls, fpath):                data = {}        if fpath and isfile(fpath):            parser = HeaderParser()            # FIXME: Is this a correct assumption for the encoding?            # This was needed due to some errors on windows            with open_utf8(fpath) as fp:                data = parser.parse(fp)        return cls._message_to_dict(data)",
        "labels_text": "Read the original format which is stored a RFC header"
    },
    {
        "input_text": "summarize: def _get_multiple_data(self, keys):                data = []        if self._data:            for key in keys:                raw_data = self._data.get(key, [])                for req in raw_data:                    data.append(req.strip())                if data:                    break        return frozenset(data)",
        "labels_text": "Helper method to get multiple data value by key Keys is an iterable including the preferred key in order to include value of key that might have been replaced deprecated for example key can be requiresdist requires where the key requires is deprecated and replaced by requiresdist"
    },
    {
        "input_text": "summarize: def get_python_requirements(self):                return self._get_multiple_data([\"requires_python\"])",
        "labels_text": "New in version This field specifies the Python version that the distribution is guaranteed to be compatible with Installation tool may look at this when picking which version of a project to install The value must be in the format specified in Version specifier This field may be followed by an environment marker after a semicolon Example frozenset sysplatform win"
    },
    {
        "input_text": "summarize: def get_extra_provides(self):                return self._get_multiple_data([\"provides_extra\"])",
        "labels_text": "New in version A string containing the name of an optional feature Must be a valid Python identifier May be used to make a dependency conditional on hether the optional feature ha been requested Example frozensetpdf doc test"
    },
    {
        "input_text": "summarize: def get_classifiers(self):                return self._get_multiple_data([\"classifier\"])",
        "labels_text": "Classifiers are described in PEP and the Python Package Index publishes a dynamic list of currently defined classifier This field may be followed by an environment marker after a semicolon Example frozensetDevelopment Status Beta Environment Console Text Based osname posix"
    },
    {
        "input_text": "summarize: def norm_package_version(version):        if version:        version = \",\".join(v.strip() for v in version.split(\",\")).strip()        if version.startswith(\"(\") and version.endswith(\")\"):            version = version[1:-1]        version = \"\".join(v for v in version if v.strip())    else:        version = \"\"    return version",
        "labels_text": "Normalize a version by removing extra space and parenthesis"
    },
    {
        "input_text": "summarize: def split_spec(spec, sep):        parts = spec.rsplit(sep, 1)    spec_start = parts[0].strip()    spec_end = \"\"    if len(parts) == 2:        spec_end = parts[-1].strip()    return spec_start, spec_end",
        "labels_text": "Split a spec by separator and return stripped start and end part"
    },
    {
        "input_text": "summarize: def interpret(marker, execution_context=None):        try:        expr, rest = parse_marker(marker)    except Exception as e:        raise SyntaxError(f\"Unable to interpret marker syntax: {marker}: {e}\")    if rest and rest[0] != \"#\":        raise SyntaxError(f\"unexpected trailing data in marker: {marker}: {rest}\")    context = DEFAULT_MARKER_CONTEXT.copy()    if execution_context:        context.update(execution_context)    return evaluator.evaluate(expr, context)",
        "labels_text": "Interpret a marker and return a result depending on environment param marker The marker to interpret type marker str param executioncontext The context used for name lookup type executioncontext mapping"
    },
    {
        "input_text": "summarize: def _wait_and_close_handle(process_handle):        try:        WaitForSingleObject(process_handle, INFINITE)        CloseHandle(process_handle)    except Exception as e:        log.info(\"%r\", e)",
        "labels_text": "Waits until spawned process finish and close the handle for it"
    },
    {
        "input_text": "summarize: def get_user_environments_txt_file(userhome: str = \"~\") -> str:        return expand(join(userhome, \".conda\", \"environments.txt\"))",
        "labels_text": "Gets the path to the user environmentstxt file param userhome The home directory of the user type userhome str return Path to the environmentstxt file rtype str"
    },
    {
        "input_text": "summarize: def query_all_prefixes(spec: str) -> Iterator[tuple[str, tuple]]:        for prefix in list_all_known_prefixes():        prefix_recs = tuple(PrefixData(prefix).query(spec))        if prefix_recs:            yield prefix, prefix_recs",
        "labels_text": "Queries all known prefix for a given specification param spec The specification to query for type spec str return An iterator of tuples containing the prefix and the query result rtype IteratorTuplestr Tuple"
    },
    {
        "input_text": "summarize: def _rewrite_environments_txt(environments_txt_file: str, prefixes: list[str]) -> None:        try:        with open_utf8(environments_txt_file, \"w\") as fh:            fh.write(\"\\n\".join(prefixes))            fh.write(\"\\n\")    except OSError as e:        log.info(\"File not cleaned: %s\", environments_txt_file)        log.debug(\"%r\", e, exc_info=True)",
        "labels_text": "Rewrites the environmentstxt file with the specified prefix param environmentstxtfile The file path of environmentstxt param prefix List of prefix to write into the file type environmentstxtfile str type prefix Liststr return None"
    },
    {
        "input_text": "summarize: def check_allowlist(channel_urls: list[str]) -> None:        if context.allowlist_channels:        allowlist_channel_urls = tuple(            chain.from_iterable(                Channel(c).base_urls for c in context.allowlist_channels            )        )        for url in channel_urls:            these_urls = Channel(url).base_urls            if not all(this_url in allowlist_channel_urls for this_url in these_urls):                raise ChannelNotAllowed(Channel(url))",
        "labels_text": "Check if the given channel URLs are allowed by the context allowlist param channelurls A list of channel URLs to check against the allowlist raise ChannelNotAllowed If any URL is not in the allowlist"
    },
    {
        "input_text": "summarize: def dist_str_in_index(index: dict[Any, Any], dist_str: str) -> bool:        match_spec = MatchSpec.from_dist_str(dist_str)    return any(match_spec.match(prec) for prec in index.values())",
        "labels_text": "Check if a distribution string match any package in the index param index The package index param diststr The distribution string to match against the index return True if there is a match False otherwise"
    },
    {
        "input_text": "summarize: def _supplement_index_with_cache(index: dict[Any, Any]) -> None:        # supplement index with packages from the cache    for pcrec in PackageCacheData.get_all_extracted_entries():        if pcrec in index:            # The downloaded repodata takes priority            current_record = index[pcrec]            index[pcrec] = PackageCacheRecord.from_objects(current_record, pcrec)        else:            index[pcrec] = pcrec",
        "labels_text": "Supplement the given index with package from the cache param index The package index to supplement"
    },
    {
        "input_text": "summarize: def _make_virtual_package(    name: str, version: str | None = None, build_string: str | None = None) -> PackageRecord:        return PackageRecord(        package_type=PackageType.VIRTUAL_SYSTEM,        name=name,        version=version or \"0\",        build_string=build_string or \"0\",        channel=\"@\",        subdir=context.subdir,        md5=\"12345678901234567890123456789012\",        build_number=0,        fn=name,    )",
        "labels_text": "Create a virtual package record param name The name of the virtual package param version The version of the virtual package default to param buildstring The build string of the virtual package default to return A PackageRecord representing the virtual package"
    },
    {
        "input_text": "summarize: def _supplement_index_with_features(    index: dict[PackageRecord, PackageRecord], features: list[str] = []) -> None:        for feature in chain(context.track_features, features):        rec = make_feature_record(feature)        index[rec] = rec",
        "labels_text": "Supplement the given index with virtual feature record param index The package index to supplement param feature A list of feature name to add to the index"
    },
    {
        "input_text": "summarize: def _supplement_index_with_system(index: dict[PackageRecord, PackageRecord]) -> None:        for package in context.plugin_manager.get_virtual_packages():        rec = _make_virtual_package(f\"__{package.name}\", package.version, package.build)        index[rec] = rec",
        "labels_text": "Loads and populates virtual package record from conda plugins and add them to the provided index unless there is a naming conflict param index The package index to supplement"
    },
    {
        "input_text": "summarize: def calculate_channel_urls(    channel_urls: tuple[str] = (),    prepend: bool = True,    platform: str | None = None,    use_local: bool = False,) -> list[str]:        if use_local:        channel_urls = [\"local\"] + list(channel_urls)    if prepend:        channel_urls += context.channels    subdirs = (platform, \"noarch\") if platform is not None else context.subdirs    return all_channel_urls(channel_urls, subdirs=subdirs)",
        "labels_text": "Calculate the full list of channel URLs to use based on the given parameter param channelurls Initial list of channel URLs param prepend Whether to prepend default channel to the list param platform The target platform for the channel param uselocal Whether to include the local channel return The calculated list of channel URLs"
    },
    {
        "input_text": "summarize: def __init__(self, link_prefs):                self.link_precs = link_prefs        log.debug(            \"instantiating ProgressiveFetchExtract with\\n  %s\\n\",            \"\\n  \".join(pkg_rec.dist_str() for pkg_rec in link_prefs),        )        self.paired_actions = {}  # Map[pref, Tuple(CacheUrlAction, ExtractPackageAction)]        self._prepared = False        self._executed = False",
        "labels_text": "Args linkprefs tuplePackageRecord A sequence of classPackageRecords to ensure available in a known package cache typically for a followon classUnlinkLinkTransaction Here available mean the package tarball is both downloaded and extracted to a package directory"
    },
    {
        "input_text": "summarize: def do_extract_action(prec, extract_action, progress_bar):        # pass None if already extracted (simplifies code)    if not extract_action:        return prec    extract_action.verify()    # currently unable to do updates on extract;    # likely too fast to bother    extract_action.execute(None)    progress_bar.update_to(1.0)    return prec",
        "labels_text": "This function get called after docacheaction completes"
    },
    {
        "input_text": "summarize: def _python_pkg_record(self):                return next(            (                prefix_record                for prefix_record in self.__prefix_records.values()                if prefix_record.name == \"python\"            ),            None,        )",
        "labels_text": "Return the prefix record for the package python"
    },
    {
        "input_text": "summarize: def delete_prefix_from_linked_data(path: str | os.PathLike | Path) -> bool:        path = Path(path)    for prefix in sorted(PrefixData._cache_, reverse=True):        try:            path.relative_to(prefix)            del PrefixData._cache_[prefix]            return True        except ValueError:            # ValueError: path is not relative to prefix            continue    return False",
        "labels_text": "Here path may be a complete prefix or a dist inside a prefix"
    },
    {
        "input_text": "summarize: def _repo(self) -> RepoInterface:                return self.repo_fetch._repo",
        "labels_text": "Changes a we mutate selfrepodatafn"
    },
    {
        "input_text": "summarize: def repo_fetch(self) -> RepodataFetch:                return RepodataFetch(            Path(self.cache_path_base),            self.channel,            self.repodata_fn,            repo_interface_cls=self.RepoInterface,        )",
        "labels_text": "Object to get repodata Not cached since selfrepodatafn is mutable Replaces selfrepo selfrepocache"
    },
    {
        "input_text": "summarize: def cache_path_state(self):                return Path(            self.cache_path_base            + (\"1\" if context.use_only_tar_bz2 else \"\")            + CACHE_STATE_SUFFIX        )",
        "labels_text": "Outofband etag and other state needed by the RepoInterface"
    },
    {
        "input_text": "summarize: def _load(self):                try:            fetcher = self.repo_fetch            repodata, state = fetcher.fetch_latest_parsed()            return self._process_raw_repodata(repodata, state)        except UnavailableInvalidChannel:            if self.repodata_fn != REPODATA_FN:                self.repodata_fn = REPODATA_FN                return self._load()            else:                raise",
        "labels_text": "Try to load repodata If eg we are downloading currentrepodatajson fall back to repodatajson when the former is unavailable"
    },
    {
        "input_text": "summarize: def _process_raw_repodata_str(        self,        raw_repodata_str,        state: RepodataState | None = None,    ):                json_obj = json.loads(raw_repodata_str or \"{}\")        return self._process_raw_repodata(json_obj, state=state)",
        "labels_text": "State contains information that wa previously inband in rawrepodatastr"
    },
    {
        "input_text": "summarize: def from_yaml(yamlstr, **kwargs):        data = yaml_safe_load(yamlstr)    filename = kwargs.get(\"filename\")    if data is None:        raise EnvironmentFileEmpty(filename)    data = validate_keys(data, kwargs)    if kwargs is not None:        for key, value in kwargs.items():            data[key] = value    _expand_channels(data)    return Environment(**data)",
        "labels_text": "Load and return a Environment from a given yaml string"
    },
    {
        "input_text": "summarize: def _expand_channels(data):        data[\"channels\"] = [        os.path.expandvars(channel) for channel in data.get(\"channels\", [])    ]",
        "labels_text": "Expands Environment variable for the channel found in the yaml data"
    },
    {
        "input_text": "summarize: def add(self, package_name):                self.raw.append(package_name)        self.parse()",
        "labels_text": "Add a package to the Environment"
    },
    {
        "input_text": "summarize: def add_channels(self, channels):                self.channels = list(unique(chain.from_iterable((channels, self.channels))))",
        "labels_text": "Add channel to the Environment"
    },
    {
        "input_text": "summarize: def remove_channels(self):                self.channels = []",
        "labels_text": "Remove all channel from the Environment"
    },
    {
        "input_text": "summarize: def to_dict(self, stream=None):                d = {\"name\": self.name}        if self.channels:            d[\"channels\"] = self.channels        if self.dependencies:            d[\"dependencies\"] = self.dependencies.raw        if self.variables:            d[\"variables\"] = self.variables        if self.prefix:            d[\"prefix\"] = self.prefix        if stream is None:            return d        stream.write(json.dumps(d))",
        "labels_text": "Convert information related to the Environment into a dictionary"
    },
    {
        "input_text": "summarize: def to_yaml(self, stream=None):                d = self.to_dict()        out = yaml_safe_dump(d, stream)        if stream is None:            return out",
        "labels_text": "Convert information related to the Environment into a yaml string"
    },
    {
        "input_text": "summarize: def save(self):                with open(self.filename, \"wb\") as fp:            self.to_yaml(stream=fp)",
        "labels_text": "Save the Environment data to a yaml file"
    },
    {
        "input_text": "summarize: def get_filename(filename):        url_scheme = filename.split(\"://\", 1)[0]    if url_scheme in CONDA_SESSION_SCHEMES:        return filename    else:        return abspath(expanduser(expandvars(filename)))",
        "labels_text": "Expand filename if local path or return the url"
    },
    {
        "input_text": "summarize: def get_pip_installed_packages(stdout):        m = re.search(r\"Successfully installed\\ (.*)\", stdout)    if m:        return m.group(1).strip().split()    else:        return None",
        "labels_text": "Return the list of pip package installed based on the command output"
    },
    {
        "input_text": "summarize: def get_installer(name):        try:        return importlib.import_module(f\"conda.env.installers.{name}\")    except ImportError:        raise InvalidInstaller(name)",
        "labels_text": "Gets the installer for the given environment Raises InvalidInstaller if unable to load installer"
    },
    {
        "input_text": "summarize: def dry_run(specs, args, env, *_, **kwargs):        solver = _solve(tempfile.mkdtemp(), specs, args, env, *_, **kwargs)    pkgs = solver.solve_final_state()    solved_env = Environment(        name=env.name, dependencies=[str(p) for p in pkgs], channels=env.channels    )    return solved_env",
        "labels_text": "Do a dry run of the environment solve"
    },
    {
        "input_text": "summarize: def can_handle(self) -> bool:                # TODO: log information about trying to find the package in binstar.org        if self.valid_name():            if not self.binstar:                self.msg = (                    \"Anaconda Client is required to interact with anaconda.org or an \"                    \"Anaconda API. Please run `conda install anaconda-client -n base`.\"                )                return False            return self.package is not None and self.valid_package()        return False",
        "labels_text": "Validates loader can process environment definition return True or False"
    },
    {
        "input_text": "summarize: def valid_name(self) -> bool:                if re.match(\"^(.+)/(.+)$\", str(self.name)) is not None:            return True        elif self.name is None:            self.msg = \"Can't process without a name\"        else:            self.msg = f\"Invalid name {self.name!r}, try the format: user/package\"        return False",
        "labels_text": "Validates name return True or False"
    },
    {
        "input_text": "summarize: def valid_package(self) -> bool:                return len(self.file_data) > 0",
        "labels_text": "Returns True if package ha an environment file return True or False"
    },
    {
        "input_text": "summarize: def detect(    name: str | None = None,    filename: str | None = None,    directory: str | None = None,) -> SpecTypes:        if filename is not None:        spec_class = get_spec_class_from_file(filename)        spec = spec_class(name=name, filename=filename, directory=directory)        if spec.can_handle():            return spec    raise SpecNotFound(spec.msg)",
        "labels_text": "Return the appropriate spec type to use raise SpecNotFound Raised if no suitable spec class could be found given the input raise EnvironmentFileExtensionNotValid EnvironmentFileNotFound"
    },
    {
        "input_text": "summarize: def filter(self, record):                if not isinstance(record.msg, str):            # This should always be the case but it's not checked so            # we avoid any potential logging errors.            return True        if record.args:            record.msg = record.msg % record.args            record.args = None        record.msg = self.TOKEN_REPLACE(record.msg)        return True",
        "labels_text": "Since Python s getMessage is incapable of handling any string that are not unicode when it interpolates the message with the argument we fix that here by doing it ourselves At the same time we replace token in the argument which wa not happening until now"
    },
    {
        "input_text": "summarize: def __init__(self, sys_stream):                super().__init__(getattr(sys, sys_stream))        self.sys_stream = sys_stream        del self.stream",
        "labels_text": "Args sysstream stream name either stdout or stderr attribute of module sys"
    },
    {
        "input_text": "summarize: def emit(self, record):                try:            msg = self.format(record)            stream = self.stream            fs = \"%s\"            stream.write(fs % msg)            terminator = getattr(record, \"terminator\", self.terminator)            stream.write(terminator)            self.flush()        # How does conda handle Ctrl-C? Find out..        # except (KeyboardInterrupt, SystemExit):        #     raise        except Exception:            self.handleError(record)",
        "labels_text": "Emit a record If a formatter is specified it is used to format the record The record is then written to the stream with a trailing newline If exception information is present it is formatted using tracebackprintexception and appended to the stream If the stream ha an encoding attribute it is used to determine how to do the output to the stream"
    },
    {
        "input_text": "summarize: def get_channel_name_from_url(url: str) -> str | None:        return Channel.from_url(url).canonical_name",
        "labels_text": "Given a URL determine the channel it belongs to and return it name"
    },
    {
        "input_text": "summarize: def get_session_storage_key(auth) -> str:        if auth is None:        return \"default\"    if isinstance(auth, tuple):        return hash(auth)    auth_type = type(auth)    return f\"{auth_type.__module__}.{auth_type.__qualname__}::{auth.channel_name}\"",
        "labels_text": "Function that determines which storage key to use for our CondaSession object caching"
    },
    {
        "input_text": "summarize: def close(self):                # Currently this is a no-op.        pass",
        "labels_text": "Dispose of any internal state"
    },
    {
        "input_text": "summarize: def retr(self, path, request):                data = BytesIO()        # To ensure the BytesIO gets cleaned up, we need to alias its close        # method. See self.list().        data.release_conn = data.close        code = self.conn.retrbinary(\"RETR \" + path, data_callback_factory(data))        response = build_binary_response(request, data, code)        # Close the connection.        self.conn.close()        return response",
        "labels_text": "Executes the FTP RETR command on the given path"
    },
    {
        "input_text": "summarize: def nlst(self, path, request):                data = StringIO()        # Alias the close method.        data.release_conn = data.close        self.conn.cwd(path)        code = self.conn.retrbinary(\"NLST\", data_callback_factory(data))        # When that call has finished executing, we'll have all our data.        response = build_text_response(request, data, code)        # Close the connection.        self.conn.close()        return response",
        "labels_text": "Executes the FTP NLST command on the given path"
    },
    {
        "input_text": "summarize: def get_host_and_path_from_url(self, request):                url = request.url        parsed = urlparse(url)        path = parsed.path        # If there is a slash on the front of the path, chuck it.        if path[0] == \"/\":            path = path[1:]        host = parsed.hostname        port = parsed.port or 0        return (host, port, path)",
        "labels_text": "Given a PreparedRequest object split the URL in such a manner a to determine the host and the path This is a separate method to wrap some of urlparses craziness"
    },
    {
        "input_text": "summarize: def data_callback_factory(variable):        def callback(data):        variable.write(data)    return callback",
        "labels_text": "Returns a callback suitable for use by the FTP library This callback will repeatedly save data into the variable provided to this function This variable should be a filelike structure"
    },
    {
        "input_text": "summarize: def build_text_response(request, data, code):        return build_response(request, data, code, \"ascii\")",
        "labels_text": "Build a response for textual data"
    },
    {
        "input_text": "summarize: def build_binary_response(request, data, code):        return build_response(request, data, code, None)",
        "labels_text": "Build a response for data whose encoding is unknown"
    },
    {
        "input_text": "summarize: def make_menu(prefix, file_path, remove=False):        try:        import menuinst        menuinst.install(            join(prefix, win_path_ok(file_path)),            remove=remove,            prefix=prefix,            root_prefix=context.root_prefix,        )    except Exception:        stdoutlog.error(\"menuinst Exception\", exc_info=True)",
        "labels_text": "Create crossplatform menu item eg Windows Start Menu Passes all menu config file PREFIXMenujson to menuinstinstall removeTrue will remove the menu item"
    },
    {
        "input_text": "summarize: def path_is_clean(path):        clean = not exists(path)    if not clean:        for root, dirs, fns in os.walk(path):            for fn in fns:                if not (                    fnmatch.fnmatch(fn, \"*.conda_trash*\")                    or fnmatch.fnmatch(fn, \"*\" + CONDA_TEMP_EXTENSION)                ):                    return False    return True",
        "labels_text": "Sometimes we cant completely remove a path because file are considered in use by python hardlinking confusion For our test it is sufficient that either the folder doesnt exist or nothing but temporary file copy are left"
    },
    {
        "input_text": "summarize: def win_hard_link(src, dst):                if not CreateHardLink(dst, src, None):            raise CondaOSError(f\"win32 hard link failed\\n  src: {src}\\n  dst: {dst}\")",
        "labels_text": "Equivalent to oslink using the win CreateHardLink call"
    },
    {
        "input_text": "summarize: def win_soft_link(src, dst):                if CreateSymbolicLink is None:            raise CondaOSError(\"win32 soft link not supported\")        if not CreateSymbolicLink(dst, src, isdir(src)):            raise CondaOSError(f\"win32 soft link failed\\n  src: {src}\\n  dst: {dst}\")",
        "labels_text": "Equivalent to ossymlink using the win CreateSymbolicLink call"
    },
    {
        "input_text": "summarize: def islink(path):                return is_reparse_point(path) and is_symlink(path)",
        "labels_text": "Determine if the given path is a symlink"
    },
    {
        "input_text": "summarize: def _patch_path(path):        r  # NOQA        if path.startswith(\"\\\\\\\\?\\\\\"):            return path        path = abspath(path)        if not path[1] == \":\":            # python doesn't include the drive letter, but \\\\?\\ requires it            path = getcwd()[:2] + path        return \"\\\\\\\\?\\\\\" + path",
        "labels_text": "Paths have a max length of apiMAXPATH character If a target path is longer than that it need to be made absolute and prepended with in order to work with API call See httpmsdnmicrosoftcomenuslibraryaavvsaspx for detail"
    },
    {
        "input_text": "summarize: def local_format(string):                context = inspect.currentframe().f_back.f_locals        return string.format_map(context)",
        "labels_text": "Format the string using variable in the caller local namespace codeblock pycon a localformata"
    },
    {
        "input_text": "summarize: def is_symlink(path):                path = _patch_path(path)        try:            return _is_symlink(next(find_files(path)))        except OSError as orig_error:  # NOQA            tmpl = \"Error accessing {path}: {orig_error.message}\"            raise OSError(local_format(tmpl))",
        "labels_text": "Assuming path is a reparse point determine if it a symlink"
    },
    {
        "input_text": "summarize: def is_reparse_point(path):                res = GetFileAttributes(path)        return res != INVALID_FILE_ATTRIBUTES and bool(            res & FILE_ATTRIBUTE_REPARSE_POINT        )",
        "labels_text": "Determine if the given path is a reparse point Return False if the file doe not exist or the file attribute cannot be determined"
    },
    {
        "input_text": "summarize: def _lock_noop(fd):        yield",
        "labels_text": "When locking is not available"
    },
    {
        "input_text": "summarize: def yield_lines(path):        try:        with open_utf8(path) as fh:            for line in fh:                line = line.strip()                if not line or line.startswith(\"#\"):                    continue                yield line    except OSError as e:        if e.errno == ENOENT:            pass        else:            raise",
        "labels_text": "Generator function for line in file Empty generator if path doe not exist Args path str path to file Returns iterator each line in file not starting with"
    },
    {
        "input_text": "summarize: def rename_context(source: str, destination: str | None = None, dry_run: bool = False):        if destination is None:        destination = tempfile.mkdtemp()    if dry_run:        print(f\"{DRY_RUN_PREFIX} rename_context {source} > {destination}\")        yield        return    try:        rename(source, destination, force=True)        yield    except Exception as exc:        # Error occurred, roll back change        rename(destination, source, force=True)        raise exc",
        "labels_text": "Used for removing a directory when there are dependent action ie you need to ensure other action succeed before removing it Example with renamecontextdirectory Do dependent action here"
    },
    {
        "input_text": "summarize: def repodata(self, state: dict) -> str:                ...",
        "labels_text": "Given a mutable state dictionary with information about the cache return repodatajson or currentrepodatajson a a str This function also update state which is expected to be saved by the caller"
    },
    {
        "input_text": "summarize: def mod(self) -> str:                return self.get(LAST_MODIFIED_KEY) or \"\"",
        "labels_text": "LastModified header or"
    },
    {
        "input_text": "summarize: def etag(self) -> str:                return self.get(ETAG_KEY) or \"\"",
        "labels_text": "Etag header or"
    },
    {
        "input_text": "summarize: def cache_control(self) -> str:                return self.get(CACHE_CONTROL_KEY) or \"\"",
        "labels_text": "CacheControl header or"
    },
    {
        "input_text": "summarize: def clear_has_format(self, format: str):                key = f\"has_{format}\"        self.pop(key, None)",
        "labels_text": "Remove hasformat instead of setting to False"
    },
    {
        "input_text": "summarize: def should_check_format(self, format: str) -> bool:                has, when = self.has_format(format)        return (            has is True            or isinstance(when, datetime.datetime)            and datetime.datetime.now(tz=datetime.timezone.utc) - when            > CHECK_ALTERNATE_FORMAT_INTERVAL        )",
        "labels_text": "Return True if named format should be attempted"
    },
    {
        "input_text": "summarize: def __init__(self, base, repodata_fn):                cache_path_base = pathlib.Path(base)        self.cache_dir = cache_path_base.parent        self.name = cache_path_base.name        # XXX can we skip repodata_fn or include the full url for debugging        self.repodata_fn = repodata_fn        self.state = RepodataState(            self.cache_path_json, self.cache_path_state, repodata_fn        )",
        "labels_text": "base directory and filename prefix for cache eg cachedirabc writes cachedirabcjson"
    },
    {
        "input_text": "summarize: def cache_path_state(self):                return self.cache_path_json.with_suffix(CACHE_STATE_SUFFIX)",
        "labels_text": "Outofband etag and other state needed by the RepoInterface"
    },
    {
        "input_text": "summarize: def load_state(self):                try:            self.load(state_only=True)        except (FileNotFoundError, json.JSONDecodeError) as e:            if isinstance(e, json.JSONDecodeError):                log.warning(f\"{e.__class__.__name__} loading {self.cache_path_state}\")            self.state.clear()        return self.state",
        "labels_text": "Update selfstate without reading repodatajson Return selfstate"
    },
    {
        "input_text": "summarize: def save(self, data: str):                temp_path = self.cache_dir / f\"{self.name}.{os.urandom(2).hex()}.tmp\"        try:            with temp_path.open(\"x\") as temp:  # exclusive mode, error if exists                temp.write(data)            return self.replace(temp_path)        finally:            try:                temp_path.unlink()            except OSError:                pass",
        "labels_text": "Write data to repodatajson cache path synchronize state"
    },
    {
        "input_text": "summarize: def refresh(self, refresh_ns=0):                # Note this is not thread-safe.        with self.lock() as state_file:            # \"a+\" creates the file if necessary, does not trunctate file.            state_file.seek(0)            state_file.truncate()            self.state[\"refresh_ns\"] = refresh_ns or time.time_ns()            state_file.write(json.dumps(dict(self.state), indent=2))",
        "labels_text": "Update access time in cache info file to indicate a HTTP Not Modified response"
    },
    {
        "input_text": "summarize: def lock(self, mode=\"a+\"):                with self.cache_path_state.open(mode) as state_file, lock(state_file):            yield state_file",
        "labels_text": "Lock infojson file Hold lock while modifying related file mode a then seek to writecreate r to read"
    },
    {
        "input_text": "summarize: def fetch_latest_parsed(self) -> tuple[dict, RepodataState]:                parsed, state = self.fetch_latest()        if isinstance(parsed, str):            try:                return json.loads(parsed), state            except json.JSONDecodeError as e:                e.args = (                    f'{e.args[0]}; got \"{parsed[:ERROR_SNIPPET_LENGTH]}\"',                    *e.args[1:],                )                raise        else:            return parsed, state",
        "labels_text": "Retrieve parsed latest or latestcached repodata a a dict update cache return repodata content state including cache header"
    },
    {
        "input_text": "summarize: def fetch_latest_path(self) -> tuple[Path, RepodataState]:                _, state = self.fetch_latest()        return self.cache_path_json, state",
        "labels_text": "Retrieve latest or latestcached repodata update cache return pathlibPath to uncompressed repodata content RepodataState"
    },
    {
        "input_text": "summarize: def cache_path_state(self):                return self.repo_cache.cache_path_state",
        "labels_text": "Outofband etag and other state needed by the RepoInterface"
    },
    {
        "input_text": "summarize: def _repo(self) -> RepoInterface:                return self.repo_interface_cls(            self.url_w_credentials,            repodata_fn=self.repodata_fn,            cache=self.repo_cache,        )",
        "labels_text": "Changes a we mutate selfrepodatafn"
    },
    {
        "input_text": "summarize: def keyed_hash(data: bytes, key: bytes):        return blake2b(data, key=key, digest_size=DIGEST_SIZE)",
        "labels_text": "Keyed hash"
    },
    {
        "input_text": "summarize: def line_and_pos(lines: Iterable[bytes], pos=0) -> Iterator[tuple[int, bytes]]:    r    for line in lines:        yield pos, line        pos += len(line) + 1",
        "labels_text": "param line iterator over input split by n with n removed param po initial position"
    },
    {
        "input_text": "summarize: def terminate(self):                _, _, iv = self[-1]        self.add(iv)        return self",
        "labels_text": "Add trailing checksum to buffer return self"
    },
    {
        "input_text": "summarize: def write(self, path: Path):                with Path(path).open(\"w\", encoding=\"utf-8\", newline=\"\\n\") as p:            return p.write(\"\\n\".join(b[1] for b in self))",
        "labels_text": "Write buffer to path"
    },
    {
        "input_text": "summarize: def body(self):                return self[1:-2]",
        "labels_text": "All line except the first and last two"
    },
    {
        "input_text": "summarize: def penultimate(self):                return self[-2]",
        "labels_text": "Nexttolast line Should contain the footer"
    },
    {
        "input_text": "summarize: def last(self):                return self[-1]",
        "labels_text": "Last line Should contain the trailing checksum"
    },
    {
        "input_text": "summarize: def hash():        return blake2b(digest_size=DIGEST_SIZE)",
        "labels_text": "Ordinary hash"
    },
    {
        "input_text": "summarize: def format_hash(hash):        return hash[:16] + \"\\N{HORIZONTAL ELLIPSIS}\"",
        "labels_text": "Abbreviate hash for formatting"
    },
    {
        "input_text": "summarize: def build_headers(json_path: pathlib.Path, state: RepodataState):        headers = {}    # simplify if we require state to be empty when json_path is missing.    if json_path.exists():        etag = state.get(\"_etag\")        if etag:            headers[\"if-none-match\"] = etag    return headers",
        "labels_text": "Caching header for a path and state"
    },
    {
        "input_text": "summarize: def _is_http_error_most_400_codes(e: HTTPError) -> bool:        if e.response is None:  # 404 e.response is falsey        return False    status_code = e.response.status_code    return 400 <= status_code < 500 and status_code != 416",
        "labels_text": "Determine whether the HTTPError is an HTTP error code except for"
    },
    {
        "input_text": "summarize: def repodata(self, state: dict | RepodataState) -> str | None:                self.repodata_parsed(state)        raise RepodataOnDisk()",
        "labels_text": "Fetch newest repodata if necessary Always writes to cachepathjson"
    },
    {
        "input_text": "summarize: def get_channel_objs(ctx: Context):        return tuple(Channel(chn) for chn in ctx.channels)",
        "labels_text": "Return current channel a Channel object"
    },
    {
        "input_text": "summarize: def match(self, rec):                if isinstance(rec, dict):            # TODO: consider AttrDict instead of PackageRecord            from .records import PackageRecord            rec = PackageRecord.from_objects(rec)        for field_name, v in self._match_components.items():            if not self._match_individual(rec, field_name, v):                return False        return True",
        "labels_text": "Accepts a PackageRecord or a dict and match can pull from any field in that record Returns True for a match and False for no match"
    },
    {
        "input_text": "summarize: def _parse_version_plus_build(v_plus_b):        parts = re.search(        r\"((?:.+?)[^><!,|]?)(?:(?<![=!|,<>~])(?:[ =])([^-=,|<>~]+?))?$\", v_plus_b    )    if parts:        version, build = parts.groups()        build = build and build.strip()    else:        version, build = v_plus_b, None    return version and version.replace(\" \", \"\"), build",
        "labels_text": "This should reliably pull the build string out of a version build string combo Examples parseversionplusbuild parseversionplusbuild parseversionplusbuild py py parseversionplusbuild py py parseversionplusbuild None parseversionplusbuild None parseversionplusbuild openblas openblas parseversionplusbuild"
    },
    {
        "input_text": "summarize: def _parse_legacy_dist(dist_str):        dist_str, _ = strip_pkg_extension(dist_str)    name, version, build = dist_str.rsplit(\"-\", 2)    return name, version, build",
        "labels_text": "Examples parselegacydistlicensepytarbz license py parselegacydistlicensepy license py"
    },
    {
        "input_text": "summarize: def exact_value(self):                raise NotImplementedError()",
        "labels_text": "If the match value is an exact specification return the value Otherwise return None"
    },
    {
        "input_text": "summarize: def _remove_node(self, node):                graph = self.graph        if node not in graph:            raise KeyError(f\"node {node} does not exist\")        graph.pop(node)        self.spec_matches.pop(node, None)        for node, edges in graph.items():            if node in edges:                edges.remove(node)",
        "labels_text": "Removes this node and all edge referencing it"
    },
    {
        "input_text": "summarize: def _toposort_pop_key(graph):                node_with_fewest_parents = sorted(            (len(parents), node.dist_str(), node) for node, parents in graph.items()        )[0][2]        graph.pop(node_with_fewest_parents)        for parents in graph.values():            parents.discard(node_with_fewest_parents)        return node_with_fewest_parents",
        "labels_text": "Pop an item from the graph that ha the fewest parent In the case of a tie use the node with the alphabeticallyfirst package name"
    },
    {
        "input_text": "summarize: def normalized_version(version: str) -> VersionOrder:        return VersionOrder(version)",
        "labels_text": "Parse a version string and return VersionOrder object"
    },
    {
        "input_text": "summarize: def is_notice_response_cache_expired(    channel_notice_response: ChannelNoticeResponse,) -> bool:        now = datetime.now(timezone.utc)    def is_channel_notice_expired(expired_at: datetime | None) -> bool:                if expired_at is None:            return True        return expired_at < now    return any(        is_channel_notice_expired(chn.expired_at)        for chn in channel_notice_response.notices    )",
        "labels_text": "This check the content of the cache response to see if it is expired If for whatever reason we encounter an exception while parsing the individual message we assume an invalid cache and return true"
    },
    {
        "input_text": "summarize: def get_notices_cache_dir() -> Path:        cache_dir = user_cache_dir(APP_NAME, appauthor=APP_NAME)    return Path(cache_dir).joinpath(NOTICES_CACHE_SUBDIR)",
        "labels_text": "Returns the location of the notice cache directory a a Path object"
    },
    {
        "input_text": "summarize: def get_notices_cache_file() -> Path:        cache_dir = get_notices_cache_dir()    cache_file = cache_dir.joinpath(NOTICES_CACHE_FN)    if not cache_file.is_file():        with open(cache_file, \"w\") as fp:            fp.write(\"\")    return cache_file",
        "labels_text": "Returns the location of the notice cache file a a Path object"
    },
    {
        "input_text": "summarize: def write_notice_response_to_cache(    channel_notice_response: ChannelNoticeResponse, cache_dir: Path) -> None:        cache_key = ChannelNoticeResponse.get_cache_key(        channel_notice_response.url, cache_dir    )    with open(cache_key, \"w\") as fp:        json.dump(channel_notice_response.json_data, fp)",
        "labels_text": "Writes our notice data to our local cache location"
    },
    {
        "input_text": "summarize: def get_viewed_channel_notice_ids(    cache_file: Path, channel_notices: Sequence[ChannelNotice]) -> set[str]:        notice_ids = {chn.id for chn in channel_notices}    with open(cache_file) as fp:        contents: str = fp.read()    contents_unique = set(filter(None, set(contents.splitlines())))    return notice_ids.intersection(contents_unique)",
        "labels_text": "Return the id of the channel notice which have already been seen"
    },
    {
        "input_text": "summarize: def get_channel_name_and_urls(    channels: Sequence[Channel | MultiChannel],) -> list[tuple[ChannelUrl, ChannelName]]:        channel_name_and_urls = []    for channel in channels:        name = channel.name or channel.location        for url in channel.base_urls:            full_url = url.rstrip(\"/\")            channel_name_and_urls.append((f\"{full_url}/{NOTICES_FN}\", name))    return channel_name_and_urls",
        "labels_text": "Return a sequence of Channel URL and name tuples This function handle both Channel and MultiChannel object type"
    },
    {
        "input_text": "summarize: def filter_notices(    channel_notices: Sequence[ChannelNotice],    limit: int | None = None,    exclude: set[str] | None = None,) -> Sequence[ChannelNotice]:        if exclude:        channel_notices = tuple(            channel_notice            for channel_notice in channel_notices            if channel_notice.id not in exclude        )    if limit is not None:        channel_notices = channel_notices[:limit]    return channel_notices",
        "labels_text": "Perform filtering action for the provided sequence of ChannelNotice object"
    },
    {
        "input_text": "summarize: def is_channel_notices_enabled(ctx: Context) -> bool:        return ctx.number_channel_notices > 0 and not ctx.offline and not ctx.json",
        "labels_text": "Determines whether channel notice are enabled and therefore displayed when invoking the notice command decorator This only happens when offline is False numberchannelnotices is greater than Args ctx The conda context object"
    },
    {
        "input_text": "summarize: def is_channel_notices_cache_expired() -> bool:        cache_file = cache.get_notices_cache_file()    cache_file_stat = cache_file.stat()    now = time.time()    seconds_since_checked = now - cache_file_stat.st_mtime    return seconds_since_checked >= NOTICES_DECORATOR_DISPLAY_INTERVAL",
        "labels_text": "Checks to see if the notice cache file we use to keep track of displayed notice is expired This involves checking the mtime attribute of the file Anything older than what is specified a the NOTICESDECORATORDISPLAYINTERVAL is considered expired"
    },
    {
        "input_text": "summarize: def _parse_notice_level(level: str | None) -> NoticeLevel:                try:            return NoticeLevel(level)        except ValueError:            # If we get an invalid value, rather than fail, we simply use a reasonable default            return NoticeLevel(NoticeLevel.INFO)",
        "labels_text": "We use this to validate notice level and provide reasonable default if any are invalid"
    },
    {
        "input_text": "summarize: def _parse_iso_timestamp(iso_timestamp: str | None) -> datetime | None:                if iso_timestamp is None:            return None        try:            return datetime.fromisoformat(iso_timestamp)        except ValueError:            return None",
        "labels_text": "Parse ISO timestamp and fail over to a default value of none"
    },
    {
        "input_text": "summarize: def get_cache_key(cls, url: str, cache_dir: Path) -> Path:                bytes_filename = url.encode()        sha256_hash = hashlib.sha256(bytes_filename)        cache_filename = f\"{sha256_hash.hexdigest()}.json\"        return cache_dir.joinpath(cache_filename)",
        "labels_text": "Returns where this channel response will be cached by hashing the URL"
    },
    {
        "input_text": "summarize: def print_notice_message(notice: ChannelNotice, indent: str = \"  \") -> None:        timestamp = f\"{notice.created_at:%c}\" if notice.created_at else \"\"    level = f\"[{notice.level}] -- {timestamp}\"    print(f\"{indent}{level}\\n{indent}{notice.message}\")",
        "labels_text": "Prints a single channel notice"
    },
    {
        "input_text": "summarize: def conda_solvers(self) -> Iterable[CondaSolver]:",
        "labels_text": "Register solver in conda Example codeblock python import logging from conda import plugins from condacore import solve log logginggetLoggername class VerboseSolversolveSolver def solvefinalstateself args kwargs loginfoMy verbose solver return supersolvefinalstateargs kwargs pluginshookimpl def condasolvers yield pluginsCondaSolver nameverboseclassic backendVerboseSolver return An iterable of solver entry"
    },
    {
        "input_text": "summarize: def conda_subcommands(self) -> Iterable[CondaSubcommand]:",
        "labels_text": "Register external subcommands in conda Example codeblock python from conda import plugins def examplecommandargs printThis is an example command pluginshookimpl def condasubcommands yield pluginsCondaSubcommand nameexample summaryexample command actionexamplecommand return An iterable of subcommand entry"
    },
    {
        "input_text": "summarize: def conda_virtual_packages(self) -> Iterable[CondaVirtualPackage]:",
        "labels_text": "Register virtual package in Conda Example codeblock python from conda import plugins pluginshookimpl def condavirtualpackages yield pluginsCondaVirtualPackage namemycustomos version buildx return An iterable of virtual package entry"
    },
    {
        "input_text": "summarize: def conda_pre_commands(self) -> Iterable[CondaPreCommand]:",
        "labels_text": "Register precommand function in conda Example codeblock python from conda import plugins def exampleprecommandcommand printprecommand action pluginshookimpl def condaprecommands yield pluginsCondaPreCommand nameexampleprecommand actionexampleprecommand runforinstall create"
    },
    {
        "input_text": "summarize: def conda_post_commands(self) -> Iterable[CondaPostCommand]:",
        "labels_text": "Register postcommand function in conda Example codeblock python from conda import plugins def examplepostcommandcommand printpostcommand action pluginshookimpl def condapostcommands yield pluginsCondaPostCommand nameexamplepostcommand actionexamplepostcommand runforinstall create"
    },
    {
        "input_text": "summarize: def conda_auth_handlers(self) -> Iterable[CondaAuthHandler]:",
        "labels_text": "Register a conda auth handler derived from the request API This plugin hook allows attaching request auth handler subclass eg when authenticating request against individual channel hosted at HTTPHTTPS service Example codeblock python import o from conda import plugins from requestsauth import AuthBase class EnvironmentHeaderAuthAuthBase def initself args kwargs selfusername osenvironEXAMPLECONDAAUTHUSERNAME selfpassword osenvironEXAMPLECONDAAUTHPASSWORD def callself request requestheadersXUsername selfusername requestheadersXPassword selfpassword return request pluginshookimpl def condaauthhandlers yield pluginsCondaAuthHandler nameenvironmentheaderauth authhandlerEnvironmentHeaderAuth"
    },
    {
        "input_text": "summarize: def conda_health_checks(self) -> Iterable[CondaHealthCheck]:",
        "labels_text": "Register health check for conda doctor This plugin hook allows you to add more health check to conda doctor that you can write to diagnose problem in your conda environment Check out the health check already shipped with conda for inspiration Example codeblock python from conda import plugins def examplehealthcheckprefix str verbose bool printThis is an example health check pluginshookimpl def condahealthchecks yield pluginsCondaHealthCheck nameexamplehealthcheck actionexamplehealthcheck"
    },
    {
        "input_text": "summarize: def conda_pre_solves(self) -> Iterable[CondaPreSolve]:",
        "labels_text": "Register presolve function in conda that are used in the general solver API before the solver process the package spec in search of a solution Example codeblock python from conda import plugins from condamodelsmatchspec import MatchSpec def examplepresolve specstoadd frozensetMatchSpec specstoremove frozensetMatchSpec printfAdding lenspecstoadd package printfRemoving lenspecstoremove package pluginshookimpl def condapresolves yield pluginsCondaPreSolve nameexamplepresolve actionexamplepresolve"
    },
    {
        "input_text": "summarize: def conda_post_solves(self) -> Iterable[CondaPostSolve]:",
        "labels_text": "Register postsolve function in conda that are used in the general solver API after the solver ha provided the package record to add or remove from the conda environment Example codeblock python from conda import plugins from condamodelsrecords import PackageRecord def examplepostsolve repodatafn str unlinkprecs tuplePackageRecord linkprecs tuplePackageRecord printfUninstalling lenunlinkprecs package printfInstalling lenlinkprecs package pluginshookimpl def condapostsolves yield pluginsCondaPostSolve nameexamplepostsolve actionexamplepostsolve"
    },
    {
        "input_text": "summarize: def conda_settings(self) -> Iterable[CondaSetting]:",
        "labels_text": "Register new setting The example below defines a simple string type parameter Example codeblock python from conda import plugins from condacommonconfiguration import PrimitiveParameter SequenceParameter pluginshookimpl def condasettings yield pluginsCondaSetting nameexampleoption descriptionThis is an example option parameterPrimitiveParameterdefaultvalue elementtypestr aliasesexampleoptionalias"
    },
    {
        "input_text": "summarize: def register(self, plugin, name: str | None = None) -> str | None:                try:            # register plugin but ignore ValueError since that means            # the plugin has already been registered            return super().register(plugin, name=name)        except ValueError:            return None        except Exception as err:            raise PluginError(                f\"Error while loading conda plugin: \"                f\"{name or self.get_canonical_name(plugin)} ({err})\"            ) from err",
        "labels_text": "Call methpluggyPluginManagerregister and return the result or ignore error raised except ValueError which mean the plugin had already been registered"
    },
    {
        "input_text": "summarize: def load_plugins(self, *plugins) -> int:                count = 0        for plugin in plugins:            if self.register(plugin):                count += 1        return count",
        "labels_text": "Load the provided list of plugins and fail gracefully on error The provided list of plugins can either be class or module with attrcondapluginshookimpl"
    },
    {
        "input_text": "summarize: def get_solvers(self) -> dict[str, CondaSolver]:                return {            solver_plugin.name.lower(): solver_plugin            for solver_plugin in self.get_hook_results(\"solvers\")        }",
        "labels_text": "Return a mapping from solver name to solver class"
    },
    {
        "input_text": "summarize: def get_auth_handler(self, name: str) -> type[AuthBase] | None:                auth_handlers = self.get_hook_results(\"auth_handlers\")        matches = tuple(            item for item in auth_handlers if item.name.lower() == name.lower().strip()        )        if len(matches) > 0:            return matches[0].handler        return None",
        "labels_text": "Get the auth handler with the given name or None"
    },
    {
        "input_text": "summarize: def get_settings(self) -> dict[str, ParameterLoader]:                return {            config_param.name.lower(): (config_param.parameter, config_param.aliases)            for config_param in self.get_hook_results(\"settings\")        }",
        "labels_text": "Return a mapping of plugin setting name to ParameterLoader class This method intentionally overwrites any duplicate that may be present"
    },
    {
        "input_text": "summarize: def invoke_pre_commands(self, command: str) -> None:                for hook in self.get_hook_results(\"pre_commands\"):            if command in hook.run_for:                hook.action(command)",
        "labels_text": "Invokes CondaPreCommandaction function registered with condaprecommands param command name of the command that is currently being invoked"
    },
    {
        "input_text": "summarize: def invoke_post_commands(self, command: str) -> None:                for hook in self.get_hook_results(\"post_commands\"):            if command in hook.run_for:                hook.action(command)",
        "labels_text": "Invokes CondaPostCommandaction function registered with condapostcommands param command name of the command that is currently being invoked"
    },
    {
        "input_text": "summarize: def disable_external_plugins(self) -> None:                for name, plugin in self.list_name_plugin():            if not name.startswith(\"conda.plugins.\") and not self.is_blocked(name):                self.set_blocked(name)",
        "labels_text": "Disables all currently registered plugins except builtin conda plugins"
    },
    {
        "input_text": "summarize: def invoke_pre_solves(        self,        specs_to_add: frozenset[MatchSpec],        specs_to_remove: frozenset[MatchSpec],    ) -> None:                for hook in self.get_hook_results(\"pre_solves\"):            hook.action(specs_to_add, specs_to_remove)",
        "labels_text": "Invokes CondaPreSolveaction function registered with condapresolves param specstoadd param specstoremove"
    },
    {
        "input_text": "summarize: def invoke_post_solves(        self,        repodata_fn: str,        unlink_precs: tuple[PackageRecord, ...],        link_precs: tuple[PackageRecord, ...],    ) -> None:                for hook in self.get_hook_results(\"post_solves\"):            hook.action(repodata_fn, unlink_precs, link_precs)",
        "labels_text": "Invokes CondaPostSolveaction function registered with condapostsolves param repodatafn param unlinkprecs param linkprecs"
    },
    {
        "input_text": "summarize: def load_settings(self) -> None:                for name, (parameter, aliases) in self.get_settings().items():            add_plugin_setting(name, parameter, aliases)",
        "labels_text": "Iterates through all registered setting and add them to the classcondacommonconfigurationPluginConfig class"
    },
    {
        "input_text": "summarize: def get_plugin_manager() -> CondaPluginManager:        plugin_manager = CondaPluginManager()    plugin_manager.add_hookspecs(CondaSpecs)    plugin_manager.load_plugins(        solvers,        *virtual_packages.plugins,        *subcommands.plugins,        health_checks,        *post_solves.plugins,    )    plugin_manager.load_entrypoints(spec_name)    return plugin_manager",
        "labels_text": "Get a cached version of the classcondapluginsmanagerCondaPluginManager instance with the builtin and entrypoints provided by the plugins loaded"
    },
    {
        "input_text": "summarize: def conda_solvers():        from ..core.solve import Solver    yield CondaSolver(        name=CLASSIC_SOLVER,        backend=Solver,    )",
        "labels_text": "The classic solver a shipped by default in conda"
    },
    {
        "input_text": "summarize: def execute(args: Namespace) -> None:        prefix = context.target_prefix    if not is_conda_environment(prefix):        raise EnvironmentLocationNotFound(prefix)    else:        print(f\"Environment Health Report for: {prefix}\\n\")        context.plugin_manager.invoke_health_checks(prefix, context.verbose)",
        "labels_text": "Run registered healthcheck plugins"
    },
    {
        "input_text": "summarize: def cached_cuda_version():        return cuda_version()",
        "labels_text": "A cached version of the cuda detection system"
    },
    {
        "input_text": "summarize: def suppress_resource_warning():        warnings.filterwarnings(\"ignore\", category=ResourceWarning)",
        "labels_text": "Suppress Unclosed Socket Warning It seems urllib keep a socket open to avoid costly recreation cost xref httpsgithubcomkennethreitzrequestsissues"
    },
    {
        "input_text": "summarize: def disable_channel_notices():        yaml_str = dals(            )    reset_context(())    rd = {        \"testdata\": YamlRawParameter.make_raw_parameters(            \"testdata\", yaml_round_trip_load(yaml_str)        )    }    context._set_raw_data(rd)    yield    reset_context(())",
        "labels_text": "Fixture that will set contextnumberchannelnotices to and then set it back to it original value This is also a good example of how to override value in the context object"
    },
    {
        "input_text": "summarize: def reset_conda_context():        yield    reset_context()",
        "labels_text": "Resets the context object after each test function is run"
    },
    {
        "input_text": "summarize: def temp_package_cache(tmp_path_factory):        pkgs_dir = tmp_path_factory.mktemp(\"pkgs\")    with env_vars(        {\"CONDA_PKGS_DIRS\": str(pkgs_dir)}, stack_callback=conda_tests_ctxt_mgmt_def_pol    ):        yield pkgs_dir",
        "labels_text": "Used to isolate package or index cache from other test"
    },
    {
        "input_text": "summarize: def parametrized_solver_fixture(    request: FixtureRequest,    monkeypatch: MonkeyPatch,) -> Iterable[Literal[\"libmamba\", \"classic\"]]:        yield from _solver_helper(request, monkeypatch, request.param)",
        "labels_text": "A parameterized fixture that set the solver backend to libmamba and classic for each test Its using autouseTrue so only import it in module that actually need it Note that skip and xfails need to be done inside the test body Decorators cant be used because they are evaluated before the fixture ha done it work So instead of pytestmarkskipifcontextsolver libmamba reason def testfoo Do def testfoo if contextsolver libmamba pytestskip"
    },
    {
        "input_text": "summarize: def make_temp_prefix(name=None, use_restricted_unicode=False, _temp_prefix=None):        if not _temp_prefix:        _temp_prefix = _get_temp_prefix(            name=name, use_restricted_unicode=use_restricted_unicode        )    try:        os.makedirs(_temp_prefix)    except:        pass    assert isdir(_temp_prefix)    return _temp_prefix",
        "labels_text": "When the env you are creating will be used to install Python on Windows only a restricted amount of Unicode will work and probably only those char in your current codepage so the character in UNICODECHARACTERSRESTRICTED should probably be randomly generated from that instead The problem here is that the current codepage need to be able to handle sysprefix otherwise ntpath will fall over"
    },
    {
        "input_text": "summarize: def index_packages(num):        # XXX: get_index_r_X should probably be refactored to avoid loading the environment like this.    get_index = getattr(helpers, f\"get_index_r_{num}\")    index, _ = get_index(context.subdir)    return list(index.values())",
        "labels_text": "Get the index data of the helpersgetindexr helper"
    },
    {
        "input_text": "summarize: def package_string_set(packages):        return {package_string(record) for record in packages}",
        "labels_text": "Transforms package container in package string set"
    },
    {
        "input_text": "summarize: def package_dict(packages):        return {record.name: record for record in packages}",
        "labels_text": "Transforms package container into a dictionary"
    },
    {
        "input_text": "summarize: def _channel_packages(self):                if isinstance(self.repo_packages, dict):            return self.repo_packages        return {\"test\": self.repo_packages}",
        "labels_text": "Helper that unfolds the repopackages into a dictionary"
    },
    {
        "input_text": "summarize: def _package_data(self, record):                data = {            key: value            for key, value in vars(record).items()            if key in self.REPO_DATA_KEYS        }        if \"subdir\" not in data:            data[\"subdir\"] = context.subdir        return data",
        "labels_text": "Turn record into data to be written in the JSON environmentrepo file"
    },
    {
        "input_text": "summarize: def solver_class(self) -> type[Solver]:                raise NotImplementedError",
        "labels_text": "Class under test"
    },
    {
        "input_text": "summarize: def assert_unsatisfiable(self, exc_info, entries):                assert issubclass(exc_info.type, UnsatisfiableError)        if exc_info.type is UnsatisfiableError:            assert (                sorted(                    tuple(map(str, entries)) for entries in exc_info.value.unsatisfiable                )                == entries            )",
        "labels_text": "Helper to assert that a pyclasscondaexceptionsUnsatisfiableError instance a a the specified set of unsatisfiable specification"
    },
    {
        "input_text": "summarize: def conda_cli(capsys: CaptureFixture) -> Iterator[CondaCLIFixture]:        yield CondaCLIFixture(capsys)",
        "labels_text": "Fixture returning CondaCLIFixture instance"
    },
    {
        "input_text": "summarize: def __call__(        self,        name: str | None = None,        prefix: str | None = None,        suffix: str | None = None,    ) -> Path:                prefix = prefix or \"\"        name = name or uuid.uuid4().hex        suffix = suffix or \"\"        return self.tmp_path / (prefix + name + suffix)",
        "labels_text": "Unique nonexistent path factory Extends pytests tmppath fixture with a new unique nonexistent path for usage in case where we need a temporary path that doesnt exist yet param name Path name to append to tmppath param prefix Prefix to prepend to unique name generated param suffix Suffix to append to unique name generated return A new unique path"
    },
    {
        "input_text": "summarize: def path_factory(tmp_path: Path) -> Iterator[PathFactoryFixture]:        yield PathFactoryFixture(tmp_path)",
        "labels_text": "Fixture returning PathFactoryFixture instance"
    },
    {
        "input_text": "summarize: def __call__(        self,        *packages: str,        prefix: str | os.PathLike | None = None,    ) -> Iterator[Path]:                prefix = Path(prefix or self.path_factory())        self.conda_cli(\"create\", \"--prefix\", prefix, *packages, \"--yes\", \"--quiet\")        yield prefix",
        "labels_text": "Generate a conda environment with the provided package param package The package to install into environment param prefix The prefix at which to install the conda environment return The conda environment prefix"
    },
    {
        "input_text": "summarize: def tmp_env(    path_factory: PathFactoryFixture,    conda_cli: CondaCLIFixture,) -> Iterator[TmpEnvFixture]:        yield TmpEnvFixture(path_factory, conda_cli)",
        "labels_text": "Fixture returning TmpEnvFixture instance"
    },
    {
        "input_text": "summarize: def tmp_channel(    path_factory: PathFactoryFixture,    conda_cli: CondaCLIFixture,) -> Iterator[TmpChannelFixture]:        yield TmpChannelFixture(path_factory, conda_cli)",
        "labels_text": "Fixture returning TmpChannelFixture instance"
    },
    {
        "input_text": "summarize: def notices_cache_dir(tmpdir):        with mock.patch(\"conda.notices.cache.user_cache_dir\") as user_cache_dir:        user_cache_dir.return_value = tmpdir        cache_dir = Path(tmpdir).joinpath(NOTICES_CACHE_SUBDIR)        cache_dir.mkdir(parents=True, exist_ok=True)        yield cache_dir",
        "labels_text": "Fixture that creates the notice cache dir while also mocking out a call to usercachedir"
    },
    {
        "input_text": "summarize: def create_notice_cache_files(    cache_dir: Path,    cache_files: Sequence[str],    messages_json_seq: Sequence[dict],) -> None:        for message_json, file in zip(messages_json_seq, cache_files):        with cache_dir.joinpath(file).open(\"w\") as fp:            json.dump(message_json, fp)",
        "labels_text": "Creates the cache file that we use in test"
    },
    {
        "input_text": "summarize: def offset_cache_file_mtime(mtime_offset) -> None:        cache_file = get_notices_cache_file()    os.utime(        cache_file,        times=(cache_file.stat().st_atime, cache_file.stat().st_mtime - mtime_offset),    )",
        "labels_text": "Allows for offsetting the mtime of the notice cache file This is often used to mock an older creation time the cache file"
    },
    {
        "input_text": "summarize: def notices_decorator_assert_message_in_stdout(    captured,    messages: Sequence[str],    dummy_mesg: str | None = None,    not_in: bool = False,):        assert captured.err == \"\"    assert dummy_mesg in captured.out    for mesg in messages:        if not_in:            assert mesg not in captured.out        else:            assert mesg in captured.out",
        "labels_text": "Tests a run of notice decorator where we expect to see the message print to stdout"
    },
    {
        "input_text": "summarize: def get_notice_cache_filenames(ctx: Context) -> tuple[str]:        channel_urls_and_names = get_channel_name_and_urls(get_channel_objs(ctx))    return tuple(        ChannelNoticeResponse.get_cache_key(url, Path(\"\")).name        for url, name in channel_urls_and_names    )",
        "labels_text": "Returns the filename of the cache file that will be searched for"
    },
    {
        "input_text": "summarize: def post_process(files, output_path):        for file in files:        with fileinput.input(            files=[os.path.join(output_path, file)], inplace=True        ) as open_file:            for line in open_file:                for old, new in replacements:                    line = line.replace(old, new)                sys.stdout.write(line)",
        "labels_text": "Replace all item from the replacement list above in the given file"
    },
    {
        "input_text": "summarize: def support_file_server():        base = Path(__file__).parents[0] / \"env\" / \"support\"    http = http_test_server.run_test_server(str(base))    yield http    # shutdown is checked at a polling interval, or the daemon thread will shut    # down when the test suite exits.    http.shutdown()",
        "labels_text": "Open a local web server to test remote support file"
    },
    {
        "input_text": "summarize: def do_not_register_envs(monkeypatch):        monkeypatch.setenv(\"CONDA_REGISTER_ENVS\", \"false\")",
        "labels_text": "Do not register environment created during test"
    },
    {
        "input_text": "summarize: def do_not_notify_outdated_conda(monkeypatch):        monkeypatch.setenv(\"CONDA_NOTIFY_OUTDATED_CONDA\", \"false\")",
        "labels_text": "Do not notify about outdated conda during test"
    },
    {
        "input_text": "summarize: def latency(delay):        global LATENCY    LATENCY = delay    return \"OK\"",
        "labels_text": "Set delay before each file response"
    },
    {
        "input_text": "summarize: def none_accept_ranges():        response = flask.Response(\"test content test content test content\")    response.headers[\"Accept-Ranges\"] = \"none\"    return response",
        "labels_text": "Returns an empty request with AcceptRanges set to none"
    },
    {
        "input_text": "summarize: def run_on_random_port():        return next(_package_server())",
        "labels_text": "Run in a new process to minimize interference with test"
    },
    {
        "input_text": "summarize: def prepare_socket() -> socket.socket:        host = \"127.0.0.1\"    port = 0  # automatically choose an available port    server_address = (host, port)    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)    s.set_inheritable(True)    s.bind(server_address)    s.listen()    return s",
        "labels_text": "Prepare a socket for use by the WSGI server Based on Werkzeug preparesocket removed in"
    },
    {
        "input_text": "summarize: def package_repository_base(tmp_path_factory):        # destination can't exist for shutil.copytree(); create its parent    destination = tmp_path_factory.mktemp(\"repo\") / TEST_REPOSITORY.name    shutil.copytree(TEST_REPOSITORY, destination)    return destination",
        "labels_text": "Copy testsindexdata to avoid writing change to repository Could be made sessionscoped if we dont mind reusing the index cache during test"
    },
    {
        "input_text": "summarize: def test_stub_exe_signatures(stub_file_name: str) -> None:        stub_file = STUB_FOLDER / stub_file_name    signtool_exe = find_signtool()    completed_process = run([signtool_exe, \"verify\", \"/pa\", \"/v\", stub_file])    assert completed_process.returncode == 0",
        "labels_text": "Verify that signtool verifies the signature of the stub ex"
    },
    {
        "input_text": "summarize: def test_function(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with nullcontext() if warning else pytest.raises(DeprecatedError):        @deprecated(\"2.0\", \"3.0\")        def foo():            return True        with pytest.warns(warning, match=message):            assert foo()",
        "labels_text": "Calling a deprecated function display associated warning or error"
    },
    {
        "input_text": "summarize: def test_method(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with nullcontext() if warning else pytest.raises(DeprecatedError):        class Bar:            @deprecated(\"2.0\", \"3.0\")            def foo(self):                return True        with pytest.warns(warning, match=message):            assert Bar().foo()",
        "labels_text": "Calling a deprecated method display associated warning or error"
    },
    {
        "input_text": "summarize: def test_class(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with nullcontext() if warning else pytest.raises(DeprecatedError):        @deprecated(\"2.0\", \"3.0\")        class Foo:            pass        with pytest.warns(warning, match=message):            assert Foo()",
        "labels_text": "Calling a deprecated class display associated warning or error"
    },
    {
        "input_text": "summarize: def test_action(    deprecated: DeprecationHandler,    warning: UserDeprecationType | None,    message: str | None,) -> None:        with nullcontext() if warning else pytest.raises(DeprecatedError):        parser = ArgumentParser()        parser.add_argument(            \"--foo\",            action=deprecated.action(\"2.0\", \"3.0\", _StoreTrueAction),        )        with pytest.warns(warning, match=message):            parser.parse_args([\"--foo\"])",
        "labels_text": "Calling a deprecated argparseAction display associated warning or error"
    },
    {
        "input_text": "summarize: def test_module(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with (        pytest.warns(warning, match=message)        if warning        else pytest.raises(DeprecatedError)    ):        deprecated.module(\"2.0\", \"3.0\")",
        "labels_text": "Importing a deprecated module display associated warning or error"
    },
    {
        "input_text": "summarize: def test_constant(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with nullcontext() if warning else pytest.raises(DeprecatedError):        deprecated.constant(\"2.0\", \"3.0\", \"SOME_CONSTANT\", 42)        module = sys.modules[__name__]        with pytest.warns(warning, match=message):            module.SOME_CONSTANT",
        "labels_text": "Using a deprecated constant display associated warning or error"
    },
    {
        "input_text": "summarize: def test_topic(    deprecated: DeprecationHandler,    warning: DevDeprecationType | None,    message: str | None,) -> None:        with (        pytest.warns(warning, match=message)        if warning        else pytest.raises(DeprecatedError)    ):        deprecated.topic(\"2.0\", \"3.0\", topic=\"Some special topic\")",
        "labels_text": "Reaching a deprecated topic display associated warning or error"
    },
    {
        "input_text": "summarize: def test_version_fallback() -> None:        deprecated = DeprecationHandler(None)  # type: ignore[arg-type]    assert deprecated._version_less_than(\"0\")    assert deprecated._version_tuple is None    version: Version = deprecated._version_object  # type: ignore[assignment]    assert version.major == version.minor == version.micro == 0",
        "labels_text": "Test that conda can run even if deprecation cant parse the version"
    },
    {
        "input_text": "summarize: def test_history_malformed(tmp_history):        with tmp_history as history:        with open(history.path, \"w\") as fp:            fp.write(\"# malformed content\")        results = history.parse()        assert results == []",
        "labels_text": "Regression test for httpsgithubcomcondacondaissues"
    },
    {
        "input_text": "summarize: def test_expected_operation_order():        expected = (        instructions.CHECK_FETCH,        instructions.FETCH,        instructions.CHECK_EXTRACT,        instructions.EXTRACT,        instructions.UNLINK,        instructions.LINK,        instructions.SYMLINK_CONDA,        instructions.RM_EXTRACTED,        instructions.RM_FETCHED,    )    assert expected == instructions.ACTION_CODES",
        "labels_text": "Ensure expected order of operation"
    },
    {
        "input_text": "summarize: def my_OR(*args):        if any(v == TRUE for v in args):        return TRUE    args = {v for v in args if v != FALSE}    if len(args) == 0:        return FALSE    if len(args) == 1:        return next(v for v in args)    if len({v if v is None else my_ABS(v) for v in args}) < len(args):        return TRUE    return None",
        "labels_text": "Implements a logical OR according to the logic positive integer are variable negative integer are negation of positive variable TRUE and FALSE are fixed value None is an unknown value TRUE OR x TRUE FALSE OR x FALSE None OR x None x OR y None"
    },
    {
        "input_text": "summarize: def which_powershell():    r    if on_win:        posh = which(\"powershell.exe\")        if posh:            return \"powershell\", posh    posh = which(\"pwsh\")    if posh:        return \"pwsh\", posh    posh = which(\"pwsh-preview\")    if posh:        return \"pwsh-preview\", posh",
        "labels_text": "Since we dont know whether PowerShell is installed a powershell pwsh or pwshpreview it helpful to have a utility function that return the name of the best PowerShell executable available or None if there no PowerShell installed If PowerShell is found this function return both the kind of PowerShell install found and a path to it main executable Eg pwsh rCProgram FilesPowerShellpwshexe"
    },
    {
        "input_text": "summarize: def test_ensure_dir(tmpdir):        new_dir = \"test_dir\"    @utils.ensure_dir_exists    def get_test_dir() -> Path:        return Path(tmpdir).joinpath(new_dir)    new_dir = get_test_dir()    assert new_dir.is_dir()",
        "labels_text": "Ensures that this decorator creates a directory"
    },
    {
        "input_text": "summarize: def test_channels_defaults(testdata: None):        reset_context(())    assert context.channels == (\"defaults\",)",
        "labels_text": "Test when no channel provided in cli"
    },
    {
        "input_text": "summarize: def test_channels_defaults_condarc(testdata: None):        reset_context(())    string = dals(            )    rd = {        \"testdata\": YamlRawParameter.make_raw_parameters(            \"testdata\", yaml_round_trip_load(string)        )    }    context._set_raw_data(rd)    assert context.channels == (\"defaults\", \"conda-forge\")",
        "labels_text": "Test when no channel provided in cli but some in condarc"
    },
    {
        "input_text": "summarize: def test_specify_channels_cli_adding_defaults_no_condarc(testdata: None):        reset_context((), argparse_args=AttrDict(channel=[\"conda-forge\"]))    assert context.channels == (\"conda-forge\", \"defaults\")",
        "labels_text": "When the channel havent been specified in condarc default should be present when specifying channel in the cli"
    },
    {
        "input_text": "summarize: def test_specify_channels_cli_condarc(testdata: None):        reset_context((), argparse_args=AttrDict(channel=[\"conda-forge\"]))    string = dals(            )    rd = {        \"testdata\": YamlRawParameter.make_raw_parameters(            \"testdata\", yaml_round_trip_load(string)        )    }    context._set_raw_data(rd)    assert context.channels == (\"defaults\", \"conda-forge\")",
        "labels_text": "When the channel have been specified in condarc these channel should be used along with the one specified"
    },
    {
        "input_text": "summarize: def test_specify_different_channels_cli_condarc(testdata: None):        reset_context((), argparse_args=AttrDict(channel=[\"other\"]))    string = dals(            )    rd = {        \"testdata\": YamlRawParameter.make_raw_parameters(            \"testdata\", yaml_round_trip_load(string)        )    }    context._set_raw_data(rd)    assert context.channels == (\"conda-forge\", \"other\")",
        "labels_text": "When the channel have been specified in condarc these channel should be used along with the one specified In this test the given channel in cli is different from condarc default should not be added"
    },
    {
        "input_text": "summarize: def test_specify_same_channels_cli_as_in_condarc(testdata: None):        reset_context((), argparse_args=AttrDict(channel=[\"conda-forge\"]))    string = dals(            )    rd = {        \"testdata\": YamlRawParameter.make_raw_parameters(            \"testdata\", yaml_round_trip_load(string)        )    }    context._set_raw_data(rd)    assert context.channels == (\"conda-forge\",)",
        "labels_text": "When the channel have been specified in condarc these channel should be used along with the one specified In this test the given channel in cli is the same a in condarc default should not be added See httpsgithubcomcondacondaissues"
    },
    {
        "input_text": "summarize: def test_channel_settings(testdata: None):        assert context.channel_settings == (        {\"channel\": \"darwin\", \"param_one\": \"value_one\", \"param_two\": \"value_two\"},        {            \"channel\": \"http://localhost\",            \"param_one\": \"value_one\",            \"param_two\": \"value_two\",        },    )",
        "labels_text": "Ensure channelsettings appears a we expect it to on the context object"
    },
    {
        "input_text": "summarize: def test_get_plugin_config_data_skip_bad_values():        path = Path(\"/tmp/\")    class Value:        def value(self, _):            return \"some_value\"    raw_data = {path: {\"plugins\": Value()}}    plugin_config_data = get_plugin_config_data(raw_data)    assert plugin_config_data == {}",
        "labels_text": "Make sure that value that are not frozendict for file source are skipped"
    },
    {
        "input_text": "summarize: def test_reporters_from_config_file(testdata):        assert context.reporters == (        {\"backend\": \"json\", \"output\": \"stdout\"},        {\"backend\": \"console\", \"output\": \"stdout\"},    )",
        "labels_text": "Ensure that the reporter property return the correct value"
    },
    {
        "input_text": "summarize: def test_reporters_json_is_true(testdata):        args = SimpleNamespace(json=True)    reset_context((), args)    assert context.reporters == (        {            \"backend\": \"json\",            \"output\": \"stdout\",            \"quiet\": False,            \"verbosity\": context.verbosity,        },    )    reset_context()",
        "labels_text": "Ensure that the reporter property return the correct value when contextjson is true"
    },
    {
        "input_text": "summarize: def test_reporters_quiet_is_true(testdata):        args = SimpleNamespace(quiet=True)    reset_context((), args)    assert context.reporters == (        {            \"backend\": \"console\",            \"output\": \"stdout\",            \"verbosity\": context.verbosity,            \"quiet\": True,        },    )    reset_context()",
        "labels_text": "Ensure that the reporter property return the correct value when contextquiet is true"
    },
    {
        "input_text": "summarize: def test_reporters_default_value():        assert context.reporters == (        {            \"backend\": \"console\",            \"output\": \"stdout\",            \"quiet\": False,            \"verbosity\": context.verbosity,        },    )",
        "labels_text": "Ensure that the reporter property return the correct value when nothing is set including value from configuration file"
    },
    {
        "input_text": "summarize: def urllib3_logger_error(caplog):        caplog.set_level(logging.ERROR, logger=\"urllib3.connectionpool\")",
        "labels_text": "Increase log level to error to prevent retries from polluting stderr"
    },
    {
        "input_text": "summarize: def chdir(tmp_path: Path, monkeypatch: MonkeyPatch) -> Iterator[Path]:        monkeypatch.chdir(tmp_path)    yield tmp_path",
        "labels_text": "Change directory to a temporary directory for conda env command since they are sensitive to the current working directory"
    },
    {
        "input_text": "summarize: def test_conda_env_create_no_file(conda_cli: CondaCLIFixture):        with pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"create\")",
        "labels_text": "Test conda env create without an environmentyml file Should fail"
    },
    {
        "input_text": "summarize: def test_conda_env_create_no_existent_file(conda_cli: CondaCLIFixture):        with pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"create\", \"--file=not_a_file.txt\")",
        "labels_text": "Test conda env create filenotafiletxt with a file that doe not exist"
    },
    {
        "input_text": "summarize: def test_conda_env_create_no_existent_file_with_name(conda_cli: CondaCLIFixture):        with pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"create\", \"--file=not_a_file.txt\", \"--name=foo\")",
        "labels_text": "Test conda env create filenotafiletxt with a file that doe not exist"
    },
    {
        "input_text": "summarize: def test_deprecated_binstar(conda_cli: CondaCLIFixture):        with pytest.warns(FutureWarning), pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"create\", \"conda-test/env-remote\")    with pytest.warns(FutureWarning), pytest.raises(EnvironmentFileNotFound):        conda_cli(\"env\", \"update\", \"conda-test/env-remote\")",
        "labels_text": "Test retrieving an environment using the BinstarSpec ie it retrieves it from anacondaorg This test the remoteorigin command line argument"
    },
    {
        "input_text": "summarize: def test_create_valid_env(env1: str, conda_cli: CondaCLIFixture):        create_env(ENVIRONMENT_CA_CERTIFICATES)    conda_cli(\"env\", \"create\")    assert env_is_created(env1)    stdout, _, _ = conda_cli(\"info\", \"--json\")    parsed = json.loads(stdout)    assert [env for env in parsed[\"envs\"] if env.endswith(env1)]",
        "labels_text": "Creates an environmentyml file and creates and environment with it"
    },
    {
        "input_text": "summarize: def test_conda_env_create_empty_file(    conda_cli: CondaCLIFixture, path_factory: PathFactoryFixture):        tmp_file = path_factory(suffix=\".yml\")    tmp_file.touch()    with pytest.raises(SpecNotFound):        conda_cli(\"env\", \"create\", f\"--file={tmp_file}\")",
        "labels_text": "Test conda env create filefilenameyml where filenameyml is empty"
    },
    {
        "input_text": "summarize: def test_conda_env_create_http(conda_cli: CondaCLIFixture, tmp_path: Path):        conda_cli(        *(\"env\", \"create\"),        f\"--prefix={tmp_path}\",        \"--file=https://raw.githubusercontent.com/conda/conda/main/tests/env/support/simple.yml\",    )    assert (tmp_path / \"conda-meta\" / \"history\").is_file()",
        "labels_text": "Test conda env create filehttpssomewebsitecomenvironmentyml"
    },
    {
        "input_text": "summarize: def test_name(env1: str, conda_cli: CondaCLIFixture):        create_env(ENVIRONMENT_CA_CERTIFICATES)    conda_cli(\"env\", \"create\", \"--file=environment.yml\", f\"--name={env1}\", \"--yes\")    stdout, _, _ = conda_cli(\"info\", \"--json\")    parsed = json.loads(stdout)    assert [env for env in parsed[\"envs\"] if env.endswith(env1)]",
        "labels_text": "smoke test for gh Test that name can override the name key inside an environmentyml"
    },
    {
        "input_text": "summarize: def test_pip_error_is_propagated(env1: str, conda_cli: CondaCLIFixture):        create_env(ENVIRONMENT_PIP_NONEXISTING)    with pytest.raises(CondaEnvException, match=\"Pip failed\"):        conda_cli(\"env\", \"create\")",
        "labels_text": "Creates an environment from an environmentyml file with conda and incorrect pip dependency The output must clearly show pip error Check the json output"
    },
    {
        "input_text": "summarize: def env_is_created(env_name):        from os.path import basename    for prefix in list_all_known_prefixes():        name = ROOT_ENV_NAME if prefix == context.root_prefix else basename(prefix)        if name == env_name:            return True    return False",
        "labels_text": "Assert an environment is created Args envname the environment name Returns True if created False otherwise"
    },
    {
        "input_text": "summarize: def test_ensure_no_command_provided_returns_help(    conda_cli: CondaCLIFixture, capsys, option):        with pytest.raises(SystemExit):        conda_cli(option)    captured = capsys.readouterr()    assert \"error: the following arguments are required: COMMAND\" in captured.err",
        "labels_text": "Regression test to make sure that invoking with just any of the option listed a parameter will not return a traceback"
    },
    {
        "input_text": "summarize: def test_main_notices_help(capsys):        parser = conda_argparse.generate_parser()    try:        args = parser.parse_args([\"notices\", \"--help\"])        notices.execute(args, parser)    except SystemExit:        pass    captured = capsys.readouterr()    assert captured.err == \"\"    assert \"Retrieve latest channel notifications.\" in captured.out    assert \"maintainers have the option of setting messages\" in captured.out",
        "labels_text": "Test to make sure help documentation ha appropriate section in it"
    },
    {
        "input_text": "summarize: def test_rename_by_name_name_already_exists_error(    conda_cli: CondaCLIFixture,    env_one: str,):        with pytest.raises(        CondaEnvException,        match=f\"The environment '{env_one}' already exists. Override with --yes\",    ):        conda_cli(\"rename\", \"--name\", env_one, env_one)    assert locate_prefix_by_name(env_one)",
        "labels_text": "Test to ensure that we do not rename if the name already exists"
    },
    {
        "input_text": "summarize: def test_rename_by_path_path_already_exists_error(    conda_cli: CondaCLIFixture,    env_one: str,    tmp_path: Path,):        with pytest.raises(        CondaEnvException,        match=f\"The environment '{tmp_path.name}' already exists. Override with --yes\",    ):        conda_cli(\"rename\", \"--name\", env_one, tmp_path)    assert locate_prefix_by_name(env_one)    assert tmp_path.exists()",
        "labels_text": "Test to ensure that we do not rename if the path already exists"
    },
    {
        "input_text": "summarize: def test_cannot_rename_base_env_by_name(conda_cli: CondaCLIFixture, env_rename: str):        with pytest.raises(        CondaEnvException,        match=\"The 'base' environment cannot be renamed\",    ):        conda_cli(\"rename\", \"--name\", \"base\", env_rename)    assert locate_prefix_by_name(\"base\")    with pytest.raises(EnvironmentNameNotFound):        locate_prefix_by_name(env_rename)",
        "labels_text": "Test to ensure that we cannot rename the base env invoked by name"
    },
    {
        "input_text": "summarize: def test_cannot_rename_base_env_by_path(conda_cli: CondaCLIFixture, env_rename: str):        with pytest.raises(        CondaEnvException,        match=\"The 'base' environment cannot be renamed\",    ):        conda_cli(\"rename\", \"--prefix\", context.root_prefix, env_rename)    assert Path(context.root_prefix).exists()    with pytest.raises(EnvironmentNameNotFound):        locate_prefix_by_name(env_rename)",
        "labels_text": "Test to ensure that we cannot rename the base env invoked by path"
    },
    {
        "input_text": "summarize: def test_rename_with_force(conda_cli: CondaCLIFixture, env_one: str, env_two: str):        # Do a force rename    conda_cli(\"rename\", \"--name\", env_one, env_two, \"--yes\")    assert locate_prefix_by_name(env_two)    with pytest.raises(EnvironmentNameNotFound):        locate_prefix_by_name(env_one)",
        "labels_text": "Runs a test where we specify the yes flag to remove an existing directory Without this flag it would return with an error message"
    },
    {
        "input_text": "summarize: def test_object_defaults_partially_empty():        config = SampleConfiguration()._set_raw_data(load_from_string_data(\"objectFile3\"))    test_object = config.test_object    assert test_object.int_field == 0    assert test_object.str_field == \"\"    assert test_object.map_field == {\"key1\": \"a1\", \"key2\": \"a2\"}    assert test_object.seq_field == tuple()",
        "labels_text": "Used to make sure that default are appropriately set for missing value when only some value have been passed in"
    },
    {
        "input_text": "summarize: def test_object_defaults_completely_empty():        config = SampleConfiguration()._set_raw_data(load_from_string_data(\"objectFile4\"))    test_object = config.test_object    assert test_object.int_field == 0    assert test_object.str_field == \"\"    assert test_object.map_field == {}    assert test_object.seq_field == tuple()",
        "labels_text": "Used to make sure that default are appropriately set for missing value when no value have been passed in"
    },
    {
        "input_text": "summarize: def test_expand_search_path(tmp_path):        target = tmp_path / \"condarc-symlink-target\"    target.touch()    symlink = tmp_path / \"condarc\"    symlink.symlink_to(target)    expanded = list(Configuration._expand_search_path([target, symlink]))    assert expanded == [symlink], expanded",
        "labels_text": "expandsearchpath wa testing the symlink target against valid condarc filename instead of the symlink itself It may or may not be necessary to limit valid condarc filename but it is even more confusing to reject based on the symlink target"
    },
    {
        "input_text": "summarize: def unique_sequence_map_test_class():        class UniqueSequenceMapTestObject(SimpleNamespace):        @unique_sequence_map(unique_key=\"backend\")        def test_prop(self):            return self._test_prop    return UniqueSequenceMapTestObject",
        "labels_text": "Creates a class that is used for uniquesequencemap test"
    },
    {
        "input_text": "summarize: def test_degenerate():        assert toposort({}) == []    assert toposort({}, safe=False) == []",
        "labels_text": "Edge case"
    },
    {
        "input_text": "summarize: def test_urlparse(test_url_str, exp_url_obj):        answer = urlparse(test_url_str)    for attr in exp_url_obj.__annotations__.keys():        assert getattr(answer, attr) == getattr(exp_url_obj, attr)    assert str(answer) == test_url_str",
        "labels_text": "Tests a variety of different use case for condacommonurlurlparse"
    },
    {
        "input_text": "summarize: def test_url_obj_unparse(test_url_obj, expected_url):        url_obj = Url(*test_url_obj)    assert str(url_obj) == expected_url",
        "labels_text": "Tests the variety of object instantiation for the condacommonurlUrl"
    },
    {
        "input_text": "summarize: def _create_test_files(test_files):        temp_path = tempfile.mkdtemp()    fpaths = []    for folder, fname, content in test_files:        fpath = os.path.join(temp_path, folder, fname)        try:            os.makedirs(os.path.dirname(fpath))        except Exception:            pass        with open(fpath, \"w\") as fh:            fh.write(content)        fpaths.append(fpath)    return temp_path, fpaths",
        "labels_text": "Helper method to create file in a folder with fname and given content testfiles folder fname content Create a file in folder with content fname content Create a file with content folder Create a folder"
    },
    {
        "input_text": "summarize: def _print_output(*args):        for arg in args:        print(arg)    print(\"\\n\")",
        "labels_text": "Helper function to print output in case of failed test"
    },
    {
        "input_text": "summarize: def test_cover_fetch_not_exists():        with pytest.raises(CondaMultiError):        ProgressiveFetchExtract(            [                MatchSpec(                    url=\"http://localhost:8080/conda-test/fakepackage-1.2.12-testing_3.conda\"                ),                MatchSpec(                    url=\"http://localhost:8080/conda-test/phonypackage-0.0.1-testing_3.conda\"                ),            ]        ).execute()",
        "labels_text": "Conda collect all exception raised during ProgressiveFetchExtract into a CondaMultiError TODO Is this necessary"
    },
    {
        "input_text": "summarize: def test_corrupt_unicode_conda_meta_json():        with pytest.raises(CorruptedEnvironmentError):        PrefixData(\"tests/data/corrupt/unicode\").load()",
        "labels_text": "Test for graceful failure if a Unicode corrupt file exists in condameta"
    },
    {
        "input_text": "summarize: def test_corrupt_json_conda_meta_json():        with pytest.raises(CorruptedEnvironmentError):        PrefixData(\"tests/data/corrupt/json\").load()",
        "labels_text": "Test for graceful failure if a JSON corrupt file exists in condameta"
    },
    {
        "input_text": "summarize: def test_subdir_data_dict_state(platform=OVERRIDE_PLATFORM):        local_channel = Channel(join(CHANNEL_DIR_V1, platform))    sd = SubdirData(channel=local_channel)    sd._read_pickled({})",
        "labels_text": "SubdirData can accept a dict instead of a RepodataState for compatibility"
    },
    {
        "input_text": "summarize: def test_s3_server_with_mock(    package_server,    tmp_env: TmpEnvFixture,    monkeypatch: MonkeyPatch,) -> None:        host, port = package_server.getsockname()    endpoint_url = f\"http://{host}:{port}\"    bucket_name = \"test\"    inner_s3_test(tmp_env, monkeypatch, endpoint_url, bucket_name)",
        "labels_text": "Use boto to fetch from a mock s server pointing at the test package repository This work since conda only GETs against s and s is http"
    },
    {
        "input_text": "summarize: def test_get_session_returns_default():        url = \"https://localhost/test\"    session_obj = get_session(url)    get_session.cache_clear()  # ensuring cleanup    assert type(session_obj) is CondaSession",
        "labels_text": "Tests to make sure that our session manager return a regular CondaSession object when no other session class are registered"
    },
    {
        "input_text": "summarize: def test_get_channel_name_from_url(url, channels, expected, monkeypatch):        monkeypatch.setenv(\"CONDA_CHANNELS\", \",\".join(channels))    reset_context()    channel_name = get_channel_name_from_url(url)    assert expected == channel_name",
        "labels_text": "Makes sure we return the correct value from the getchannelnamefromurl function"
    },
    {
        "input_text": "summarize: def test_jlap_coverage():        class JlapCoverMe(interface.JlapRepoInterface):        def repodata_parsed(self, state):            return    with pytest.raises(RepodataOnDisk):        JlapCoverMe(\"\", \"\", cache=None).repodata({})",
        "labels_text": "Force raise RepodataOnDisk at end of JlapRepoInterfacerepodata function"
    },
    {
        "input_text": "summarize: def test_hashwriter():        closed = False    class backing:        def close(self):            nonlocal closed            closed = True    writer = fetch.HashWriter(backing(), None)    with writer:        pass    assert closed",
        "labels_text": "Test that HashWriter close it backing file in a context manager"
    },
    {
        "input_text": "summarize: def test_get_cache_control_max_age():        assert get_cache_control_max_age('cache_control = \"public, max-age=30\"') == 30    assert get_cache_control_max_age(None) == 0",
        "labels_text": "Test that we are robust against None cachecontrolmaxage"
    },
    {
        "input_text": "summarize: def testdata() -> None:        reset_context()    context._set_raw_data(        {            \"testdata\": YamlRawParameter.make_raw_parameters(                \"testdata\",                yaml_round_trip_load(                    dals(                                            )                ),            )        }    )    Channel._reset_state()",
        "labels_text": "Some note about the test in this class The pkgsanaconda channel is migrated while the pkgspro channel is not Thus testpkgsfree and testpkgspro have substantially different behavior"
    },
    {
        "input_text": "summarize: def test_local_identifier():        # a valid versionstr should match itself    versions = (        \"1.7.0\",        \"1.7.0.post123\",        \"1.7.0.post123.gabcdef9\",        \"1.7.0.post123+gabcdef9\",    )    for version in versions:        m = VersionSpec(version)        assert m.match(version)",
        "labels_text": "The separator for the local identifier should be either or"
    },
    {
        "input_text": "summarize: def test_notice_response_cache_expired():        class ExpiredAtNone:        expired_at = None    class Notices:        notices = [ExpiredAtNone]    assert is_notice_response_cache_expired(Notices)",
        "labels_text": "Channel notice is expired if expiredat is None"
    },
    {
        "input_text": "summarize: def test_channel_notice_response():        messages = tuple(f\"Test {idx}\" for idx in range(1, 4, 1))    expected_num_notices = len(messages)    json_data = get_test_notices(messages)    response = ChannelNoticeResponse(\"http://localhost\", \"local\", json_data)    notices = response.notices    assert len(notices) == expected_num_notices",
        "labels_text": "Tests a normal invocation of the ChannelNoticeResponse class"
    },
    {
        "input_text": "summarize: def test_channel_notice_undefined_id():        json_data = {        \"notices\": [            {                \"message\": \"test\",                \"level\": \"warning\",                \"created_at\": \"2023-01-01 00:00:00\",                \"updated_at\": \"2023-01-01 00:00:00\",            }        ]    }    response = ChannelNoticeResponse(\"http://localhost\", \"local\", json_data)    assert response.notices[0].id == UNDEFINED_MESSAGE_ID",
        "labels_text": "Test to make sure that when a message doe not define an id it appears a the string value undefined"
    },
    {
        "input_text": "summarize: def test_get_auth_handler(plugin_manager):        plugin = CustomAuthPlugin()    plugin_manager.register(plugin)    auth_handler_cls = plugin_manager.get_auth_handler(PLUGIN_NAME)    assert auth_handler_cls is CustomCondaAuth    auth_handler_cls = plugin_manager.get_auth_handler(\"DOES_NOT_EXIST\")    assert auth_handler_cls is None",
        "labels_text": "Return the correct auth backend class or return None"
    },
    {
        "input_text": "summarize: def test_get_auth_handler_multiple(plugin_manager):        plugin_one = CustomAuthPlugin()    plugin_two = CustomAltAuthPlugin()    plugin_manager.register(plugin_one)    plugin_manager.register(plugin_two)    auth_class = plugin_manager.get_auth_handler(PLUGIN_NAME)    assert auth_class is CustomCondaAuth    auth_class = plugin_manager.get_auth_handler(PLUGIN_NAME_ALT)    assert auth_class is CustomAltCondaAuth",
        "labels_text": "Tests to make sure we can retrieve auth backends when there are multiple hook registered"
    },
    {
        "input_text": "summarize: def test_duplicated(plugin_manager):        plugin_manager.register(CustomAuthPlugin())    plugin_manager.register(CustomAuthPlugin())    with pytest.raises(        PluginError, match=re.escape(\"Conflicting `auth_handlers` plugins found\")    ):        plugin_manager.get_auth_handler(PLUGIN_NAME)",
        "labels_text": "Make sure that a PluginError is raised if we register the same auth backend twice"
    },
    {
        "input_text": "summarize: def plugin_manager_with_doctor_command(plugin_manager):        plugin_manager.load_plugins(doctor)    return plugin_manager",
        "labels_text": "Registers the conda doctor subcommand"
    },
    {
        "input_text": "summarize: def test_health_check_ran(mocker, health_check_plugin, conda_cli):        conda_cli(\"doctor\")    assert len(health_check_plugin.health_check_action.mock_calls) == 1",
        "labels_text": "Test for the case when the health check successfully ran"
    },
    {
        "input_text": "summarize: def test_health_check_not_ran(health_check_plugin, conda_cli):        conda_cli(\"info\")    assert len(health_check_plugin.health_check_action.mock_calls) == 0",
        "labels_text": "Test for the case when the health check did not run"
    },
    {
        "input_text": "summarize: def test_load_entrypoints_register_valueerror(plugin_manager: CondaPluginManager):        assert plugin_manager.load_entrypoints(\"test_plugin\", \"success\") == 1    assert plugin_manager.load_entrypoints(\"test_plugin\", \"success\") == 0",
        "labels_text": "Cover check when selfregister raise ValueError because the plugin wa loaded already"
    },
    {
        "input_text": "summarize: def test_unknown_solver(plugin_manager: CondaPluginManager):        with pytest.raises(CondaValueError):        plugin_manager.get_solver_backend(\"p_equals_np\")",
        "labels_text": "Cover getting a solver that doesnt exist"
    },
    {
        "input_text": "summarize: def test_known_solver(plugin_manager: CondaPluginManager):        assert plugin_manager.load_plugins(VerboseSolverPlugin) == 1    assert plugin_manager.get_solver_backend(\"verbose-classic\") == VerboseSolver",
        "labels_text": "Cover getting a solver that exists"
    },
    {
        "input_text": "summarize: def test_disable_external_plugins(plugin_manager: CondaPluginManager, plugin: object):        assert plugin_manager.load_plugins(plugin) == 1    assert plugin_manager.get_plugins() == {plugin}    plugin_manager.disable_external_plugins()    if pluggy_v100 or pluggy_v150:        assert plugin_manager.get_plugins() == set()    else:        assert plugin_manager.get_plugins() == {None}",
        "labels_text": "Run a test to ensure we can successfully disable externally registered plugins"
    },
    {
        "input_text": "summarize: def test_post_command_invoked(post_command_plugin, conda_cli):        conda_cli(\"info\")    assert len(post_command_plugin.post_command_action.mock_calls) == 1",
        "labels_text": "Makes sure that we successfully invoked our postcommand action"
    },
    {
        "input_text": "summarize: def test_post_command_not_invoked(post_command_plugin, conda_cli):        conda_cli(\"config\")    assert len(post_command_plugin.post_command_action.mock_calls) == 0",
        "labels_text": "Makes sure that we successfully did not invoke our postcommand action"
    },
    {
        "input_text": "summarize: def test_post_command_action_raises_exception(post_command_plugin, conda_cli):        exc_message = \"\ud83d\udca5\"    post_command_plugin.post_command_action.side_effect = [Exception(exc_message)]    with pytest.raises(Exception, match=exc_message):        conda_cli(\"info\")    assert len(post_command_plugin.post_command_action.mock_calls) == 1",
        "labels_text": "When the plugin action fails or raise an exception we want to make sure that it bubble up to the top and isnt caught anywhere This will ensure that it go through our normal exception catchingreporting mechanism"
    },
    {
        "input_text": "summarize: def test_pre_command_invoked(pre_command_plugin, conda_cli):        conda_cli(\"info\")    assert len(pre_command_plugin.pre_command_action.mock_calls) == 1",
        "labels_text": "Makes sure that we successfully invoked our precommand action"
    },
    {
        "input_text": "summarize: def test_pre_command_not_invoked(pre_command_plugin, conda_cli):        conda_cli(\"config\")    assert len(pre_command_plugin.pre_command_action.mock_calls) == 0",
        "labels_text": "Makes sure that we successfully did not invoke our precommand action"
    },
    {
        "input_text": "summarize: def test_pre_command_action_raises_exception(pre_command_plugin, conda_cli):        exc_message = \"\ud83d\udca5\"    pre_command_plugin.pre_command_action.side_effect = [Exception(exc_message)]    with pytest.raises(Exception, match=exc_message):        conda_cli(\"info\")    assert len(pre_command_plugin.pre_command_action.mock_calls) == 1",
        "labels_text": "When the plugin action fails or raise an exception we want to make sure that it bubble up to the top and isnt caught anywhere This will ensure that it go through our normal exception catchingreporting mechanism"
    },
    {
        "input_text": "summarize: def clear_plugins_context_cache():        try:        del context.plugins  # clear cached property    except AttributeError:        pass",
        "labels_text": "This fixture is used to ensure that the cache on the property plugins for the context object is cleared before each test run More info httpsdocspythonorglibraryfunctoolshtmlfunctoolscachedproperty"
    },
    {
        "input_text": "summarize: def setting_plugin_manager(    plugin_manager: CondaPluginManager, clear_plugins_context_cache) -> CondaPluginManager:        plugin = SettingPlugin()    plugin_manager.register(plugin)    yield plugin_manager",
        "labels_text": "Loads our SettingPlugin class using the pluginmanager fixture"
    },
    {
        "input_text": "summarize: def condarc_plugin_manager(setting_plugin_manager):        reset_context()    context._set_raw_data(        {            \"testdata\": YamlRawParameter.make_raw_parameters(                \"testdata\", yaml_round_trip_load(CONDARC_TEST_ONE)            )        }    )    return setting_plugin_manager",
        "labels_text": "Update the context object to load our test condarc file containing a setting defined by a plugin"
    },
    {
        "input_text": "summarize: def test_get_settings(setting_plugin_manager):        config_params = setting_plugin_manager.get_settings()    assert len(config_params) == 1    assert config_params.get(STRING_PARAMETER_NAME) == (        string_parameter,        (STRING_PARAMETER_ALIAS,),    )",
        "labels_text": "Ensure the setting method return what we expect"
    },
    {
        "input_text": "summarize: def test_load_configuration_parameters(setting_plugin_manager):        setting_plugin_manager.load_settings()    assert hasattr(context.plugins, STRING_PARAMETER_NAME)",
        "labels_text": "Ensure that the setting is available via the context object"
    },
    {
        "input_text": "summarize: def test_load_plugin_settings_with_condarc(condarc_plugin_manager):        assert getattr(context.plugins, STRING_PARAMETER_NAME) == STRING_PARAMETER_VALUE",
        "labels_text": "Ensure that when we define a custom plugin setting in a condarc file that the value show up on the context object"
    },
    {
        "input_text": "summarize: def test_load_plugin_config_with_env_var(    monkeypatch: MonkeyPatch, setting_plugin_manager):        monkeypatch.setenv(        f\"CONDA_PLUGINS_{STRING_PARAMETER_NAME.upper()}\", STRING_PARAMETER_ENV_VAR_VALUE    )    reset_context()    assert (        getattr(context.plugins, STRING_PARAMETER_NAME)        == STRING_PARAMETER_ENV_VAR_VALUE    )",
        "labels_text": "Ensure that when an environment variable is set for a plugin setting it is read correctly"
    },
    {
        "input_text": "summarize: def test_help(plugin_manager, conda_cli: CondaCLIFixture, capsys: CaptureFixture):        # setup    plugin_manager.register(SubcommandPlugin(name=\"custom\", summary=\"Summary.\"))    # test    with pytest.raises(SystemExit, match=\"0\"):        conda_cli(\"--help\")    stdout, stderr = capsys.readouterr()    # assertions; make sure our command appears with the help blurb    assert \"custom            Summary.\" in stdout    assert not stderr",
        "labels_text": "Ensures the command appears on the help page"
    },
    {
        "input_text": "summarize: def test_duplicated(plugin_manager, conda_cli: CondaCLIFixture):        # setup    plugin = SubcommandPlugin(name=\"custom\", summary=\"Summary.\")    assert plugin_manager.load_plugins(plugin) == 1    # invalid, identical plugins, error ignored    assert plugin_manager.load_plugins(plugin) == 0    # invalid, similar plugins, error ignored    plugin2 = SubcommandPlugin(name=\"custom\", summary=\"Summary.\")    assert plugin_manager.load_plugins(plugin2) == 0",
        "labels_text": "Ensures we get an error when attempting to register command with the same name property"
    },
    {
        "input_text": "summarize: def test_archspec_override(    monkeypatch: MonkeyPatch,    version: str | None,    expected: bool,):        monkeypatch.setenv(\"CONDA_OVERRIDE_ARCHSPEC\", version or \"\")    reset_context()    assert any(prec.name == \"__archspec\" for prec in get_virtual_precs()) is expected",
        "labels_text": "Conda should not produce a archspec virtual package when CONDAOVERRIDEARCHSPEC"
    },
    {
        "input_text": "summarize: def test_linux_override(monkeypatch: MonkeyPatch, version: str | None, expected: bool):        monkeypatch.setenv(\"CONDA_SUBDIR\", \"linux-64\")    monkeypatch.setenv(\"CONDA_OVERRIDE_LINUX\", version or \"\")    reset_context()    assert context.subdir == \"linux-64\"    assert any(prec.name == \"__linux\" for prec in get_virtual_precs()) is expected",
        "labels_text": "Conda will still produce a linux virtual package when CONDAOVERRIDELINUX"
    },
    {
        "input_text": "summarize: def test_glibc_override(monkeypatch: MonkeyPatch, version: str | None, expected: bool):        monkeypatch.setenv(\"CONDA_SUBDIR\", \"linux-64\")    monkeypatch.setenv(\"CONDA_OVERRIDE_GLIBC\", version or \"\")    reset_context()    assert context.subdir == \"linux-64\"    assert any(prec.name == \"__glibc\" for prec in get_virtual_precs()) == expected",
        "labels_text": "Conda should not produce a libc virtual package when CONDAOVERRIDEGLIBC"
    },
    {
        "input_text": "summarize: def test_osx_override(monkeypatch: MonkeyPatch, version: str | None, expected: bool):        monkeypatch.setenv(\"CONDA_SUBDIR\", \"osx-64\")    monkeypatch.setenv(\"CONDA_OVERRIDE_OSX\", version or \"\")    reset_context()    assert context.subdir == \"osx-64\"    assert any(prec.name == \"__osx\" for prec in get_virtual_precs()) == expected",
        "labels_text": "Conda should not produce a osx virtual package when CONDAOVERRIDEOSX"
    },
    {
        "input_text": "summarize: def test_conda_virtual_package():        assert any(        prec.name == \"__conda\" and prec.version == __version__        for prec in get_virtual_precs()    )",
        "labels_text": "Conda always produce a conda virtual package"
    },
    {
        "input_text": "summarize: def conda_solvers():        yield plugins.CondaSolver(        name=\"test\",        backend=Solver,    )",
        "labels_text": "The conda plugin hook implementation to load the solver into conda"
    },
    {
        "input_text": "summarize: def test_conda_doctor_happy_path(conda_cli: CondaCLIFixture):        out, err, code = conda_cli(\"doctor\")    assert not err  # no error message    assert not code",
        "labels_text": "Make sure that we are able to call the conda doctor command"
    },
    {
        "input_text": "summarize: def test_conda_doctor_happy_path_verbose(conda_cli: CondaCLIFixture):        out, err, code = conda_cli(\"doctor\", \"--verbose\")    assert not err  # no error message    assert not code",
        "labels_text": "Make sure that we are able to run conda doctor command with the verbose flag"
    },
    {
        "input_text": "summarize: def test_conda_doctor_happy_path_show_help(conda_cli: CondaCLIFixture):        with pytest.raises(SystemExit, match=\"0\"):  # 0 is the return code \u00af\\_(\u30c4)_/\u00af        conda_cli(\"doctor\", \"--help\")",
        "labels_text": "Make sure that we are able to run conda doctor command with the help flag"
    },
    {
        "input_text": "summarize: def test_conda_doctor_with_test_environment(    conda_cli: CondaCLIFixture,    tmp_env: TmpEnvFixture,):        with tmp_env() as prefix:        out, err, code = conda_cli(\"doctor\", \"--prefix\", prefix)        assert \"There are no packages with missing files.\" in out        assert not err  # no error message        assert not code",
        "labels_text": "Make sure that we are able to call conda doctor command for a specific environment"
    },
    {
        "input_text": "summarize: def test_conda_doctor_with_non_existent_environment(conda_cli: CondaCLIFixture):        # with pytest.raises(EnvironmentLocationNotFound):    out, err, exception = conda_cli(        \"doctor\",        \"--prefix\",        Path(\"non/existent/path\"),        raises=EnvironmentLocationNotFound,    )    assert not out    assert not err  # no error message    assert exception",
        "labels_text": "Make sure that conda doctor detects a non existent environment path"
    },
    {
        "input_text": "summarize: def env_missing_files(    env_ok: tuple[Path, str, str, str, str],) -> tuple[Path, str, str, str, str]:        prefix, bin_doctor, _, ignored_doctor, _ = env_ok    (prefix / bin_doctor).unlink()  # file bin_doctor becomes \"missing\"    (prefix / ignored_doctor).unlink()  # file ignored_doctor becomes \"missing\"    return env_ok",
        "labels_text": "Fixture that return a testing environment with missing file"
    },
    {
        "input_text": "summarize: def env_altered_files(    env_ok: tuple[Path, str, str, str, str],) -> tuple[Path, str, str, str, str]:        prefix, _, lib_doctor, _, _ = env_ok    # Altering the lib_doctor.py file so that it's sha256 checksum will change    with open(prefix / lib_doctor, \"w\") as f:        f.write(\"print('Hello, World!')\")    return env_ok",
        "labels_text": "Fixture that return a testing environment with altered file"
    },
    {
        "input_text": "summarize: def test_no_missing_files(env_ok: tuple[Path, str, str, str, str]):        prefix, _, _, _, _ = env_ok    assert find_packages_with_missing_files(prefix) == {}",
        "labels_text": "Test that run for the case with no missing file"
    },
    {
        "input_text": "summarize: def test_no_altered_files(env_ok: tuple[Path, str, str, str, str]):        prefix, _, _, _, _ = env_ok    assert find_altered_packages(prefix) == {}",
        "labels_text": "Test that run for the case with no altered file"
    },
    {
        "input_text": "summarize: def test_json_keys_missing(env_ok: tuple[Path, str, str, str, str], capsys):        prefix, _, _, _, package = env_ok    file = prefix / \"conda-meta\" / f\"{package}.json\"    with open(file) as f:        data = json.load(f)    del data[\"paths_data\"]    with open(file, \"w\") as f:        json.dump(data, f)    assert find_altered_packages(prefix) == {}",
        "labels_text": "Test that run for the case with empty json"
    },
    {
        "input_text": "summarize: def test_json_cannot_be_loaded(env_ok: tuple[Path, str, str, str, str]):        prefix, _, _, _, package = env_ok    # passing a None type to json.loads() so that it fails    assert find_altered_packages(prefix) == {}",
        "labels_text": "Test that run for the case when json file is missing"
    },
    {
        "input_text": "summarize: def register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator        def decorate(f: Callable) -> Callable:                HANDLERS.setdefault(vcs, {})[method] = f        return f    return decorate",
        "labels_text": "Create decorator to mark a method a the handler of a VCS"
    },
    {
        "input_text": "summarize: def write_to_version_file(filename: str, versions: Dict[str, Any]) -> None:        contents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))    with open(filename, \"w\") as f:        f.write(SHORT_VERSION_PY % contents)    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))",
        "labels_text": "Write the given version number to the given versionpy file"
    },
    {
        "input_text": "summarize: def plus_or_dot(pieces: Dict[str, Any]) -> str:        if \"+\" in pieces.get(\"closest-tag\", \"\"):        return \".\"    return \"+\"",
        "labels_text": "Return a if we dont already have one else return a"
    },
    {
        "input_text": "summarize: def pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:        vc = str.split(ver, \".post\")    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None",
        "labels_text": "Split pep version string at the postrelease segment Returns the release segment before the postrelease and the postrelease version number or if no postrelease segment is present"
    },
    {
        "input_text": "summarize: def render_pep440_old(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"] or pieces[\"dirty\"]:            rendered += \".post%d\" % pieces[\"distance\"]            if pieces[\"dirty\"]:                rendered += \".dev0\"    else:        # exception #1        rendered = \"0.post%d\" % pieces[\"distance\"]        if pieces[\"dirty\"]:            rendered += \".dev0\"    return rendered",
        "labels_text": "TAGpostDISTANCEdev The dev mean dirty Exceptions no tag postDISTANCEdev"
    },
    {
        "input_text": "summarize: def render_git_describe(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"]:            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])    else:        # exception #1        rendered = pieces[\"short\"]    if pieces[\"dirty\"]:        rendered += \"-dirty\"    return rendered",
        "labels_text": "TAGDISTANCEgHEXdirty Like git describe tag dirty always Exceptions no tag HEXdirty note no g prefix"
    },
    {
        "input_text": "summarize: def render_git_describe_long(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])    else:        # exception #1        rendered = pieces[\"short\"]    if pieces[\"dirty\"]:        rendered += \"-dirty\"    return rendered",
        "labels_text": "TAGDISTANCEgHEXdirty Like git describe tag dirty always long The distancehash is unconditional Exceptions no tag HEXdirty note no g prefix"
    },
    {
        "input_text": "summarize: def get_version() -> str:        return get_versions()[\"version\"]",
        "labels_text": "Get the short version string for this project"
    },
    {
        "input_text": "summarize: def setup_command() -> NoReturn:        errors = do_setup()    errors += scan_setup_py()    sys.exit(1 if errors else 0)",
        "labels_text": "Set up Versioneer and exit with appropriate error code"
    },
    {
        "input_text": "summarize: def handle_saving(*args, **kwargs):        try:        return _handle_saving(*args, **kwargs)    except Exception:        print(\"Failed to save benchmark results:\")        traceback.print_exc()",
        "labels_text": "Patches pytestbenchmarks save handler to avoid raising exception on failure An upstream bug cause failure to generate the benchmark JSON when test fail"
    },
    {
        "input_text": "summarize: def skip_remote_run():        import os    in_gha = os.environ.get(\"CI\", False)    secret_not_set = os.environ.get(\"PREFECT_API_KEY\", \"\") == \"\"    return in_gha and secret_not_set",
        "labels_text": "Github Actions will not populate secret if the workflow is triggered by external collaborator including dependabot This function check if were in a CI environment AND if the secret wa not populated if those condition are true we wont try to run the flow against the remote API"
    },
    {
        "input_text": "summarize: def current_version() -> str:        version = Version(prefect.__version__)    return f\"{version.major}.{version.minor}{version.pre[0] if version.pre else ''}\"",
        "labels_text": "Return a highlevel version string for the current Prefect version such a or rc"
    },
    {
        "input_text": "summarize: def write_mint(update_mint_json: Mint):        with open(docs_path() / \"mint.json\", \"w\") as f:        json.dump(update_mint_json, f, indent=2, ensure_ascii=False)",
        "labels_text": "Write updated mintjson file out"
    },
    {
        "input_text": "summarize: def add_needs_attention_label(issue_number: int, headers: dict):        url = f\"https://api.github.com/repos/{GITHUB_REPO}/issues/{issue_number}/labels\"    response = httpx.post(url, headers=headers, json=[\"needs:attention\"])    response.raise_for_status()    print(f\"Added 'needs:attention' label to issue #{issue_number}\")",
        "labels_text": "Add a needsattention label to the issue Args issuenumber int The number of the issue header dict HTTP header for GitHub API request"
    },
    {
        "input_text": "summarize: def instantiate_config(cls, value: Union[Config, Dict[str, Any]]) -> Dict[str, Any]:                if isinstance(value, Config):            return value.__dict__[\"_user_provided_options\"]        return value",
        "labels_text": "Casts list to Config instance"
    },
    {
        "input_text": "summarize: def deprecated_verify_cert_path(cls, values: Dict[str, Any]) -> Dict[str, Any]:                verify = values.get(\"verify\")        # deprecate using verify in favor of verify_cert_path        # so the UI looks nicer        if verify is not None and not isinstance(verify, bool):            warnings.warn(                (                    \"verify should be a boolean. \"                    \"If you want to use a CA cert bundle, use verify_cert_path instead.\"                ),                DeprecationWarning,            )        return values",
        "labels_text": "If verify is not a bool raise a warning"
    },
    {
        "input_text": "summarize: def _get_client_cached(ctx, client_type: Union[str, ClientType]) -> Any:        with _LOCK:        if isinstance(client_type, ClientType):            client_type = client_type.value        client = ctx.get_boto3_session().client(            service_name=client_type,            **ctx.aws_client_parameters.get_params_override(),        )    return client",
        "labels_text": "Helper method to cache and dynamically get a client type Args clienttype The client service name Returns An authenticated client Raises ValueError if the client is not supported"
    },
    {
        "input_text": "summarize: def get_client(self, client_type: Union[str, ClientType]):                if isinstance(client_type, ClientType):            client_type = client_type.value        return _get_client_cached(ctx=self, client_type=client_type)",
        "labels_text": "Helper method to dynamically get a client type Args clienttype The client service name Returns An authenticated client Raises ValueError if the client is not supported"
    },
    {
        "input_text": "summarize: def get_s3_client(self) -> S3Client:                return self.get_client(client_type=ClientType.S3)",
        "labels_text": "Gets an authenticated S client Returns An authenticated S client"
    },
    {
        "input_text": "summarize: def get_secrets_manager_client(self) -> SecretsManagerClient:                return self.get_client(client_type=ClientType.SECRETS_MANAGER)",
        "labels_text": "Gets an authenticated Secrets Manager client Returns An authenticated Secrets Manager client"
    },
    {
        "input_text": "summarize: def get_boto3_session(self) -> boto3.Session:                minio_root_password = (            self.minio_root_password.get_secret_value()            if self.minio_root_password            else None        )        return boto3.Session(            aws_access_key_id=self.minio_root_user,            aws_secret_access_key=minio_root_password,            region_name=self.region_name,        )",
        "labels_text": "Returns an authenticated boto session that can be used to create client and perform object operation on MinIO server Example Create an S client from an authorized boto session python miniocredentials MinIOCredentials miniorootuser miniorootuser miniorootpassword miniorootpassword sclient miniocredentialsgetbotosessionclient service endpointurlhttplocalhost"
    },
    {
        "input_text": "summarize: def get_client(self, client_type: Union[str, ClientType]):                if isinstance(client_type, ClientType):            client_type = client_type.value        return _get_client_cached(ctx=self, client_type=client_type)",
        "labels_text": "Helper method to dynamically get a client type Args clienttype The client service name Returns An authenticated client Raises ValueError if the client is not supported"
    },
    {
        "input_text": "summarize: def get_s3_client(self) -> S3Client:                return self.get_client(client_type=ClientType.S3)",
        "labels_text": "Gets an authenticated S client Returns An authenticated S client"
    },
    {
        "input_text": "summarize: def _get_job_run(self):                return self.client.get_job_run(JobName=self.job_name, RunId=self.job_id)",
        "labels_text": "get glue job"
    },
    {
        "input_text": "summarize: def _get_client(self) -> _GlueJobClient:                boto_session = self.aws_credentials.get_boto3_session()        return boto_session.client(\"glue\")",
        "labels_text": "Retrieve a Glue Job Client"
    },
    {
        "input_text": "summarize: def _get_lambda_client(self):                boto_session = self.aws_credentials.get_boto3_session()        lambda_client = boto_session.client(\"lambda\")        return lambda_client",
        "labels_text": "Retrieve a boto session and Lambda client"
    },
    {
        "input_text": "summarize: def _list_objects_sync(page_iterator: PageIterator):        return [content for page in page_iterator for content in page.get(\"Contents\", [])]",
        "labels_text": "Synchronous method to collect S object into a list Args pageiterator AWS Paginator for S object Returns ListDict List of object information"
    },
    {
        "input_text": "summarize: def basepath(self) -> str:                return self.bucket_folder",
        "labels_text": "The base path of the S bucket Returns str The base path of the S bucket"
    },
    {
        "input_text": "summarize: def _resolve_path(self, path: str) -> str:                # If bucket_folder provided, it means we won't write to the root dir of        # the bucket. So we need to add it on the front of the path.        #        # AWS object key naming guidelines require '/' for bucket folders.        # Get POSIX path to prevent `pathlib` from inferring '\\' on Windows OS        path = (            (Path(self.bucket_folder) / path).as_posix() if self.bucket_folder else path        )        return path",
        "labels_text": "A helper function used in writepath to join selfbasepath and path Args path Name of the key eg file Each object in your bucket ha a unique key or key name"
    },
    {
        "input_text": "summarize: def _get_s3_client(self) -> boto3.client:                return self.credentials.get_client(\"s3\")",
        "labels_text": "Authenticate MinIO credential or AWS credential and return an S client This is a helper function called by readpath or writepath"
    },
    {
        "input_text": "summarize: def _get_bucket_resource(self) -> boto3.resource:                params_override = self.credentials.aws_client_parameters.get_params_override()        bucket = (            self.credentials.get_boto3_session()            .resource(\"s3\", **params_override)            .Bucket(self.bucket_name)        )        return bucket",
        "labels_text": "Retrieves boto resource object for the configured bucket"
    },
    {
        "input_text": "summarize: def _read_sync(self, key: str) -> bytes:                s3_client = self._get_s3_client()        with io.BytesIO() as stream:            s3_client.download_fileobj(Bucket=self.bucket_name, Key=key, Fileobj=stream)            stream.seek(0)            output = stream.read()            return output",
        "labels_text": "Called by readpath Creates an S client and retrieves the content from a specified path"
    },
    {
        "input_text": "summarize: def _write_sync(self, key: str, data: bytes) -> None:                s3_client = self._get_s3_client()        with io.BytesIO(data) as stream:            s3_client.upload_fileobj(Fileobj=stream, Bucket=self.bucket_name, Key=key)",
        "labels_text": "Called by writepath Creates an S client and uploads a file object"
    },
    {
        "input_text": "summarize: def _list_objects_sync(page_iterator: PageIterator) -> List[Dict[str, Any]]:                return [            content for page in page_iterator for content in page.get(\"Contents\", [])        ]",
        "labels_text": "Synchronous method to collect S object into a list Args pageiterator AWS Paginator for S object Returns ListDict List of object information"
    },
    {
        "input_text": "summarize: def hash_collection(collection) -> int:        def make_hashable(item):                if isinstance(item, dict):            return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))        elif isinstance(item, list):            return tuple(make_hashable(v) for v in item)        return item    hashable_collection = visit_collection(        collection, visit_fn=make_hashable, return_data=True    )    return hash(hashable_collection)",
        "labels_text": "Use visitcollection to transform and hash a collection Args collection Any The collection to hash Returns int The hash of the transformed collection Example python from prefectawsutilities import hashcollection hashcollectiona b"
    },
    {
        "input_text": "summarize: def assemble_document_for_patches(patches):        document = {}    for patch in patches:        operation = patch[\"op\"]        path = patch[\"path\"].lstrip(\"/\").split(\"/\")        if operation == \"add\":            # Ensure all but the last element of the path exists            ensure_path_exists(document, path[:-1])        elif operation in [\"remove\", \"replace\"]:            # For remove and replace, the entire path should exist            ensure_path_exists(document, path)    return document",
        "labels_text": "Assembles an initial document that can successfully accept the given JSON Patch operation Args patch A list of JSON Patch operation Returns An initial document structured to accept the patch Example python patch op replace path name value Jane op add path contactaddress value Main St op remove path age initialdocument assembledocumentforpatchespatches output name contact age"
    },
    {
        "input_text": "summarize: def push_project_to_s3(*args, **kwargs):        push_to_s3(*args, **kwargs)",
        "labels_text": "Deprecated Use pushtos instead"
    },
    {
        "input_text": "summarize: def pull_project_from_s3(*args, **kwargs):        pull_from_s3(*args, **kwargs)",
        "labels_text": "Deprecated Use pullfroms instead"
    },
    {
        "input_text": "summarize: def _default_task_definition_template() -> dict:        return yaml.safe_load(DEFAULT_TASK_DEFINITION_TEMPLATE)",
        "labels_text": "The default task definition template for ECS job"
    },
    {
        "input_text": "summarize: def _default_task_run_request_template() -> dict:        return yaml.safe_load(DEFAULT_TASK_RUN_REQUEST_TEMPLATE)",
        "labels_text": "The default task run request template for ECS job"
    },
    {
        "input_text": "summarize: def _drop_empty_keys_from_dict(taskdef: dict):        for key, value in tuple(taskdef.items()):        if not value:            taskdef.pop(key)        if isinstance(value, dict):            _drop_empty_keys_from_dict(value)        if isinstance(value, list) and key != \"capacity_provider_strategy\":            for v in value:                if isinstance(v, dict):                    _drop_empty_keys_from_dict(v)",
        "labels_text": "Recursively drop key with empty value from a task definition dict Mutates the task definition in place Only support recursion into dicts and list"
    },
    {
        "input_text": "summarize: def _get_container(containers: List[dict], name: str) -> Optional[dict]:        for container in containers:        if container.get(\"name\") == name:            return container    return None",
        "labels_text": "Extract a container from a list of container or container definition If not found None is returned"
    },
    {
        "input_text": "summarize: def parse_identifier(identifier: str) -> ECSIdentifier:        cluster, task = identifier.split(\"::\", maxsplit=1)    return ECSIdentifier(cluster, task)",
        "labels_text": "Splits identifier into it cluster and task component eg input clusternametaskarn output clustername taskarn"
    },
    {
        "input_text": "summarize: def task_run_request_requires_arn_if_no_task_definition_given(self) -> Self:                if (            not (self.task_run_request or {}).get(\"taskDefinition\")            and not self.task_definition        ):            raise ValueError(                \"A task definition must be provided if a task definition ARN is not \"                \"present on the task run request.\"            )        return self",
        "labels_text": "If no task definition is provided a task definition ARN must be present on the task run request"
    },
    {
        "input_text": "summarize: def container_name_default_from_task_definition(self) -> Self:                if self.container_name is None:            self.container_name = _container_name_from_task_definition(                self.task_definition            )            # We may not have a name here still; for example if someone is using a task            # definition arn. In that case, we'll perform similar logic later to find            # the name to treat as the \"orchestration\" container.        return self",
        "labels_text": "Infers the container name from the task definition if not provided"
    },
    {
        "input_text": "summarize: def set_default_configure_cloudwatch_logs(self) -> Self:                configure_cloudwatch_logs = self.configure_cloudwatch_logs        if configure_cloudwatch_logs is None:            self.configure_cloudwatch_logs = self.stream_output        return self",
        "labels_text": "Streaming output generally requires CloudWatch log to be configured To avoid entangled argument in the simple case configurecloudwatchlogs default to matching the value of streamoutput"
    },
    {
        "input_text": "summarize: def cloudwatch_logs_options_requires_configure_cloudwatch_logs(        self,    ) -> Self:                if self.cloudwatch_logs_options and not self.configure_cloudwatch_logs:            raise ValueError(                \"`configure_cloudwatch_log` must be enabled to use \"                \"`cloudwatch_logs_options`.\"            )        return self",
        "labels_text": "Enforces that an execution role arn is provided or could be provided by a runtime task definition when configuring logging"
    },
    {
        "input_text": "summarize: def network_configuration_requires_vpc_id(self) -> Self:                if self.network_configuration and not self.vpc_id:            raise ValueError(                \"You must provide a `vpc_id` to enable custom `network_configuration`.\"            )        return self",
        "labels_text": "Enforces a vpcid is provided when custom network configuration mode is enabled for network setting"
    },
    {
        "input_text": "summarize: def _get_client(        self, configuration: ECSJobConfiguration, client_type: Union[str, ClientType]    ) -> _ECSClient:                return configuration.aws_credentials.get_client(client_type)",
        "labels_text": "Get a boto client of clienttype Will use a cached client if one exists"
    },
    {
        "input_text": "summarize: def _register_task_definition(        self,        logger: logging.Logger,        ecs_client: _ECSClient,        task_definition: dict,    ) -> str:                logger.info(\"Registering ECS task definition...\")        logger.debug(            \"Task definition request\"            f\"{json.dumps(task_definition, indent=2, default=str)}\"        )        response = ecs_client.register_task_definition(**task_definition)        return response[\"taskDefinition\"][\"taskDefinitionArn\"]",
        "labels_text": "Register a new task definition with AWS Returns the ARN"
    },
    {
        "input_text": "summarize: def _create_task_run(self, ecs_client: _ECSClient, task_run_request: dict) -> str:                task = ecs_client.run_task(**task_run_request)        if task[\"failures\"]:            raise RuntimeError(                f\"Failed to run ECS task: {task['failures'][0]['reason']}\"            )        elif not task[\"tasks\"]:            raise RuntimeError(                \"Failed to run ECS task: no tasks or failures were returned.\"            )        return task[\"tasks\"][0]",
        "labels_text": "Create a run of a task definition Returns the task run ARN"
    },
    {
        "input_text": "summarize: def test_aws_credentials_get_boto3_session():        with mock_s3():        aws_credentials_block = AwsCredentials()        boto3_session = aws_credentials_block.get_boto3_session()        assert isinstance(boto3_session, Session)",
        "labels_text": "Asserts that instantiated AwsCredentials block creates an authenticated boto session"
    },
    {
        "input_text": "summarize: def test_minio_credentials_get_boto3_session():        minio_credentials_block = MinIOCredentials(        minio_root_user=\"root_user\", minio_root_password=\"root_password\"    )    boto3_session = minio_credentials_block.get_boto3_session()    assert isinstance(boto3_session, Session)",
        "labels_text": "Asserts that instantiated MinIOCredentials block creates an authenticated boto session"
    },
    {
        "input_text": "summarize: def test_get_client_cached(credentials, client_type):        _get_client_cached.cache_clear()    assert _get_client_cached.cache_info().hits == 0, \"Initial call count should be 0\"    credentials.get_client(client_type)    credentials.get_client(client_type)    credentials.get_client(client_type)    assert _get_client_cached.cache_info().misses == 1    assert _get_client_cached.cache_info().hits == 2",
        "labels_text": "Test to ensure that getclientcached function return the same instance for multiple call with the same parameter and properly utilizes lrucache"
    },
    {
        "input_text": "summarize: def make_patched_invocation(client, handler):        true_invoke = client.invoke    def invoke(*args, **kwargs):                result = true_invoke(*args, **kwargs)        blob = json.dumps(            handler(                event=kwargs.get(\"Payload\"),                context=kwargs.get(\"ClientContext\"),            )        ).encode()        result[\"Payload\"] = StreamingBody(io.BytesIO(blob), len(blob))        return result    return invoke",
        "labels_text": "Creates a patched invoke method for moto lambda The method replaces the response Payload with the result of the handler function"
    },
    {
        "input_text": "summarize: def mock_invoke(    lambda_function: LambdaFunction, handler, monkeypatch: pytest.MonkeyPatch):        client = lambda_function._get_lambda_client()    monkeypatch.setattr(        client,        \"invoke\",        make_patched_invocation(client, handler),    )    def _get_lambda_client():        return client    monkeypatch.setattr(        lambda_function,        \"_get_lambda_client\",        _get_lambda_client,    )    yield",
        "labels_text": "Fixture to patch the invocation response Payload field When resultPayloadread is called moto attempt to run the function in a Docker container and return the result This is total overkill so we actually call the handler with the given argument"
    },
    {
        "input_text": "summarize: def s3():        with mock_s3():        yield boto3.client(            service_name=\"s3\",            region_name=\"us-east-1\",            aws_access_key_id=\"minioadmin\",            aws_secret_access_key=\"testing\",            aws_session_token=\"testing\",        )",
        "labels_text": "Mock connection to AWS S with boto client"
    },
    {
        "input_text": "summarize: def test_read_path_in_sync_context(s3_bucket_with_file):        s3_bucket, key = s3_bucket_with_file    content = s3_bucket.read_path(key)    assert content == b\"hello\"",
        "labels_text": "Test that read path work in a sync context"
    },
    {
        "input_text": "summarize: def test_write_path_in_sync_context(s3_bucket):        key = s3_bucket.write_path(\"test.txt\", content=b\"hello\")    content = s3_bucket.read_path(key)    assert content == b\"hello\"",
        "labels_text": "Test that write path work in a sync context"
    },
    {
        "input_text": "summarize: def patch_run_task(mock, run_task, *args, **kwargs):        mock(*args, **kwargs)    return run_task(*args, **kwargs)",
        "labels_text": "Track call to runtask by calling a mock a well"
    },
    {
        "input_text": "summarize: def patch_calculate_task_resource_requirements(    _calculate_task_resource_requirements, task_definition):        for container_definition in task_definition.container_definitions:        container_definition.setdefault(\"memory\", 0)    return _calculate_task_resource_requirements(task_definition)",
        "labels_text": "Adds support for nonEC execution mode to motos calculation of task definition"
    },
    {
        "input_text": "summarize: def create_test_ecs_cluster(ecs_client, cluster_name) -> str:        return ecs_client.create_cluster(clusterName=cluster_name)[\"cluster\"][\"clusterArn\"]",
        "labels_text": "Create an ECS cluster and return it ARN"
    },
    {
        "input_text": "summarize: def describe_task(ecs_client, task_arn, **kwargs) -> dict:        return ecs_client.describe_tasks(tasks=[task_arn], include=[\"TAGS\"], **kwargs)[        \"tasks\"    ][0]",
        "labels_text": "Describe a single ECS task"
    },
    {
        "input_text": "summarize: def get_client(self) -> \"BlobServiceClient\":                if self.connection_string is None:            return BlobServiceClient(                account_url=self.account_url,                credential=ADefaultAzureCredential(),            )        return BlobServiceClient.from_connection_string(            self.connection_string.get_secret_value()        )",
        "labels_text": "Returns an authenticated base Blob Service client that can be used to create other client for Azure service Example Create an authorized Blob Service session python import o import asyncio from prefect import flow from prefectazure import AzureBlobStorageCredentials flow async def examplegetclientflow connectionstring osgetenvAZURESTORAGECONNECTIONSTRING azurecredentials AzureBlobStorageCredentials connectionstringconnectionstring async with azurecredentialsgetclient a blobserviceclient run other code here pas asynciorunexamplegetclientflow"
    },
    {
        "input_text": "summarize: def get_blob_client(self, container, blob) -> \"BlobClient\":                if self.connection_string is None:            return BlobClient(                account_url=self.account_url,                container_name=container,                credential=ADefaultAzureCredential(),                blob_name=blob,            )        blob_client = BlobClient.from_connection_string(            self.connection_string.get_secret_value(), container, blob        )        return blob_client",
        "labels_text": "Returns an authenticated Blob client that can be used to download and upload blob Args container Name of the Blob Storage container to retrieve from blob Name of the blob within this container to retrieve Example Create an authorized Blob session python import o import asyncio from prefect import flow from prefectazure import AzureBlobStorageCredentials flow async def examplegetblobclientflow connectionstring osgetenvAZURESTORAGECONNECTIONSTRING azurecredentials AzureBlobStorageCredentials connectionstringconnectionstring async with azurecredentialsgetblobclient container blob a blobclient run other code here pas asynciorunexamplegetblobclientflow"
    },
    {
        "input_text": "summarize: def get_container_client(self, container) -> \"ContainerClient\":                if self.connection_string is None:            return ContainerClient(                account_url=self.account_url,                container_name=container,                credential=ADefaultAzureCredential(),            )        container_client = ContainerClient.from_connection_string(            self.connection_string.get_secret_value(), container        )        return container_client",
        "labels_text": "Returns an authenticated Container client that can be used to create client for Azure service Args container Name of the Blob Storage container to retrieve from Example Create an authorized Container session python import o import asyncio from prefect import flow from prefectazure import AzureBlobStorageCredentials flow async def examplegetcontainerclientflow connectionstring osgetenvAZURESTORAGECONNECTIONSTRING azurecredentials AzureBlobStorageCredentials connectionstringconnectionstring async with azurecredentialsgetcontainerclient container a containerclient run other code here pas asynciorunexamplegetcontainerclientflow"
    },
    {
        "input_text": "summarize: def get_client(self) -> \"CosmosClient\":                return CosmosClient.from_connection_string(            self.connection_string.get_secret_value()        )",
        "labels_text": "Returns an authenticated Cosmos client that can be used to create other client for Azure service Example Create an authorized Cosmos session python import o from prefect import flow from prefectazure import AzureCosmosDbCredentials flow def examplegetclientflow connectionstring osgetenvAZURECOSMOSCONNECTIONSTRING azurecredentials AzureCosmosDbCredentials connectionstringconnectionstring cosmosclient azurecredentialsgetclient return cosmosclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def get_database_client(self, database: str) -> \"DatabaseProxy\":                cosmos_client = self.get_client()        database_client = cosmos_client.get_database_client(database=database)        return database_client",
        "labels_text": "Returns an authenticated Database client Args database Name of the database Example Create an authorized Cosmos session python import o from prefect import flow from prefectazure import AzureCosmosDbCredentials flow def examplegetclientflow connectionstring osgetenvAZURECOSMOSCONNECTIONSTRING azurecredentials AzureCosmosDbCredentials connectionstringconnectionstring cosmosclient azurecredentialsgetdatabaseclient return cosmosclient examplegetdatabaseclientflow"
    },
    {
        "input_text": "summarize: def get_container_client(self, container: str, database: str) -> \"ContainerProxy\":                database_client = self.get_database_client(database)        container_client = database_client.get_container_client(container=container)        return container_client",
        "labels_text": "Returns an authenticated Container client used for querying Args container Name of the Cosmos DB container to retrieve from database Name of the Cosmos DB database Example Create an authorized Container session python import o from prefect import flow from prefectazure import AzureBlobStorageCredentials flow def examplegetcontainerclientflow connectionstring osgetenvAZURECOSMOSCONNECTIONSTRING azurecredentials AzureCosmosDbCredentials connectionstringconnectionstring containerclient azurecredentialsgetcontainerclientcontainer return containerclient examplegetcontainerclientflow"
    },
    {
        "input_text": "summarize: def get_container_client(self, subscription_id: str):                return ContainerInstanceManagementClient(            credential=self._create_credential(),            subscription_id=subscription_id,        )",
        "labels_text": "Creates an Azure Container Instances client initialized with data from this block field and a provided Azure subscription ID Args subscriptionid A valid Azure subscription ID Returns An initialized ContainerInstanceManagementClient"
    },
    {
        "input_text": "summarize: def get_resource_client(self, subscription_id: str):                return ResourceManagementClient(            credential=self._create_credential(),            subscription_id=subscription_id,        )",
        "labels_text": "Creates an Azure resource management client initialized with data from this block field and a provided Azure subscription ID Args subscriptionid A valid Azure subscription ID Returns An initialized ResourceManagementClient"
    },
    {
        "input_text": "summarize: def _create_credential(self):                auth_args = (self.client_id, self.tenant_id, self.client_secret)        if auth_args == (None, None, None):            return DefaultAzureCredential(**self.credential_kwargs)        return ClientSecretCredential(            tenant_id=self.tenant_id,            client_id=self.client_id,            client_secret=self.client_secret.get_secret_value(),            **self.credential_kwargs,        )",
        "labels_text": "Creates an Azure credential initialized with data from this block field Returns An initialized Azure TokenCredential ready to use with Azure SDK client class"
    },
    {
        "input_text": "summarize: def ml_list_datastores(ml_credentials: \"AzureMlCredentials\") -> Dict:        logger = get_run_logger()    logger.info(\"Listing datastores\")    workspace = ml_credentials.get_workspace()    results = workspace.datastores    return results",
        "labels_text": "Lists the Datastores in the Workspace Args mlcredentials Credentials to use for authentication with Azure Example List Datastore object python from prefect import flow from prefectazure import AzureMlCredentials from prefectazuremldatastore import mllistdatastores flow def examplemllistdatastoresflow mlcredentials AzureMlCredentials tenantidtenantid serviceprincipalidserviceprincipalid serviceprincipalpasswordserviceprincipalpassword subscriptionidsubscriptionid resourcegroupresourcegroup workspacenameworkspacename result mllistdatastoresmlcredentials return result"
    },
    {
        "input_text": "summarize: def _add_image(self):                try:            self.arm_template[\"resources\"][0][\"properties\"][\"containers\"][0][                \"properties\"            ][\"image\"] = self.image        except KeyError:            raise ValueError(\"Unable to add image due to invalid job ARM template.\")",
        "labels_text": "Add the image to the arm template"
    },
    {
        "input_text": "summarize: def _add_subnets(self, subnet_ids: List[str]):                self.arm_template[\"resources\"][0][\"properties\"][\"subnetIds\"] = [            {\"id\": subnet_id} for subnet_id in subnet_ids        ]",
        "labels_text": "Add subnets to the container group Args subnetids A list of subnet id to add to the container group"
    },
    {
        "input_text": "summarize: def _add_dns_servers(self, dns_servers: List[str]):                self.arm_template[\"resources\"][0][\"properties\"][\"dnsConfig\"] = {            \"nameServers\": dns_servers        }",
        "labels_text": "Add dns server to the container group Args dnsservers A list of dns server to add to the container group"
    },
    {
        "input_text": "summarize: def _get_arm_environment(self):                env = {**self._base_environment(), **self.env}        azure_env = [            (                {\"name\": key, \"secureValue\": value}                if key in ENV_SECRETS                else {\"name\": key, \"value\": value}            )            for key, value in env.items()        ]        return azure_env",
        "labels_text": "Returns the environment variable to pas to the ARM template"
    },
    {
        "input_text": "summarize: def _get_container(self, container_group: ContainerGroup) -> Container:                return container_group.containers[0]",
        "labels_text": "Extracts the job container from a container group"
    },
    {
        "input_text": "summarize: def _get_container_group(        client: ContainerInstanceManagementClient,        resource_group_name: str,        container_group_name: str,    ) -> ContainerGroup:                return client.container_groups.get(            resource_group_name=resource_group_name,            container_group_name=container_group_name,        )",
        "labels_text": "Gets the container group from Azure"
    },
    {
        "input_text": "summarize: def _get_and_stream_output(        self,        client: ContainerInstanceManagementClient,        configuration: AzureContainerJobConfiguration,        container_group: ContainerGroup,        last_log_time: datetime.datetime,    ) -> datetime.datetime:                logs = self._get_logs(            client=client, configuration=configuration, container_group=container_group        )        return self._stream_output(logs, last_log_time)",
        "labels_text": "Fetches log output from the job container and writes all entry after a given time to stderr Args client An initialized ContainerInstanceManagementClient containergroup The container group that hold the job container lastlogtime The timestamp of the last output line already streamed Returns The time of the most recent output line written by this call"
    },
    {
        "input_text": "summarize: def _log_prefix(self) -> str:                if self.name is not None:            return f\"AzureContainerInstanceJob {self.name!r}\"        else:            return \"AzureContainerInstanceJob\"",
        "labels_text": "Internal property for generating a prefix for log where name may be null"
    },
    {
        "input_text": "summarize: def _provisioning_succeeded(container_group: Union[ContainerGroup, None]) -> bool:                if not container_group:            return False        return (            container_group.provisioning_state            == ContainerGroupProvisioningState.SUCCEEDED            and len(container_group.containers) == 1        )",
        "labels_text": "Determines whether ACI container group provisioning wa successful Args containergroup a container group returned by the Azure SDK Returns True if provisioning wa successful False otherwise"
    },
    {
        "input_text": "summarize: def _write_output_line(line: str):                print(line, file=sys.stderr)",
        "labels_text": "Writes a line of output to stderr"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def credential_values(    credentials: AzureContainerInstanceCredentials,) -> Tuple[str, str, str]:        return (        credentials.client_id,        credentials.client_secret.get_secret_value(),        credentials.tenant_id,    )",
        "labels_text": "Helper function to extract value from an Azure container instance credential block Args credential The credential to extract value from Returns A tuple containing clientid clientsecret tenantid from the credential block"
    },
    {
        "input_text": "summarize: def create_mock_container_group(state: str, exit_code: Union[int, None]):        container_group = Mock()    container = Mock()    container.instance_view.current_state.state = state    container.instance_view.current_state.exit_code = exit_code    containers = [container]    container_group.containers = containers    # Azure assigns all provisioned container groups a stringified    # UUID name.    container_group.name = str(uuid.uuid4())    return container_group",
        "labels_text": "Creates a mock container group with a single container to serve a a standin for an Azure ContainerInstanceManagementClients containergroup property Args state The state the single container in the group should report exitcode The container exit code or None Returns A mock container group"
    },
    {
        "input_text": "summarize: def running_worker_container_group():        container_group = create_mock_container_group(state=\"Running\", exit_code=None)    container_group.provisioning_state = ContainerGroupProvisioningState.SUCCEEDED    return container_group",
        "labels_text": "A fixture that return a mock container group simulating a a container group that is currently running a flow run"
    },
    {
        "input_text": "summarize: def completed_worker_container_group():        container_group = create_mock_container_group(state=\"Terminated\", exit_code=0)    container_group.provisioning_state = ContainerGroupProvisioningState.SUCCEEDED    return container_group",
        "labels_text": "A fixture that return a mock container group simulating a a container group that successfully completed it flow run"
    },
    {
        "input_text": "summarize: def job_configuration(aci_credentials, worker_flow_run):        return asyncio.run(create_job_configuration(aci_credentials, worker_flow_run))",
        "labels_text": "Returns a basic initialized ACI infrastructure block suitable for use in a variety of test"
    },
    {
        "input_text": "summarize: def mock_prefect_client(monkeypatch, worker_flow):        mock_client = Mock()    mock_client.read_flow = AsyncMock()    mock_client.read_flow.return_value = worker_flow    monkeypatch.setattr(        prefect_azure.workers.container_instance,        \"get_client\",        Mock(return_value=mock_client),    )    return mock_client",
        "labels_text": "A fixture that provides a mock Prefect client"
    },
    {
        "input_text": "summarize: def _validate_username(cls, value: str) -> str:                pattern = \"^[A-Za-z0-9_-]*$\"        if not re.match(pattern, value):            raise ValueError(                \"Username must be alpha, num, dash and/or underscore only.\"            )        if not len(value) <= 30:            raise ValueError(\"Username cannot be longer than 30 chars.\")        return value",
        "labels_text": "When username provided will validate it"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def test_bitbucket_credentials(token):        credentials_block = BitBucketCredentials(token=token)    assert isinstance(credentials_block, Block)",
        "labels_text": "Test credential is Block type"
    },
    {
        "input_text": "summarize: def ensure_valid_bitbucket_username_passes():        try:        BitBucketCredentials(token=\"token\", username=\"validusername\")    except Exception as exc:        assert False, f\"Valid username raised an exception {exc}\"",
        "labels_text": "Ensure invalid char username raise"
    },
    {
        "input_text": "summarize: def test_bitbucket_username_invalid_char():        with pytest.raises(ValueError):        BitBucketCredentials(token=\"token\", username=\"invalid!username\")",
        "labels_text": "Ensure invalid char username raise"
    },
    {
        "input_text": "summarize: def test_bitbucket_username_over_max_length():        with pytest.raises(ValueError):        BitBucketCredentials(            token=\"token\", username=\"usernamethatisoverthirtycharacters\"        )",
        "labels_text": "Ensure username of greater than max allowed length raise"
    },
    {
        "input_text": "summarize: def __enter__(self):                        return self.dir",
        "labels_text": "Create mock directory and return it"
    },
    {
        "input_text": "summarize: def __exit__(self, *args, **kwargs):",
        "labels_text": "Delete mock directory"
    },
    {
        "input_text": "summarize: def __eq__(self, other: object) -> bool:                if isinstance(other, DaskTaskRunner):            return (                self.address == other.address                and self.cluster_class == other.cluster_class                and self.cluster_kwargs == other.cluster_kwargs                and self.adapt_kwargs == other.adapt_kwargs                and self.client_kwargs == other.client_kwargs            )        else:            return False",
        "labels_text": "Check if an instance ha the same setting a this task runner"
    },
    {
        "input_text": "summarize: def duplicate(self):                return type(self)(            address=self.address,            cluster_class=self.cluster_class,            cluster_kwargs=self.cluster_kwargs,            adapt_kwargs=self.adapt_kwargs,            client_kwargs=self.client_kwargs,        )",
        "labels_text": "Create a new instance of the task runner with the same setting"
    },
    {
        "input_text": "summarize: def get_dask_client(    timeout: Optional[Union[int, float, str, timedelta]] = None,    **client_kwargs: Dict[str, Any],) -> Generator[Client, None, None]:        client_kwargs = _generate_client_kwargs(        async_client=False, timeout=timeout, **client_kwargs    )    with Client(**client_kwargs) as client:        yield client",
        "labels_text": "Yields a temporary synchronous dask client this is useful for parallelizing operation on dask collection such a a daskDataFrame or daskBag Without invoking this worker do not automatically get a client to connect to the full cluster Therefore it will attempt perform work within the worker itself serially and potentially overwhelming the single worker When in an async context we recommend using getasyncdaskclient instead Args timeout Timeout after which to error out ha no effect in flow run context because the client ha already started Defaults to the distributedcommtimeoutsconnect configuration value clientkwargs Additional keyword argument to pas to distributedClient and overwrites inherited keyword argument from the task runner if any Yields A temporary synchronous dask client Examples Use getdaskclient to distribute work across worker python import dask from prefect import flow task from prefectdask import DaskTaskRunner getdaskclient task def computetask with getdaskclienttimeouts a client df daskdatasetstimeseries partitionfreqw summarydf clientcomputedfdescriberesult return summarydf flowtaskrunnerDaskTaskRunner def daskflow prefectfuture computetasksubmit return prefectfutureresult daskflow"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def dask_task_runner_with_existing_cluster(use_hosted_api_server):  # noqa        with distributed.LocalCluster(n_workers=2) as cluster:        yield DaskTaskRunner(cluster=cluster)",
        "labels_text": "Generate a dask task runner thats connected to a local cluster"
    },
    {
        "input_text": "summarize: def dask_task_runner_with_existing_cluster_address(use_hosted_api_server):  # noqa        with distributed.LocalCluster(n_workers=2) as cluster:        with distributed.Client(cluster) as client:            address = client.scheduler.address            yield DaskTaskRunner(address=address)",
        "labels_text": "Generate a dask task runner thats connected to a local cluster"
    },
    {
        "input_text": "summarize: def get_client(self) -> AsyncClient:                base_url = f\"https://{self.databricks_instance}/api/\"        client_kwargs = self.client_kwargs or {}        client_kwargs[\"headers\"] = {            \"Authorization\": f\"Bearer {self.token.get_secret_value()}\"        }        client = AsyncClient(base_url=base_url, **client_kwargs)        return client",
        "labels_text": "Gets an Databricks REST AsyncClient Returns An Databricks REST AsyncClient Example Gets a Databricks REST AsyncClient python from prefect import flow from prefectdatabricks import DatabricksCredentials flow def examplegetclientflow token consumerkey databrickscredentials DatabricksCredentialstokentoken client databrickscredentialsgetclient return client examplegetclientflow"
    },
    {
        "input_text": "summarize: def serialize_model(obj: Any) -> Any:        if isinstance(obj, list):        return [serialize_model(o) for o in obj]    elif isinstance(obj, Dict):        return {k: serialize_model(v) for k, v in obj.items()}    if isinstance(obj, BaseModel):        return obj.model_dump(mode=\"json\")    elif isinstance(obj, Enum):        return obj.value    return obj",
        "labels_text": "Recursively serializes pydanticBaseModel into JSON return original obj if not a BaseModel Args obj Input object to serialize Returns Serialized version of object"
    },
    {
        "input_text": "summarize: def strip_kwargs(**kwargs: Dict) -> Dict:        stripped_dict = {}    for k, v in kwargs.items():        v = serialize_model(v)        if isinstance(v, dict):            v = strip_kwargs(**v)        if v is not None:            stripped_dict[k] = v    return stripped_dict or {}",
        "labels_text": "Recursively drop keyword argument if value is None and serializes any pydanticBaseModel type Args kwargs Input keyword argument Returns Stripped version of kwargs"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def _append_dirs_to_commands(self, profiles_dir) -> List[str]:                project_dir = self.project_dir        commands = []        for command in self.commands:            command += f\" --profiles-dir {profiles_dir}\"            if project_dir is not None:                project_dir = Path(project_dir).expanduser()                command += f\" --project-dir {project_dir}\"            commands.append(command)        return commands",
        "labels_text": "Append profilesdir and projectdir option to dbt command"
    },
    {
        "input_text": "summarize: def _create_node_info_md(node_name, resource_type, message, path, compiled_code) -> str:        markdown = f    if compiled_code:        markdown += f    return markdown",
        "labels_text": "Creates template for unsuccessful node information"
    },
    {
        "input_text": "summarize: def _create_node_summary_table_md(run_results: dict) -> str:        markdown = f    return markdown",
        "labels_text": "Creates a table for node summary"
    },
    {
        "input_text": "summarize: def get_profile(self) -> Dict[str, Any]:                profile = {            \"config\": self.global_configs.get_configs() if self.global_configs else {},            self.name: {                \"target\": self.target,                \"outputs\": {self.target: self.target_configs.get_configs()},            },        }        return profile",
        "labels_text": "Returns the dbt profile likely used for writing to profilesyml Returns A JSON compatible dictionary with the expected format of profilesyml"
    },
    {
        "input_text": "summarize: def get_configs(self) -> Dict[str, Any]:                return self._populate_configs_json({}, self.model_fields, model=self)",
        "labels_text": "Returns the dbt configs likely used eventually for writing to profilesyml Returns A configs JSON"
    },
    {
        "input_text": "summarize: def query(        self,        query: str,        variables: Optional[Dict] = None,        operation_name: Optional[str] = None,    ) -> Dict[str, Any]:                return self._http_endpoint(            query=query, variables=variables, operation_name=operation_name        )",
        "labels_text": "Run a GraphQL query against the dbt Cloud metadata API Args query The GraphQL query to run variable The value of any variable defined in the GraphQL query operationname The name of the operation to run if multiple operation are defined in the provided query Returns The result of the GraphQL query"
    },
    {
        "input_text": "summarize: def get_administrative_client(self) -> DbtCloudAdministrativeClient:                return DbtCloudAdministrativeClient(            api_key=self.api_key.get_secret_value(),            account_id=self.account_id,            domain=self.domain,        )",
        "labels_text": "Returns a newly instantiated client for working with the dbt Cloud administrative API Returns An authenticated dbt Cloud administrative API client"
    },
    {
        "input_text": "summarize: def get_metadata_client(self) -> DbtCloudMetadataClient:                return DbtCloudMetadataClient(            api_key=self.api_key.get_secret_value(),            domain=f\"metadata.{self.domain}\",        )",
        "labels_text": "Returns a newly instantiated client for working with the dbt Cloud metadata API Example Sending query via the returned metadata client python from prefectdbt import DbtCloudCredentials credentialsblock DbtCloudCredentialsloadtestaccount metadataclient credentialsblockgetmetadataclient query metricsjobId uniqueId name packageName tag label runId description type sql timestamp timeGrains dimension meta resourceType filter field operator value model name metadataclientqueryquery Result data metric uniqueId metrictpchtotalrevenue name totalrevenue packageName tpch tag label Total Revenue runId description type sum sql netitemsalesamount timestamp orderdate timeGrains day week month dimension statuscode prioritycode meta resourceType metric filter model name fctorders Returns An authenticated dbt Cloud metadata API client"
    },
    {
        "input_text": "summarize: def get_client(        self, client_type: Literal[\"administrative\", \"metadata\"]    ) -> Union[DbtCloudAdministrativeClient, DbtCloudMetadataClient]:                get_client_method = getattr(self, f\"get_{client_type}_client\", None)        if get_client_method is None:            raise ValueError(f\"'{client_type}' is not a supported client type.\")        return get_client_method()",
        "labels_text": "Returns a newly instantiated client for working with the dbt Cloud API Args clienttype Type of client to return Accepts either administrative or metadata Returns The authenticated client of the requested type"
    },
    {
        "input_text": "summarize: def get_run_id(obj: Dict):        id = obj.get(\"id\")    if id is None:        raise RuntimeError(\"Unable to determine run ID for triggered job.\")    return id",
        "labels_text": "Task that extract the run ID from a trigger job run API response This task is mainly used to maintain dependency tracking between the triggerdbtcloudjobrun task and downstream tasksflows that use the run ID Args obj The JSON body from the trigger job run response Example python from prefect import flow from prefectdbtcloud import DbtCloudCredentials from prefectdbtcloudjobs import triggerdbtcloudjobrun getrunid flow def triggerrunandgetid dbtcloudcredentialsDbtCloudCredentials apikeymyapikey accountid triggeredrundata triggerdbtcloudjobrun dbtcloudcredentialsdbtcloudcredentials jobidjobid optionstriggerjobrunoptions runid getrunidtriggeredrundata return runid triggerrunandgetid"
    },
    {
        "input_text": "summarize: def default_cause_factory():        cause = \"Triggered via Prefect\"    try:        context = get_run_context()        if isinstance(context, FlowRunContext):            cause = f\"{cause} in flow run {context.flow_run.name}\"        elif isinstance(context, TaskRunContext):            cause = f\"{cause} in task run {context.task_run.name}\"    except RuntimeError:        pass    return cause",
        "labels_text": "Factory function to populate the default cause for a job run to include information from the Prefect run context"
    },
    {
        "input_text": "summarize: def is_terminal_status_code(cls, status_code: Any) -> bool:                return status_code in [cls.SUCCESS.value, cls.FAILED.value, cls.CANCELLED.value]",
        "labels_text": "Returns True if a status code is terminal for a job run Returns False otherwise"
    },
    {
        "input_text": "summarize: def extract_user_message(ex: HTTPStatusError) -> Optional[str]:        response_payload = ex.response.json()    status = response_payload.get(\"status\", {})    return status.get(\"user_message\")",
        "labels_text": "Extracts user message from a error response from the dbt Cloud administrative API Args ex An HTTPStatusError raised by httpx Returns usermessage from dbt Cloud administrative API response or None if a usermessage cannot be extracted"
    },
    {
        "input_text": "summarize: def extract_developer_message(ex: HTTPStatusError) -> Optional[str]:        response_payload = ex.response.json()    status = response_payload.get(\"status\", {})    return status.get(\"developer_message\")",
        "labels_text": "Extracts developer message from a error response from the dbt Cloud administrative API Args ex An HTTPStatusError raised by httpx Returns developermessage from dbt Cloud administrative API response or None if a developermessage cannot be extracted"
    },
    {
        "input_text": "summarize: def prefect_db():        with temporary_settings({PREFECT_API_SERVICES_TRIGGERS_ENABLED: False}):        with prefect_test_harness():            yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def google_auth_mock(monkeypatch):        google_auth_default_mock = MagicMock()    monkeypatch.setattr(\"google.auth.default\", google_auth_default_mock)",
        "labels_text": "Mocks out the googleauth module"
    },
    {
        "input_text": "summarize: def __enter__(self):                return self",
        "labels_text": "Enters the context manager"
    },
    {
        "input_text": "summarize: def __exit__(self, exc_type, exc_val, exc_tb):                self.close()",
        "labels_text": "Exits the context manager and close the DockerClient"
    },
    {
        "input_text": "summarize: def _convert_labels_to_docker_format(self, labels: Dict[str, str]):                labels = labels or {}        new_labels = {}        for name, value in labels.items():            if \"/\" in name:                namespace, key = name.split(\"/\", maxsplit=1)                new_namespace = \".\".join(reversed(namespace.split(\".\")))                new_labels[f\"{new_namespace}.{key}\"] = value            else:                new_labels[name] = value        return new_labels",
        "labels_text": "Converts label to the format expected by Docker"
    },
    {
        "input_text": "summarize: def _determine_image_pull_policy(self) -> ImagePullPolicy:                if not self.image_pull_policy:            _, tag = parse_image_tag(self.image)            if tag == \"latest\" or not tag:                return ImagePullPolicy.ALWAYS            return ImagePullPolicy.IF_NOT_PRESENT        return ImagePullPolicy(self.image_pull_policy)",
        "labels_text": "Determine the appropriate image pull policy If they specified an image pull policy use that If they did not specify an image pull policy and gave u the latest tag use ImagePullPolicyalways If they did not specify an image pull policy and did not specify a tag use ImagePullPolicyalways If they did not specify an image pull policy and gave u a tag other than latest use ImagePullPolicyifnotpresent This logic match the behavior of Kubernetes Seehttpskubernetesiodocsconceptscontainersimagesimagepullpolicydefaulting"
    },
    {
        "input_text": "summarize: def _get_client(self):                try:            with warnings.catch_warnings():                # Silence warnings due to use of deprecated methods within dockerpy                # See https://github.com/docker/docker-py/pull/2931                warnings.filterwarnings(                    \"ignore\",                    message=\"distutils Version classes are deprecated.*\",                    category=DeprecationWarning,                )                docker_client = docker.from_env()        except docker.errors.DockerException as exc:            raise RuntimeError(\"Could not connect to Docker.\") from exc        return docker_client",
        "labels_text": "Returns a docker client"
    },
    {
        "input_text": "summarize: def _get_infrastructure_pid(self, container_id: str) -> str:                docker_client = self._get_client()        base_url = docker_client.api.base_url        docker_client.close()        return f\"{base_url}:{container_id}\"",
        "labels_text": "Generates a Docker infrastructurepid string in the form of dockerhostbaseurlcontainerid"
    },
    {
        "input_text": "summarize: def _parse_infrastructure_pid(self, infrastructure_pid: str) -> Tuple[str, str]:                # base_url can contain `:` so we only want the last item of the split        base_url, container_id = infrastructure_pid.rsplit(\":\", 1)        return base_url, str(container_id)",
        "labels_text": "Splits a Docker infrastructurepid into it component part"
    },
    {
        "input_text": "summarize: def _pull_image(        self, docker_client: \"DockerClient\", configuration: DockerWorkerJobConfiguration    ):                image, tag = parse_image_tag(configuration.image)        return docker_client.images.pull(image, tag)",
        "labels_text": "Pull the image were going to use to create the container"
    },
    {
        "input_text": "summarize: def _container_as_resource(self, container: \"Container\") -> Dict[str, str]:                return {            \"prefect.resource.id\": f\"prefect.docker.container.{container.id}\",            \"prefect.resource.name\": container.name,        }",
        "labels_text": "Convert a container to a resource dictionary"
    },
    {
        "input_text": "summarize: def _emit_container_creation_failed_event(        self, configuration: DockerWorkerJobConfiguration    ) -> Event:                return emit_event(            event=\"prefect.docker.container.creation-failed\",            resource=self._event_resource(),            related=self._event_related_resources(configuration=configuration),        )",
        "labels_text": "Emit a Prefect event when a docker container fails to be created"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        asyncio.run(to_thread.run_sync(alembic_upgrade))        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def _cast_smtp_server(cls, value: str):                return _cast_to_enum(value, SMTPServer)",
        "labels_text": "Cast the smtpserver to an SMTPServer Enum member if valid"
    },
    {
        "input_text": "summarize: def _cast_smtp_type(cls, value: str):                if isinstance(value, int):            return SMTPType(value)        return _cast_to_enum(value, SMTPType, restrict=True)",
        "labels_text": "Cast the smtptype to an SMTPType Enum member if valid"
    },
    {
        "input_text": "summarize: def _result_sync(func, *args, **kwargs):        result = func(*args, **kwargs).result()    return result",
        "labels_text": "Helper function to ensure result is run on a single thread"
    },
    {
        "input_text": "summarize: def _start_connection(self):                with self.gcp_credentials.get_bigquery_client() as client:            self._connection = Connection(client=client)",
        "labels_text": "Starts a connection"
    },
    {
        "input_text": "summarize: def get_connection(self) -> \"Connection\":                return self._connection",
        "labels_text": "Get the opened connection to BigQuery"
    },
    {
        "input_text": "summarize: def reset_cursors(self) -> None:                input_hashes = tuple(self._unique_cursors.keys())        for input_hash in input_hashes:            cursor = self._unique_cursors.pop(input_hash)            try:                cursor.close()            except Exception as exc:                self.logger.warning(                    f\"Failed to close cursor for input hash {input_hash!r}: {exc}\"                )",
        "labels_text": "Tries to close all opened cursor"
    },
    {
        "input_text": "summarize: def close(self):                try:            self.reset_cursors()        finally:            if self._connection is not None:                self._connection.close()                self._connection = None",
        "labels_text": "Closes connection and it cursor"
    },
    {
        "input_text": "summarize: def __enter__(self):                return self",
        "labels_text": "Start a connection upon entry"
    },
    {
        "input_text": "summarize: def __exit__(self, *args):                self.close()",
        "labels_text": "Closes connection and it cursor upon exit"
    },
    {
        "input_text": "summarize: def _get_bucket(    bucket: str,    gcp_credentials: GcpCredentials,    project: Optional[str] = None,) -> \"Bucket\":        client = gcp_credentials.get_cloud_storage_client(project=project)    bucket_obj = client.get_bucket(bucket)    return bucket_obj",
        "labels_text": "Helper function to retrieve a bucket"
    },
    {
        "input_text": "summarize: def format(self) -> str:                return self.value[0]",
        "labels_text": "The file format of the current instance"
    },
    {
        "input_text": "summarize: def compression(self) -> Union[str, None]:                return self.value[1]",
        "labels_text": "The compression type of the current instance"
    },
    {
        "input_text": "summarize: def content_type(self) -> str:                return self.value[2]",
        "labels_text": "The content type of the current instance"
    },
    {
        "input_text": "summarize: def suffix(self) -> str:                return self.value[3]",
        "labels_text": "The suffix of the file format of the current instance"
    },
    {
        "input_text": "summarize: def fix_extension_with(self, gcs_blob_path: str) -> str:                gcs_blob_path = PurePosixPath(gcs_blob_path)        folder = gcs_blob_path.parent        filename = PurePosixPath(gcs_blob_path.stem).with_suffix(self.suffix)        return str(folder.joinpath(filename))",
        "labels_text": "Fix the extension of a GCS blob Args gcsblobpath The path to the GCS blob to be modified Returns The modified path to the GCS blob with the new extension"
    },
    {
        "input_text": "summarize: def basepath(self) -> str:                return self.bucket_folder",
        "labels_text": "Readonly property that mirror the bucket folder Used for deployment"
    },
    {
        "input_text": "summarize: def _bucket_folder_suffix(cls, value):                if value != \"\" and not value.endswith(\"/\"):            value = f\"{value}/\"        return value",
        "labels_text": "Ensures that the bucket folder is suffixed with a forward slash"
    },
    {
        "input_text": "summarize: def _raise_help_msg(key: str):        def outer(func):                @functools.wraps(func)        def inner(*args, **kwargs):                        try:                return func(*args, **kwargs)            except NameError as exc:                raise ImportError(                    f\"To use prefect_gcp.{key}, install prefect-gcp with the \"                    f\"'{key}' extra: `pip install 'prefect_gcp[{key}]'`\"                ) from exc        return inner    return outer",
        "labels_text": "Raises a helpful error message Args key the key to access HELPURLS"
    },
    {
        "input_text": "summarize: def _get_job_service_async_client_cached(    ctx, client_options: tuple) -> \"JobServiceAsyncClient\":        with _LOCK:        client_options = dict(client_options)        credentials = ctx.get_credentials_from_service_account()        job_service_client = JobServiceAsyncClient(            credentials=credentials, client_options=client_options        )    return job_service_client",
        "labels_text": "Gets an authenticated Job Service async client for Vertex AI Returns An authenticated Job Service async client"
    },
    {
        "input_text": "summarize: def _provide_one_service_account_source(self):                if self.service_account_info and self.service_account_file:            raise ValueError(                \"Only one of service_account_info or service_account_file \"                \"can be specified at once\"            )        return self",
        "labels_text": "Ensure that only a service account file or service account info ia provided"
    },
    {
        "input_text": "summarize: def _check_service_account_file(cls, file):                if not file:            return file        service_account_file = Path(file).expanduser()        if not service_account_file.exists():            raise ValueError(\"The provided path to the service account is invalid\")        return service_account_file",
        "labels_text": "Get full path of provided file and make sure that it exists"
    },
    {
        "input_text": "summarize: def _convert_json_string_json_service_account_info(cls, value):                if isinstance(value, str):            try:                service_account_info = json.loads(value)                return service_account_info            except Exception:                raise ValueError(\"Unable to decode service_account_info\")        else:            return value",
        "labels_text": "Converts service account info provided a a json formatted string to a dictionary"
    },
    {
        "input_text": "summarize: def get_access_token(self):          # noqa        request = google.auth.transport.requests.Request()        credentials = self.get_credentials_from_service_account()        credentials.refresh(request)        return credentials.token",
        "labels_text": "See httpsstackoverflowcoma Also httpswwwjhanleycomgooglecloudcreatingoauthaccesstokensforrestapicalls"
    },
    {
        "input_text": "summarize: def get_client(        self,        client_type: Union[str, ClientType],        **get_client_kwargs: Dict[str, Any],    ) -> Any:                if isinstance(client_type, str):            client_type = ClientType(client_type)        client_type = client_type.value        get_client_method = getattr(self, f\"get_{client_type}_client\")        return get_client_method(**get_client_kwargs)",
        "labels_text": "Helper method to dynamically get a client type Args clienttype The name of the client to get getclientkwargs Additional keyword argument to pas to the getclient method Returns An authenticated client Raises ValueError if the client is not supported"
    },
    {
        "input_text": "summarize: def get_cloud_storage_client(        self, project: Optional[str] = None    ) -> \"StorageClient\":                credentials = self.get_credentials_from_service_account()        # override class project if method project is provided        project = project or self.project        storage_client = StorageClient(credentials=credentials, project=project)        return storage_client",
        "labels_text": "Gets an authenticated Cloud Storage client Args project Name of the project to use override the base class project if provided Returns An authenticated Cloud Storage client Examples Gets a GCP Cloud Storage client from a path python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountfile secretsprefectserviceaccountjson client GcpCredentials serviceaccountfileserviceaccountfile getcloudstorageclient examplegetclientflow Gets a GCP Cloud Storage client from a dictionary python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountinfo type serviceaccount projectid projectid privatekeyid privatekeyid privatekey privatekey clientemail clientemail clientid clientid authuri authuri tokenuri tokenuri authproviderxcerturl authproviderxcerturl clientxcerturl clientxcerturl client GcpCredentials serviceaccountinfoserviceaccountinfo getcloudstorageclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def get_bigquery_client(        self, project: Optional[str] = None, location: Optional[str] = None    ) -> \"BigQueryClient\":                credentials = self.get_credentials_from_service_account()        # override class project if method project is provided        project = project or self.project        big_query_client = BigQueryClient(            credentials=credentials, project=project, location=location        )        return big_query_client",
        "labels_text": "Gets an authenticated BigQuery client Args project Name of the project to use override the base class project if provided location Location to use Returns An authenticated BigQuery client Examples Gets a GCP BigQuery client from a path python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountfile secretsprefectserviceaccountjson client GcpCredentials serviceaccountfileserviceaccountfile getbigqueryclient examplegetclientflow Gets a GCP BigQuery client from a dictionary python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountinfo type serviceaccount projectid projectid privatekeyid privatekeyid privatekey privatekey clientemail clientemail clientid clientid authuri authuri tokenuri tokenuri authproviderxcerturl authproviderxcerturl clientxcerturl clientxcerturl client GcpCredentials serviceaccountinfoserviceaccountinfo getbigqueryclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def get_secret_manager_client(self) -> \"SecretManagerServiceClient\":                credentials = self.get_credentials_from_service_account()        # doesn't accept project; must pass in project in tasks        secret_manager_client = SecretManagerServiceClient(credentials=credentials)        return secret_manager_client",
        "labels_text": "Gets an authenticated Secret Manager Service client Returns An authenticated Secret Manager Service client Examples Gets a GCP Secret Manager client from a path python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountfile secretsprefectserviceaccountjson client GcpCredentials serviceaccountfileserviceaccountfile getsecretmanagerclient examplegetclientflow Gets a GCP Cloud Storage client from a dictionary python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountinfo type serviceaccount projectid projectid privatekeyid privatekeyid privatekey privatekey clientemail clientemail clientid clientid authuri authuri tokenuri tokenuri authproviderxcerturl authproviderxcerturl clientxcerturl clientxcerturl client GcpCredentials serviceaccountinfoserviceaccountinfo getsecretmanagerclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def get_job_service_client(        self, client_options: Union[Dict[str, Any], ClientOptions] = None    ) -> \"JobServiceClient\":                if isinstance(client_options, dict):            client_options = from_dict(client_options)        credentials = self.get_credentials_from_service_account()        return JobServiceClient(credentials=credentials, client_options=client_options)",
        "labels_text": "Gets an authenticated Job Service client for Vertex AI Returns An authenticated Job Service client Examples Gets a GCP Job Service client from a path python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountfile secretsprefectserviceaccountjson client GcpCredentials serviceaccountfileserviceaccountfile getjobserviceclient examplegetclientflow Gets a GCP Cloud Storage client from a dictionary python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountinfo type serviceaccount projectid projectid privatekeyid privatekeyid privatekey privatekey clientemail clientemail clientid clientid authuri authuri tokenuri tokenuri authproviderxcerturl authproviderxcerturl clientxcerturl clientxcerturl client GcpCredentials serviceaccountinfoserviceaccountinfo getjobserviceclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def get_job_service_async_client(        self, client_options: Union[Dict[str, Any], ClientOptions] = None    ) -> \"JobServiceAsyncClient\":                if isinstance(client_options, dict):            client_options = from_dict(client_options)        return _get_job_service_async_client_cached(            self, tuple(client_options.__dict__.items())        )",
        "labels_text": "Gets an authenticated Job Service async client for Vertex AI Returns An authenticated Job Service async client Examples Gets a GCP Job Service client from a path python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountfile secretsprefectserviceaccountjson client GcpCredentials serviceaccountfileserviceaccountfile getjobserviceasyncclient examplegetclientflow Gets a GCP Cloud Storage client from a dictionary python from prefect import flow from prefectgcpcredentials import GcpCredentials flow def examplegetclientflow serviceaccountinfo type serviceaccount projectid projectid privatekeyid privatekeyid privatekey privatekey clientemail clientemail clientid clientid authuri authuri tokenuri tokenuri authproviderxcerturl authproviderxcerturl clientxcerturl clientxcerturl client GcpCredentials serviceaccountinfoserviceaccountinfo getjobserviceasyncclient examplegetclientflow"
    },
    {
        "input_text": "summarize: def slugify_name(name: str, max_length: int = 30) -> Optional[str]:        slug = slugify(        name,        max_length=max_length,        regex_pattern=r\"[^a-zA-Z0-9-]+\",    )    return slug if slug else None",
        "labels_text": "Slugify text for use a a name Keeps only alphanumeric character and dash and cap the length of the slug at char The character length allows room to add a uuid for generating a unique name for the job while keeping the total length of a name below character which is the limit for Cloud Run job name Args name The name of the job Returns The slugified job name or None if the slugified name is empty"
    },
    {
        "input_text": "summarize: def _is_missing_container(self):                if (            self.ready_condition.get(\"status\") == \"False\"            and self.ready_condition.get(\"reason\") == \"ContainerMissing\"        ):            return True        return False",
        "labels_text": "Check if Job status is not ready because the specified container cannot be found"
    },
    {
        "input_text": "summarize: def is_ready(self) -> bool:                if self._is_missing_container():            raise Exception(f\"{self.ready_condition['message']}\")        return self.ready_condition.get(\"status\") == \"True\"",
        "labels_text": "Whether a job is finished registering and ready to be executed"
    },
    {
        "input_text": "summarize: def has_execution_in_progress(self) -> bool:                return (            self.execution_status == {}            or self.execution_status.get(\"completionTimestamp\") is None        )",
        "labels_text": "See if job ha a run in progress"
    },
    {
        "input_text": "summarize: def _get_ready_condition(job: dict) -> dict:                if job[\"status\"].get(\"conditions\"):            for condition in job[\"status\"][\"conditions\"]:                if condition[\"type\"] == \"Ready\":                    return condition        return {}",
        "labels_text": "Utility to access JSON field containing ready condition"
    },
    {
        "input_text": "summarize: def _get_execution_status(job: dict):                if job[\"status\"].get(\"latestCreatedExecution\"):            return job[\"status\"][\"latestCreatedExecution\"]        return {}",
        "labels_text": "Utility to access JSON field containing execution status"
    },
    {
        "input_text": "summarize: def get(cls, client: Resource, namespace: str, job_name: str):                request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")        response = request.execute()        return cls(            metadata=response[\"metadata\"],            spec=response[\"spec\"],            status=response[\"status\"],            name=response[\"metadata\"][\"name\"],            ready_condition=cls._get_ready_condition(response),            execution_status=cls._get_execution_status(response),        )",
        "labels_text": "Make a get request to the GCP job API and return a Job instance"
    },
    {
        "input_text": "summarize: def create(client: Resource, namespace: str, body: dict):                request = client.jobs().create(parent=f\"namespaces/{namespace}\", body=body)        response = request.execute()        return response",
        "labels_text": "Make a create request to the GCP job API"
    },
    {
        "input_text": "summarize: def delete(client: Resource, namespace: str, job_name: str):                request = client.jobs().delete(name=f\"namespaces/{namespace}/jobs/{job_name}\")        response = request.execute()        return response",
        "labels_text": "Make a delete request to the GCP job API"
    },
    {
        "input_text": "summarize: def run(client: Resource, namespace: str, job_name: str):                request = client.jobs().run(name=f\"namespaces/{namespace}/jobs/{job_name}\")        response = request.execute()        return response",
        "labels_text": "Make a run request to the GCP job API"
    },
    {
        "input_text": "summarize: def is_running(self) -> bool:                return self.status.get(\"completionTime\") is None",
        "labels_text": "Returns True if Execution is not completed"
    },
    {
        "input_text": "summarize: def condition_after_completion(self):                for condition in self.status[\"conditions\"]:            if condition[\"type\"] == \"Completed\":                return condition",
        "labels_text": "Returns Execution condition if Execution ha completed"
    },
    {
        "input_text": "summarize: def succeeded(self):                completed_condition = self.condition_after_completion()        if completed_condition and completed_condition[\"status\"] == \"True\":            return True        return False",
        "labels_text": "Whether or not the Execution completed is a successful state"
    },
    {
        "input_text": "summarize: def get(cls, client: Resource, namespace: str, execution_name: str):                request = client.executions().get(            name=f\"namespaces/{namespace}/executions/{execution_name}\"        )        response = request.execute()        return cls(            name=response[\"metadata\"][\"name\"],            namespace=response[\"metadata\"][\"namespace\"],            metadata=response[\"metadata\"],            spec=response[\"spec\"],            status=response[\"status\"],            log_uri=response[\"status\"][\"logUri\"],        )",
        "labels_text": "Make a get request to the GCP execution API and return an Execution instance"
    },
    {
        "input_text": "summarize: def push_project_to_gcs(*args, **kwargs) -> PushToGcsOutput:        return push_to_gcs(*args, **kwargs)",
        "labels_text": "Deprecated Use pushtogcs instead"
    },
    {
        "input_text": "summarize: def pull_project_from_gcs(*args, **kwargs) -> PullProjectFromGcsOutput:        return pull_from_gcs(*args, **kwargs)",
        "labels_text": "Deprecated Use pullfromgcs instead"
    },
    {
        "input_text": "summarize: def is_ready(self) -> bool:                ready_condition = self.get_ready_condition()        if self._is_missing_container(ready_condition=ready_condition):            raise Exception(f\"{ready_condition.get('message')}\")        return ready_condition.get(\"state\") == \"CONDITION_SUCCEEDED\"",
        "labels_text": "Check if the job is ready to run Returns Whether the job is ready to run"
    },
    {
        "input_text": "summarize: def get_ready_condition(self) -> Dict:                if self.terminalCondition.get(\"type\") == \"Ready\":            return self.terminalCondition        return {}",
        "labels_text": "Get the ready condition for the job Returns The ready condition for the job"
    },
    {
        "input_text": "summarize: def create(        cr_client: Resource,        project: str,        location: str,        job_id: str,        body: Dict,    ) -> Dict:                # noinspection PyUnresolvedReferences        request = cr_client.jobs().create(            parent=f\"projects/{project}/locations/{location}\",            jobId=job_id,            body=body,        )        response = request.execute()        return response",
        "labels_text": "Create a job on Cloud Run with the V API Args crclient The base client needed for interacting with GCP Cloud Run V API project The GCP project ID location The GCP region jobid The ID of the job to create body The job body Returns The response from the Cloud Run V API"
    },
    {
        "input_text": "summarize: def run(        cr_client: Resource,        project: str,        location: str,        job_name: str,    ):                # noinspection PyUnresolvedReferences        request = cr_client.jobs().run(            name=f\"projects/{project}/locations/{location}/jobs/{job_name}\",        )        response = request.execute()        return response",
        "labels_text": "Run a job on Cloud Run with the V API Args crclient The base client needed for interacting with GCP Cloud Run V API project The GCP project ID location The GCP region jobname The name of the job to run"
    },
    {
        "input_text": "summarize: def _is_missing_container(ready_condition: Dict) -> bool:                if (            ready_condition.get(\"state\") == \"CONTAINER_FAILED\"            and ready_condition.get(\"reason\") == \"ContainerMissing\"        ):            return True        return False",
        "labels_text": "Check if the job is missing a container Args readycondition The ready condition for the job Returns Whether the job is missing a container"
    },
    {
        "input_text": "summarize: def is_running(self) -> bool:                return self.completionTime is None",
        "labels_text": "Return whether the execution is running Returns Whether the execution is running"
    },
    {
        "input_text": "summarize: def succeeded(self):                completed_condition = self.condition_after_completion()        if (            completed_condition            and completed_condition[\"state\"] == \"CONDITION_SUCCEEDED\"        ):            return True        return False",
        "labels_text": "Whether or not the Execution completed is a successful state"
    },
    {
        "input_text": "summarize: def condition_after_completion(self) -> Dict:                if isinstance(self.conditions, List):            for condition in self.conditions:                if condition[\"type\"] == \"Completed\":                    return condition",
        "labels_text": "Return the condition after completion Returns The condition after completion"
    },
    {
        "input_text": "summarize: def project(self) -> str:                return self.credentials.project",
        "labels_text": "property for accessing the project from the credential"
    },
    {
        "input_text": "summarize: def job_name(self) -> str:                return self.job_body[\"metadata\"][\"name\"]",
        "labels_text": "property for accessing the name from the job metadata"
    },
    {
        "input_text": "summarize: def prepare_for_flow_run(        self,        flow_run: \"FlowRun\",        deployment: Optional[\"DeploymentResponse\"] = None,        flow: Optional[\"Flow\"] = None,    ):                super().prepare_for_flow_run(flow_run, deployment, flow)        self._populate_envs()        self._populate_or_format_command()        self._format_args_if_present()        self._populate_image_if_not_present()        self._populate_name_if_not_present()",
        "labels_text": "Prepares the job configuration for a flow run Ensures that necessary value are present in the job body and that the job body is valid Args flowrun The flow run to prepare the job configuration for deployment The deployment associated with the flow run used for preparation flow The flow associated with the flow run used for preparation"
    },
    {
        "input_text": "summarize: def _populate_envs(self):                envs = [{\"name\": k, \"value\": v} for k, v in self.env.items()]        self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][            \"env\"        ] = envs",
        "labels_text": "Populate environment variable BaseWorkerprepareforflowrun handle putting the environment variable in the env attribute This method move them into the job body"
    },
    {
        "input_text": "summarize: def _populate_name_if_not_present(self):                try:            if \"name\" not in self.job_body[\"metadata\"]:                base_job_name = slugify_name(self.name)                job_name = f\"{base_job_name}-{uuid4().hex}\"                self.job_body[\"metadata\"][\"name\"] = job_name        except KeyError:            raise ValueError(\"Unable to verify name due to invalid job body template.\")",
        "labels_text": "Adds the flow run name to the job if one is not already provided"
    },
    {
        "input_text": "summarize: def _ensure_job_includes_all_required_components(cls, value: Dict[str, Any]):                patch = JsonPatch.from_diff(value, _get_base_job_body())        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])        if missing_paths:            raise ValueError(                \"Job is missing required attributes at the following paths: \"                f\"{', '.join(missing_paths)}\"            )        return value",
        "labels_text": "Ensures that the job body includes all required component"
    },
    {
        "input_text": "summarize: def _ensure_job_has_compatible_values(cls, value: Dict[str, Any]):                patch = JsonPatch.from_diff(value, _get_base_job_body())        incompatible = sorted(            [                f\"{op['path']} must have value {op['value']!r}\"                for op in patch                if op[\"op\"] == \"replace\"            ]        )        if incompatible:            raise ValueError(                \"Job has incompatible values for the following attributes: \"                f\"{', '.join(incompatible)}\"            )        return value",
        "labels_text": "Ensure that the job body ha compatible value"
    },
    {
        "input_text": "summarize: def _get_client(self, configuration: CloudRunWorkerJobConfiguration) -> Resource:                # region needed for 'v1' API        api_endpoint = f\"https://{configuration.region}-run.googleapis.com\"        gcp_creds = configuration.credentials.get_credentials_from_service_account()        options = ClientOptions(api_endpoint=api_endpoint)        return discovery.build(            \"run\", \"v1\", client_options=options, credentials=gcp_creds        ).namespaces()",
        "labels_text": "Get the base client needed for interacting with GCP APIs"
    },
    {
        "input_text": "summarize: def _watch_job_execution(        self, client, job_execution: Execution, poll_interval: int = 5    ):                while job_execution.is_running():            job_execution = Execution.get(                client=client,                namespace=job_execution.namespace,                execution_name=job_execution.name,            )            time.sleep(poll_interval)        return job_execution",
        "labels_text": "Update jobexecution status until it is no longer running"
    },
    {
        "input_text": "summarize: def _get_base_job_body() -> Dict[str, Any]:        return {        \"template\": {            \"template\": {                \"containers\": [],            },        },    }",
        "labels_text": "Returns the base job body for the Cloud Run worker job body validation Returns The base job body"
    },
    {
        "input_text": "summarize: def project(self) -> str:                return self.credentials.project",
        "labels_text": "Returns the GCP project associated with the credential Returns str The GCP project associated with the credential"
    },
    {
        "input_text": "summarize: def job_name(self) -> str:                if self._job_name is None:            base_job_name = slugify_name(self.name)            job_name = f\"{base_job_name}-{uuid4().hex}\"            self._job_name = job_name        return self._job_name",
        "labels_text": "Returns the name of the job Returns str The name of the job"
    },
    {
        "input_text": "summarize: def _populate_timeout(self):                self.job_body[\"template\"][\"template\"][\"timeout\"] = f\"{self.timeout}s\"",
        "labels_text": "Populates the job body with the timeout"
    },
    {
        "input_text": "summarize: def _populate_env(self):                envs = [{\"name\": k, \"value\": v} for k, v in self.env.items()]        self.job_body[\"template\"][\"template\"][\"containers\"][0][\"env\"] = envs",
        "labels_text": "Populates the job body with environment variable"
    },
    {
        "input_text": "summarize: def _populate_image_if_not_present(self):                if \"image\" not in self.job_body[\"template\"][\"template\"][\"containers\"][0]:            self.job_body[\"template\"][\"template\"][\"containers\"][0][                \"image\"            ] = f\"docker.io/{get_prefect_image_name()}\"",
        "labels_text": "Populates the job body with the image if not present"
    },
    {
        "input_text": "summarize: def _format_args_if_present(self):                args = self.job_body[\"template\"][\"template\"][\"containers\"][0].get(\"args\")        if args is not None and isinstance(args, str):            self.job_body[\"template\"][\"template\"][\"containers\"][0][                \"args\"            ] = shlex.split(args)",
        "labels_text": "Formats the job body args if present"
    },
    {
        "input_text": "summarize: def _ensure_job_includes_all_required_components(cls, value: Dict[str, Any]):                patch = JsonPatch.from_diff(value, _get_base_job_body())        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])        if missing_paths:            raise ValueError(                f\"Job body is missing required components: {', '.join(missing_paths)}\"            )        return value",
        "labels_text": "Ensures that the job body includes all required component Args value The job body to validate Returns The validated job body"
    },
    {
        "input_text": "summarize: def _ensure_job_has_compatible_values(cls, value: Dict[str, Any]):                patch = JsonPatch.from_diff(value, _get_base_job_body())        incompatible = sorted(            [                f\"{op['path']} must have value {op['value']!r}\"                for op in patch                if op[\"op\"] == \"replace\"            ]        )        if incompatible:            raise ValueError(                \"Job has incompatible values for the following attributes: \"                f\"{', '.join(incompatible)}\"            )        return value",
        "labels_text": "Ensure that the job body ha compatible value"
    },
    {
        "input_text": "summarize: def _watch_job_execution(        cr_client: Resource,        configuration: CloudRunWorkerJobV2Configuration,        execution: ExecutionV2,        poll_interval: int,    ) -> ExecutionV2:                while execution.is_running():            execution = ExecutionV2.get(                cr_client=cr_client,                execution_id=execution.name,            )            time.sleep(poll_interval)        return execution",
        "labels_text": "Update execution status until it is no longer running Args crclient Resource The base client needed for interacting with GCP Cloud Run V API configuration CloudRunWorkerJobVConfiguration The configuration for the job execution ExecutionV The execution to watch pollinterval int The number of second to wait between poll Returns The execution"
    },
    {
        "input_text": "summarize: def project(self) -> str:                return self.credentials.project",
        "labels_text": "property for accessing the project from the credential"
    },
    {
        "input_text": "summarize: def job_name(self) -> str:          # noqa        unique_suffix = uuid4().hex        job_name = f\"{self.name}-{unique_suffix}\"        return job_name",
        "labels_text": "The name can be up to character long and can be consist of any UTF character Reference httpscloudgooglecompythondocsreferenceaiplatformlatestgooglecloudaiplatformCustomJobgooglecloudaiplatformCustomJobdisplayname"
    },
    {
        "input_text": "summarize: def _inject_formatted_env_vars(self):                worker_pool_specs = self.job_spec[\"worker_pool_specs\"]        formatted_env_vars = [            {\"name\": key, \"value\": value} for key, value in self.env.items()        ]        worker_pool_specs[0][\"container_spec\"][\"env\"] = formatted_env_vars",
        "labels_text": "Inject environment variable in the Vertex jobspec configuration in the correct format which is sourced from the BaseJobConfiguration This method is invoked by prepareforflowrun"
    },
    {
        "input_text": "summarize: def _ensure_job_spec_includes_required_attributes(cls, value: Dict[str, Any]):                patch = JsonPatch.from_diff(value, _get_base_job_spec())        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])        if missing_paths:            raise ValueError(                \"Job is missing required attributes at the following paths: \"                f\"{', '.join(missing_paths)}\"            )        return value",
        "labels_text": "Ensures that the job spec includes all required component"
    },
    {
        "input_text": "summarize: def remove_server_url_from_env(env):        return [        env_var        for env_var in env        if env_var[\"name\"]        not in [            \"PREFECT_API_DATABASE_CONNECTION_URL\",            \"PREFECT_ORION_DATABASE_CONNECTION_URL\",            \"PREFECT_SERVER_DATABASE_CONNECTION_URL\",        ]    ]",
        "labels_text": "For convenience since the testing database URL is nondeterministic"
    },
    {
        "input_text": "summarize: def test_get_credentials_from_service_account_file(    service_account_file, oauth2_credentials):        credentials = GcpCredentials(        service_account_file=service_account_file, project=\"my-project\"    ).get_credentials_from_service_account()    assert isinstance(credentials, Path)    assert credentials == Path(service_account_file).expanduser()",
        "labels_text": "Expected behavior serviceaccountfile is typed a a path so we expect either input to be a PosixPath In our conftest we define a fixture oauthcredentials that patch GCPs Credential method to return it input We expect our getcredentialsfromserviceaccount method to call GCPs method with the path we pas in"
    },
    {
        "input_text": "summarize: def get_configs(self):                configs = self.credentials.model_dump()        for key in Block().model_dump():            configs.pop(key, None)        for key in configs.copy():            if key.startswith(\"_\"):                configs.pop(key)            elif hasattr(configs[key], \"get_secret_value\"):                configs[key] = configs[key].get_secret_value()        return configs",
        "labels_text": "Returns the dbt configs likely used eventually for writing to profilesyml Returns A configs JSON"
    },
    {
        "input_text": "summarize: def test_succeeded_responds_true(self):                execution = Execution(            name=\"Test\",            namespace=\"test-namespace\",            metadata={},            spec={},            status={\"conditions\": [{\"type\": \"Completed\", \"status\": \"True\"}]},            log_uri=\"\",        )        assert execution.succeeded()",
        "labels_text": "Desired behavior succeeded should return true if executionstatus contains a list element that is a dict with a key type and a value of Completed and a key status and a value of True"
    },
    {
        "input_text": "summarize: def test_succeeded_responds_false(self, conditions):                execution = Execution(            name=\"Test\",            namespace=\"test-namespace\",            metadata={},            spec={},            status={\"conditions\": conditions},            log_uri=\"\",        )        assert not execution.succeeded()",
        "labels_text": "Desired behavior succeeded should return False if executionstatus lack a list element that is a dict with a key type and a value of Completed and a key status and a value of True This could be a situation where there is no element containing the key or the element with the key ha a status that is not True"
    },
    {
        "input_text": "summarize: def test_is_running(self, status, expected_value):                execution = Execution(            name=\"Test\",            namespace=\"test-namespace\",            metadata={},            spec={},            status=status,            log_uri=\"\",        )        assert execution.is_running() == expected_value",
        "labels_text": "Desired behavior isrunning should return True if there if executionstatus lack a key completionTime otherwise return False"
    },
    {
        "input_text": "summarize: def test_condition_after_completion_returns_correct_condition(        self, conditions, expected_value    ):                execution = Execution(            name=\"Test\",            namespace=\"test-namespace\",            metadata={},            spec={},            status={\"conditions\": conditions},            log_uri=\"\",        )        assert execution.condition_after_completion() == expected_value",
        "labels_text": "Desired behavior conditionaftercompletion should return the list element from executionstatus that contains a dict with a key type and a value of Completed if it exists else None"
    },
    {
        "input_text": "summarize: def get_client(self) -> HTTPEndpoint:                if self.token is not None:            base_headers = {\"Authorization\": f\"Bearer {self.token.get_secret_value()}\"}        else:            base_headers = None        endpoint = HTTPEndpoint(            \"https://api.github.com/graphql\", base_headers=base_headers        )        return endpoint",
        "labels_text": "Gets an authenticated GitHub GraphQL HTTPEndpoint client Returns An authenticated GitHub GraphQL HTTPEndpoint client Example Gets an authenticated GitHub GraphQL HTTPEndpoint client python from prefectgithub import GitHubCredentials githubcredentials GitHubCredentialstokentoken client githubcredentialsgetclient"
    },
    {
        "input_text": "summarize: def camel_to_snake_case(string: str) -> str:        string = SNAKE_CASE_REGEX1.sub(r\"\\1_\\2\", string)    return SNAKE_CASE_REGEX2.sub(r\"\\1_\\2\", string).lower()",
        "labels_text": "Converts CamelCase and lowerCamelCase to snakecase Args string The string in CamelCase or lowerCamelCase to convert Returns A snakecase version of the string"
    },
    {
        "input_text": "summarize: def strip_kwargs(**kwargs: Dict) -> Dict:        stripped_dict = {}    for k, v in kwargs.items():        if isinstance(v, dict):            v = strip_kwargs(**v)        if v is not None:            stripped_dict[k] = v    return stripped_dict or {}",
        "labels_text": "Drops keyword argument if value is None because sgqlcOperation error out if a keyword argument is provided but set to None Args kwargs Input keyword argument Returns Stripped version of kwargs"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def get_client(self) -> Gitlab:                # ref: https://python-gitlab.readthedocs.io/en/stable/        gitlab = Gitlab(url=self.url, oauth_token=self.token.get_secret_value())        gitlab.auth()        return gitlab",
        "labels_text": "Gets an authenticated GitLab client Returns An authenticated GitLab client"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def _pod_as_resource(self, pod: \"V1Pod\") -> Dict[str, str]:                return {            \"prefect.resource.id\": f\"prefect.kubernetes.pod.{pod.metadata.uid}\",            \"prefect.resource.name\": pod.metadata.name,            \"kubernetes.namespace\": pod.metadata.namespace,        }",
        "labels_text": "Convert a pod to a resource dictionary"
    },
    {
        "input_text": "summarize: def run_namespaced_job(    kubernetes_job: KubernetesJob, print_func: Optional[Callable] = None) -> Dict[str, Any]:        kubernetes_job_run = task(kubernetes_job.trigger)()    task(kubernetes_job_run.wait_for_completion)(print_func)    return task(kubernetes_job_run.fetch_result)()",
        "labels_text": "Flow for running a namespaced Kubernetes job Args kubernetesjob The KubernetesJob block that specifies the job to run printfunc A function to print the log from the job pod Returns A dict of log from each pod in the job eg podname podlogstr Raises RuntimeError If the created Kubernetes job attains a failed status Example python from prefectkubernetes import KubernetesJob runnamespacedjob from prefectkubernetescredentials import KubernetesCredentials runnamespacedjob kubernetesjobKubernetesJobfromyamlfile credentialsKubernetesCredentialsloadkscreds manifestpathpathtojobyaml"
    },
    {
        "input_text": "summarize: def from_yaml_file(        cls: Type[Self], manifest_path: Union[Path, str], **kwargs    ) -> Self:                with open(manifest_path, \"r\") as yaml_stream:            yaml_dict = yaml.safe_load(yaml_stream)        return cls(v1_job=yaml_dict, **kwargs)",
        "labels_text": "Create a KubernetesJob from a YAML file Args manifestpath The YAML file to create the KubernetesJob from Returns A KubernetesJob object"
    },
    {
        "input_text": "summarize: def _slugify_name(name: str, max_length: int = 45) -> Optional[str]:        slug = slugify(        name,        max_length=max_length,  # Leave enough space for generateName        regex_pattern=r\"[^a-zA-Z0-9-]+\",    )    return slug if slug else None",
        "labels_text": "Slugify text for use a a name Keeps only alphanumeric character and dash and cap the length of the slug at char The character length allows room for the k utility generateName to generate a unique name from the slug while keeping the total length of a name below character which is the limit for eg label name that follow RFC hostnames and RFC domain name Args name The name of the job Returns The slugified job name or None if the slugified name is empty"
    },
    {
        "input_text": "summarize: def _slugify_label_value(value: str, max_length: int = 63) -> str:      # noqa    slug = (        slugify(value, max_length=max_length, regex_pattern=r\"[^a-zA-Z0-9-_\\.]+\").strip(            \"_-.\"  # Must start or end with alphanumeric characters        )        or value    )    # Fallback to the original if we end up with an empty slug, this will allow    # Kubernetes to throw the validation error    return slug",
        "labels_text": "Slugify text for use a a label value Keeps only alphanumeric character dash underscore and period Limits the total length of label text to below character See httpskubernetesiodocsconceptsoverviewworkingwithobjectslabelssyntaxandcharacterset Args value The text for the label Returns The slugified value"
    },
    {
        "input_text": "summarize: def _get_base_job_manifest():        return {        \"apiVersion\": \"batch/v1\",        \"kind\": \"Job\",        \"metadata\": {\"labels\": {}},        \"spec\": {            \"template\": {                \"spec\": {                    \"parallelism\": 1,                    \"completions\": 1,                    \"restartPolicy\": \"Never\",                    \"containers\": [                        {                            \"name\": \"prefect-job\",                        }                    ],                }            }        },    }",
        "labels_text": "Returns a base job manifest to use for manifest validation"
    },
    {
        "input_text": "summarize: def _base_flow_run_labels(flow_run: \"FlowRun\") -> Dict[str, str]:                return {            \"prefect.io/flow-run-id\": str(flow_run.id),            \"prefect.io/flow-run-name\": flow_run.name,            \"prefect.io/version\": _slugify_label_value(                prefect.__version__.split(\"+\")[0]            ),        }",
        "labels_text": "Generate a dictionary of label for a flow run job"
    },
    {
        "input_text": "summarize: def _update_prefect_api_url_if_local_server(self):                if self.env.get(\"PREFECT_API_URL\") and self._api_dns_name:            self.env[\"PREFECT_API_URL\"] = (                self.env[\"PREFECT_API_URL\"]                .replace(\"localhost\", self._api_dns_name)                .replace(\"127.0.0.1\", self._api_dns_name)            )",
        "labels_text": "If the API URL ha been set by the base environment rather than the by the user update the value to ensure connectivity when using a bridge network by updating local connection to use the internal host"
    },
    {
        "input_text": "summarize: def _slugify_labels(self):                all_labels = {**self.job_manifest[\"metadata\"].get(\"labels\", {}), **self.labels}        self.job_manifest[\"metadata\"][\"labels\"] = {            _slugify_label_key(k): _slugify_label_value(v)            for k, v in all_labels.items()        }",
        "labels_text": "Slugifies the label in the job manifest"
    },
    {
        "input_text": "summarize: def _populate_image_if_not_present(self):                try:            if (                \"image\"                not in self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0]            ):                self.job_manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][                    \"image\"                ] = get_prefect_image_name()        except KeyError:            raise ValueError(                \"Unable to verify image due to invalid job manifest template.\"            )",
        "labels_text": "Ensures that the image is present in the job manifest Populates the image with the default Prefect image if it is not present"
    },
    {
        "input_text": "summarize: def _parse_infrastructure_pid(        self, infrastructure_pid: str    ) -> Tuple[str, str, str]:                cluster_uid, namespace, job_name = infrastructure_pid.split(\":\", 2)        return cluster_uid, namespace, job_name",
        "labels_text": "Parse a Kubernetes infrastructure PID into it component part Returns a cluster UID namespace and job name"
    },
    {
        "input_text": "summarize: def best_event_time(event: CoreV1Event) -> datetime:                        return event.event_time or event.last_timestamp",
        "labels_text": "Choose the best timestamp from a Kubernetes event"
    },
    {
        "input_text": "summarize: def prefect_db():        try:        with prefect_test_harness():            yield    except OSError as e:        if \"Directory not empty\" in str(e):            pass        else:            raise e",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def disable_api_logging():        with temporary_settings(updates={PREFECT_LOGGING_TO_API_ENABLED: False}):        yield",
        "labels_text": "Disables API logging for all test"
    },
    {
        "input_text": "summarize: def get(cls) -> \"RemoteOptionsContext\":                return cls.__var__.get(RemoteOptionsContext())",
        "labels_text": "Return an empty RemoteOptionsContext instead of None if no context exists"
    },
    {
        "input_text": "summarize: def remote_options(    **new_remote_options: Dict[str, Any],) -> Generator[None, Dict[str, Any], None]:        current_remote_options = RemoteOptionsContext.get().current_remote_options    with RemoteOptionsContext(        current_remote_options={**current_remote_options, **new_remote_options}    ):        yield",
        "labels_text": "Context manager to add keyword argument to Ray remote call for task run If context are nested new option are merged with option in the outer context If a key is present in both the new option will be used Yields The current set of remote option Examples Use CPUs and GPUs for the process task python from prefect import flow task from prefectraytaskrunners import RayTaskRunner from prefectraycontext import remoteoptions task def processx return x flowtaskrunnerRayTaskRunner def myflow equivalent to setting rayremotenumcpus numgpus with remoteoptionsnumcpus numgpus processsubmit"
    },
    {
        "input_text": "summarize: def duplicate(self):                return type(self)(address=self.address, init_kwargs=self.init_kwargs)",
        "labels_text": "Return a new instance of with the same setting a this one"
    },
    {
        "input_text": "summarize: def __eq__(self, other: object) -> bool:                if isinstance(other, RayTaskRunner):            return (                self.address == other.address and self.init_kwargs == other.init_kwargs            )        else:            return False",
        "labels_text": "Check if an instance ha the same setting a this task runner"
    },
    {
        "input_text": "summarize: def __exit__(self, *exc_info):                self.logger.debug(\"Shutting down Ray cluster...\")        ray.shutdown()        super().__exit__(*exc_info)",
        "labels_text": "Shuts down the cluster"
    },
    {
        "input_text": "summarize: def ray_task_runner_with_existing_cluster(    machine_ray_instance,    use_hosted_api_server,  # noqa: F811    hosted_api_server,  # noqa: F811):        yield RayTaskRunner(        address=machine_ray_instance,        init_kwargs={            \"runtime_env\": {                # Ship the 'tests' module to the workers or they will not be able to                # deserialize test tasks / flows                \"py_modules\": [tests]            }        },    )",
        "labels_text": "Generate a ray task runner thats connected to a ray instance running in a separate process This test connection via ray which is a clientbased connection"
    },
    {
        "input_text": "summarize: def inprocess_ray_cluster():        cluster = ray.cluster_utils.Cluster(initialize_head=True)    try:        cluster.add_node()  # We need to add a second node for parallelism        yield cluster    finally:        cluster.shutdown()",
        "labels_text": "Starts a ray cluster inprocess"
    },
    {
        "input_text": "summarize: def ray_task_runner_with_temporary_cluster(    use_hosted_api_server,  # noqa: F811    hosted_api_server,  # noqa: F811):        yield RayTaskRunner(        init_kwargs={            \"runtime_env\": {                # Ship the 'tests' module to the workers or they will not be able to                # deserialize test tasks / flows                \"py_modules\": [tests]            }        },    )",
        "labels_text": "Generate a ray task runner that creates a temporary cluster This test connection via localhost which is not a clientbased connection"
    },
    {
        "input_text": "summarize: def get_sleep_time(self) -> float:                return 5.0",
        "labels_text": "Return an amount of time to sleep for concurrency test The RayTaskRunner is prone to flaking on concurrency test"
    },
    {
        "input_text": "summarize: def block_initialization(self) -> None:                if self.connection_string:            return        if not self.host:            raise ValueError(\"Missing hostname\")        if self.username and not self.password:            raise ValueError(\"Missing password\")",
        "labels_text": "Validate parameter"
    },
    {
        "input_text": "summarize: def get_client(self) -> redis.Redis:                if self.connection_string:            return redis.Redis.from_url(self.connection_string.get_secret_value())        return redis.Redis(            host=self.host,            port=self.port,            username=self.username.get_secret_value() if self.username else None,            password=self.password.get_secret_value() if self.password else None,            db=self.db,        )",
        "labels_text": "Get Redis Client Returns An initialized Redis async client"
    },
    {
        "input_text": "summarize: def from_host(        cls,        host: str,        username: Union[None, str, SecretStr],        password: Union[None, str, SecretStr],        port: int = DEFAULT_PORT,    ) -> \"RedisCredentials\":                return cls(host=host, username=username, password=password, port=port)",
        "labels_text": "Create block from hostname username and password Args host Redis hostname username Redis username password Redis password port Redis port Returns RedisCredentials instance"
    },
    {
        "input_text": "summarize: def from_connection_string(        cls, connection_string: Union[str, SecretStr]    ) -> \"RedisCredentials\":                return cls(connection_string=connection_string)",
        "labels_text": "Create block from a Redis connection string Supports the following URL scheme redis creates a TCP socket connection rediss creates a SSL wrapped TCP socket connection unix creates a Unix Domain Socket connection See Redis docshttpsredisreadthedocsioenstableexamples connectionexampleshtmlConnectingtoRedisinstancesbyspecifyingaURL scheme for more info Args connectionstring Redis connection string Returns RedisCredentials instance"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def environ_credentials() -> Dict:        return {        \"host\": os.environ.get(\"TEST_REDIS_HOST\", \"localhost\"),        \"port\": int(os.environ.get(\"TEST_REDIS_PORT\", 6379)),        \"db\": int(os.environ.get(\"TEST_REDIS_DB\", 0)),        \"username\": os.environ.get(\"TEST_REDIS_USERNAME\"),        \"password\": os.environ.get(\"TEST_REDIS_PASSWORD\"),    }",
        "labels_text": "Get redis credential from environment Returns Redis credential a a dict can be piped directly into RedisCredentials"
    },
    {
        "input_text": "summarize: def redis_credentials(environ_credentials: Dict) -> RedisCredentials:        return RedisCredentials(**environ_credentials)",
        "labels_text": "Get RedisCredentials object from environment Returns RedisCredentials object"
    },
    {
        "input_text": "summarize: def random_key() -> str:        return \"\".join(random.sample(string.ascii_lowercase, 10))",
        "labels_text": "Generate a random key Returns A random string of length"
    },
    {
        "input_text": "summarize: def pid(self) -> int:                return self._process.pid",
        "labels_text": "The PID of the process Returns The PID of the process"
    },
    {
        "input_text": "summarize: def return_code(self) -> Optional[int]:                return self._process.returncode",
        "labels_text": "The return code of the process Returns The return code of the process or None if the process is still running"
    },
    {
        "input_text": "summarize: def __enter__(self) -> \"ShellOperation\":                return self",
        "labels_text": "Enter the context of the job block"
    },
    {
        "input_text": "summarize: def __exit__(self, *exc_info):                self.close()",
        "labels_text": "Exit the context of the job block"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def get_client(self) -> AsyncWebClient:                return AsyncWebClient(token=self.token.get_secret_value())",
        "labels_text": "Returns an authenticated AsyncWebClient to interact with the Slack API"
    },
    {
        "input_text": "summarize: def get_client(self) -> AsyncWebhookClient:                return AsyncWebhookClient(url=self.url.get_secret_value())",
        "labels_text": "Returns an authenticated AsyncWebhookClient to interact with the configured Slack webhook"
    },
    {
        "input_text": "summarize: def _validate_token_kwargs(cls, values):                authenticator = values.get(\"authenticator\")        token = values.get(\"token\")        if authenticator == \"oauth\" and not token:            raise ValueError(                \"If authenticator is set to `oauth`, `token` must be provided\"            )        return values",
        "labels_text": "Ensure an authorization value ha been provided by the user"
    },
    {
        "input_text": "summarize: def _decode_secret(secret: Union[SecretStr, SecretBytes]) -> Optional[bytes]:                if isinstance(secret, (SecretStr, SecretBytes)):            secret = secret.get_secret_value()        if not isinstance(secret, (bytes, str)) or len(secret) == 0 or secret.isspace():            return None        return secret if isinstance(secret, bytes) else secret.encode()",
        "labels_text": "Decode the provided secret into byte If the secret is not a string or byte or it is whitespace then return None Args secret The value to decode Returns The decoded secret a byte"
    },
    {
        "input_text": "summarize: def _compose_pem(private_key: bytes) -> bytes:                pem_parts = re.match(_SIMPLE_PEM_CERTIFICATE_REGEX, private_key.decode())        if pem_parts is None:            raise InvalidPemFormat()        body = \"\\n\".join(re.split(r\"\\s+\", pem_parts[2].strip()))        # reassemble header+body+footer        return f\"{pem_parts[1]}\\n{body}\\n{pem_parts[3]}\".encode()",
        "labels_text": "Validate structure of PEM certificate The original key passed from Prefect is sometimes malformed This function recomposes the key into a valid key that will pas the serialization step when resolving the key to a DER Args privatekey A valid PEM format byte encoded string Returns byte encoded certificate Raises InvalidPemFormat if private key is an invalid format"
    },
    {
        "input_text": "summarize: def get_connection(self, **connect_kwargs: Any) -> SnowflakeConnection:                if self._connection is not None:            return self._connection        connect_params = {            \"database\": self.database,            \"warehouse\": self.warehouse,            \"schema\": self.schema_,        }        connection = self.credentials.get_client(**connect_kwargs, **connect_params)        self._connection = connection        self.logger.info(\"Started a new connection to Snowflake.\")        return connection",
        "labels_text": "Returns an authenticated connection that can be used to query from Snowflake database Args connectkwargs Additional argument to pas to snowflakeconnectorconnect Returns The authenticated SnowflakeConnection Examples python from prefectsnowflakecredentials import SnowflakeCredentials from prefectsnowflakedatabase import SnowflakeConnector snowflakecredentials SnowflakeCredentials accountaccount useruser passwordpassword snowflakeconnector SnowflakeConnector databasedatabase warehousewarehouse schemaschema credentialssnowflakecredentials with snowflakeconnectorgetconnection a connection"
    },
    {
        "input_text": "summarize: def _start_connection(self):                self.get_connection()        if self._unique_cursors is None:            self._unique_cursors = {}",
        "labels_text": "Starts Snowflake database connection"
    },
    {
        "input_text": "summarize: def execute_many(        self,        operation: str,        seq_of_parameters: List[Dict[str, Any]],    ) -> None:          # noqa        self._start_connection()        inputs = dict(            command=operation,            seqparams=seq_of_parameters,        )        with self._connection.cursor() as cursor:            cursor.executemany(**inputs)        self.logger.info(            f\"Executed {len(seq_of_parameters)} operations off {operation!r}.\"        )",
        "labels_text": "Executes many operation on the database This method is intended to be used for operation that do not return data such a INSERT UPDATE or DELETE Unlike the fetch method this method will always execute the operation upon calling Args operation The SQL query or other operation to be executed seqofparameters The sequence of parameter for the operation Examples Create table and insert three row into it python from prefectsnowflakedatabase import SnowflakeConnector with SnowflakeConnectorloadBLOCKNAME a conn connexecute CREATE TABLE IF NOT EXISTS customer name varchar address varchar connexecutemany INSERT INTO customer name address VALUES name address seqofparameters name Marvin address Highway name Ford address Highway name Unknown address Space"
    },
    {
        "input_text": "summarize: def close(self):                try:            self.reset_cursors()        finally:            if self._connection is None:                self.logger.info(\"There was no connection open to be closed.\")                return            self._connection.close()            self._connection = None            self.logger.info(\"Successfully closed the Snowflake connection.\")",
        "labels_text": "Closes connection and it cursor"
    },
    {
        "input_text": "summarize: def __enter__(self):                return self",
        "labels_text": "Start a connection upon entry"
    },
    {
        "input_text": "summarize: def __exit__(self, *args):                self.close()",
        "labels_text": "Closes connection and it cursor upon exit"
    },
    {
        "input_text": "summarize: def __getstate__(self):                data = self.__dict__.copy()        data.update({k: None for k in {\"_connection\", \"_unique_cursors\"}})        return data",
        "labels_text": "Allows block to be pickled and dumped"
    },
    {
        "input_text": "summarize: def __setstate__(self, data: dict):                self.__dict__.update(data)        self._start_connection()",
        "labels_text": "Reset connection and cursor upon loading"
    },
    {
        "input_text": "summarize: def _read_test_file(name: str) -> bytes:        full_name = os.path.join(os.path.split(__file__)[0], \"test_data\", name)    with open(full_name, \"rb\") as fd:        return fd.read()",
        "labels_text": "Args name File to load from testdata folder Returns File content a binary"
    },
    {
        "input_text": "summarize: def prefect_db():        try:        with prefect_test_harness():            yield    except OSError as e:        if \"Directory not empty\" in str(e):            pass        else:            raise e",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def get_engine(self) -> Union[\"Connection\", \"AsyncConnection\"]:                engine_kwargs = dict(            url=self.rendered_url,            connect_args=self.connect_args or {},            poolclass=NullPool,        )        if self._driver_is_async:            engine = create_async_engine(**engine_kwargs)        else:            engine = create_engine(**engine_kwargs)        return engine",
        "labels_text": "Returns an authenticated engine that can be used to query from database Returns The authenticated SQLAlchemy Connection AsyncConnection Examples Create an asynchronous engine to PostgreSQL using URL params python from prefect import flow from prefectsqlalchemy import DatabaseCredentials AsyncDriver flow def sqlalchemycredentialsflow sqlalchemycredentials DatabaseCredentials driverAsyncDriverPOSTGRESQLASYNCPG usernameprefect passwordprefectpassword databasepostgres printsqlalchemycredentialsgetengine sqlalchemycredentialsflow Create a synchronous engine to Snowflake using the url kwarg python from prefect import flow from prefectsqlalchemy import DatabaseCredentials AsyncDriver flow def sqlalchemycredentialsflow url snowflakeuserloginnamepassword accountidentifierdatabasename warehousewarehousename sqlalchemycredentials DatabaseCredentialsurlurl printsqlalchemycredentialsgetengine sqlalchemycredentialsflow"
    },
    {
        "input_text": "summarize: def dict(self, *args, **kwargs) -> Dict:                # Support serialization of the 'URL' type        d = super().dict(*args, **kwargs)        d[\"rendered_url\"] = SecretStr(            self.rendered_url.render_as_string(hide_password=False)        )        return d",
        "labels_text": "Convert to a dictionary"
    },
    {
        "input_text": "summarize: def _start_exit_stack(self):                self._exit_stack = AsyncExitStack() if self._driver_is_async else ExitStack()",
        "labels_text": "Starts an AsyncExitStack or ExitStack depending on whether driver is async"
    },
    {
        "input_text": "summarize: def get_connection(        self, begin: bool = True, **connect_kwargs: Dict[str, Any]    ) -> Union[Connection, AsyncConnection]:          # noqa: E501        engine = self.get_engine()        if begin:            connection = engine.begin(**connect_kwargs)        else:            connection = engine.connect(**connect_kwargs)        self.logger.info(\"Created a new connection.\")        return connection",
        "labels_text": "Returns a connection that can be used to query from database Args begin Whether to begin a transaction on the connection if True if any operation fail the entire transaction will be rolled back connectkwargs Additional keyword argument to pas to either enginebegin or engineconnect Returns The SQLAlchemy Connection AsyncConnection Examples Create an synchronous connection a a contextmanaged transaction python from prefectsqlalchemy import SqlAlchemyConnector sqlalchemyconnector SqlAlchemyConnectorloadBLOCKNAME with sqlalchemyconnectorgetconnectionbeginFalse a connection connectionexecuteSELECT FROM table LIMIT Create an asynchronous connection a a contextmanaged transacation python import asyncio from prefectsqlalchemy import SqlAlchemyConnector sqlalchemyconnector SqlAlchemyConnectorloadBLOCKNAME async with sqlalchemyconnectorgetconnectionbeginFalse a connection asynciorunconnectionexecuteSELECT FROM table LIMIT"
    },
    {
        "input_text": "summarize: def _reset_cursor_results(self) -> None:                input_hashes = tuple(self._unique_results.keys())        for input_hash in input_hashes:            try:                cursor_result = self._unique_results.pop(input_hash)                cursor_result.close()            except Exception as exc:                self.logger.warning(                    f\"Failed to close connection for input hash {input_hash!r}: {exc}\"                )",
        "labels_text": "Closes all the existing cursor result"
    },
    {
        "input_text": "summarize: def __enter__(self):                if self._driver_is_async:            raise RuntimeError(                f\"{self._rendered_url.drivername} cannot be run synchronously. \"                f\"Please use the `async with` syntax.\"            )        return self",
        "labels_text": "Start an synchronous database engine upon entry"
    },
    {
        "input_text": "summarize: def __exit__(self, *args):                self.close()",
        "labels_text": "Dispose the synchronous database engine upon exit"
    },
    {
        "input_text": "summarize: def close(self):                if self._driver_is_async:            raise RuntimeError(                f\"{self._rendered_url.drivername} is not synchronous. \"                f\"Please use the `aclose` method instead.\"            )        try:            self.reset_connections()        finally:            if self._engine is not None:                self._engine.dispose()                self._engine = None                self.logger.info(\"Disposed the engine.\")",
        "labels_text": "Closes sync connection and it cursor"
    },
    {
        "input_text": "summarize: def __getstate__(self):                data = self.__dict__.copy()        data.update({k: None for k in {\"_engine\", \"_exit_stack\", \"_unique_results\"}})        return data",
        "labels_text": "Allows the block to be pickleable"
    },
    {
        "input_text": "summarize: def __setstate__(self, data: dict):                self.__dict__.update(data)        if self._unique_results is None:            self._unique_results = {}        if self._exit_stack is None:            self._start_exit_stack()",
        "labels_text": "Upon loading back restart the engine and result"
    },
    {
        "input_text": "summarize: def prefect_db():        with prefect_test_harness():        yield",
        "labels_text": "Sets up test harness for temporary DB during test run"
    },
    {
        "input_text": "summarize: def _sanitize(        cls, item: Union[Dict[str, Any], List[Any], float]    ) -> Union[Dict[str, Any], List[Any], int, float, None]:                if isinstance(item, list):            return [cls._sanitize(sub_item) for sub_item in item]        elif isinstance(item, dict):            return {k: cls._sanitize(v) for k, v in item.items()}        elif isinstance(item, float) and math.isnan(item):            return None        else:            return item",
        "labels_text": "Sanitize NaN value in a given item The item can be a dict list or float"
    },
    {
        "input_text": "summarize: def from_cache_key_fn(        cls, cache_key_fn: Callable[[\"TaskRunContext\", Dict[str, Any]], Optional[str]]    ) -> \"CacheKeyFnPolicy\":                return CacheKeyFnPolicy(cache_key_fn=cache_key_fn)",
        "labels_text": "Given a function generates a key policy"
    },
    {
        "input_text": "summarize: def get(cls: Type[Self]) -> Optional[Self]:                return cls.__var__.get(None)",
        "labels_text": "Get the current context instance"
    },
    {
        "input_text": "summarize: def model_copy(        self: Self, *, update: Optional[Dict[str, Any]] = None, deep: bool = False    ):                new = super().model_copy(update=update, deep=deep)        # Remove the token on copy to avoid re-entrance errors        new._token = None        return new",
        "labels_text": "Duplicate the context model optionally choosing which field to include exclude or change Attributes include Fields to include in new model exclude Fields to exclude from new model a with value this take precedence over include update Values to changeadd in the new model Note the data is not validated before creating the new model you should trust this data deep Set to True to make a deep copy of the model Returns A new model instance"
    },
    {
        "input_text": "summarize: def serialize(self) -> Dict[str, Any]:                return self.model_dump(exclude_unset=True)",
        "labels_text": "Serialize the context model to a dictionary that can be pickled with cloudpickle"
    },
    {
        "input_text": "summarize: def __enter__(self):                return_value = super().__enter__()        try:            prefect_home = Path(self.settings.value_of(PREFECT_HOME))            prefect_home.mkdir(mode=0o0700, exist_ok=True)        except OSError:            warnings.warn(                (                    \"Failed to create the Prefect home directory at \"                    f\"{self.settings.value_of(PREFECT_HOME)}\"                ),                stacklevel=2,            )        return return_value",
        "labels_text": "Upon entrance we ensure the home directory for the profile exists"
    },
    {
        "input_text": "summarize: def get_run_context() -> Union[FlowRunContext, TaskRunContext]:        task_run_ctx = TaskRunContext.get()    if task_run_ctx:        return task_run_ctx    flow_run_ctx = FlowRunContext.get()    if flow_run_ctx:        return flow_run_ctx    raise MissingContextError(        \"No run context available. You are not in a flow or task run context.\"    )",
        "labels_text": "Get the current run context from within a task or flow function Returns A FlowRunContext or TaskRunContext depending on the function type Raises RuntimeError If called outside of a flow or task run"
    },
    {
        "input_text": "summarize: def get_settings_context() -> SettingsContext:        settings_ctx = SettingsContext.get()    if not settings_ctx:        raise MissingContextError(\"No settings context found.\")    return settings_ctx",
        "labels_text": "Get the current setting context which contains profile information and the setting that are being used Generally the setting that are being used are a combination of value from the profile and environment See prefectcontextuseprofile for more detail"
    },
    {
        "input_text": "summarize: def tags(*new_tags: str) -> Generator[Set[str], None, None]:        current_tags = TagsContext.get().current_tags    new_tags = current_tags.union(new_tags)    with TagsContext(current_tags=new_tags):        yield new_tags",
        "labels_text": "Context manager to add tag to flow and task run call Tags are always combined with any existing tag Yields The current set of tag Examples from prefect import tag task flow task def mytask pas Run a task with tag flow def myflow with tagsa b mytask ha tag a b Run a flow with tag flow def myflow pas with tagsa b myflow ha tag a b Run a task with nested tag context flow def myflow with tagsa b with tagsc d mytask ha tag a b c d mytask ha tag a b Inspect the current tag flow def myflow with tagsc d with tagse f a currenttags printcurrenttags with tagsa b myflow a b c d e f"
    },
    {
        "input_text": "summarize: def _trim_traceback(    tb: TracebackType, remove_modules: Iterable[ModuleType]) -> Optional[TracebackType]:        strip_paths = [module.__file__ for module in remove_modules]    while tb and any(        module_path in str(tb.tb_frame.f_globals.get(\"__file__\", \"\"))        for module_path in strip_paths    ):        tb = tb.tb_next    return tb",
        "labels_text": "Utility to remove frame from specific module from a traceback Only frame from the front of the traceback are removed Once a traceback frame is reached that doe not originate from removemodules it is returned Args tb The traceback to trim removemodules An iterable of module object to remove Returns A traceback or None if all traceback frame originate from an excluded module"
    },
    {
        "input_text": "summarize: def exception_traceback(exc: Exception) -> str:        tb = traceback.TracebackException.from_exception(exc)    return \"\".join(list(tb.format()))",
        "labels_text": "Convert an exception to a printable string with a traceback"
    },
    {
        "input_text": "summarize: def __get__(self, instance, owner):                # if no instance is provided, it's being accessed on the class        if instance is None:            return self        # if the flow is being accessed on an instance, bind the instance to the __prefect_self__ attribute        # of the flow's function. This will allow it to be automatically added to the flow's parameters        else:            bound_flow = copy(self)            bound_flow.fn.__prefect_self__ = instance            return bound_flow",
        "labels_text": "Implement the descriptor protocol so that the flow can be used a an instance method When an instance method is loaded this method is called with the self instance a an argument We return a copy of the flow with that instance bound to the flow function"
    },
    {
        "input_text": "summarize: def _raise_on_name_with_banned_characters(name: str) -> str:        if name is None:        return name    if not re.match(WITHOUT_BANNED_CHARACTERS, name):        raise InvalidNameError(            f\"Name {name!r} contains an invalid character. \"            f\"Must not contain any of: {BANNED_CHARACTERS}.\"        )    return name",
        "labels_text": "Raise an InvalidNameError if the given name contains any invalid character"
    },
    {
        "input_text": "summarize: def is_entrypoint_async(entrypoint: str) -> bool:        func_def, _ = _entrypoint_definition_and_source(entrypoint)    return isinstance(func_def, ast.AsyncFunctionDef)",
        "labels_text": "Determine if the function specified in the entrypoint is asynchronous Args entrypoint A string in the format pathtoscriptfuncname or a module path to a function Returns True if the function is asynchronous False otherwise"
    },
    {
        "input_text": "summarize: def call_flow_fn(self) -> Union[R, Coroutine[Any, Any, R]]:                if self.flow.isasync:            async def _call_flow_fn():                result = await call_with_parameters(self.flow.fn, self.parameters)                self.handle_success(result)            return _call_flow_fn()        else:            result = call_with_parameters(self.flow.fn, self.parameters)            self.handle_success(result)",
        "labels_text": "Convenience method to call the flow function Returns a coroutine if the flow is async"
    },
    {
        "input_text": "summarize: def task_run_id(self) -> uuid.UUID:                return self._task_run_id",
        "labels_text": "The ID of the task run associated with this future"
    },
    {
        "input_text": "summarize: def state(self) -> State:                if self._final_state:            return self._final_state        client = get_client(sync_client=True)        try:            task_run = cast(TaskRun, client.read_task_run(task_run_id=self.task_run_id))        except ObjectNotFound:            # We'll be optimistic and assume this task will eventually start            # TODO: Consider using task run events to wait for the task to start            return Pending()        return task_run.state or Pending()",
        "labels_text": "The current state of the task run associated with this future"
    },
    {
        "input_text": "summarize: def add_done_callback(self, fn):                ...",
        "labels_text": "Add a callback to be run when the future completes or is cancelled Args fn A callable that will be called with this future a it only argument when the future completes or is cancelled"
    },
    {
        "input_text": "summarize: def wrapped_future(self) -> F:                return self._wrapped_future",
        "labels_text": "The underlying future object wrapped by this Prefect future"
    },
    {
        "input_text": "summarize: def wait(self, timeout: Optional[float] = None) -> None:                wait(self, timeout=timeout)",
        "labels_text": "Wait for all future in the list to complete Args timeout The maximum number of second to wait for all future to complete This method will not raise if the timeout is reached"
    },
    {
        "input_text": "summarize: def resolve_serializer(serializer: ResultSerializer) -> Serializer:        if isinstance(serializer, Serializer):        return serializer    elif isinstance(serializer, str):        return Serializer(type=serializer)    else:        raise TypeError(            \"Result serializer must be one of the following types: 'Serializer', \"            f\"'str'. Got unsupported type {type(serializer).__name__!r}.\"        )",
        "labels_text": "Resolve one of the valid ResultSerializer input type into a serializer instance"
    },
    {
        "input_text": "summarize: def get_default_result_serializer() -> Serializer:        return resolve_serializer(PREFECT_RESULTS_DEFAULT_SERIALIZER.value())",
        "labels_text": "Generate a default file system for result storage"
    },
    {
        "input_text": "summarize: def get_default_persist_setting() -> bool:        return PREFECT_RESULTS_PERSIST_BY_DEFAULT.value()",
        "labels_text": "Return the default option for result persistence False"
    },
    {
        "input_text": "summarize: def generate_default_holder() -> str:                hostname = socket.gethostname()        pid = os.getpid()        thread_name = threading.current_thread().name        thread_id = threading.get_ident()        return f\"{hostname}:{pid}:{thread_id}:{thread_name}\"",
        "labels_text": "Generate a default holder string using hostname PID and thread ID Returns str A unique identifier string"
    },
    {
        "input_text": "summarize: def exists(self, key: str) -> bool:                return self._exists(key=key, _sync=True)",
        "labels_text": "Check if a result record exists in storage Args key The key to check for the existence of a result record Returns bool True if the result record exists False otherwise"
    },
    {
        "input_text": "summarize: def read(self, key: str, holder: Optional[str] = None) -> \"ResultRecord\":                holder = holder or self.generate_default_holder()        return self._read(key=key, holder=holder, _sync=True)",
        "labels_text": "Read a result record from storage Args key The key to read the result record from holder The holder of the lock if a lock wa set on the record Returns A result record"
    },
    {
        "input_text": "summarize: def create_result_record(        self,        key: str,        obj: Any,        expiration: Optional[DateTime] = None,    ):                key = key or self.storage_key_fn()        return ResultRecord(            result=obj,            metadata=ResultRecordMetadata(                serializer=self.serializer,                expiration=expiration,                storage_key=key,                storage_block_id=self.result_storage_block_id,            ),        )",
        "labels_text": "Create a result record Args key The key to create the result record for obj The object to create the result record for expiration The expiration time for the result record"
    },
    {
        "input_text": "summarize: def write(        self,        key: str,        obj: Any,        expiration: Optional[DateTime] = None,        holder: Optional[str] = None,    ):                holder = holder or self.generate_default_holder()        return self.persist_result_record(            result_record=self.create_result_record(                key=key, obj=obj, expiration=expiration            ),            holder=holder,        )",
        "labels_text": "Write a result to storage Handles the creation of a ResultRecord and it serialization to storage Args key The key to write the result record to obj The object to write to storage expiration The expiration time for the result record holder The holder of the lock if a lock wa set on the record"
    },
    {
        "input_text": "summarize: def persist_result_record(        self, result_record: \"ResultRecord\", holder: Optional[str] = None    ):                holder = holder or self.generate_default_holder()        return self._persist_result_record(            result_record=result_record, holder=holder, _sync=True        )",
        "labels_text": "Persist a result record to storage Args resultrecord The result record to persist"
    },
    {
        "input_text": "summarize: def supports_isolation_level(self, level: \"IsolationLevel\") -> bool:                from prefect.transactions import IsolationLevel        if level == IsolationLevel.READ_COMMITTED:            return True        elif level == IsolationLevel.SERIALIZABLE:            return self.lock_manager is not None        else:            raise ValueError(f\"Unsupported isolation level: {level}\")",
        "labels_text": "Check if the result store support a given isolation level Args level The isolation level to check Returns bool True if the isolation level is supported False otherwise"
    },
    {
        "input_text": "summarize: def acquire_lock(        self, key: str, holder: Optional[str] = None, timeout: Optional[float] = None    ) -> bool:                holder = holder or self.generate_default_holder()        if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.acquire_lock(key, holder, timeout)",
        "labels_text": "Acquire a lock for a result record Args key The key to acquire the lock for holder The holder of the lock If not provided a default holder based on the current host process and thread will be used timeout The timeout for the lock Returns bool True if the lock wa successfully acquired False otherwise"
    },
    {
        "input_text": "summarize: def release_lock(self, key: str, holder: Optional[str] = None):                holder = holder or self.generate_default_holder()        if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.release_lock(key, holder)",
        "labels_text": "Release a lock for a result record Args key The key to release the lock for holder The holder of the lock Must match the holder that acquired the lock If not provided a default holder based on the current host process and thread will be used"
    },
    {
        "input_text": "summarize: def is_locked(self, key: str) -> bool:                if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.is_locked(key)",
        "labels_text": "Check if a result record is locked"
    },
    {
        "input_text": "summarize: def is_lock_holder(self, key: str, holder: Optional[str] = None) -> bool:                holder = holder or self.generate_default_holder()        if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.is_lock_holder(key, holder)",
        "labels_text": "Check if the current holder is the lock holder for the result record Args key The key to check the lock for holder The holder of the lock If not provided a default holder based on the current host process and thread will be used Returns bool True if the current holder is the lock holder False otherwise"
    },
    {
        "input_text": "summarize: def wait_for_lock(self, key: str, timeout: Optional[float] = None) -> bool:                if self.lock_manager is None:            raise ConfigurationError(                \"Result store is not configured with a lock manager. Please set\"                \" a lock manager when creating the result store to enable locking.\"            )        return self.lock_manager.wait_for_lock(key, timeout)",
        "labels_text": "Wait for the corresponding transaction record to become free"
    },
    {
        "input_text": "summarize: def get_current_result_store() -> ResultStore:        from prefect.context import get_run_context    try:        run_context = get_run_context()    except MissingContextError:        result_store = ResultStore()    else:        result_store = run_context.result_store    return result_store",
        "labels_text": "Get the current result store"
    },
    {
        "input_text": "summarize: def dump_bytes(self) -> bytes:                return self.model_dump_json(serialize_as_any=True).encode()",
        "labels_text": "Serialize the metadata to byte Returns byte the serialized metadata"
    },
    {
        "input_text": "summarize: def load_bytes(cls, data: bytes) -> \"ResultRecordMetadata\":                return cls.model_validate_json(data)",
        "labels_text": "Deserialize metadata from byte Args data the serialized metadata Returns ResultRecordMetadata the deserialized metadata"
    },
    {
        "input_text": "summarize: def serialize(        self,    ) -> bytes:                return (            self.model_copy(update={\"result\": self.serialize_result()})            .model_dump_json(serialize_as_any=True)            .encode()        )",
        "labels_text": "Serialize the record to byte Returns byte the serialized record"
    },
    {
        "input_text": "summarize: def deserialize(cls, data: bytes) -> \"ResultRecord[R]\":                instance = cls.model_validate_json(data)        if isinstance(instance.result, bytes):            instance.result = instance.serializer.loads(instance.result)        elif isinstance(instance.result, str):            instance.result = instance.serializer.loads(instance.result.encode())        return instance",
        "labels_text": "Deserialize a record from byte Args data the serialized record Returns ResultRecord the deserialized record"
    },
    {
        "input_text": "summarize: def deserialize_from_result_and_metadata(        cls, result: bytes, metadata: bytes    ) -> \"ResultRecord[R]\":                result_record_metadata = ResultRecordMetadata.load_bytes(metadata)        return cls(            metadata=result_record_metadata,            result=result_record_metadata.serializer.loads(result),        )",
        "labels_text": "Deserialize a record from separate result and metadata byte Args result the result metadata the serialized metadata Returns ResultRecord the deserialized record"
    },
    {
        "input_text": "summarize: def _infer_path(storage_block, key) -> str:                if hasattr(storage_block, \"_resolve_path\"):            return storage_block._resolve_path(key)        if hasattr(storage_block, \"_remote_file_system\"):            return storage_block._remote_file_system._resolve_path(key)",
        "labels_text": "Attempts to infer a path associated with a storage block key this method will defer to the block in the future"
    },
    {
        "input_text": "summarize: def prefect_json_object_encoder(obj: Any) -> Any:        if isinstance(obj, BaseException):        return {\"__exc_type__\": to_qualified_name(obj.__class__), \"message\": str(obj)}    else:        return {            \"__class__\": to_qualified_name(obj.__class__),            \"data\": custom_pydantic_encoder({}, obj),        }",
        "labels_text": "JSONEncoderdefault for encoding object into JSON with extended type support Raises a TypeError to fallback on other encoders on failure"
    },
    {
        "input_text": "summarize: def prefect_json_object_decoder(result: dict):        if \"__class__\" in result:        return TypeAdapter(from_qualified_name(result[\"__class__\"])).validate_python(            result[\"data\"]        )    elif \"__exc_type__\" in result:        return from_qualified_name(result[\"__exc_type__\"])(result[\"message\"])    else:        return result",
        "labels_text": "JSONDecoderobjecthook for decoding object from JSON when previously encoded with prefectjsonobjectencoder"
    },
    {
        "input_text": "summarize: def dumps(self, obj: D) -> bytes:",
        "labels_text": "Encode the object into a blob of byte"
    },
    {
        "input_text": "summarize: def loads(self, blob: bytes) -> D:",
        "labels_text": "Decode the blob of byte into an object"
    },
    {
        "input_text": "summarize: def value(self, bypass_callback: bool = False) -> T:                return self.value_from(get_current_settings(), bypass_callback=bypass_callback)",
        "labels_text": "Get the current value of a setting Example python from prefectsettings import PREFECTAPIURL PREFECTAPIURLvalue"
    },
    {
        "input_text": "summarize: def __bool__(self) -> bool:                return bool(self.value())",
        "labels_text": "Returns a truthy check of the current value"
    },
    {
        "input_text": "summarize: def get_extra_loggers(_: \"Settings\", value: str) -> List[str]:        return [name.strip() for name in value.split(\",\")] if value else []",
        "labels_text": "valuecallback for PREFECTLOGGINGEXTRALOGGERSthat parses the CSV string into a list and trim whitespace from logger name"
    },
    {
        "input_text": "summarize: def debug_mode_log_level(settings, value):        if PREFECT_DEBUG_MODE.value_from(settings):        return \"DEBUG\"    else:        return value",
        "labels_text": "valuecallback for PREFECTLOGGINGLEVEL that override the log level to DEBUG when debug mode is enabled"
    },
    {
        "input_text": "summarize: def only_return_value_in_test_mode(settings, value):        if PREFECT_TEST_MODE.value_from(settings):        return value    else:        return None",
        "labels_text": "valuecallback for PREFECTTESTSETTING that only allows access during test mode"
    },
    {
        "input_text": "summarize: def default_ui_api_url(settings, value):        if value is None:        # Set a default value        value = \"/api\"    return template_with_settings(        PREFECT_SERVER_API_HOST, PREFECT_SERVER_API_PORT, PREFECT_API_URL    )(settings, value)",
        "labels_text": "valuecallback for PREFECTUIAPIURL that set the default value to relative path api otherwise it construct an API URL from the API setting"
    },
    {
        "input_text": "summarize: def max_log_size_smaller_than_batch_size(values):        if (        values[\"PREFECT_LOGGING_TO_API_BATCH_SIZE\"]        < values[\"PREFECT_LOGGING_TO_API_MAX_LOG_SIZE\"]    ):        raise ValueError(            \"`PREFECT_LOGGING_TO_API_MAX_LOG_SIZE` cannot be larger than\"            \" `PREFECT_LOGGING_TO_API_BATCH_SIZE`\"        )    return values",
        "labels_text": "Validator for setting asserting the batch size and match log size are compatible"
    },
    {
        "input_text": "summarize: def value_of(self, setting: Setting[T], bypass_callback: bool = False) -> T:                value = getattr(self, setting.name)        if setting.value_callback and not bypass_callback:            value = setting.value_callback(self, value)        return value",
        "labels_text": "Retrieve a setting value"
    },
    {
        "input_text": "summarize: def hash_key(self) -> str:                env_variables = self.to_environment_variables()        return str(hash(tuple((key, value) for key, value in env_variables.items())))",
        "labels_text": "Return a hash key for the setting object This is needed since some setting may be unhashable An example is list"
    },
    {
        "input_text": "summarize: def get_current_settings() -> Settings:        from prefect.context import SettingsContext    settings_context = SettingsContext.get()    if settings_context is not None:        return settings_context.settings    return get_settings_from_env()",
        "labels_text": "Returns a setting object populated with value from the current setting context or if no setting context is active the environment"
    },
    {
        "input_text": "summarize: def get_settings_from_env() -> Settings:        # Since os.environ is a Dict[str, str] we can safely hash it by contents, but we    # must be careful to avoid hashing a generator instead of a tuple    cache_key = hash(tuple((key, value) for key, value in os.environ.items()))    if cache_key not in _FROM_ENV_CACHE:        _FROM_ENV_CACHE[cache_key] = Settings()    return _FROM_ENV_CACHE[cache_key]",
        "labels_text": "Returns a setting object populated with default value and override from environment variable ignoring any value in profile Calls with the same environment return a cached object instead of reconstructing to avoid validation overhead"
    },
    {
        "input_text": "summarize: def get_default_settings() -> Settings:        global _DEFAULTS_CACHE    if not _DEFAULTS_CACHE:        old = os.environ        try:            os.environ = {}            settings = get_settings_from_env()        finally:            os.environ = old        _DEFAULTS_CACHE = settings    return _DEFAULTS_CACHE",
        "labels_text": "Returns a setting object populated with default value ignoring any override from environment variable or profile This is cached since the default should not change during the lifetime of the module"
    },
    {
        "input_text": "summarize: def validate_settings(self) -> None:                # Create a new `Settings` instance with the settings from this profile relying        # on Pydantic validation to raise an error.        # We do not return the `Settings` object because this is not the recommended        # path for constructing settings with a profile. See `use_profile` instead.        Settings(**{setting.name: value for setting, value in self.settings.items()})",
        "labels_text": "Validate the setting contained in this profile Raises pydanticValidationError When setting do not have valid value"
    },
    {
        "input_text": "summarize: def convert_deprecated_renamed_settings(self) -> List[Tuple[Setting, Setting]]:                changed = []        for setting in tuple(self.settings):            if (                setting.deprecated                and setting.deprecated_renamed_to                and setting.deprecated_renamed_to not in self.settings            ):                self.settings[setting.deprecated_renamed_to] = self.settings.pop(                    setting                )                changed.append((setting, setting.deprecated_renamed_to))        return changed",
        "labels_text": "Update setting in place to replace deprecated setting with new setting when renamed Returns a list of tuples with the old and new setting"
    },
    {
        "input_text": "summarize: def names(self) -> Set[str]:                return set(self.profiles_by_name.keys())",
        "labels_text": "Return a set of profile name in this collection"
    },
    {
        "input_text": "summarize: def active_profile(self) -> Optional[Profile]:                if self.active_name is None:            return None        return self[self.active_name]",
        "labels_text": "Retrieve the active profile in this collection"
    },
    {
        "input_text": "summarize: def set_active(self, name: Optional[str], check: bool = True):                if check and name is not None and name not in self.names:            raise ValueError(f\"Unknown profile name {name!r}.\")        self.active_name = name",
        "labels_text": "Set the active profile name in the collection A null value may be passed to indicate that this collection doe not determine the active profile"
    },
    {
        "input_text": "summarize: def add_profile(self, profile: Profile) -> None:                if profile.name in self.profiles_by_name:            raise ValueError(                f\"Profile name {profile.name!r} already exists in collection.\"            )        self.profiles_by_name[profile.name] = profile",
        "labels_text": "Add a profile to the collection If the profile name already exists an exception will be raised"
    },
    {
        "input_text": "summarize: def remove_profile(self, name: str) -> None:                self.profiles_by_name.pop(name)",
        "labels_text": "Remove a profile from the collection"
    },
    {
        "input_text": "summarize: def without_profile_source(self, path: Optional[Path]) -> \"ProfilesCollection\":                return ProfilesCollection(            [                profile                for profile in self.profiles_by_name.values()                if profile.source != path            ],            active=self.active_name,        )",
        "labels_text": "Remove profile that were loaded from a given path Returns a new collection"
    },
    {
        "input_text": "summarize: def to_dict(self):                return {            \"active\": self.active_name,            \"profiles\": {                profile.name: {                    setting.name: value for setting, value in profile.settings.items()                }                for profile in self.profiles_by_name.values()            },        }",
        "labels_text": "Convert to a dictionary suitable for writing to disk"
    },
    {
        "input_text": "summarize: def _read_profiles_from(path: Path) -> ProfilesCollection:        contents = toml.loads(path.read_text())    active_profile = contents.get(\"active\")    raw_profiles = contents.get(\"profiles\", {})    profiles = []    for name, settings in raw_profiles.items():        profiles.append(Profile(name=name, settings=settings, source=path))    return ProfilesCollection(profiles, active=active_profile)",
        "labels_text": "Read profile from a path into a new ProfilesCollection Profiles are expected to be written in TOML with the following schema active name Optionalstr profilesname str SETTING str value Any"
    },
    {
        "input_text": "summarize: def _write_profiles_to(path: Path, profiles: ProfilesCollection) -> None:        if not path.exists():        path.touch(mode=0o600)    return path.write_text(toml.dumps(profiles.to_dict()))",
        "labels_text": "Write profile in the given collection to a path a TOML Any existing data not present in the given profile will be deleted"
    },
    {
        "input_text": "summarize: def load_current_profile():        import prefect.context    profiles = load_profiles()    context = prefect.context.get_settings_context()    if context:        profiles.set_active(context.profile.name)    return profiles.active_profile",
        "labels_text": "Load the current profile from the default and current profile path This will not include setting from the current setting context Only setting that have been persisted to the profile file will be saved"
    },
    {
        "input_text": "summarize: def save_profiles(profiles: ProfilesCollection) -> None:        profiles_path = PREFECT_PROFILES_PATH.value()    profiles = profiles.without_profile_source(DEFAULT_PROFILES_PATH)    return _write_profiles_to(profiles_path, profiles)",
        "labels_text": "Writes all nondefault profile to the current profile path"
    },
    {
        "input_text": "summarize: def load_profile(name: str) -> Profile:        profiles = load_profiles()    try:        return profiles[name]    except KeyError:        raise ValueError(f\"Profile {name!r} not found.\")",
        "labels_text": "Load a single profile by name"
    },
    {
        "input_text": "summarize: def is_state_iterable(obj: Any) -> TypeGuard[Iterable[State]]:        # We do not check for arbitrary iterables because this is not intended to be used    # for things like dictionaries, dataframes, or pydantic models    if (        not isinstance(obj, BaseAnnotation)        and isinstance(obj, (list, set, tuple))        and obj    ):        return all([isinstance(o, State) for o in obj])    else:        return False",
        "labels_text": "Check if a the given object is an iterable of state type Supported iterables are set list tuple Other iterables will return False even if they contain state"
    },
    {
        "input_text": "summarize: def Completed(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.COMPLETED, **kwargs)",
        "labels_text": "Convenience function for creating Completed state Returns State a Completed state"
    },
    {
        "input_text": "summarize: def Running(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.RUNNING, **kwargs)",
        "labels_text": "Convenience function for creating Running state Returns State a Running state"
    },
    {
        "input_text": "summarize: def Failed(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.FAILED, **kwargs)",
        "labels_text": "Convenience function for creating Failed state Returns State a Failed state"
    },
    {
        "input_text": "summarize: def Crashed(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.CRASHED, **kwargs)",
        "labels_text": "Convenience function for creating Crashed state Returns State a Crashed state"
    },
    {
        "input_text": "summarize: def Cancelling(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.CANCELLING, **kwargs)",
        "labels_text": "Convenience function for creating Cancelling state Returns State a Cancelling state"
    },
    {
        "input_text": "summarize: def Cancelled(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.CANCELLED, **kwargs)",
        "labels_text": "Convenience function for creating Cancelled state Returns State a Cancelled state"
    },
    {
        "input_text": "summarize: def Pending(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.PENDING, **kwargs)",
        "labels_text": "Convenience function for creating Pending state Returns State a Pending state"
    },
    {
        "input_text": "summarize: def Suspended(    cls: Type[State[R]] = State,    timeout_seconds: Optional[int] = None,    pause_expiration_time: Optional[datetime.datetime] = None,    pause_key: Optional[str] = None,    **kwargs: Any,):        return Paused(        cls=cls,        name=\"Suspended\",        reschedule=True,        timeout_seconds=timeout_seconds,        pause_expiration_time=pause_expiration_time,        pause_key=pause_key,        **kwargs,    )",
        "labels_text": "Convenience function for creating Suspended state Returns State a Suspended state"
    },
    {
        "input_text": "summarize: def AwaitingRetry(    cls: Type[State[R]] = State,    scheduled_time: Optional[datetime.datetime] = None,    **kwargs: Any,) -> State[R]:        return Scheduled(        cls=cls, scheduled_time=scheduled_time, name=\"AwaitingRetry\", **kwargs    )",
        "labels_text": "Convenience function for creating AwaitingRetry state Returns State a AwaitingRetry state"
    },
    {
        "input_text": "summarize: def AwaitingConcurrencySlot(    cls: Type[State[R]] = State,    scheduled_time: Optional[datetime.datetime] = None,    **kwargs: Any,) -> State[R]:        return Scheduled(        cls=cls, scheduled_time=scheduled_time, name=\"AwaitingConcurrencySlot\", **kwargs    )",
        "labels_text": "Convenience function for creating AwaitingConcurrencySlot state Returns State a AwaitingConcurrencySlot state"
    },
    {
        "input_text": "summarize: def Retrying(cls: Type[State[R]] = State, **kwargs: Any) -> State[R]:        return cls(type=StateType.RUNNING, name=\"Retrying\", **kwargs)",
        "labels_text": "Convenience function for creating Retrying state Returns State a Retrying state"
    },
    {
        "input_text": "summarize: def Late(    cls: Type[State[R]] = State,    scheduled_time: Optional[datetime.datetime] = None,    **kwargs: Any,) -> State[R]:        return Scheduled(cls=cls, scheduled_time=scheduled_time, name=\"Late\", **kwargs)",
        "labels_text": "Convenience function for creating Late state Returns State a Late state"
    },
    {
        "input_text": "summarize: def task_input_hash(    context: \"TaskRunContext\", arguments: Dict[str, Any]) -> Optional[str]:        return hash_objects(        # We use the task key to get the qualified name for the task and include the        # task functions `co_code` bytes to avoid caching when the underlying function        # changes        context.task.task_key,        context.task.fn.__code__.co_code.hex(),        arguments,    )",
        "labels_text": "A task cache key implementation which hash all input to the task using a JSON or cloudpickle serializer If any argument are not JSON serializable the pickle serializer is used a a fallback If cloudpickle fails this will return a null key indicating that a cache key could not be generated for the given input Arguments context the active TaskRunContext argument a dictionary of argument to be passed to the underlying task Returns a string hash if hashing succeeded else None"
    },
    {
        "input_text": "summarize: def exponential_backoff(backoff_factor: float) -> Callable[[int], List[float]]:        def retry_backoff_callable(retries: int) -> List[float]:        # no more than 50 retry delays can be configured on a task        retries = min(retries, 50)        return [backoff_factor * max(0, 2**r) for r in range(retries)]    return retry_backoff_callable",
        "labels_text": "A task retry backoff utility that configures exponential backoff for task retries The exponential backoff design match the urllib implementation Arguments backofffactor the base delay for the first retry subsequent retries will increase the delay time by power of Returns a callable that can be passed to the task constructor"
    },
    {
        "input_text": "summarize: def __get__(self, instance, owner):                # if no instance is provided, it's being accessed on the class        if instance is None:            return self        # if the task is being accessed on an instance, bind the instance to the __prefect_self__ attribute        # of the task's function. This will allow it to be automatically added to the task's parameters        else:            bound_task = copy(self)            bound_task.fn.__prefect_self__ = instance            return bound_task",
        "labels_text": "Implement the descriptor protocol so that the task can be used a an instance method When an instance method is loaded this method is called with the self instance a an argument We return a copy of the task with that instance bound to the task function"
    },
    {
        "input_text": "summarize: def delay(self, *args: P.args, **kwargs: P.kwargs) -> PrefectDistributedFuture:                return self.apply_async(args=args, kwargs=kwargs)",
        "labels_text": "An alias for applyasync with simpler calling semantics Avoids having to use explicit args and kwargs argument Arguments will pas through asis to the task Examples Define a task from prefect import task task def mytaskname str world return fhello name Create a pending task run for the task from prefect import flow flow def myflow mytaskdelaymarvin Wait for a task to finish flow def myflow mytaskdelaymarvinwait Use the result from a task in a flow flow def myflow printmytaskdelaymarvinresult myflow hello marvin"
    },
    {
        "input_text": "summarize: def is_running(self) -> bool:                if (task_run := getattr(self, \"task_run\", None)) is None:            return False        return task_run.state.is_running() or task_run.state.is_scheduled()",
        "labels_text": "Whether or not the engine is currently running a task"
    },
    {
        "input_text": "summarize: def name(self):                return type(self).__name__.lower().replace(\"taskrunner\", \"\")",
        "labels_text": "The name of this task runner"
    },
    {
        "input_text": "summarize: def duplicate(self) -> Self:                ...",
        "labels_text": "Return a new instance of this task runner with the same configuration"
    },
    {
        "input_text": "summarize: def submit(        self,        task: \"Task\",        parameters: Dict[str, Any],        wait_for: Optional[Iterable[PrefectFuture]] = None,        dependencies: Optional[Dict[str, Set[TaskRunInput]]] = None,    ) -> F:                ...",
        "labels_text": "Submit a task to the task run engine Args task The task to submit parameter The parameter to use when running the task waitfor A list of future that the task depends on Returns A future object that can be used to wait for the task to complete and retrieve the result"
    },
    {
        "input_text": "summarize: def stop(self):                self.logger.debug(\"Stopping TaskRunWaiter\")        if self._consumer_task:            self._consumer_task.cancel()            self._consumer_task = None        self.__class__._instance = None        self._started = False",
        "labels_text": "Stop the TaskRunWaiter service"
    },
    {
        "input_text": "summarize: def add_done_callback(cls, task_run_id: uuid.UUID, callback):                instance = cls.instance()        with instance._observed_completed_task_runs_lock:            if task_run_id in instance._observed_completed_task_runs:                callback()                return        with instance._completion_events_lock:            # Cache the event for the task run ID so the consumer can set it            # when the event is received            instance._completion_callbacks[task_run_id] = callback",
        "labels_text": "Add a callback to be called when a task run finish Args taskrunid The ID of the task run to wait for callback The callback to call when the task run finish"
    },
    {
        "input_text": "summarize: def instance(cls):                with cls._instance_lock:            if cls._instance is None:                cls._instance = cls._new_instance()            return cls._instance",
        "labels_text": "Get the singleton instance of TaskRunWaiter"
    },
    {
        "input_text": "summarize: def should_try_to_read_parameters(task: Task, task_run: TaskRun) -> bool:        new_enough_state_details = hasattr(        task_run.state.state_details, \"task_parameters_id\"    )    task_accepts_parameters = bool(inspect.signature(task.fn).parameters)    return new_enough_state_details and task_accepts_parameters",
        "labels_text": "Determines whether a task run should read parameter from the result store"
    },
    {
        "input_text": "summarize: def handle_sigterm(self, signum, frame):                logger.info(\"SIGTERM received, initiating graceful shutdown...\")        from_sync.call_in_loop_thread(create_call(self.stop))        sys.exit(0)",
        "labels_text": "Shuts down the task worker when a SIGTERM is received"
    },
    {
        "input_text": "summarize: def get_config() -> VersioneerConfig:        # these strings are filled in when 'setup.py versioneer' creates    # _version.py    cfg = VersioneerConfig()    cfg.VCS = \"git\"    cfg.style = \"pep440\"    cfg.tag_prefix = \"\"    cfg.parentdir_prefix = \"\"    cfg.versionfile_source = \"src/prefect/_version.py\"    cfg.verbose = False    return cfg",
        "labels_text": "Create populate and return the VersioneerConfig object"
    },
    {
        "input_text": "summarize: def register_vcs_handler(vcs: str, method: str) -> Callable:  # decorator        def decorate(f: Callable) -> Callable:                if vcs not in HANDLERS:            HANDLERS[vcs] = {}        HANDLERS[vcs][method] = f        return f    return decorate",
        "labels_text": "Create decorator to mark a method a the handler of a VCS"
    },
    {
        "input_text": "summarize: def plus_or_dot(pieces: Dict[str, Any]) -> str:        if \"+\" in pieces.get(\"closest-tag\", \"\"):        return \".\"    return \"+\"",
        "labels_text": "Return a if we dont already have one else return a"
    },
    {
        "input_text": "summarize: def pep440_split_post(ver: str) -> Tuple[str, Optional[int]]:        vc = str.split(ver, \".post\")    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None",
        "labels_text": "Split pep version string at the postrelease segment Returns the release segment before the postrelease and the postrelease version number or if no postrelease segment is present"
    },
    {
        "input_text": "summarize: def render_pep440_old(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"] or pieces[\"dirty\"]:            rendered += \".post%d\" % pieces[\"distance\"]            if pieces[\"dirty\"]:                rendered += \".dev0\"    else:        # exception #1        rendered = \"0.post%d\" % pieces[\"distance\"]        if pieces[\"dirty\"]:            rendered += \".dev0\"    return rendered",
        "labels_text": "TAGpostDISTANCEdev The dev mean dirty Exceptions no tag postDISTANCEdev"
    },
    {
        "input_text": "summarize: def render_git_describe(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        if pieces[\"distance\"]:            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])    else:        # exception #1        rendered = pieces[\"short\"]    if pieces[\"dirty\"]:        rendered += \"-dirty\"    return rendered",
        "labels_text": "TAGDISTANCEgHEXdirty Like git describe tag dirty always Exceptions no tag HEXdirty note no g prefix"
    },
    {
        "input_text": "summarize: def render_git_describe_long(pieces: Dict[str, Any]) -> str:        if pieces[\"closest-tag\"]:        rendered = pieces[\"closest-tag\"]        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])    else:        # exception #1        rendered = pieces[\"short\"]    if pieces[\"dirty\"]:        rendered += \"-dirty\"    return rendered",
        "labels_text": "TAGDISTANCEgHEXdirty Like git describe tag dirty always long The distancehash is unconditional Exceptions no tag HEXdirty note no g prefix"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the CredentialsBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def get_client(self, *args, **kwargs):",
        "labels_text": "Returns a client for interacting with the external system If a service offer various client this method can accept a clienttype keyword argument to get the desired client within the service"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the NotificationBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def raise_on_failure(self) -> Generator[None, None, None]:                self._raise_on_failure = True        try:            yield        finally:            self._raise_on_failure = False",
        "labels_text": "Context manager that while active cause the block to raise error if it encounter a failure sending notification"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the JobRun is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the JobBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the DatabaseBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def __enter__(self) -> Self:                raise NotImplementedError(            f\"{self.__class__.__name__} does not support context management.\"        )",
        "labels_text": "Context management method for database"
    },
    {
        "input_text": "summarize: def __exit__(self, *args) -> None:                raise NotImplementedError(            f\"{self.__class__.__name__} does not support context management.\"        )",
        "labels_text": "Context management method for database"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the ObjectStorageBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def logger(self) -> Logger:                try:            return get_run_logger()        except MissingContextError:            return get_logger(self.__class__.__name__)",
        "labels_text": "Returns a logger based on whether the SecretBlock is called from within a flow or task run context If a run context is present the logger property return a run logger Else it return a default logger labeled with the class name Returns The run logger or a default logger with the class name"
    },
    {
        "input_text": "summarize: def block_schema_to_key(schema: BlockSchema) -> str:        return f\"{schema.block_type.slug}\"",
        "labels_text": "Defines the unique key used to lookup the Block class for a given schema"
    },
    {
        "input_text": "summarize: def _is_subclass(cls, parent_cls) -> bool:        # For python<=3.11 inspect.isclass() will return True for parametrized types (e.g. list[str])    # so we need to check for get_origin() to avoid TypeError for issubclass.    return inspect.isclass(cls) and not get_origin(cls) and issubclass(cls, parent_cls)",
        "labels_text": "Checks if a given class is a subclass of another class Unlike issubclass this will not throw an exception if cl is an instance instead of a type"
    },
    {
        "input_text": "summarize: def get_block_capabilities(cls) -> FrozenSet[str]:                return frozenset(            {                c                for base in (cls,) + cls.__mro__                for c in getattr(base, \"_block_schema_capabilities\", []) or []            }        )",
        "labels_text": "Returns the block capability for this Block Recursively collect all block capability of all parent class into a single frozenset"
    },
    {
        "input_text": "summarize: def _parse_docstring(cls) -> List[DocstringSection]:                with disable_logger(\"griffe\"):            docstring = Docstring(cls.__doc__)            parsed = parse(docstring, Parser.google)        return parsed",
        "labels_text": "Parses the docstring into list of DocstringSection object Helper method used primarily to suppress irrelevant log eg module No type or annotation for parameter writejson because griffe is unable to parse the type from pydanticBaseModel"
    },
    {
        "input_text": "summarize: def _generate_code_example(cls) -> str:                qualified_name = to_qualified_name(cls)        module_str = \".\".join(qualified_name.split(\".\")[:-1])        class_name = cls.__name__        block_variable_name = f'{cls.get_block_type_slug().replace(\"-\", \"_\")}_block'        return dedent(            f        )",
        "labels_text": "Generates a default code example for the current class"
    },
    {
        "input_text": "summarize: def _to_block_type(cls) -> BlockType:                return BlockType(            id=cls._block_type_id or uuid4(),            slug=cls.get_block_type_slug(),            name=cls.get_block_type_name(),            logo_url=cls._logo_url,            documentation_url=cls._documentation_url,            description=cls.get_description(),            code_example=cls.get_code_example(),        )",
        "labels_text": "Creates the corresponding block type of the block Returns BlockType The corresponding block type"
    },
    {
        "input_text": "summarize: def get_block_class_from_schema(cls: Type[Self], schema: BlockSchema) -> Type[Self]:                return cls.get_block_class_from_key(block_schema_to_key(schema))",
        "labels_text": "Retrieve the block class implementation given a schema"
    },
    {
        "input_text": "summarize: def get_block_class_from_key(cls: Type[Self], key: str) -> Type[Self]:                # Ensure collections are imported and have the opportunity to register types        # before looking up the block class        prefect.plugins.load_prefect_collections()        return lookup_type(cls, key)",
        "labels_text": "Retrieve the block class implementation given a key"
    },
    {
        "input_text": "summarize: def __new__(cls: Type[Self], **kwargs) -> Self:                block_type_slug = kwargs.pop(\"block_type_slug\", None)        if block_type_slug:            subcls = lookup_type(cls, dispatch_key=block_type_slug)            return super().__new__(subcls)        else:            return super().__new__(cls)",
        "labels_text": "Create an instance of the Block subclass type if a blocktypeslug is present in the data payload"
    },
    {
        "input_text": "summarize: def get_block_placeholder(self) -> str:                block_document_name = self._block_document_name        if not block_document_name:            raise BlockNotSavedError(                \"Could not generate block placeholder for unsaved block.\"            )        return f\"prefect.blocks.{self.get_block_type_slug()}.{block_document_name}\"",
        "labels_text": "Returns the block placeholder for the current block which can be used for templating Returns str The block placeholder for the current block in the format prefectblocksblocktypenameblockdocumentname Raises BlockNotSavedError Raised if the block ha not been saved If a block ha not been saved the return value will be None"
    },
    {
        "input_text": "summarize: def block_initialization(self) -> None:                from apprise.plugins.workflows import NotifyWorkflows        if not (            parsed_url := NotifyWorkflows.parse_native_url(self.url.get_secret_value())        ):            raise ValueError(\"Invalid Microsoft Teams Workflow URL provided.\")        parsed_url |= {\"include_image\": self.include_image, \"wrap\": self.wrap}        self._start_apprise_client(SecretStr(NotifyWorkflows(**parsed_url).url()))",
        "labels_text": "see httpsgithubcomcaroncapprisepull"
    },
    {
        "input_text": "summarize: def from_host(        cls,        host: str,        port: int = 6379,        db: int = 0,        username: Union[None, str, SecretStr] = None,        password: Union[None, str, SecretStr] = None,    ) -> Self:                username = SecretStr(username) if isinstance(username, str) else username        password = SecretStr(password) if isinstance(password, str) else password        return cls(host=host, port=port, db=db, username=username, password=password)",
        "labels_text": "Create block from hostname username and password Args host Redis hostname username Redis username password Redis password port Redis port Returns RedisStorageContainer instance"
    },
    {
        "input_text": "summarize: def from_connection_string(cls, connection_string: Union[str, SecretStr]) -> Self:                connection_string = (            SecretStr(connection_string)            if isinstance(connection_string, str)            else connection_string        )        return cls(connection_string=connection_string)",
        "labels_text": "Create block from a Redis connection string Supports the following URL scheme redis creates a TCP socket connection rediss creates a SSL wrapped TCP socket connection unix creates a Unix Domain Socket connection See Redis docshttpsredisreadthedocsioenstableexamples connectionexampleshtmlConnectingtoRedisinstancesbyspecifyingaURL scheme for more info Args connectionstring Redis connection string Returns RedisStorageContainer instance"
    },
    {
        "input_text": "summarize: def _construct_schedules(    deploy_config: Dict,) -> List[DeploymentScheduleCreate]:        schedule_configs = deploy_config.get(\"schedules\", NotSet) or []    if schedule_configs is not NotSet:        schedules = [            _schedule_config_to_deployment_schedule(schedule_config)            for schedule_config in schedule_configs        ]    elif schedule_configs is NotSet:        if is_interactive():            schedules = prompt_schedules(app.console)        else:            schedules = []    return schedules",
        "labels_text": "Constructs a schedule from a deployment configuration Args deployconfig A deployment configuration Returns A list of schedule object"
    },
    {
        "input_text": "summarize: def str_presenter(dumper, data):        if len(data.splitlines()) > 1:  # check for multiline string        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)",
        "labels_text": "configures yaml for dumping multiline string Ref httpsstackoverflowcomquestionshowcanicontrolwhatscalarformpyyamlusesformydata"
    },
    {
        "input_text": "summarize: def kubernetes_manifest():        exit_with_error_if_not_editable_install()    template = Template(        (            prefect.__module_path__ / \"cli\" / \"templates\" / \"kubernetes-dev.yaml\"        ).read_text()    )    manifest = template.substitute(        {            \"prefect_root_directory\": prefect.__development_base_path__,            \"image_name\": get_prefect_image_name(),        }    )    print(manifest)",
        "labels_text": "Generates a Kubernetes manifest for development Example prefect dev kubernetesmanifest kubectl apply f"
    },
    {
        "input_text": "summarize: def get_prefect_integrations() -> Dict[str, str]:        from importlib.metadata import distributions    integrations = {}    for dist in distributions():        if dist.metadata[\"Name\"].startswith(\"prefect-\"):            author_email = dist.metadata.get(\"Author-email\", \"\").strip()            if author_email.endswith(\"@prefect.io>\"):                integrations[dist.metadata[\"Name\"]] = dist.version    return integrations",
        "labels_text": "Get information about installed Prefect integration"
    },
    {
        "input_text": "summarize: def display(object: Dict[str, Any], nesting: int = 0):        for key, value in object.items():        key += \":\"        if isinstance(value, dict):            app.console.print(\" \" * nesting + key)            display(value, nesting + 2)        else:            prefix = \" \" * nesting            app.console.print(f\"{prefix}{key.ljust(20 - len(prefix))} {value}\")",
        "labels_text": "Recursive display of a dictionary with nesting"
    },
    {
        "input_text": "summarize: def output_stream(pipe, logger_function):        with pipe:        for line in iter(pipe.readline, \"\"):            logger_function(line.strip())",
        "labels_text": "Read from a pipe line by line and log using the provided logging function Args pipe IO A filelike object for reading process output loggerfunction function A logging function from the logger"
    },
    {
        "input_text": "summarize: def output_collect(pipe, container):        for line in iter(pipe.readline, \"\"):        container.append(line)",
        "labels_text": "Collects output from a subprocess pipe and store it in a container list Args pipe The output pipe of the subprocess either stdout or stderr container A list to store the collected output line"
    },
    {
        "input_text": "summarize: def has_provisioner_for_type(work_pool_type: str) -> bool:        return work_pool_type in _provisioners",
        "labels_text": "Check if there is a provisioner for the given work pool type Args workpooltype str The type of the work pool Returns bool True if a provisioner exists for the given type False otherwise"
    },
    {
        "input_text": "summarize: def prompt(message, **kwargs):        return Prompt.ask(f\"[bold][green]?[/] {message}[/]\", **kwargs)",
        "labels_text": "Utility to prompt the user for input with consistent styling"
    },
    {
        "input_text": "summarize: def confirm(message, **kwargs):        return Confirm.ask(f\"[bold][green]?[/] {message}[/]\", **kwargs)",
        "labels_text": "Utility to prompt the user for confirmation with consistent styling"
    },
    {
        "input_text": "summarize: def prompt_for_schedule_max_active_runs(console) -> int:        return prompt(        \"Maximum number of active runs for this schedule (leave blank for unlimited)\",        console=console,        default=None,    )",
        "labels_text": "Prompt the user for the maximum number of active run for a schedule"
    },
    {
        "input_text": "summarize: def prompt_for_schedule_catchup(console) -> bool:        return Confirm.ask(        \"[bold][green]?[/] Catch up on late flow runs?\",        console=console,        default=False,    )",
        "labels_text": "Prompt the user for whether to catchup on missed run for a schedule"
    },
    {
        "input_text": "summarize: def prompt_cron_schedule(console):        cron = CronStringPrompt.ask(        \"[bold][green]?[/] Cron string\",        console=console,        default=\"0 0 * * *\",    )    timezone = CronTimezonePrompt.ask(        \"[bold][green]?[/] Timezone\", console=console, default=\"UTC\"    )    return CronSchedule(cron=cron, timezone=timezone)",
        "labels_text": "Prompt the user for a cron string and timezone"
    },
    {
        "input_text": "summarize: def prompt_rrule_schedule(console):        rrule = RRuleStringPrompt.ask(        \"[bold][green]?[/] RRule string\",        console=console,        default=\"RRULE:FREQ=DAILY;INTERVAL=1\",    )    timezone = CronTimezonePrompt.ask(        \"[bold][green]?[/] Timezone\", console=console, default=\"UTC\"    )    return RRuleSchedule(rrule=rrule, timezone=timezone)",
        "labels_text": "Prompts the user to enter an RRule string and timezone"
    },
    {
        "input_text": "summarize: def SettingsArgument(setting: Setting, *args, **kwargs) -> typer.Argument:        # See comments in `SettingsOption`    return typer.Argument(        setting.value,        *args,        show_default=f\"from {setting.name}\",        **kwargs,    )",
        "labels_text": "Custom typerArgument factory to load the default value from setting"
    },
    {
        "input_text": "summarize: def exit_with_error(message, code=1, **kwargs) -> NoReturn:        from prefect.cli.root import app    kwargs.setdefault(\"style\", \"red\")    app.console.print(message, **kwargs)    raise typer.Exit(code)",
        "labels_text": "Utility to print a stylized error message and exit with a nonzero code"
    },
    {
        "input_text": "summarize: def exit_with_success(message, **kwargs) -> NoReturn:        from prefect.cli.root import app    kwargs.setdefault(\"style\", \"green\")    app.console.print(message, **kwargs)    raise typer.Exit(0)",
        "labels_text": "Utility to print a stylized success message and exit with a zero code"
    },
    {
        "input_text": "summarize: def require_access_to_ip_allowlisting(ctx: typer.Context):        asyncio.run(_require_access_to_ip_allowlisting(ctx))",
        "labels_text": "Enforce access to IP allowlisting for all subcommands"
    },
    {
        "input_text": "summarize: def raise_for_status(self) -> None:                try:            return super().raise_for_status()        except HTTPStatusError as exc:            raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__",
        "labels_text": "Raise an exception if the response contains an HTTPStatusError The PrefectHTTPStatusError contains useful additional information that is not contained in the HTTPStatusError"
    },
    {
        "input_text": "summarize: def from_httpx_response(cls: Type[Self], response: httpx.Response) -> Self:                new_response = copy.copy(response)        new_response.__class__ = cls        return new_response",
        "labels_text": "Create a PrefectReponse from an httpxResponse By changing the class attribute of the Response we change the method resolution order to look for method defined in PrefectResponse while leaving everything else about the original Response instance intact"
    },
    {
        "input_text": "summarize: def determine_server_type() -> ServerType:        api_url = PREFECT_API_URL.value()    if api_url is None:        if PREFECT_SERVER_ALLOW_EPHEMERAL_MODE.value():            return ServerType.EPHEMERAL        else:            return ServerType.UNCONFIGURED    if api_url.startswith(PREFECT_CLOUD_API_URL.value()):        return ServerType.CLOUD    else:        return ServerType.SERVER",
        "labels_text": "Determine the server type based on the current setting Returns ServerTypeEPHEMERAL if the ephemeral server is enabled ServerTypeSERVER if a API URL is configured and it is not a cloud URL ServerTypeCLOUD if an API URL is configured and it is a cloud URL ServerTypeUNCONFIGURED if no API URL is configured and ephemeral mode is not enabled"
    },
    {
        "input_text": "summarize: def get_collections_metadata_client(    httpx_settings: Optional[Dict] = None,) -> \"CollectionsMetadataClient\":        orchestration_client = get_client(httpx_settings=httpx_settings)    if orchestration_client.server_type == ServerType.CLOUD:        return get_cloud_client(httpx_settings=httpx_settings, infer_cloud_url=True)    else:        return orchestration_client",
        "labels_text": "Creates a client that can be used to fetch metadata for Prefect collection Will return a CloudClient if profile is set to connect to Prefect Cloud otherwise will return an OrchestrationClient"
    },
    {
        "input_text": "summarize: def api_url(self) -> httpx.URL:                return self._client.base_url",
        "labels_text": "Get the base URL for the API"
    },
    {
        "input_text": "summarize: def api_url(self) -> httpx.URL:                return self._client.base_url",
        "labels_text": "Get the base URL for the API"
    },
    {
        "input_text": "summarize: def __enter__(self) -> \"SyncPrefectClient\":                if self._closed:            # httpx.Client does not allow reuse so we will not either.            raise RuntimeError(                \"The client cannot be started again after closing. \"                \"Retrieve a new client with `get_client()` instead.\"            )        self._context_stack += 1        if self._started:            # allow reentrancy            return self        self._client.__enter__()        self._started = True        return self",
        "labels_text": "Start the client If the client is already started this will raise an exception If the client is already closed this will raise an exception Use a new client instance instead"
    },
    {
        "input_text": "summarize: def __exit__(self, *exc_info) -> None:                self._context_stack -= 1        if self._context_stack > 0:            return        self._closed = True        self._client.__exit__(*exc_info)",
        "labels_text": "Shutdown the client"
    },
    {
        "input_text": "summarize: def api_healthcheck(self) -> Optional[Exception]:                try:            self._client.get(\"/health\")            return None        except Exception as exc:            return exc",
        "labels_text": "Attempts to connect to the API and return the encountered exception if not successful If successful return None"
    },
    {
        "input_text": "summarize: def hello(self) -> httpx.Response:                return self._client.get(\"/hello\")",
        "labels_text": "Send a GET request to hello for testing purpose"
    },
    {
        "input_text": "summarize: def create_flow(self, flow: \"FlowObject\") -> UUID:                return self.create_flow_from_name(flow.name)",
        "labels_text": "Create a flow in the Prefect API Args flow a FlowprefectflowsFlow object Raises httpxRequestError if a flow wa not created for any reason Returns the ID of the flow in the backend"
    },
    {
        "input_text": "summarize: def create_flow_from_name(self, flow_name: str) -> UUID:                flow_data = FlowCreate(name=flow_name)        response = self._client.post(\"/flows/\", json=flow_data.model_dump(mode=\"json\"))        flow_id = response.json().get(\"id\")        if not flow_id:            raise httpx.RequestError(f\"Malformed response: {response}\")        # Return the id of the created flow        return UUID(flow_id)",
        "labels_text": "Create a flow in the Prefect API Args flowname the name of the new flow Raises httpxRequestError if a flow wa not created for any reason Returns the ID of the flow in the backend"
    },
    {
        "input_text": "summarize: def read_flow_run(self, flow_run_id: UUID) -> FlowRun:                try:            response = self._client.get(f\"/flow_runs/{flow_run_id}\")        except httpx.HTTPStatusError as e:            if e.response.status_code == 404:                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e            else:                raise        return FlowRun.model_validate(response.json())",
        "labels_text": "Query the Prefect API for a flow run by id Args flowrunid the flow run ID of interest Returns a Flow Run model representation of the flow run"
    },
    {
        "input_text": "summarize: def read_task_run(self, task_run_id: UUID) -> TaskRun:                try:            response = self._client.get(f\"/task_runs/{task_run_id}\")            return TaskRun.model_validate(response.json())        except httpx.HTTPStatusError as e:            if e.response.status_code == status.HTTP_404_NOT_FOUND:                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e            else:                raise",
        "labels_text": "Query the Prefect API for a task run by id Args taskrunid the task run ID of interest Returns a Task Run model representation of the task run"
    },
    {
        "input_text": "summarize: def read_task_run_states(self, task_run_id: UUID) -> List[prefect.states.State]:                response = self._client.get(            \"/task_run_states/\", params=dict(task_run_id=str(task_run_id))        )        return pydantic.TypeAdapter(List[prefect.states.State]).validate_python(            response.json()        )",
        "labels_text": "Query for the state of a task run Args taskrunid the id of the task run Returns a list of State model representation of the task run state"
    },
    {
        "input_text": "summarize: def read_deployment(        self,        deployment_id: UUID,    ) -> DeploymentResponse:                try:            response = self._client.get(f\"/deployments/{deployment_id}\")        except httpx.HTTPStatusError as e:            if e.response.status_code == status.HTTP_404_NOT_FOUND:                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e            else:                raise        return DeploymentResponse.model_validate(response.json())",
        "labels_text": "Query the Prefect API for a deployment by id Args deploymentid the deployment ID of interest Returns a Deployment modelprefectclientschemasobjectsDeployment representation of the deployment"
    },
    {
        "input_text": "summarize: def read_deployment_by_name(        self,        name: str,    ) -> DeploymentResponse:                try:            response = self._client.get(f\"/deployments/name/{name}\")        except httpx.HTTPStatusError as e:            if e.response.status_code == status.HTTP_404_NOT_FOUND:                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e            else:                raise        return DeploymentResponse.model_validate(response.json())",
        "labels_text": "Query the Prefect API for a deployment by name Args name A deployed flow name FLOWNAMEDEPLOYMENTNAME Raises prefectexceptionsObjectNotFound If request return httpxRequestError If request fails Returns a Deployment model representation of the deployment"
    },
    {
        "input_text": "summarize: def create_artifact(        self,        artifact: ArtifactCreate,    ) -> Artifact:                response = self._client.post(            \"/artifacts/\",            json=artifact.model_dump(mode=\"json\", exclude_unset=True),        )        return Artifact.model_validate(response.json())",
        "labels_text": "Creates an artifact with the provided configuration Args artifact Desired configuration for the new artifact Returns Information about the newly created artifact"
    },
    {
        "input_text": "summarize: def release_concurrency_slots(        self, names: List[str], slots: int, occupancy_seconds: float    ) -> httpx.Response:                return self._client.post(            \"/v2/concurrency_limits/decrement\",            json={                \"names\": names,                \"slots\": slots,                \"occupancy_seconds\": occupancy_seconds,            },        )",
        "labels_text": "Release concurrency slot for the specified limit Args name Liststr A list of limit name for which to release slot slot int The number of concurrency slot to release occupancyseconds float The duration in second that the slot were occupied Returns httpxResponse The HTTP response from the server"
    },
    {
        "input_text": "summarize: def decrement_v1_concurrency_slots(        self, names: List[str], occupancy_seconds: float, task_run_id: UUID    ) -> httpx.Response:                return self._client.post(            \"/concurrency_limits/decrement\",            json={                \"names\": names,                \"occupancy_seconds\": occupancy_seconds,                \"task_run_id\": str(task_run_id),            },        )",
        "labels_text": "Release the specified concurrency limit Args name Liststr A list of limit name to decrement occupancyseconds float The duration in second that the slot were held taskrunid UUID The task run ID that incremented the limit Returns httpxResponse The HTTP response from the server"
    },
    {
        "input_text": "summarize: def result(        self,        raise_on_failure: bool = True,        fetch: Optional[bool] = None,        retry_result_failure: bool = True,    ) -> Union[R, Exception]:                from prefect.states import get_state_result        return get_state_result(            self,            raise_on_failure=raise_on_failure,            fetch=fetch,            retry_result_failure=retry_result_failure,        )",
        "labels_text": "Retrieve the result attached to this state Args raiseonfailure a boolean specifying whether to raise an exception if the state is of type FAILED and the underlying data is an exception fetch a boolean specifying whether to resolve reference to persisted result into data For synchronous user this default to True For asynchronous user this default to False for backwards compatibility retryresultfailure a boolean specifying whether to retry on failure to load the result from result storage Raises TypeError If the state is failed but the result is not an exception Returns The result of the run Examples from prefect import flow task task def mytaskx return x Get the result from a task future in a flow flow def myflow future mytaskhello state futurewait result stateresult printresult myflow hello Get the result from a flow state flow def myflow return hello myflowreturnstateTrueresult hello Get the result from a failed state flow def myflow raise ValueErroroh no state myflowreturnstateTrue Error is wrapped in FAILED state stateresult Raises ValueError Get the result from a failed state without erroring flow def myflow raise ValueErroroh no state myflowreturnstateTrue result stateresultraiseonfailureFalse printresult ValueErroroh no Get the result from a flow state in an async context flow async def myflow return hello state await myflowreturnstateTrue await stateresult hello"
    },
    {
        "input_text": "summarize: def to_state_create(self):                from prefect.client.schemas.actions import StateCreate        from prefect.results import BaseResult        if isinstance(self.data, BaseResult) and self.data.serialize_to_none is False:            data = self.data        else:            data = None        return StateCreate(            type=self.type,            name=self.name,            message=self.message,            data=data,            state_details=self.state_details,        )",
        "labels_text": "Convert this state to a StateCreate type which can be used to set the state of a run in the API This method will drop this state data if it is not a result type Only result should be sent to the API Other data is only available locally"
    },
    {
        "input_text": "summarize: def default_name_from_type(self) -> Self:                # if `type` is not in `values` it means the `type` didn't pass its own        # validation check and an error will be raised after this function is called        name = self.name        if name is None and self.type:            self.name = \" \".join([v.capitalize() for v in self.type.value.split(\"_\")])        return self",
        "labels_text": "If a name is not provided use the type"
    },
    {
        "input_text": "summarize: def model_copy(        self, *, update: Optional[Dict[str, Any]] = None, deep: bool = False    ):                update = update or {}        update.setdefault(\"timestamp\", self.model_fields[\"timestamp\"].get_default())        return super().model_copy(update=update, deep=deep)",
        "labels_text": "Copying API model should return an object that could be inserted into the database again The timestamp is reset using the default factory"
    },
    {
        "input_text": "summarize: def fresh_copy(self, **kwargs) -> Self:                return self.model_copy(            update={                \"id\": uuid4(),                \"created\": pendulum.now(\"utc\"),                \"updated\": pendulum.now(\"utc\"),                \"timestamp\": pendulum.now(\"utc\"),            },            **kwargs,        )",
        "labels_text": "Return a fresh copy of the state with a new ID"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                result = self.data        display = dict(            message=repr(self.message),            type=str(self.type.value),            result=repr(result),        )        return f\"{self.name}({', '.join(f'{k}={v}' for k, v in display.items())})\"",
        "labels_text": "Generates a complete state representation appropriate for introspection and debugging including the result MyCompletedStatemessagemy message typeCOMPLETED result"
    },
    {
        "input_text": "summarize: def __str__(self) -> str:                display = []        if self.message:            display.append(repr(self.message))        if self.type.value.lower() != self.name.lower():            display.append(f\"type={self.type.value}\")        return f\"{self.name}({', '.join(display)})\"",
        "labels_text": "Generates a simple state representation appropriate for logging MyCompletedStatemy message typeCOMPLETED"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                if isinstance(other, FlowRun):            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}            return self.model_dump(exclude=exclude_fields) == other.model_dump(                exclude=exclude_fields            )        return super().__eq__(other)",
        "labels_text": "Check for equality to another flow run schema Estimates time are rolling and will always change with repeated query for a flow run so we ignore them during equality check"
    },
    {
        "input_text": "summarize: def populate_deprecated_fields(self):                # We have marked these fields as deprecated, so we need to filter out the        # deprecation warnings _we're_ generating here        with warnings.catch_warnings():            warnings.simplefilter(\"ignore\", DeprecationWarning)            if not self.retries and self.max_retries != 0:                self.retries = self.max_retries            if not self.retry_delay and self.retry_delay_seconds != 0:                self.retry_delay = self.retry_delay_seconds        return self",
        "labels_text": "If deprecated field are provided populate the corresponding new field to preserve orchestration behavior"
    },
    {
        "input_text": "summarize: def handle(self) -> str:                return self.account_handle + \"/\" + self.workspace_handle",
        "labels_text": "The full handle of the workspace a accounthandle workspacehandle"
    },
    {
        "input_text": "summarize: def api_url(self) -> str:                return (            f\"{PREFECT_CLOUD_API_URL.value()}\"            f\"/accounts/{self.account_id}\"            f\"/workspaces/{self.workspace_id}\"        )",
        "labels_text": "Generate the API URL for accessing this workspace"
    },
    {
        "input_text": "summarize: def ui_url(self) -> str:                return (            f\"{PREFECT_CLOUD_UI_URL.value()}\"            f\"/account/{self.account_id}\"            f\"/workspace/{self.workspace_id}\"        )",
        "labels_text": "Generate the UI URL for accessing this workspace"
    },
    {
        "input_text": "summarize: def decoded_value(self) -> Any:                return orjson.loads(self.value)",
        "labels_text": "Decode the value of the input Returns Any the decoded value"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                if isinstance(other, objects.FlowRun):            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}            return self.model_dump(exclude=exclude_fields) == other.model_dump(                exclude=exclude_fields            )        return super().__eq__(other)",
        "labels_text": "Check for equality to another flow run schema Estimates time are rolling and will always change with repeated query for a flow run so we ignore them during equality check"
    },
    {
        "input_text": "summarize: def valid_timezone(cls, v):                from prefect._internal.pytz import HAS_PYTZ        if HAS_PYTZ:            import pytz        else:            from prefect._internal import pytz        if v and v not in pytz.all_timezones_set:            raise ValueError(f'Invalid timezone: \"{v}\"')        elif v is None:            return \"UTC\"        return v",
        "labels_text": "Validate that the provided timezone is a valid IANA timezone Unfortunately this list is slightly different from the list of valid timezones in pendulum that we use for cron and interval timezone validation"
    },
    {
        "input_text": "summarize: def _get_git_remote_origin_url() -> Optional[str]:        try:        origin_url = subprocess.check_output(            [\"git\", \"config\", \"--get\", \"remote.origin.url\"],            shell=sys.platform == \"win32\",            stderr=subprocess.DEVNULL,        )        origin_url = origin_url.decode().strip()    except subprocess.CalledProcessError:        return None    return origin_url",
        "labels_text": "Returns the git remote origin URL for the current directory"
    },
    {
        "input_text": "summarize: def _get_git_branch() -> Optional[str]:        try:        branch = subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"])        branch = branch.decode().strip()    except subprocess.CalledProcessError:        return None    return branch",
        "labels_text": "Returns the git branch for the current directory"
    },
    {
        "input_text": "summarize: def _interval_schedule_to_dict(schedule: IntervalSchedule) -> Dict:        schedule_config = schedule.model_dump()    schedule_config[\"interval\"] = schedule_config[\"interval\"].total_seconds()    schedule_config[\"anchor_date\"] = schedule_config[\"anchor_date\"].isoformat()    return schedule_config",
        "labels_text": "Converts an IntervalSchedule to a dictionary Args schedule IntervalSchedule the schedule to convert Returns Dict the schedule a a dictionary"
    },
    {
        "input_text": "summarize: def validate_automation_names(self):                trigger: Union[DeploymentTriggerTypes, TriggerTypes]        for i, trigger in enumerate(self.triggers, start=1):            if trigger.name is None:                trigger.name = f\"{self.name}__automation_{i}\"        return self",
        "labels_text": "Ensure that each trigger ha a name for it automation if none is provided"
    },
    {
        "input_text": "summarize: def create_deployment_schedule_create(    schedule: \"SCHEDULE_TYPES\",    active: Optional[bool] = True,    max_active_runs: Optional[int] = None,    catchup: bool = False,) -> DeploymentScheduleCreate:        return DeploymentScheduleCreate(        schedule=schedule,        active=active if active is not None else True,        max_active_runs=max_active_runs,        catchup=catchup,    )",
        "labels_text": "Create a DeploymentScheduleCreate object from common schedule parameter"
    },
    {
        "input_text": "summarize: def _strip_version(requirement: str) -> str:        # split on any of the characters in the set [<>=!~]    # and return the first element which will be the package name    return re.split(r\"[<>=!~]\", requirement)[0].strip()",
        "labels_text": "Strips the version from a requirement string Args requirement A requirement string eg request Returns The package name eg request Examples python stripversionsfs sfs"
    },
    {
        "input_text": "summarize: def set_working_directory(directory: str) -> dict:        os.chdir(directory)    return dict(directory=directory)",
        "labels_text": "Sets the working directory work with both absolute and relative path Args directory str the directory to set a the working directory Returns dict a dictionary containing a directory key of the directory that wa set"
    },
    {
        "input_text": "summarize: def describe_for_cli(self) -> str:                return self.type.replace(\"-\", \" \").capitalize()",
        "labels_text": "A humanreadable description of the action"
    },
    {
        "input_text": "summarize: def reset(cls) -> None:                cls.last = None        cls.all = []",
        "labels_text": "Reset all captured instance and their event For use between test"
    },
    {
        "input_text": "summarize: def includes(self, event: Event) -> bool:                return all(filter.includes(event) for filter in self.get_filters())",
        "labels_text": "Does the given event match the criterion of this filter"
    },
    {
        "input_text": "summarize: def excludes(self, event: Event) -> bool:                return not self.includes(event)",
        "labels_text": "Would the given filter exclude this event"
    },
    {
        "input_text": "summarize: def describe_for_cli(self, indent: int = 0) -> str:",
        "labels_text": "Return a humanreadable description of this trigger for the CLI"
    },
    {
        "input_text": "summarize: def describe_for_cli(self, indent: int = 0) -> str:                m = self.metric        return textwrap.indent(            \"\\n\".join(                [                    f\"Metric: {m.name.value} {m.operator.value} {m.threshold} for {m.range}\",                ]            ),            prefix=\"  \" * indent,        )",
        "labels_text": "Return a humanreadable description of this trigger for the CLI"
    },
    {
        "input_text": "summarize: def describe_for_cli(self, indent: int = 0) -> str:                return textwrap.indent(            \"\\n\".join(                [                    f\"{str(self.require).capitalize()} of:\",                    \"\\n\".join(                        [                            trigger.describe_for_cli(indent=indent + 1)                            for trigger in self.triggers                        ]                    ),                ]            ),            prefix=\"  \" * indent,        )",
        "labels_text": "Return a humanreadable description of this trigger for the CLI"
    },
    {
        "input_text": "summarize: def describe_for_cli(self, indent: int = 0) -> str:                return textwrap.indent(            \"\\n\".join(                [                    \"In this order:\",                    \"\\n\".join(                        [                            trigger.describe_for_cli(indent=indent + 1)                            for trigger in self.triggers                        ]                    ),                ]            ),            prefix=\"  \" * indent,        )",
        "labels_text": "Return a humanreadable description of this trigger for the CLI"
    },
    {
        "input_text": "summarize: def prefect_object_id(self, kind: str) -> UUID:                prefix = f\"{kind}.\" if not kind.endswith(\".\") else kind        if not self.id.startswith(prefix):            raise ValueError(f\"Resource ID {self.id} does not start with {prefix}\")        return UUID(self.id[len(prefix) :])",
        "labels_text": "Extracts the UUID from an event resource ID if it the expected kind of prefect resource"
    },
    {
        "input_text": "summarize: def resource_in_role(self) -> Mapping[str, RelatedResource]:                return {related.role: related for related in reversed(self.related)}",
        "labels_text": "Returns a mapping of role to the first related resource in that role"
    },
    {
        "input_text": "summarize: def resources_in_role(self) -> Mapping[str, Sequence[RelatedResource]]:                resources: Dict[str, List[RelatedResource]] = defaultdict(list)        for related in self.related:            resources[related.role].append(related)        return resources",
        "labels_text": "Returns a mapping of role to related resource in that role"
    },
    {
        "input_text": "summarize: def find_resource_label(self, label: str) -> Optional[str]:                directive, _, related_label = label.rpartition(\":\")        directive, _, role = directive.partition(\":\")        if directive == \"related\":            for related in self.related:                if related.role == role:                    return related.get(related_label)        return self.resource.get(label)",
        "labels_text": "Finds the value of the given label in this event resource or one of it related resource If the label start with relatedrole search for the first matching label in a related resource with that role"
    },
    {
        "input_text": "summarize: def matches(expected: str, value: Optional[str]) -> bool:        if value is None:        return False    positive = True    if expected.startswith(\"!\"):        expected = expected[1:]        positive = False    if expected.endswith(\"*\"):        match = value.startswith(expected[:-1])    else:        match = value == expected    return match if positive else not match",
        "labels_text": "Returns true if the given value match the expected string which may include a a negation prefix thisvalue or a wildcard suffix anyvaluestartingwith"
    },
    {
        "input_text": "summarize: def is_boto3_installed():                try:            importlib.import_module(\"boto3\")            return True        except ModuleNotFoundError:            return False",
        "labels_text": "Check if boto is installed"
    },
    {
        "input_text": "summarize: def _is_modal_installed() -> bool:                try:            importlib.import_module(\"modal\")            return True        except ModuleNotFoundError:            return False",
        "labels_text": "Checks if the modal package is installed Returns True if the modal package is installed False otherwise"
    },
    {
        "input_text": "summarize: def get_infrastructure_provisioner_for_work_pool_type(    work_pool_type: str,) -> Type[Provisioner]:        provisioner = _provisioners.get(work_pool_type)    if provisioner is None:        raise ValueError(f\"Unsupported work pool type: {work_pool_type}\")    return provisioner()",
        "labels_text": "Retrieve an instance of the infrastructure provisioner for the given work pool type Args workpooltype the work pool type Returns an instance of the infrastructure provisioner for the given work pool type Raises ValueError if the work pool type is not supported"
    },
    {
        "input_text": "summarize: def keyset_from_paused_state(state: \"State\") -> Keyset:        if not state.is_paused():        raise RuntimeError(f\"{state.type.value!r} is unsupported.\")    state_name = state.name or \"\"    base_key = f\"{state_name.lower()}-{str(state.state_details.pause_key)}\"    return keyset_from_base_key(base_key)",
        "labels_text": "Get the keyset for the given Paused state Args state State the state to get the keyset for"
    },
    {
        "input_text": "summarize: def keyset_from_base_key(base_key: str) -> Keyset:        return {        \"description\": f\"{base_key}-description\",        \"response\": f\"{base_key}-response\",        \"schema\": f\"{base_key}-schema\",    }",
        "labels_text": "Get the keyset for the given base key Args basekey str the base key to get the keyset for Returns Dictstr str the keyset"
    },
    {
        "input_text": "summarize: def load_from_flow_run_input(cls, flow_run_input: \"FlowRunInput\"):                instance = cls(**flow_run_input.decoded_value)        instance._metadata = RunInputMetadata(            key=flow_run_input.key,            sender=flow_run_input.sender,            receiver=flow_run_input.flow_run_id,        )        return instance",
        "labels_text": "Load the run input from a FlowRunInput object Args flowruninput FlowRunInput the flow run input to load the input for"
    },
    {
        "input_text": "summarize: def with_initial_data(        cls: Type[R], description: Optional[str] = None, **kwargs: Any    ) -> Type[R]:                fields: Dict[str, Any] = {}        for key, value in kwargs.items():            fields[key] = (type(value), value)        model = pydantic.create_model(cls.__name__, **fields, __base__=cls)        if description is not None:            model._description = description        return model",
        "labels_text": "Create a new RunInput subclass with the given initial data a field default Args description str optional a description to show when resuming a flow run that requires input kwargs Any the initial data to populate the subclass"
    },
    {
        "input_text": "summarize: def subclass_from_base_model_type(        cls, model_cls: Type[pydantic.BaseModel]    ) -> Type[\"RunInput\"]:                return type(f\"{model_cls.__name__}RunInput\", (RunInput, model_cls), {})",
        "labels_text": "Create a new RunInput subclass from the given pydanticBaseModel subclass Args modelcls pydanticBaseModel subclass the class from which to create the new RunInput subclass"
    },
    {
        "input_text": "summarize: def _expire_lock(self, key: str):                with self._locks_dict_lock:            if key in self._locks:                lock_info = self._locks[key]                if lock_info[\"lock\"].locked():                    lock_info[\"lock\"].release()                if lock_info[\"expiration_timer\"]:                    lock_info[\"expiration_timer\"].cancel()                del self._locks[key]",
        "labels_text": "Expire the lock for the given key Used a a callback for the expiration timer of a lock Args key The key of the lock to expire"
    },
    {
        "input_text": "summarize: def acquire_lock(        self,        key: str,        holder: str,        acquire_timeout: Optional[float] = None,        hold_timeout: Optional[float] = None,    ) -> bool:                ...",
        "labels_text": "Acquire a lock for a transaction record with the given key Will block other actor from updating this transaction record until the lock is released Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock acquiretimeout Max number of second to wait for the record to become available if it is locked while attempting to acquire a lock Pass to attempt to acquire a lock without waiting Blocks indefinitely by default holdtimeout Max number of second to hold the lock for Holds the lock indefinitely by default Returns bool True if the lock wa successfully acquired False otherwise"
    },
    {
        "input_text": "summarize: def release_lock(self, key: str, holder: str):                ...",
        "labels_text": "Releases the lock on the corresponding transaction record Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock Must match the holder provided when acquiring the lock"
    },
    {
        "input_text": "summarize: def is_locked(self, key: str) -> bool:                ...",
        "labels_text": "Simple check to see if the corresponding record is currently locked Args key Unique identifier for the transaction record Returns True is the record is locked False otherwise"
    },
    {
        "input_text": "summarize: def is_lock_holder(self, key: str, holder: str) -> bool:                ...",
        "labels_text": "Check if the current holder is the lock holder for the transaction record Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock Returns bool True if the current holder is the lock holder False otherwise"
    },
    {
        "input_text": "summarize: def wait_for_lock(self, key: str, timeout: Optional[float] = None) -> bool:                ...",
        "labels_text": "Wait for the corresponding transaction record to become free Args key Unique identifier for the transaction record timeout Maximum time to wait None mean to wait indefinitely Returns bool True if the lock becomes free within the timeout False otherwise"
    },
    {
        "input_text": "summarize: def redact_substr(obj: Any, substr: str):        def redact_item(item):        if isinstance(item, str):            return item.replace(substr, obfuscate(substr))        return item    redacted_obj = visit_collection(obj, visit_fn=redact_item, return_data=True)    return redacted_obj",
        "labels_text": "Redact a string from a potentially nested object Args obj The object to redact the string from substr The string to redact Returns Any The object with the API key redacted"
    },
    {
        "input_text": "summarize: def disable_logger(name: str):        logger = logging.getLogger(name=name)    # determine if it's already disabled    base_state = logger.disabled    try:        # disable the logger        logger.disabled = True        yield    finally:        # return to base state        logger.disabled = base_state",
        "labels_text": "Get a logger by name and disables it within the context manager Upon exiting the context manager the logger is returned to it original state"
    },
    {
        "input_text": "summarize: def disable_run_logger():        with disable_logger(\"prefect.flow_run\"), disable_logger(\"prefect.task_run\"):        yield",
        "labels_text": "Gets both prefectflowrun and prefecttaskrun and disables them within the context manager Upon exiting the context manager both logger are returned to it original state"
    },
    {
        "input_text": "summarize: def patch_print():        import builtins    original = builtins.print    try:        builtins.print = print_as_log        yield    finally:        builtins.print = original",
        "labels_text": "Patches the Python builtin print method to use printaslog"
    },
    {
        "input_text": "summarize: def __init__(self, eavesdrop_on: str, level: int = logging.NOTSET):                super().__init__(level=level)        self.eavesdrop_on = eavesdrop_on        self._target_logger = None        # It's important that we use a very minimalistic formatter for use cases where        # we may present these logs back to the user.  We shouldn't leak filenames,        # versions, or other environmental information.        self.formatter = logging.Formatter(\"[%(levelname)s]: %(message)s\")",
        "labels_text": "Args eavesdropon str the name of the logger to eavesdrop on level int the minimum log level to eavesdrop on if omitted all level are captured"
    },
    {
        "input_text": "summarize: def emit(self, record: LogRecord) -> None:                self._lines.append(self.format(record))",
        "labels_text": "The loggingHandler implementation not intended to be called directly"
    },
    {
        "input_text": "summarize: def text(self) -> str:                return \"\\n\".join(self._lines)",
        "labels_text": "Return the collected log a a single newlinedelimited string"
    },
    {
        "input_text": "summarize: def read(        self, key: str, holder: Optional[str] = None    ) -> Optional[TransactionRecord]:                ...",
        "labels_text": "Read the transaction record with the given key Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock If a lock exists on the record being written the read will be blocked until the lock is released if the provided holder doe not match the holder of the lock If not provided a default holder based on the current host process and thread will be used Returns TransactionRecord The transaction record with the given key"
    },
    {
        "input_text": "summarize: def write(self, key: str, result: \"BaseResult\", holder: Optional[str] = None):                ...",
        "labels_text": "Write the transaction record with the given key Args key Unique identifier for the transaction record record The transaction record to write holder Unique identifier for the holder of the lock If a lock exists on the record being written the write will be rejected if the provided holder doe not match the holder of the lock If not provided a default holder based on the current host process and thread will be used"
    },
    {
        "input_text": "summarize: def exists(self, key: str) -> bool:                ...",
        "labels_text": "Check if the transaction record with the given key exists Args key Unique identifier for the transaction record Returns bool True if the record exists False otherwise"
    },
    {
        "input_text": "summarize: def supports_isolation_level(self, isolation_level: \"IsolationLevel\") -> bool:                ...",
        "labels_text": "Check if the record store support the given isolation level Args isolationlevel The isolation level to check Returns bool True if the record store support the isolation level False otherwise"
    },
    {
        "input_text": "summarize: def acquire_lock(        self,        key: str,        holder: Optional[str] = None,        acquire_timeout: Optional[float] = None,        hold_timeout: Optional[float] = None,    ) -> bool:                raise NotImplementedError",
        "labels_text": "Acquire a lock for a transaction record with the given key Will block other actor from updating this transaction record until the lock is released Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock If not provided a default holder based on the current host process and thread will be used acquiretimeout Max number of second to wait for the record to become available if it is locked while attempting to acquire a lock Pass to attempt to acquire a lock without waiting Blocks indefinitely by default holdtimeout Max number of second to hold the lock for Holds the lock indefinitely by default Returns bool True if the lock wa successfully acquired False otherwise"
    },
    {
        "input_text": "summarize: def release_lock(self, key: str, holder: Optional[str] = None):                raise NotImplementedError",
        "labels_text": "Releases the lock on the corresponding transaction record Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock Must match the holder provided when acquiring the lock"
    },
    {
        "input_text": "summarize: def is_locked(self, key: str) -> bool:                raise NotImplementedError",
        "labels_text": "Simple check to see if the corresponding record is currently locked Args key Unique identifier for the transaction record Returns True is the record is locked False otherwise"
    },
    {
        "input_text": "summarize: def is_lock_holder(self, key: str, holder: Optional[str] = None) -> bool:                raise NotImplementedError",
        "labels_text": "Check if the current holder is the lock holder for the transaction record Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock If not provided a default holder based on the current host process and thread will be used Returns bool True if the current holder is the lock holder False otherwise"
    },
    {
        "input_text": "summarize: def wait_for_lock(self, key: str, timeout: Optional[float] = None) -> bool:                ...",
        "labels_text": "Wait for the corresponding transaction record to become free Args key Unique identifier for the transaction record timeout Maximum time to wait None mean to wait indefinitely Returns bool True if the lock becomes free within the timeout False otherwise"
    },
    {
        "input_text": "summarize: def generate_default_holder() -> str:                hostname = socket.gethostname()        pid = os.getpid()        thread_name = threading.current_thread().name        thread_id = threading.get_ident()        return f\"{hostname}:{pid}:{thread_id}:{thread_name}\"",
        "labels_text": "Generate a default holder string using hostname PID and thread ID Returns str A unique identifier string"
    },
    {
        "input_text": "summarize: def lock(        self,        key: str,        holder: Optional[str] = None,        acquire_timeout: Optional[float] = None,        hold_timeout: Optional[float] = None,    ):                self.acquire_lock(            key=key,            holder=holder,            acquire_timeout=acquire_timeout,            hold_timeout=hold_timeout,        )        try:            yield        finally:            self.release_lock(key=key, holder=holder)",
        "labels_text": "Context manager to lock the transaction record during the execution of the nested code block Args key Unique identifier for the transaction record holder Unique identifier for the holder of the lock If not provided a default holder based on the current host process and thread will be used acquiretimeout Max number of second to wait for the record to become available if it is locked while attempting to acquire a lock Pass to attempt to acquire a lock without waiting Blocks indefinitely by default holdtimeout Max number of second to hold the lock for Holds the lock indefinitely by default Example Hold a lock while during an operation python with TransactionRecordkeymytransactionrecordkeylock dostuff"
    },
    {
        "input_text": "summarize: def _expire_lock(self, key: str):                with self._locks_dict_lock:            if key in self._locks:                lock_info = self._locks[key]                if lock_info[\"lock\"].locked():                    lock_info[\"lock\"].release()                if lock_info[\"expiration_timer\"]:                    lock_info[\"expiration_timer\"].cancel()                del self._locks[key]",
        "labels_text": "Expire the lock for the given key Used a a callback for the expiration timer of a lock Args key The key of the lock to expire"
    },
    {
        "input_text": "summarize: def handle_sigterm(self, signum, frame):                self._logger.info(\"SIGTERM received, initiating graceful shutdown...\")        from_sync.call_in_loop_thread(create_call(self.stop))        sys.exit(0)",
        "labels_text": "Gracefully shuts down the runner when a SIGTERM is received"
    },
    {
        "input_text": "summarize: def execute_in_background(self, func, *args, **kwargs):                return asyncio.run_coroutine_threadsafe(func(*args, **kwargs), self._loop)",
        "labels_text": "Executes a function in the background"
    },
    {
        "input_text": "summarize: def has_slots_available(self) -> bool:                return self._limiter.available_tokens > 0",
        "labels_text": "Determine if the flow run limit ha been reached Returns bool True if the limit ha not been reached False otherwise"
    },
    {
        "input_text": "summarize: def _release_limit_slot(self, flow_run_id: str) -> None:                if self._limiter:            self._limiter.release_on_behalf_of(flow_run_id)            self._logger.debug(\"Limit slot released for flow run '%s'\", flow_run_id)",
        "labels_text": "Frees up a slot taken by the given flow run id"
    },
    {
        "input_text": "summarize: def _flow_in_schemas(flow: Flow, schemas: Dict[str, Dict]) -> bool:        flow_name_with_dashes = flow.name.replace(\"_\", \"-\")    return flow.name in schemas or flow_name_with_dashes in schemas",
        "labels_text": "Check if a flow is in the schema dict either by name or by name with dash replaced with underscore"
    },
    {
        "input_text": "summarize: def _flow_schema_changed(flow: Flow, schemas: Dict[str, Dict]) -> bool:        flow_name_with_dashes = flow.name.replace(\"_\", \"-\")    schema = schemas.get(flow.name, None) or schemas.get(flow_name_with_dashes, None)    if schema is not None and flow.parameters.model_dump() != schema:        return True    return False",
        "labels_text": "Check if a flow schema have changed either by bame of by name with dash replaced with underscore"
    },
    {
        "input_text": "summarize: def start_webserver(runner: \"Runner\", log_level: Optional[str] = None) -> None:        host = PREFECT_RUNNER_SERVER_HOST.value()    port = PREFECT_RUNNER_SERVER_PORT.value()    log_level = log_level or PREFECT_RUNNER_SERVER_LOG_LEVEL.value()    webserver = build_server(runner)    uvicorn.run(webserver, host=host, port=port, log_level=log_level)",
        "labels_text": "Run a FastAPI server for a runner Args runner Runner the runner this server interacts with and monitor loglevel str the log level to use for the server"
    },
    {
        "input_text": "summarize: def set_base_path(self, path: Path):                ...",
        "labels_text": "Sets the base path to use when pulling content from remote storage to local storage"
    },
    {
        "input_text": "summarize: def pull_interval(self) -> Optional[int]:                ...",
        "labels_text": "The interval at which content from remote storage should be pulled to local storage If None remote storage will perform a onetime sync"
    },
    {
        "input_text": "summarize: def destination(self) -> Path:                ...",
        "labels_text": "The local file path to pull content from remote storage to"
    },
    {
        "input_text": "summarize: def to_pull_step(self) -> dict:                ...",
        "labels_text": "Returns a dictionary representation of the storage object that can be used a a deployment pull step"
    },
    {
        "input_text": "summarize: def __eq__(self, __value) -> bool:                ...",
        "labels_text": "Equality check for runner storage object"
    },
    {
        "input_text": "summarize: def pull_interval(self) -> Optional[int]:                return self._pull_interval",
        "labels_text": "The interval at which content from remote storage should be pulled to local storage If None remote storage will perform a onetime sync"
    },
    {
        "input_text": "summarize: def destination(self) -> Path:                return self._storage_base_path / self._remote_path",
        "labels_text": "The local file path to pull content from remote storage to"
    },
    {
        "input_text": "summarize: def _remote_path(self) -> Path:                _, netloc, urlpath, _, _ = urlsplit(self._url)        return Path(netloc) / Path(urlpath.lstrip(\"/\"))",
        "labels_text": "The remote file path to pull content from remote storage to"
    },
    {
        "input_text": "summarize: def __eq__(self, __value) -> bool:                if isinstance(__value, RemoteStorage):            return self._url == __value._url and self._settings == __value._settings        return False",
        "labels_text": "Equality check for runner storage object"
    },
    {
        "input_text": "summarize: def to_pull_step(self) -> dict:                step = {            \"prefect.deployments.steps.set_working_directory\": {                \"directory\": str(self.destination)            }        }        return step",
        "labels_text": "Returns a dictionary representation of the storage object that can be used a a deployment pull step"
    },
    {
        "input_text": "summarize: def inject_schemas_into_openapi(    webserver: FastAPI, schemas_to_inject: Dict[str, Any]) -> Dict[str, Any]:        openapi_schema = get_openapi(        title=\"FastAPI Prefect Runner\", version=PREFECT_VERSION, routes=webserver.routes    )    augmented_schema = merge_definitions(schemas_to_inject, openapi_schema)    return update_refs_to_components(augmented_schema)",
        "labels_text": "Augments the webservers OpenAPI schema with additional schema from deployment flow task Args webserver The FastAPI instance representing the webserver schemastoinject A dictionary of OpenAPI schema to integrate Returns The augmented OpenAPI schema dictionary"
    },
    {
        "input_text": "summarize: def _pendulum_parse(dt: str) -> pendulum.DateTime:        return pendulum.parse(dt, tz=None, strict=False).set(tz=\"UTC\")",
        "labels_text": "Use pendulum to cast different format date string to pendulumDateTime tzinfo is ignored UTC forced"
    },
    {
        "input_text": "summarize: def reset(cls) -> None:                cls._task_queues.clear()        cls._scheduled_tasks_already_restored = False",
        "labels_text": "A unit testing utility to reset the state of the task queue subsystem"
    },
    {
        "input_text": "summarize: def is_ephemeral_request(request: Request):        return \"ephemeral-prefect\" in str(request.base_url)",
        "labels_text": "A dependency that return whether the request is to an ephemeral server"
    },
    {
        "input_text": "summarize: def __new__(cls, port: Optional[int] = None, *args, **kwargs):                if port not in cls._instances:            instance = super().__new__(cls)            cls._instances[port] = instance        return cls._instances[port]",
        "labels_text": "Return an instance of the server associated with the provided port Prevents multiple instance from being created for the same port"
    },
    {
        "input_text": "summarize: def with_alembic_lock(fn):        @wraps(fn)    def wrapper(*args, **kwargs):        with ALEMBIC_LOCK:            return fn(*args, **kwargs)    return wrapper",
        "labels_text": "Decorator that prevents alembic command from running concurrently This is necessary because alembic us a global configuration object that is not threadsafe This issue occurred in httpsgithubcomPrefectHQprefectdaskpull where dask thread were simultaneously performing alembic upgrade and causing cryptic KeyError config when del globalsattrname"
    },
    {
        "input_text": "summarize: def alembic_upgrade(revision: str = \"head\", dry_run: bool = False):        # lazy import for performance    import alembic.command    # don't display reflection warnings that pop up during schema migrations    with warnings.catch_warnings():        warnings.filterwarnings(            \"ignore\",            message=\"Skipped unsupported reflection of expression-based index\",            category=SAWarning,        )        alembic.command.upgrade(alembic_config(), revision, sql=dry_run)",
        "labels_text": "Run alembic upgrade on Prefect REST API database Args revision The revision passed to alembic downgrade Defaults to head upgrading all revision dryrun Show what migration would be made without applying them Will emit sql statement to stdout"
    },
    {
        "input_text": "summarize: def alembic_downgrade(revision: str = \"-1\", dry_run: bool = False):        # lazy import for performance    import alembic.command    alembic.command.downgrade(alembic_config(), revision, sql=dry_run)",
        "labels_text": "Run alembic downgrade on Prefect REST API database Args revision The revision passed to alembic downgrade Defaults to base downgrading all revision dryrun Show what migration would be made without applying them Will emit sql statement to stdout"
    },
    {
        "input_text": "summarize: def alembic_revision(    message: Optional[str] = None, autogenerate: bool = False, **kwargs):        # lazy import for performance    import alembic.command    alembic.command.revision(        alembic_config(), message=message, autogenerate=autogenerate, **kwargs    )",
        "labels_text": "Create a new revision file for the database Args message string message to apply to the revision autogenerate whether or not to autogenerate the script from the database"
    },
    {
        "input_text": "summarize: def alembic_stamp(revision):        # lazy import for performance    import alembic.command    alembic.command.stamp(alembic_config(), revision=revision)",
        "labels_text": "Stamp the revision table with the given revision dont run any migration Args revision The revision passed to alembic stamp"
    },
    {
        "input_text": "summarize: def _unique_key(self) -> Tuple[Hashable, ...]:                return (self.__class__, self.connection_url)",
        "labels_text": "Returns a key used to determine whether to instantiate a new DB interface"
    },
    {
        "input_text": "summarize: def is_inmemory(self) -> bool:",
        "labels_text": "Returns true if database is run in memory"
    },
    {
        "input_text": "summarize: def is_inmemory(self) -> Literal[False]:                return False",
        "labels_text": "Returns true if database is run in memory"
    },
    {
        "input_text": "summarize: def is_inmemory(self):                return \":memory:\" in self.connection_url or \"mode=memory\" in self.connection_url",
        "labels_text": "Returns true if database is run in memory"
    },
    {
        "input_text": "summarize: def temporary_database_config(tmp_database_config: BaseDatabaseConfiguration):        starting_config = MODELS_DEPENDENCIES[\"database_config\"]    try:        MODELS_DEPENDENCIES[\"database_config\"] = tmp_database_config        yield    finally:        MODELS_DEPENDENCIES[\"database_config\"] = starting_config",
        "labels_text": "Temporarily override the Prefect REST API database configuration When the context is closed the existing database configuration will be restored Args tmpdatabaseconfig Prefect REST API database configuration to inject"
    },
    {
        "input_text": "summarize: def temporary_query_components(tmp_queries: BaseQueryComponents):        starting_queries = MODELS_DEPENDENCIES[\"query_components\"]    try:        MODELS_DEPENDENCIES[\"query_components\"] = tmp_queries        yield    finally:        MODELS_DEPENDENCIES[\"query_components\"] = starting_queries",
        "labels_text": "Temporarily override the Prefect REST API database query component When the context is closed the existing query component will be restored Args tmpqueries Prefect REST API query component to inject"
    },
    {
        "input_text": "summarize: def temporary_orm_config(tmp_orm_config: BaseORMConfiguration):        starting_orm_config = MODELS_DEPENDENCIES[\"orm\"]    try:        MODELS_DEPENDENCIES[\"orm\"] = tmp_orm_config        yield    finally:        MODELS_DEPENDENCIES[\"orm\"] = starting_orm_config",
        "labels_text": "Temporarily override the Prefect REST API ORM configuration When the context is closed the existing orm configuration will be restored Args tmpormconfig Prefect REST API ORM configuration to inject"
    },
    {
        "input_text": "summarize: def temporary_interface_class(tmp_interface_class: Type[PrefectDBInterface]):        starting_interface_class = MODELS_DEPENDENCIES[\"interface_class\"]    try:        MODELS_DEPENDENCIES[\"interface_class\"] = tmp_interface_class        yield    finally:        MODELS_DEPENDENCIES[\"interface_class\"] = starting_interface_class",
        "labels_text": "Temporarily override the Prefect REST API interface class When the context is closed the existing interface will be restored Args tmpinterfaceclass Prefect REST API interface class to inject"
    },
    {
        "input_text": "summarize: def set_database_config(database_config: BaseDatabaseConfiguration):        MODELS_DEPENDENCIES[\"database_config\"] = database_config",
        "labels_text": "Set Prefect REST API database configuration"
    },
    {
        "input_text": "summarize: def set_query_components(query_components: BaseQueryComponents):        MODELS_DEPENDENCIES[\"query_components\"] = query_components",
        "labels_text": "Set Prefect REST API query component"
    },
    {
        "input_text": "summarize: def set_orm_config(orm_config: BaseORMConfiguration):        MODELS_DEPENDENCIES[\"orm\"] = orm_config",
        "labels_text": "Set Prefect REST API orm configuration"
    },
    {
        "input_text": "summarize: def set_interface_class(interface_class: Type[PrefectDBInterface]):        MODELS_DEPENDENCIES[\"interface_class\"] = interface_class",
        "labels_text": "Set Prefect REST API interface class"
    },
    {
        "input_text": "summarize: def Base(self):                return orm_models.Base",
        "labels_text": "Base class for orm model"
    },
    {
        "input_text": "summarize: def Flow(self):                return orm_models.Flow",
        "labels_text": "A flow orm model"
    },
    {
        "input_text": "summarize: def FlowRun(self):                return orm_models.FlowRun",
        "labels_text": "A flow run orm model"
    },
    {
        "input_text": "summarize: def FlowRunState(self):                return orm_models.FlowRunState",
        "labels_text": "A flow run state orm model"
    },
    {
        "input_text": "summarize: def TaskRun(self):                return orm_models.TaskRun",
        "labels_text": "A task run orm model"
    },
    {
        "input_text": "summarize: def TaskRunState(self):                return orm_models.TaskRunState",
        "labels_text": "A task run state orm model"
    },
    {
        "input_text": "summarize: def Artifact(self):                return orm_models.Artifact",
        "labels_text": "An artifact orm model"
    },
    {
        "input_text": "summarize: def ArtifactCollection(self):                return orm_models.ArtifactCollection",
        "labels_text": "An artifact collection orm model"
    },
    {
        "input_text": "summarize: def TaskRunStateCache(self):                return orm_models.TaskRunStateCache",
        "labels_text": "A task run state cache orm model"
    },
    {
        "input_text": "summarize: def Deployment(self):                return orm_models.Deployment",
        "labels_text": "A deployment orm model"
    },
    {
        "input_text": "summarize: def DeploymentSchedule(self):                return orm_models.DeploymentSchedule",
        "labels_text": "A deployment schedule orm model"
    },
    {
        "input_text": "summarize: def SavedSearch(self):                return orm_models.SavedSearch",
        "labels_text": "A saved search orm model"
    },
    {
        "input_text": "summarize: def WorkPool(self):                return orm_models.WorkPool",
        "labels_text": "A work pool orm model"
    },
    {
        "input_text": "summarize: def Worker(self):                return orm_models.Worker",
        "labels_text": "A worker process orm model"
    },
    {
        "input_text": "summarize: def Log(self):                return orm_models.Log",
        "labels_text": "A log orm model"
    },
    {
        "input_text": "summarize: def ConcurrencyLimit(self):                return orm_models.ConcurrencyLimit",
        "labels_text": "A concurrency model"
    },
    {
        "input_text": "summarize: def ConcurrencyLimitV2(self):                return orm_models.ConcurrencyLimitV2",
        "labels_text": "A v concurrency model"
    },
    {
        "input_text": "summarize: def CsrfToken(self):                return orm_models.CsrfToken",
        "labels_text": "A csrf token model"
    },
    {
        "input_text": "summarize: def WorkQueue(self):                return orm_models.WorkQueue",
        "labels_text": "A work queue model"
    },
    {
        "input_text": "summarize: def Agent(self):                return orm_models.Agent",
        "labels_text": "An agent model"
    },
    {
        "input_text": "summarize: def BlockType(self):                return orm_models.BlockType",
        "labels_text": "A block type model"
    },
    {
        "input_text": "summarize: def BlockSchema(self):                return orm_models.BlockSchema",
        "labels_text": "A block schema model"
    },
    {
        "input_text": "summarize: def BlockSchemaReference(self):                return orm_models.BlockSchemaReference",
        "labels_text": "A block schema reference model"
    },
    {
        "input_text": "summarize: def BlockDocument(self):                return orm_models.BlockDocument",
        "labels_text": "A block document model"
    },
    {
        "input_text": "summarize: def BlockDocumentReference(self):                return orm_models.BlockDocumentReference",
        "labels_text": "A block document reference model"
    },
    {
        "input_text": "summarize: def FlowRunNotificationPolicy(self):                return orm_models.FlowRunNotificationPolicy",
        "labels_text": "A flow run notification policy model"
    },
    {
        "input_text": "summarize: def FlowRunNotificationQueue(self):                return orm_models.FlowRunNotificationQueue",
        "labels_text": "A flow run notification queue model"
    },
    {
        "input_text": "summarize: def Configuration(self):                return orm_models.Configuration",
        "labels_text": "An configuration model"
    },
    {
        "input_text": "summarize: def Variable(self):                return orm_models.Variable",
        "labels_text": "A variable model"
    },
    {
        "input_text": "summarize: def FlowRunInput(self):                return orm_models.FlowRunInput",
        "labels_text": "A flow run input model"
    },
    {
        "input_text": "summarize: def Automation(self):                return orm_models.Automation",
        "labels_text": "An automation model"
    },
    {
        "input_text": "summarize: def AutomationBucket(self):                return orm_models.AutomationBucket",
        "labels_text": "An automation bucket model"
    },
    {
        "input_text": "summarize: def AutomationRelatedResource(self):                return orm_models.AutomationRelatedResource",
        "labels_text": "An automation related resource model"
    },
    {
        "input_text": "summarize: def CompositeTriggerChildFiring(self):                return orm_models.CompositeTriggerChildFiring",
        "labels_text": "A model capturing a composite trigger child firing"
    },
    {
        "input_text": "summarize: def AutomationEventFollower(self):                return orm_models.AutomationEventFollower",
        "labels_text": "A model capturing one event following another event"
    },
    {
        "input_text": "summarize: def Event(self):                return orm_models.Event",
        "labels_text": "An event model"
    },
    {
        "input_text": "summarize: def EventResource(self):                return orm_models.EventResource",
        "labels_text": "An event resource model"
    },
    {
        "input_text": "summarize: def deployment_unique_upsert_columns(self):                return self.orm.deployment_unique_upsert_columns",
        "labels_text": "Unique column for upserting a Deployment"
    },
    {
        "input_text": "summarize: def concurrency_limit_unique_upsert_columns(self):                return self.orm.concurrency_limit_unique_upsert_columns",
        "labels_text": "Unique column for upserting a ConcurrencyLimit"
    },
    {
        "input_text": "summarize: def flow_run_unique_upsert_columns(self):                return self.orm.flow_run_unique_upsert_columns",
        "labels_text": "Unique column for upserting a FlowRun"
    },
    {
        "input_text": "summarize: def artifact_collection_unique_upsert_columns(self):                return self.orm.artifact_collection_unique_upsert_columns",
        "labels_text": "Unique column for upserting an ArtifactCollection"
    },
    {
        "input_text": "summarize: def block_type_unique_upsert_columns(self):                return self.orm.block_type_unique_upsert_columns",
        "labels_text": "Unique column for upserting a BlockType"
    },
    {
        "input_text": "summarize: def block_schema_unique_upsert_columns(self):                return self.orm.block_schema_unique_upsert_columns",
        "labels_text": "Unique column for upserting a BlockSchema"
    },
    {
        "input_text": "summarize: def flow_unique_upsert_columns(self):                return self.orm.flow_unique_upsert_columns",
        "labels_text": "Unique column for upserting a Flow"
    },
    {
        "input_text": "summarize: def saved_search_unique_upsert_columns(self):                return self.orm.saved_search_unique_upsert_columns",
        "labels_text": "Unique column for upserting a SavedSearch"
    },
    {
        "input_text": "summarize: def task_run_unique_upsert_columns(self):                return self.orm.task_run_unique_upsert_columns",
        "labels_text": "Unique column for upserting a TaskRun"
    },
    {
        "input_text": "summarize: def block_document_unique_upsert_columns(self):                return self.orm.block_document_unique_upsert_columns",
        "labels_text": "Unique column for upserting a BlockDocument"
    },
    {
        "input_text": "summarize: def insert(self, model):                return self.queries.insert(model)",
        "labels_text": "INSERTs a model into the database"
    },
    {
        "input_text": "summarize: def set_state_id_on_inserted_flow_runs_statement(        self, inserted_flow_run_ids, insert_flow_run_states    ):                return self.queries.set_state_id_on_inserted_flow_runs_statement(            orm_models.FlowRun,            orm_models.FlowRunState,            inserted_flow_run_ids,            insert_flow_run_states,        )",
        "labels_text": "Given a list of flow run id and associated state set the stateid to the appropriate state for all flow run"
    },
    {
        "input_text": "summarize: def clear_configuration_value_cache_for_key(self, key: str):                return self.queries.clear_configuration_value_cache_for_key(key=key)",
        "labels_text": "Removes a configuration key from the cache"
    },
    {
        "input_text": "summarize: def __tablename__(cls):                return camel_to_snake.sub(\"_\", cls.__name__).lower()",
        "labels_text": "By default turn the model camelcase class name into a snakecase table name Override by providing an explicit tablename class property"
    },
    {
        "input_text": "summarize: def estimated_run_time(self):                if self.state_type and self.state_type == schemas.states.StateType.RUNNING:            return self.total_run_time + (pendulum.now(\"UTC\") - self.state_timestamp)        else:            return self.total_run_time",
        "labels_text": "Total run time is incremented in the database whenever a RUNNING state is exited To give uptodate estimate we estimate incremental run time for any run currently in a RUNNING state"
    },
    {
        "input_text": "summarize: def set_state(self, state):                if state is not None:            state.flow_run_id = self.id        self._state = state",
        "labels_text": "If a state is assigned to this run populate it run id This would normally be handled by the backpopulated SQLAlchemy relationship but because this is a onetoone pointer to a onetomany relationship SQLAlchemy cant figure it out"
    },
    {
        "input_text": "summarize: def set_state(self, state):                if state is not None:            state.task_run_id = self.id        self._state = state",
        "labels_text": "If a state is assigned to this run populate it run id This would normally be handled by the backpopulated SQLAlchemy relationship but because this is a onetoone pointer to a onetomany relationship SQLAlchemy cant figure it out"
    },
    {
        "input_text": "summarize: def sort_expression(cls, value: AutomationSort) -> ColumnElement:                sort_mapping = {            AutomationSort.CREATED_DESC: cls.created.desc(),            AutomationSort.UPDATED_DESC: cls.updated.desc(),            AutomationSort.NAME_ASC: cast(sa.Column, cls.name).asc(),            AutomationSort.NAME_DESC: cast(sa.Column, cls.name).desc(),        }        return sort_mapping[value]",
        "labels_text": "Return an expression used to sort Automations"
    },
    {
        "input_text": "summarize: def _unique_key(self) -> Tuple[Hashable, ...]:                return (self.__class__, Base.metadata)",
        "labels_text": "Returns a key used to determine whether to instantiate a new DB interface"
    },
    {
        "input_text": "summarize: def versions_dir(self):                ...",
        "labels_text": "Directory containing migration"
    },
    {
        "input_text": "summarize: def deployment_unique_upsert_columns(self):                return [Deployment.flow_id, Deployment.name]",
        "labels_text": "Unique column for upserting a Deployment"
    },
    {
        "input_text": "summarize: def concurrency_limit_unique_upsert_columns(self):                return [ConcurrencyLimit.tag]",
        "labels_text": "Unique column for upserting a ConcurrencyLimit"
    },
    {
        "input_text": "summarize: def flow_run_unique_upsert_columns(self):                return [FlowRun.flow_id, FlowRun.idempotency_key]",
        "labels_text": "Unique column for upserting a FlowRun"
    },
    {
        "input_text": "summarize: def block_type_unique_upsert_columns(self):                return [BlockType.slug]",
        "labels_text": "Unique column for upserting a BlockType"
    },
    {
        "input_text": "summarize: def artifact_collection_unique_upsert_columns(self):                return [ArtifactCollection.key]",
        "labels_text": "Unique column for upserting an ArtifactCollection"
    },
    {
        "input_text": "summarize: def block_schema_unique_upsert_columns(self):                return [BlockSchema.checksum, BlockSchema.version]",
        "labels_text": "Unique column for upserting a BlockSchema"
    },
    {
        "input_text": "summarize: def flow_unique_upsert_columns(self):                return [Flow.name]",
        "labels_text": "Unique column for upserting a Flow"
    },
    {
        "input_text": "summarize: def saved_search_unique_upsert_columns(self):                return [SavedSearch.name]",
        "labels_text": "Unique column for upserting a SavedSearch"
    },
    {
        "input_text": "summarize: def task_run_unique_upsert_columns(self):                return [            TaskRun.flow_run_id,            TaskRun.task_key,            TaskRun.dynamic_key,        ]",
        "labels_text": "Unique column for upserting a TaskRun"
    },
    {
        "input_text": "summarize: def block_document_unique_upsert_columns(self):                return [BlockDocument.block_type_id, BlockDocument.name]",
        "labels_text": "Unique column for upserting a BlockDocument"
    },
    {
        "input_text": "summarize: def versions_dir(self) -> Path:                return (            Path(prefect.server.database.__file__).parent            / \"migrations\"            / \"versions\"            / \"postgresql\"        )",
        "labels_text": "Directory containing migration"
    },
    {
        "input_text": "summarize: def versions_dir(self) -> Path:                return (            Path(prefect.server.database.__file__).parent            / \"migrations\"            / \"versions\"            / \"sqlite\"        )",
        "labels_text": "Directory containing migration"
    },
    {
        "input_text": "summarize: def _unique_key(self) -> Tuple[Hashable, ...]:                return (self.__class__,)",
        "labels_text": "Returns a key used to determine whether to instantiate a new DB interface"
    },
    {
        "input_text": "summarize: def insert(self, obj) -> Union[postgresql.Insert, sqlite.Insert]:",
        "labels_text": "dialectspecific insert statement"
    },
    {
        "input_text": "summarize: def greatest(self, *values):",
        "labels_text": "dialectspecific SqlAlchemy binding"
    },
    {
        "input_text": "summarize: def least(self, *values):",
        "labels_text": "dialectspecific SqlAlchemy binding"
    },
    {
        "input_text": "summarize: def uses_json_strings(self) -> bool:",
        "labels_text": "specifies whether the configured dialect return JSON a string"
    },
    {
        "input_text": "summarize: def cast_to_json(self, json_obj):",
        "labels_text": "cast to JSON object if necessary"
    },
    {
        "input_text": "summarize: def build_json_object(self, *args):",
        "labels_text": "build a JSON object from sequential keyvalue pair"
    },
    {
        "input_text": "summarize: def json_arr_agg(self, json_array):",
        "labels_text": "aggregate a JSON array"
    },
    {
        "input_text": "summarize: def _flow_run_work_queue_join_clause(self, flow_run, work_queue):                return sa.and_(flow_run.work_queue_name == work_queue.name)",
        "labels_text": "On clause for for joining flow run to work queue Used by selfgetscheduledflowrunsfromworkqueue allowing just this function to be changed on a perdialect basis"
    },
    {
        "input_text": "summarize: def _get_scheduled_flow_runs_from_work_pool_template_path(self):",
        "labels_text": "Template for the query to get scheduled flow run from a work pool"
    },
    {
        "input_text": "summarize: def clear_configuration_value_cache_for_key(self, key: str):                self.CONFIGURATION_CACHE.pop(key, None)",
        "labels_text": "Removes a configuration key from the cache"
    },
    {
        "input_text": "summarize: def _get_scheduled_flow_runs_from_work_pool_template_path(self):                return \"postgres/get-runs-from-worker-queues.sql.jinja\"",
        "labels_text": "Template for the query to get scheduled flow run from a work pool"
    },
    {
        "input_text": "summarize: def _get_scheduled_flow_runs_from_work_pool_template_path(self):                return \"sqlite/get-runs-from-worker-queues.sql.jinja\"",
        "labels_text": "Template for the query to get scheduled flow run from a work pool"
    },
    {
        "input_text": "summarize: def disable_sqlite_foreign_keys(context):        if dialect.name == \"sqlite\":        context.execute(\"COMMIT\")        context.execute(\"PRAGMA foreign_keys=OFF\")        context.execute(\"BEGIN IMMEDIATE\")    yield    if dialect.name == \"sqlite\":        context.execute(\"END\")        context.execute(\"PRAGMA foreign_keys=ON\")        context.execute(\"BEGIN IMMEDIATE\")",
        "labels_text": "Disable foreign key constraint on sqlite"
    },
    {
        "input_text": "summarize: def downgrade():",
        "labels_text": "Data only migration No action on downgrade"
    },
    {
        "input_text": "summarize: def downgrade():",
        "labels_text": "Dataonly migration no action needed"
    },
    {
        "input_text": "summarize: def downgrade():",
        "labels_text": "Data only migration No action on downgrade"
    },
    {
        "input_text": "summarize: def downgrade():",
        "labels_text": "Dataonly migration no action needed"
    },
    {
        "input_text": "summarize: def ensure_payload_is_a_string(        cls, value: Union[str, Dict[str, Any], None]    ) -> Optional[str]:                if value is None:            return value        if isinstance(value, str):            return value        return orjson.dumps(value, option=orjson.OPT_INDENT_2).decode()",
        "labels_text": "Temporary measure while we migrate payload from being a dictionary to a string template This cover both reading from the database where value may currently be a dictionary a well a the API where older version of the frontend may be sending a JSON object with the single message key"
    },
    {
        "input_text": "summarize: def validate_payload_templates(cls, value: Optional[str]) -> Optional[str]:                if not value:            return value        cls.validate_template(value, \"payload\")        return value",
        "labels_text": "Validate userprovided payload template"
    },
    {
        "input_text": "summarize: def reset(cls) -> None:                cls.last = None        cls.all = []",
        "labels_text": "Reset all captured instance and their event For use this between test"
    },
    {
        "input_text": "summarize: def assert_emitted_event_count(cls, count: int) -> None:                total_num_events = cls.emitted_events_count()        assert (            total_num_events == count        ), f\"The number of emitted events did not match the expected count: {total_num_events=} != {count=}\"",
        "labels_text": "Assert that the given number of event were emitted"
    },
    {
        "input_text": "summarize: def includes(self, event: Event) -> bool:                return all(filter.includes(event) for filter in self.get_filters())",
        "labels_text": "Does the given event match the criterion of this filter"
    },
    {
        "input_text": "summarize: def excludes(self, event: Event) -> bool:                return not self.includes(event)",
        "labels_text": "Would the given filter exclude this event"
    },
    {
        "input_text": "summarize: def build_where_clauses(self) -> Sequence[\"ColumnExpressionArgument[bool]\"]:                clauses: List[\"ColumnExpressionArgument[bool]\"] = []        for filter in self.get_filters():            clauses.extend(filter.build_where_clauses())        return clauses",
        "labels_text": "Convert the criterion to a WHERE clause"
    },
    {
        "input_text": "summarize: def clamp(self, max_duration: timedelta):                earliest = pendulum.now(\"UTC\") - max_duration        self.since = max(earliest, cast(pendulum.DateTime, self.since))",
        "labels_text": "Limit how far the query can look back based on the given duration"
    },
    {
        "input_text": "summarize: def _scoped_event_resources(self) -> Select:                query = sa.select(orm_models.EventResource.event_id).where(            orm_models.EventResource.occurred >= self.occurred.since,            orm_models.EventResource.occurred <= self.occurred.until,        )        return query",
        "labels_text": "Returns an eventresources query that is scoped to this filter scope by occurred"
    },
    {
        "input_text": "summarize: def logical_limit(self) -> int:                if self.id and self.id.id:            # If we're asking for a specific set of IDs, the most we could get back is            # that number of rows            return len(self.id.id)        return sys.maxsize",
        "labels_text": "The logical limit for this query which is a maximum number of row that it could return regardless of what the caller ha requested May be used a an optimization for DB query"
    },
    {
        "input_text": "summarize: def ui_url(ctx: Mapping[str, Any], obj: Any) -> Optional[str]:        return url_for(obj, url_type=\"ui\")",
        "labels_text": "Return the UI URL for the given object"
    },
    {
        "input_text": "summarize: def load_automation(automation: Optional[Automation]):        if not automation:        return    event_triggers = automation.triggers_of_type(EventTrigger)    if not automation.enabled or not event_triggers:        forget_automation(automation.id)        return    automations_by_id[automation.id] = automation    for trigger in event_triggers:        triggers[trigger.id] = trigger        next_proactive_runs.pop(trigger.id, None)",
        "labels_text": "Loads the given automation into memory so that it is available for evaluation"
    },
    {
        "input_text": "summarize: def forget_automation(automation_id: UUID):        if automation := automations_by_id.pop(automation_id, None):        for trigger in automation.triggers():            triggers.pop(trigger.id, None)            next_proactive_runs.pop(trigger.id, None)",
        "labels_text": "Unloads the given automation from memory"
    },
    {
        "input_text": "summarize: def reset_ids(self) -> None:                self.id = uuid4()        for trigger in self.all_triggers():            trigger.id = uuid4()",
        "labels_text": "Resets the ID of this trigger and all of it child"
    },
    {
        "input_text": "summarize: def all_triggers(self) -> Sequence[Trigger]:                return [self]",
        "labels_text": "Returns all trigger within this trigger"
    },
    {
        "input_text": "summarize: def immediate(self) -> bool:                return self.posture == Posture.Reactive and self.within == timedelta(0)",
        "labels_text": "Does this reactive trigger fire immediately for all event"
    },
    {
        "input_text": "summarize: def triggers(self) -> Sequence[Trigger]:                return self.trigger.all_triggers()",
        "labels_text": "Returns all trigger within this automation"
    },
    {
        "input_text": "summarize: def triggers_of_type(self, trigger_type: Type[T]) -> Sequence[T]:                return [t for t in self.triggers() if isinstance(t, trigger_type)]",
        "labels_text": "Returns all trigger of the specified type within this automation"
    },
    {
        "input_text": "summarize: def trigger_by_id(self, trigger_id: UUID) -> Optional[Trigger]:                for trigger in self.triggers():            if trigger.id == trigger_id:                return trigger        return None",
        "labels_text": "Returns the trigger with the given ID or None if no such trigger exists"
    },
    {
        "input_text": "summarize: def idempotency_key(self) -> str:                return \", \".join(            [                f\"automation {self.automation.id}\",                f\"action {self.action_index}\",                f\"invocation {self.id}\",            ]        )",
        "labels_text": "Produce a humanfriendly idempotency key for this action"
    },
    {
        "input_text": "summarize: def prefect_object_id(self, kind: str) -> UUID:                prefix = f\"{kind}.\" if not kind.endswith(\".\") else kind        if not self.id.startswith(prefix):            raise ValueError(f\"Resource ID {self.id} does not start with {prefix}\")        return UUID(self.id[len(prefix) :])",
        "labels_text": "Extracts the UUID from an event resource ID if it the expected kind of prefect resource"
    },
    {
        "input_text": "summarize: def resource_in_role(self) -> Mapping[str, RelatedResource]:                return {related.role: related for related in reversed(self.related)}",
        "labels_text": "Returns a mapping of role to the first related resource in that role"
    },
    {
        "input_text": "summarize: def resources_in_role(self) -> Mapping[str, Sequence[RelatedResource]]:                resources: Dict[str, List[RelatedResource]] = defaultdict(list)        for related in self.related:            resources[related.role].append(related)        return resources",
        "labels_text": "Returns a mapping of role to related resource in that role"
    },
    {
        "input_text": "summarize: def find_resource_label(self, label: str) -> Optional[str]:                directive, _, related_label = label.rpartition(\":\")        directive, _, role = directive.partition(\":\")        if directive == \"related\":            for related in self.related:                if related.role == role:                    return related.get(related_label)        return self.resource.get(label)",
        "labels_text": "Finds the value of the given label in this event resource or one of it related resource If the label start with relatedrole search for the first matching label in a related resource with that role"
    },
    {
        "input_text": "summarize: def matches(expected: str, value: Optional[str]) -> bool:        if value is None:        return False    positive = True    if expected.startswith(\"!\"):        expected = expected[1:]        positive = False    if expected.endswith(\"*\"):        match = value.startswith(expected[:-1])    else:        match = value == expected    return match if positive else not match",
        "labels_text": "Returns true if the given value match the expected string which may include a a negation prefix thisvalue or a wildcard suffix anyvaluestartingwith"
    },
    {
        "input_text": "summarize: def _find_root_block_schema(    block_schemas_with_references: List[        Tuple[BlockSchema, Optional[str], Optional[UUID]]    ],):        return next(        (            copy(block_schema)            for (                block_schema,                _,                parent_block_schema_id,            ) in block_schemas_with_references            if parent_block_schema_id is None        ),        None,    )",
        "labels_text": "Attempts to find the root block schema from a list of block schema with reference Returns None if a root block schema is not found Returns only the first potential root block schema if multiple are found"
    },
    {
        "input_text": "summarize: def _find_block_schema_via_checksum(    block_schemas_with_references: List[        Tuple[BlockSchema, Optional[str], Optional[UUID]]    ],    checksum: str,) -> Optional[BlockSchema]:        return next(        (            block_schema            for block_schema, _, _ in block_schemas_with_references            if block_schema.checksum == checksum        ),        None,    )",
        "labels_text": "Attempt to find a block schema via a given checksum Returns None if not found"
    },
    {
        "input_text": "summarize: def reset(self):                self.workers.clear()        self.task_keys.clear()        self.worker_timestamps.clear()",
        "labels_text": "Testing utility to reset the state of the task worker tracker"
    },
    {
        "input_text": "summarize: def priority():                return []",
        "labels_text": "A list of orchestration rule in priority order"
    },
    {
        "input_text": "summarize: def compile_transition_rules(cls, from_state=None, to_state=None):                transition_rules = []        for rule in cls.priority():            if from_state in rule.FROM_STATES and to_state in rule.TO_STATES:                transition_rules.append(rule)        return transition_rules",
        "labels_text": "Returns rule in policy that are valid for the specified state transition"
    },
    {
        "input_text": "summarize: def initial_state_type(self) -> Optional[states.StateType]:                return self.initial_state.type if self.initial_state else None",
        "labels_text": "The state type of selfinitialstate if it exists"
    },
    {
        "input_text": "summarize: def proposed_state_type(self) -> Optional[states.StateType]:                return self.proposed_state.type if self.proposed_state else None",
        "labels_text": "The state type of selfproposedstate if it exists"
    },
    {
        "input_text": "summarize: def validated_state_type(self) -> Optional[states.StateType]:                return self.validated_state.type if self.validated_state else None",
        "labels_text": "The state type of selfvalidatedstate if it exists"
    },
    {
        "input_text": "summarize: def entry_context(self):                safe_context = self.safe_copy()        return safe_context.initial_state, safe_context.proposed_state, safe_context",
        "labels_text": "A convenience method that generates input parameter for orchestration rule An OrchestrationContext defines a state transition that is managed by orchestration rule which can fire hook before a transition ha been committed to the database These hook have a consistent interface which can be generated with this method"
    },
    {
        "input_text": "summarize: def exit_context(self):                safe_context = self.safe_copy()        return safe_context.initial_state, safe_context.validated_state, safe_context",
        "labels_text": "A convenience method that generates input parameter for orchestration rule An OrchestrationContext defines a state transition that is managed by orchestration rule which can fire hook after a transition ha been committed to the database These hook have a consistent interface which can be generated with this method"
    },
    {
        "input_text": "summarize: def safe_copy(self):                return super().safe_copy()",
        "labels_text": "Creates a mostlymutationsafe copy for use in orchestration rule Orchestration rule govern state transition using information stored in an OrchestrationContext However mutating object stored on the context directly can have unintended sideeffects To guard against this selfsafecopy can be used to pas information to orchestration rule without risking mutation Note selfrun is an ORM model and even when copied is unsafe to mutate Returns A mutationsafe copy of FlowOrchestrationContext"
    },
    {
        "input_text": "summarize: def run_settings(self) -> Dict:                return self.run.empirical_policy",
        "labels_text": "Runlevel setting used to orchestrate the state transition"
    },
    {
        "input_text": "summarize: def safe_copy(self):                return super().safe_copy()",
        "labels_text": "Creates a mostlymutationsafe copy for use in orchestration rule Orchestration rule govern state transition using information stored in an OrchestrationContext However mutating object stored on the context directly can have unintended sideeffects To guard against this selfsafecopy can be used to pas information to orchestration rule without risking mutation Note selfrun is an ORM model and even when copied is unsafe to mutate Returns A mutationsafe copy of TaskOrchestrationContext"
    },
    {
        "input_text": "summarize: def run_settings(self) -> Dict:                return self.run.empirical_policy",
        "labels_text": "Runlevel setting used to orchestrate the state transition"
    },
    {
        "input_text": "summarize: def nullified_transition(self) -> bool:                return self.context.proposed_state is None",
        "labels_text": "Determines if the transition ha been nullified Transitions are nullified if the proposed state is None indicating that nothing should be written to the database Returns True if the transition is nullified False otherwise"
    },
    {
        "input_text": "summarize: def exception_in_transition(self) -> bool:                return self.context.orchestration_error is not None",
        "labels_text": "Determines if the transition ha encountered an exception Returns True if the transition is encountered an exception False otherwise"
    },
    {
        "input_text": "summarize: def check_valid_configuration(self, base_job_template: dict):                # This import is here to avoid a circular import        from prefect.utilities.schema_tools import validate        variables_schema = deepcopy(base_job_template.get(\"variables\"))        if variables_schema is not None:            validate(                self.job_variables,                variables_schema,                raise_on_error=True,                preprocess=True,                ignore_required=True,            )",
        "labels_text": "Check that the combination of basejobtemplate default and jobvariables conforms to the specified schema NOTE This method doe not hydrate block reference in default value within the base job template to validate them Failing to do this can cause userfacing error Instead of this method use validatejobvariablesfordeployment function from prefectcloudorionapivalidation"
    },
    {
        "input_text": "summarize: def check_valid_configuration(self, base_job_template: dict):                # This import is here to avoid a circular import        from prefect.utilities.schema_tools import validate        variables_schema = deepcopy(base_job_template.get(\"variables\"))        if variables_schema is not None:            errors = validate(                self.job_variables,                variables_schema,                raise_on_error=False,                preprocess=True,                ignore_required=True,            )            if errors:                for error in errors:                    raise error",
        "labels_text": "Check that the combination of basejobtemplate default and jobvariables conforms to the schema specified in the basejobtemplate NOTE This method doe not hydrate block reference in default value within the base job template to validate them Failing to do this can cause userfacing error Instead of this method use validatejobvariablesfordeployment function from prefectcloudorionapivalidation"
    },
    {
        "input_text": "summarize: def default_name_from_type(self):                # if `type` is not in `values` it means the `type` didn't pass its own        # validation check and an error will be raised after this function is called        name = self.name        if name is None and self.type:            self.name = \" \".join([v.capitalize() for v in self.type.value.split(\"_\")])        return self",
        "labels_text": "If a name is not provided use the type"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                if isinstance(other, FlowRun):            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}            return self.model_dump(exclude=exclude_fields) == other.model_dump(                exclude=exclude_fields            )        return super().__eq__(other)",
        "labels_text": "Check for equality to another flow run schema Estimates time are rolling and will always change with repeated query for a flow run so we ignore them during equality check"
    },
    {
        "input_text": "summarize: def as_sql_filter(self) -> \"BooleanClauseList\":                filters = self._get_filter_list()        if not filters:            return True        return sa.and_(*filters)",
        "labels_text": "Generate SQL filter from provided filter parameter If no filter parameter are available return a TRUE filter"
    },
    {
        "input_text": "summarize: def _get_filter_list(self) -> List:                raise NotImplementedError(\"_get_filter_list must be implemented\")",
        "labels_text": "Return a list of boolean filter statement based on filter parameter"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                if isinstance(other, FlowRunResponse):            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}            return self.model_dump(exclude=exclude_fields) == other.model_dump(                exclude=exclude_fields            )        return super().__eq__(other)",
        "labels_text": "Check for equality to another flow run schema Estimates time are rolling and will always change with repeated query for a flow run so we ignore them during equality check"
    },
    {
        "input_text": "summarize: def _prepare_scheduling_start_and_end(    start: Any, end: Any, timezone: str) -> Tuple[pendulum.datetime, Optional[pendulum.datetime]]:        timezone = timezone or \"UTC\"    if start is not None:        start = pendulum.instance(start).in_tz(timezone)    if end is not None:        end = pendulum.instance(end).in_tz(timezone)    return start, end",
        "labels_text": "Uniformly prepares the start and end date for any Schedules getdates call coercing the argument into timezoneaware pendulum datetimes"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"TIMESTAMP_ASC\": orm_models.Log.timestamp.asc(),            \"TIMESTAMP_DESC\": orm_models.Log.timestamp.desc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort task run"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"CREATED_DESC\": orm_models.Flow.created.desc(),            \"UPDATED_DESC\": orm_models.Flow.updated.desc(),            \"NAME_ASC\": orm_models.Flow.name.asc(),            \"NAME_DESC\": orm_models.Flow.name.desc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort flow"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"CREATED_DESC\": orm_models.Deployment.created.desc(),            \"UPDATED_DESC\": orm_models.Deployment.updated.desc(),            \"NAME_ASC\": orm_models.Deployment.name.asc(),            \"NAME_DESC\": orm_models.Deployment.name.desc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort deployment"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"CREATED_DESC\": orm_models.Artifact.created.desc(),            \"UPDATED_DESC\": orm_models.Artifact.updated.desc(),            \"ID_DESC\": orm_models.Artifact.id.desc(),            \"KEY_DESC\": orm_models.Artifact.key.desc(),            \"KEY_ASC\": orm_models.Artifact.key.asc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort artifact"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"CREATED_DESC\": orm_models.ArtifactCollection.created.desc(),            \"UPDATED_DESC\": orm_models.ArtifactCollection.updated.desc(),            \"ID_DESC\": orm_models.ArtifactCollection.id.desc(),            \"KEY_DESC\": orm_models.ArtifactCollection.key.desc(),            \"KEY_ASC\": orm_models.ArtifactCollection.key.asc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort artifact collection"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"CREATED_DESC\": orm_models.Variable.created.desc(),            \"UPDATED_DESC\": orm_models.Variable.updated.desc(),            \"NAME_DESC\": orm_models.Variable.name.desc(),            \"NAME_ASC\": orm_models.Variable.name.asc(),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort variable"
    },
    {
        "input_text": "summarize: def as_sql_sort(self) -> \"ColumnElement\":                sort_mapping = {            \"NAME_DESC\": orm_models.BlockDocument.name.desc(),            \"NAME_ASC\": orm_models.BlockDocument.name.asc(),            \"BLOCK_TYPE_AND_NAME_ASC\": sa.text(\"block_type_name asc, name asc\"),        }        return sort_mapping[self.value]",
        "labels_text": "Return an expression used to sort block document"
    },
    {
        "input_text": "summarize: def orm_dict(self, *args, **kwargs) -> dict:                schema_dict = self.model_dump(*args, **kwargs)        # remove the data field in order to construct a state ORM model        schema_dict.pop(\"data\", None)        return schema_dict",
        "labels_text": "This method is used a a convenience method for constructing fixtues by first building a State schema object and converting it into an ORMcompatible format Because the data field is not writable on ORM state this method omits the data field entirely for the purpose of constructing an ORM model If state data is required an artifact must be created separately"
    },
    {
        "input_text": "summarize: def from_orm_without_result(        cls,        orm_state: Union[\"ORMFlowRunState\", \"ORMTaskRunState\"],        with_data: Optional[Any] = None,    ):                field_keys = cls.model_json_schema()[\"properties\"].keys()        state_data = {            field: getattr(orm_state, field, None)            for field in field_keys            if field != \"data\"        }        state_data[\"data\"] = with_data        return cls(**state_data)",
        "labels_text": "During orchestration ORM state can be instantiated prior to inserting result into the artifact table and the data field will not be eagerly loaded In these case sqlalchemy will attempt to lazily load the the relationship which will fail when called within a synchronous pydantic method This method will construct a State object from an ORM model without a loaded artifact and attach data passed using the withdata argument to the data field"
    },
    {
        "input_text": "summarize: def default_name_from_type(self):                # if `type` is not in `values` it means the `type` didn't pass its own        # validation check and an error will be raised after this function is called        name = self.name        if name is None and self.type:            self.name = \" \".join([v.capitalize() for v in self.type.value.split(\"_\")])        return self",
        "labels_text": "If a name is not provided use the type"
    },
    {
        "input_text": "summarize: def fresh_copy(self, **kwargs) -> Self:                return self.model_copy(            update={                \"id\": uuid4(),                \"created\": pendulum.now(\"utc\"),                \"updated\": pendulum.now(\"utc\"),                \"timestamp\": pendulum.now(\"utc\"),            },            deep=True,            **kwargs,        )",
        "labels_text": "Return a fresh copy of the state with a new ID"
    },
    {
        "input_text": "summarize: def __repr__(self) -> str:                result = self.data        display = dict(            message=repr(self.message),            type=str(self.type.value),            result=repr(result),        )        return f\"{self.name}({', '.join(f'{k}={v}' for k, v in display.items())})\"",
        "labels_text": "Generates a complete state representation appropriate for introspection and debugging including the result MyCompletedStatemessagemy message typeCOMPLETED result"
    },
    {
        "input_text": "summarize: def __str__(self) -> str:                display = []        if self.message:            display.append(repr(self.message))        if self.type.value.lower() != self.name.lower():            display.append(f\"type={self.type.value}\")        return f\"{self.name}({', '.join(display)})\"",
        "labels_text": "Generates a simple state representation appropriate for logging MyCompletedStatemy message typeCOMPLETED"
    },
    {
        "input_text": "summarize: def Completed(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.COMPLETED, **kwargs)",
        "labels_text": "Convenience function for creating Completed state Returns State a Completed state"
    },
    {
        "input_text": "summarize: def Running(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.RUNNING, **kwargs)",
        "labels_text": "Convenience function for creating Running state Returns State a Running state"
    },
    {
        "input_text": "summarize: def Failed(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.FAILED, **kwargs)",
        "labels_text": "Convenience function for creating Failed state Returns State a Failed state"
    },
    {
        "input_text": "summarize: def Crashed(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.CRASHED, **kwargs)",
        "labels_text": "Convenience function for creating Crashed state Returns State a Crashed state"
    },
    {
        "input_text": "summarize: def Cancelling(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.CANCELLING, **kwargs)",
        "labels_text": "Convenience function for creating Cancelling state Returns State a Cancelling state"
    },
    {
        "input_text": "summarize: def Cancelled(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.CANCELLED, **kwargs)",
        "labels_text": "Convenience function for creating Cancelled state Returns State a Cancelled state"
    },
    {
        "input_text": "summarize: def Pending(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.PENDING, **kwargs)",
        "labels_text": "Convenience function for creating Pending state Returns State a Pending state"
    },
    {
        "input_text": "summarize: def Suspended(    cls: Type[State] = State,    timeout_seconds: Optional[int] = None,    pause_expiration_time: Optional[datetime.datetime] = None,    pause_key: Optional[str] = None,    **kwargs,):        return Paused(        cls=cls,        name=\"Suspended\",        reschedule=True,        timeout_seconds=timeout_seconds,        pause_expiration_time=pause_expiration_time,        pause_key=pause_key,        **kwargs,    )",
        "labels_text": "Convenience function for creating Suspended state Returns State a Suspended state"
    },
    {
        "input_text": "summarize: def AwaitingRetry(    scheduled_time: datetime.datetime = None, cls: Type[State] = State, **kwargs) -> State:        return Scheduled(        cls=cls, scheduled_time=scheduled_time, name=\"AwaitingRetry\", **kwargs    )",
        "labels_text": "Convenience function for creating AwaitingRetry state Returns State a AwaitingRetry state"
    },
    {
        "input_text": "summarize: def Retrying(cls: Type[State] = State, **kwargs) -> State:        return cls(type=StateType.RUNNING, name=\"Retrying\", **kwargs)",
        "labels_text": "Convenience function for creating Retrying state Returns State a Retrying state"
    },
    {
        "input_text": "summarize: def Late(    scheduled_time: datetime.datetime = None, cls: Type[State] = State, **kwargs) -> State:        return Scheduled(cls=cls, scheduled_time=scheduled_time, name=\"Late\", **kwargs)",
        "labels_text": "Convenience function for creating Late state Returns State a Late state"
    },
    {
        "input_text": "summarize: def get_ui_url_for_flow_run_id(self, flow_run_id: UUID) -> str:                return urls.url_for(            \"flow-run\",            obj_id=flow_run_id,            default_base_url=\"http://ephemeral-prefect/api\",        )",
        "labels_text": "Returns a link to the flow run view of the given flow run id Args flowrunid the flow run id"
    },
    {
        "input_text": "summarize: def _stop(self, *_) -> None:                self._should_stop = True",
        "labels_text": "Private synchronous method for setting the shouldstop flag Takes arbitrary argument so it can be used a a signal handler"
    },
    {
        "input_text": "summarize: def _generate_uuid_postgresql(element, compiler, **kwargs):        return \"(GEN_RANDOM_UUID())\"",
        "labels_text": "Generates a random UUID in Postgres requires the pgcrypto extension"
    },
    {
        "input_text": "summarize: def _generate_uuid_sqlite(element, compiler, **kwargs):        return",
        "labels_text": "Generates a random UUID in other database SQLite by concatenating byte in a way that approximates a UUID hex representation This is sufficient for our purpose of having a random clientgenerated ID that is compatible with a UUID spec"
    },
    {
        "input_text": "summarize: def _current_timestamp_sqlite(element, compiler, **kwargs):        return \"strftime('%Y-%m-%d %H:%M:%f000', 'now')\"",
        "labels_text": "Generates the current timestamp for SQLite We need to add three zero to the string representation because SQLAlchemy us a regex expression which is expecting decimal place microsecond but SQLite by default only store millisecond This cause SQLAlchemy to interpret a if it were By forcing SQLite to store an extra three s we work around his issue Note this only affect timestamps that we ask SQLite to issue in SQL like the default value for a timestamp column not datetimes provided by SQLAlchemy itself"
    },
    {
        "input_text": "summarize: def _current_timestamp(element, compiler, **kwargs):        return \"CURRENT_TIMESTAMP\"",
        "labels_text": "Generates the current timestamp in standard SQL"
    },
    {
        "input_text": "summarize: def get_dialect(    obj: Union[str, sa.orm.Session, sa.engine.Engine],) -> sa.engine.Dialect:        if isinstance(obj, sa.orm.Session):        url = obj.bind.url    elif isinstance(obj, sa.engine.Engine):        url = obj.url    else:        url = sa.engine.url.make_url(obj)    return url.get_dialect()",
        "labels_text": "Get the dialect of a session engine or connection url Primary use case is figuring out whether the Prefect REST API is communicating with SQLite or Postgres Example python import prefectsettings from prefectserverutilitiesdatabase import getdialect dialect getdialectPREFECTAPIDATABASECONNECTIONURLvalue if dialectname sqlite printUsing SQLite else printUsing Postgres"
    },
    {
        "input_text": "summarize: def method_paths_from_routes(routes: Sequence[BaseRoute]) -> Set[str]:        method_paths = set()    for route in routes:        if isinstance(route, (APIRoute, StarletteRoute)):            for method in route.methods:                method_paths.add(f\"{method} {route.path}\")    return method_paths",
        "labels_text": "Generate a set of string describing the given route in the format method path For example GET log"
    },
    {
        "input_text": "summarize: def register_user_template_filters(filters: Dict[str, Any]):        _template_environment.filters.update(filters)",
        "labels_text": "Register additional filter that will be available to user template"
    },
    {
        "input_text": "summarize: def create_cache() -> Cache:        module = importlib.import_module(PREFECT_MESSAGING_CACHE.value())    assert isinstance(module, CacheModule)    return module.Cache()",
        "labels_text": "Creates a new cache with the application default setting Returns a new Cache instance"
    },
    {
        "input_text": "summarize: def create_publisher(    topic: str, cache: Optional[Cache] = None, deduplicate_by: Optional[str] = None) -> Publisher:        cache = cache or create_cache()    module = importlib.import_module(PREFECT_MESSAGING_BROKER.value())    assert isinstance(module, BrokerModule)    return module.Publisher(topic, cache, deduplicate_by=deduplicate_by)",
        "labels_text": "Creates a new publisher with the application default setting Args topic the topic to publish to Returns a new Consumer instance"
    },
    {
        "input_text": "summarize: def create_consumer(topic: str, **kwargs) -> Consumer:        module = importlib.import_module(PREFECT_MESSAGING_BROKER.value())    assert isinstance(module, BrokerModule)    return module.Consumer(topic, **kwargs)",
        "labels_text": "Creates a new consumer with the application default setting Args topic the topic to consume from Returns a new Consumer instance"
    },
    {
        "input_text": "summarize: def get_class_fields_only(model: Type[BaseModel]) -> set:        subclass_class_fields = set(model.__annotations__.keys())    parent_class_fields = set()    for base in model.__class__.__bases__:        if issubclass(base, BaseModel):            parent_class_fields.update(base.__annotations__.keys())    return (subclass_class_fields - parent_class_fields) | (        subclass_class_fields & parent_class_fields    )",
        "labels_text": "Gets all the field name defined on the model class but not any parent class Any field that are on the parent but redefined on the subclass are included"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                copy_dict = self.model_dump(exclude=self._reset_fields)        if isinstance(other, PrefectBaseModel):            return copy_dict == other.model_dump(exclude=other._reset_fields)        if isinstance(other, BaseModel):            return copy_dict == other.model_dump()        else:            return copy_dict == other",
        "labels_text": "Equaltiy operator that ignores the resettable field of the PrefectBaseModel NOTE this equality operator will only be applied if the PrefectBaseModel is the lefthand operand This is a limitation of Python"
    },
    {
        "input_text": "summarize: def reset_fields(self: Self) -> Self:                return self.model_copy(            update={                field: self.model_fields[field].get_default(call_default_factory=True)                for field in self._reset_fields            }        )",
        "labels_text": "Reset the field of the model that are in the resetfields set Returns PrefectBaseModel A new instance of the model with the reset field"
    },
    {
        "input_text": "summarize: def orjson_dumps(v: Any, *, default: Any) -> str:        return orjson.dumps(v, default=default).decode()",
        "labels_text": "Utility for dumping a value to JSON using orjson orjsondumps return byte to match standard jsondumps we need to decode"
    },
    {
        "input_text": "summarize: def orjson_dumps_extra_compatible(v: Any, *, default: Any) -> str:        return orjson.dumps(        v, default=default, option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY    ).decode()",
        "labels_text": "Utility for dumping a value to JSON using orjson but allows for nonstring key this is helpful for situation like panda dataframes which can result in nonstring key numpy type for serializing numpy array orjsondumps return byte to match standard jsondumps we need to decode"
    },
    {
        "input_text": "summarize: def capture_builders() -> Generator[List[ImageBuilder], None, None]:        builders = []    original_init = ImageBuilder.__init__    def capture(self, *args, **kwargs):        builders.append(self)        original_init(self, *args, **kwargs)    with mock.patch.object(ImageBuilder, \"__init__\", capture):        yield builders",
        "labels_text": "Captures any instance of ImageBuilder created while this context is active"
    },
    {
        "input_text": "summarize: def use_hosted_api_server(hosted_api_server):        with temporary_settings(        {            PREFECT_API_URL: hosted_api_server,            PREFECT_SERVER_CSRF_PROTECTION_ENABLED: False,        }    ):        yield hosted_api_server",
        "labels_text": "Sets PREFECTAPIURL to the test session hosted API endpoint"
    },
    {
        "input_text": "summarize: def disable_hosted_api_server():        with temporary_settings(        {            PREFECT_API_URL: None,        }    ):        yield hosted_api_server",
        "labels_text": "Disables the hosted API server by setting PREFECTAPIURL to None"
    },
    {
        "input_text": "summarize: def enable_ephemeral_server(disable_hosted_api_server):        with temporary_settings(        {            PREFECT_SERVER_ALLOW_EPHEMERAL_MODE: True,        }    ):        yield hosted_api_server    SubprocessASGIServer().stop()",
        "labels_text": "Enables the ephemeral server by setting PREFECTSERVERALLOWEPHEMERALMODE to True"
    },
    {
        "input_text": "summarize: def exceptions_equal(a, b):        if a == b:        return True    return type(a) == type(b) and getattr(a, \"args\", None) == getattr(b, \"args\", None)",
        "labels_text": "Exceptions cannot be compared by They can be compared using is but this will fail if the exception is serializeddeserialized so this utility doe it best to assert equality using the type and args used to initialize the exception"
    },
    {
        "input_text": "summarize: def assert_does_not_warn(ignore_warnings=[]):        with warnings.catch_warnings():        warnings.simplefilter(\"error\")        for warning_type in ignore_warnings:            warnings.filterwarnings(\"ignore\", category=warning_type)        try:            yield        except Warning as warning:            raise AssertionError(f\"Warning was raised. {warning!r}\") from warning",
        "labels_text": "Converts warning to error within this context to assert warning are not raised except for those specified in ignorewarnings Parameters ignorewarnings List of warning type to ignore Example DeprecationWarning UserWarning"
    },
    {
        "input_text": "summarize: def is_async_fn(    func: Union[Callable[P, R], Callable[P, Awaitable[R]]],) -> TypeGuard[Callable[P, Awaitable[R]]]:        while hasattr(func, \"__wrapped__\"):        func = func.__wrapped__    return inspect.iscoroutinefunction(func)",
        "labels_text": "Returns True if a function return a coroutine See httpsgithubcommicrosoftpyrightissues for an example use"
    },
    {
        "input_text": "summarize: def is_async_gen_fn(func):        while hasattr(func, \"__wrapped__\"):        func = func.__wrapped__    return inspect.isasyncgenfunction(func)",
        "labels_text": "Returns True if a function is an async generator"
    },
    {
        "input_text": "summarize: def create_task(coroutine: Coroutine) -> asyncio.Task:        task = asyncio.create_task(coroutine)    # Add task to the set. This creates a strong reference.    # Take a lock because this might be done from multiple threads.    with background_task_lock:        BACKGROUND_TASKS.add(task)    # To prevent keeping references to finished tasks forever,    # make each task remove its own reference from the set after    # completion:    task.add_done_callback(BACKGROUND_TASKS.discard)    return task",
        "labels_text": "Replacement for asynciocreatetask that will ensure that task arent garbage collected before they complete Allows for fire and forget behavior in which task can be created and the application can move on Tasks can also be awaited normally See httpsdocspythonorglibraryasynciotaskhtmlasynciocreatetask for detail and essentially this implementation"
    },
    {
        "input_text": "summarize: def run_async_from_worker_thread(    __fn: Callable[..., Awaitable[T]], *args: Any, **kwargs: Any) -> T:        call = partial(__fn, *args, **kwargs)    return anyio.from_thread.run(call)",
        "labels_text": "Runs an async function in the main thread event loop blocking the worker thread until completion"
    },
    {
        "input_text": "summarize: def create_gather_task_group() -> GatherTaskGroup:        # This function matches the AnyIO API which uses callables since the concrete    # task group class depends on the async library being used and cannot be    # determined until runtime    return GatherTaskGroup(anyio.create_task_group())",
        "labels_text": "Create a new task group that gather result"
    },
    {
        "input_text": "summarize: def get_parameter_defaults(    fn: Callable,) -> Dict[str, Any]:        signature = inspect.signature(fn)    parameter_defaults = {}    for name, param in signature.parameters.items():        if param.default is not signature.empty:            parameter_defaults[name] = param.default    return parameter_defaults",
        "labels_text": "Get default parameter value for a callable"
    },
    {
        "input_text": "summarize: def call_with_parameters(fn: Callable, parameters: Dict[str, Any]):        args, kwargs = parameters_to_args_kwargs(fn, parameters)    return fn(*args, **kwargs)",
        "labels_text": "Call a function with parameter extracted with getcallparameters The function must have an identical signature to the original function or this will fail If you need to send to a function with a different signature extract the argskwargs using parameterstopositionalandkeyword directly"
    },
    {
        "input_text": "summarize: def cloudpickle_wrapped_call(    __fn: Callable, *args: Any, **kwargs: Any) -> Callable[[], bytes]:        payload = cloudpickle.dumps((__fn, args, kwargs))    return partial(_run_serialized_call, payload)",
        "labels_text": "Serializes a function call using cloudpickle then return a callable which will execute that call and return a cloudpickle serialized return value This is particularly useful for sending call to library that only use the Python builtin pickler eg anyiotoprocess and multiprocessing but may require a wider range of pickling support"
    },
    {
        "input_text": "summarize: def _run_serialized_call(payload) -> bytes:        fn, args, kwargs = cloudpickle.loads(payload)    retval = fn(*args, **kwargs)    return cloudpickle.dumps(retval)",
        "labels_text": "Defined at the toplevel so it can be pickled by the Python pickler Used by cloudpicklewrappedcall"
    },
    {
        "input_text": "summarize: def parameter_docstrings(docstring: Optional[str]) -> Dict[str, str]:        param_docstrings = {}    if not docstring:        return param_docstrings    with disable_logger(\"griffe\"):        parsed = parse(Docstring(docstring), Parser.google)        for section in parsed:            if section.kind != DocstringSectionKind.parameters:                continue            param_docstrings = {                parameter.name: parameter.description for parameter in section.value            }    return param_docstrings",
        "labels_text": "Given a docstring in Google docstring format parse the parameter section and return a dictionary that map parameter name to docstring Args docstring The function docstring Returns Mapping from parameter name to docstrings"
    },
    {
        "input_text": "summarize: def parameter_schema(fn: Callable) -> ParameterSchema:        try:        signature = inspect.signature(fn, eval_str=True)  # novm    except (NameError, TypeError):        # `eval_str` is not available in Python < 3.10        signature = inspect.signature(fn)    docstrings = parameter_docstrings(inspect.getdoc(fn))    return generate_parameter_schema(signature, docstrings)",
        "labels_text": "Given a function generates an OpenAPIcompatible description of the function argument including name typing information whether it is required a default value additional constraint like possible enum value Args fn Callable The function whose argument will be serialized Returns ParameterSchema the argument schema"
    },
    {
        "input_text": "summarize: def raise_for_reserved_arguments(fn: Callable, reserved_arguments: Iterable[str]):        function_paremeters = inspect.signature(fn).parameters    for argument in reserved_arguments:        if argument in function_paremeters:            raise ReservedArgumentError(                f\"{argument!r} is a reserved argument name and cannot be used.\"            )",
        "labels_text": "Raise a ReservedArgumentError if fn ha any parameter that conflict with the name contained in reservedarguments"
    },
    {
        "input_text": "summarize: def auto():                return auto()",
        "labels_text": "Exposes enumauto to avoid requiring a second import to use AutoEnum"
    },
    {
        "input_text": "summarize: def isiterable(obj: Any) -> bool:        try:        iter(obj)    except TypeError:        return False    else:        return not isinstance(obj, (str, bytes, io.IOBase))",
        "labels_text": "Return a boolean indicating if an object is iterable Excludes type that are iterable but typically used a singleton str byte IO object"
    },
    {
        "input_text": "summarize: def batched_iterable(iterable: Iterable[T], size: int) -> Iterator[Tuple[T, ...]]:        it = iter(iterable)    while True:        batch = tuple(itertools.islice(it, size))        if not batch:            break        yield batch",
        "labels_text": "Yield batch of a certain size from an iterable Args iterable Iterable An iterable size int The batch size to return Yields tuple A batch of the iterable"
    },
    {
        "input_text": "summarize: def remove_nested_keys(keys_to_remove: List[Hashable], obj):        if not isinstance(obj, dict):        return obj    return {        key: remove_nested_keys(keys_to_remove, value)        for key, value in obj.items()        if key not in keys_to_remove    }",
        "labels_text": "Recurses a dictionary return a copy without all key that match an entry in keytoremove Return obj unchanged if not a dictionary Args keystoremove A list of key to remove from obj obj The object to remove key from Returns obj without key matching an entry in keystoremove if obj is a dictionary obj if obj is not a dictionary"
    },
    {
        "input_text": "summarize: def get_task_and_flow_run_ids() -> Tuple[Optional[UUID], Optional[UUID]]:        return get_task_run_id(), get_flow_run_id()",
        "labels_text": "Get the task run and flow run id from the context if available Returns TupleOptionalUUID OptionalUUID a tuple of the task run id and flow run id"
    },
    {
        "input_text": "summarize: def get_registry_for_type(cls: T) -> Optional[Dict[str, T]]:        return next(        filter(            lambda registry: registry is not None,            (_TYPE_REGISTRIES.get(cls) for cls in cls.mro()),        ),        None,    )",
        "labels_text": "Get the first matching registry for a class or any of it base class If not found None is returned"
    },
    {
        "input_text": "summarize: def lookup_type(cls: T, dispatch_key: str) -> T:        # Get the first matching registry for the class or one of its bases    registry = get_registry_for_type(cls)    # Look up this type in the registry    subcls = registry.get(dispatch_key)    if subcls is None:        raise KeyError(            f\"No class found for dispatch key {dispatch_key!r} in registry for type \"            f\"{cls.__name__!r}.\"        )    return subcls",
        "labels_text": "Look up a dispatch key in the type registry for the given class"
    },
    {
        "input_text": "summarize: def docker_client() -> Generator[\"DockerClient\", None, None]:        client = None    try:        with silence_docker_warnings():            client = docker.DockerClient.from_env()            yield client    except docker.errors.DockerException as exc:        raise RuntimeError(            \"This error is often thrown because Docker is not running. Please ensure Docker is running.\"        ) from exc    finally:        client is not None and client.close()",
        "labels_text": "Get the environmentallyconfigured Docker client"
    },
    {
        "input_text": "summarize: def add_line(self, line: str) -> None:                self.add_lines([line])",
        "labels_text": "Add a line to this image Dockerfile"
    },
    {
        "input_text": "summarize: def add_lines(self, lines: Iterable[str]) -> None:                self.dockerfile_lines.extend(lines)",
        "labels_text": "Add line to this image Dockerfile"
    },
    {
        "input_text": "summarize: def assert_has_line(self, line: str) -> None:                all_lines = \"\\n\".join(            [f\"  {i+1:>3}: {line}\" for i, line in enumerate(self.dockerfile_lines)]        )        message = (            f\"Expected {line!r} not found in Dockerfile.  Dockerfile:\\n{all_lines}\"        )        assert line in self.dockerfile_lines, message",
        "labels_text": "Asserts that the given line is in the Dockerfile"
    },
    {
        "input_text": "summarize: def assert_line_after(self, second: str, first: str) -> None:                self.assert_line_before(first, second)",
        "labels_text": "Asserts that the second line appears after the first line"
    },
    {
        "input_text": "summarize: def assert_has_file(self, source: Path, container_path: PurePosixPath) -> None:                if source.is_absolute():            source = source.relative_to(self.base_directory)        self.assert_has_line(f\"COPY {source} {container_path}\")",
        "labels_text": "Asserts that the given file or directory will be copied into the container at the given path"
    },
    {
        "input_text": "summarize: def format_outlier_version_name(version: str):        return version.replace(\"-ce\", \"\").replace(\"-ee\", \"\")",
        "labels_text": "Formats outlier docker version name to pas packagingversionparse validation Current case are simple but creates stub for more complicated formatting if eventually needed Example outlier version that throw a parsing exception ce variant of community edition label ee variant of enterprise edition label Args version str raw docker version value Returns str value that can pas packagingversionparse validation"
    },
    {
        "input_text": "summarize: def get_state_for_result(obj: Any) -> Optional[State]:        flow_run_context = FlowRunContext.get()    if flow_run_context:        return flow_run_context.task_run_results.get(id(obj))",
        "labels_text": "Get the state related to a result object linkstatetoresult must have been called first"
    },
    {
        "input_text": "summarize: def create_default_ignore_file(path: str) -> bool:        path = pathlib.Path(path)    ignore_file = path / \".prefectignore\"    if ignore_file.exists():        return False    default_file = pathlib.Path(prefect.__module_path__) / \".prefectignore\"    with ignore_file.open(mode=\"w\") as f:        f.write(default_file.read_text())    return True",
        "labels_text": "Creates default ignore file in the provided path if one doe not already exist return boolean specifying whether a file wa created"
    },
    {
        "input_text": "summarize: def tmpchdir(path: str):        path = os.path.abspath(path)    if os.path.isfile(path) or (not os.path.exists(path) and not path.endswith(\"/\")):        path = os.path.dirname(path)    owd = os.getcwd()    with chdir_lock:        try:            os.chdir(path)            yield path        finally:            os.chdir(owd)",
        "labels_text": "Change currentworking directory for the duration of the context"
    },
    {
        "input_text": "summarize: def filename(path: str) -> str:        try:        of: OpenFile = fsspec.open(path)        sep = of.fs.sep    except (ImportError, AttributeError):        sep = \"\\\\\" if \"\\\\\" in path else \"/\"    return path.split(sep)[-1]",
        "labels_text": "Extract the file name from a path with remote file system support"
    },
    {
        "input_text": "summarize: def to_display_path(    path: Union[pathlib.Path, str], relative_to: Union[pathlib.Path, str] = None) -> str:        path, relative_to = (        pathlib.Path(path).resolve(),        pathlib.Path(relative_to or \".\").resolve(),    )    relative_path = str(path.relative_to(relative_to))    absolute_path = str(path)    return relative_path if len(relative_path) < len(absolute_path) else absolute_path",
        "labels_text": "Convert a path to a displayable path The absolute path or relative path to the current or given directory will be returned whichever is shorter"
    },
    {
        "input_text": "summarize: def relative_path_to_current_platform(path_str: str) -> Path:        return Path(PureWindowsPath(path_str).as_posix())",
        "labels_text": "Converts a relative path generated on any platform to a relative path for the current platform"
    },
    {
        "input_text": "summarize: def get_open_file_limit() -> int:        try:        if os.name == \"nt\":            import ctypes            return ctypes.cdll.ucrtbase._getmaxstdio()        else:            import resource            soft_limit, _ = resource.getrlimit(resource.RLIMIT_NOFILE)            return soft_limit    except Exception:        # Catch all exceptions, as ctypes can raise several errors        # depending on what went wrong. Return a safe default if we        # can't get the limit from the OS.        return 200",
        "labels_text": "Get the maximum number of open file allowed for the current process"
    },
    {
        "input_text": "summarize: def stable_hash(*args: Union[str, bytes], hash_algo=_md5) -> str:        h = hash_algo()    for a in args:        if isinstance(a, str):            a = a.encode()        h.update(a)    return h.hexdigest()",
        "labels_text": "Given some argument produce a stable bit hash of their content Supports byte and string Strings will be UTF encoded Args args Items to include in the hash hashalgo Hash algorithm from hashlib to use Returns A hex hash"
    },
    {
        "input_text": "summarize: def file_hash(path: str, hash_algo=_md5) -> str:        contents = Path(path).read_bytes()    return stable_hash(contents, hash_algo=hash_algo)",
        "labels_text": "Given a path to a file produce a stable hash of the file content Args path str the path to a file hashalgo Hash algorithm from hashlib to use Returns str a hash of the file content"
    },
    {
        "input_text": "summarize: def hash_objects(*args, hash_algo=_md5, **kwargs) -> Optional[str]:        try:        serializer = JSONSerializer(dumps_kwargs={\"sort_keys\": True})        return stable_hash(serializer.dumps((args, kwargs)), hash_algo=hash_algo)    except Exception:        pass    try:        return stable_hash(cloudpickle.dumps((args, kwargs)), hash_algo=hash_algo)    except Exception:        pass    return None",
        "labels_text": "Attempt to hash object by dumping to JSON or serializing with cloudpickle On failure of both None will be returned"
    },
    {
        "input_text": "summarize: def to_qualified_name(obj: Any) -> str:        if sys.version_info < (3, 10):        # These attributes are only available in Python 3.10+        if isinstance(obj, (classmethod, staticmethod)):            obj = obj.__func__    return obj.__module__ + \".\" + obj.__qualname__",
        "labels_text": "Given an object return it fullyqualified name a string that represents it Python import path Args obj Any an importable Python object Returns str the qualified name"
    },
    {
        "input_text": "summarize: def load_module(module_name: str) -> ModuleType:        # Ensure relative imports within the imported module work if the user is in the    # correct working directory    working_directory = os.getcwd()    sys.path.insert(0, working_directory)    try:        return importlib.import_module(module_name)    finally:        sys.path.remove(working_directory)",
        "labels_text": "Import a module with support for relative import within the module"
    },
    {
        "input_text": "summarize: def __init__(self, aliases: Iterable[AliasedModuleDefinition]):                self.aliases = aliases",
        "labels_text": "See AliasedModuleDefinition for alias specification Aliases apply to all module nested within an alias"
    },
    {
        "input_text": "summarize: def poisson_interval(average_interval, lower=0, upper=1):        # note that we ensure the argument to the logarithm is stabilized to prevent    # calling log(0), which results in a DomainError    return -math.log(max(1 - random.uniform(lower, upper), 1e-10)) * average_interval",
        "labels_text": "Generates an interarrival time for a Poisson process Draws a random variable from an exponential distribution using the inverseCDF method Can optionally be passed a lower and upper bound between to clamp the potential output value"
    },
    {
        "input_text": "summarize: def lower_clamp_multiple(k):        if k >= 50:        # return 0 for large values of `k` to prevent numerical overflow        return 0.0    return math.log(max(2**k / (2**k - 1), 1e-10), 2)",
        "labels_text": "Computes a lower clamp multiple that can be used to bound a random variate drawn from an exponential distribution Given an upper clamp multiple k and corresponding upper bound k averageinterval this function computes a lower clamp multiple c corresponding to a lower bound c averageinterval where the probability mass between the lower bound and the median is equal to the probability mass between the median and the upper bound"
    },
    {
        "input_text": "summarize: def bounded_poisson_interval(lower_bound, upper_bound):        average = (float(lower_bound) + float(upper_bound)) / 2.0    upper_rv = exponential_cdf(upper_bound, average)    lower_rv = exponential_cdf(lower_bound, average)    return poisson_interval(average, lower_rv, upper_rv)",
        "labels_text": "Bounds Poisson interarrival time to a range Unlike clampedpoissoninterval this doe not take a target average interval Instead the interval is predetermined and the average is calculated a their midpoint This allows Poisson interval to be used in case where a lower bound must be enforced"
    },
    {
        "input_text": "summarize: def generate_slug(n_words: int) -> str:        words = coolname.generate(n_words)    # regenerate words if they include ignored words    while IGNORE_LIST.intersection(words):        words = coolname.generate(n_words)    return \"-\".join(words)",
        "labels_text": "Generates a random slug Args nwords int the number of word in the slug"
    },
    {
        "input_text": "summarize: def obfuscate(s: Any, show_tail=False) -> str:        if s is None:        return OBFUSCATED_PREFIX + \"*\" * 4    return obfuscate_string(str(s), show_tail=show_tail)",
        "labels_text": "Obfuscates any data type string representation See obfuscatestring"
    },
    {
        "input_text": "summarize: def obfuscate_string(s: str, show_tail=False) -> str:        result = OBFUSCATED_PREFIX + \"*\" * 4    # take up to 4 characters, but only after the 10th character    suffix = s[10:][-4:]    if suffix and show_tail:        result = f\"{result[:-len(suffix)]}{suffix}\"    return result",
        "labels_text": "Obfuscates a string by returning a new string of character If the input string is longer than character and showtail is True then up to of it final character will become final character of the obfuscated string all other character are abc abcdefgh abcdefghijk k abcdefghijklmnopqrs pqrs"
    },
    {
        "input_text": "summarize: def _win32_ctrl_handler(dwCtrlType):                for pid in _windows_process_group_pids:            try:                os.kill(pid, signal.CTRL_BREAK_EVENT)            except OSError:                # process is already terminated                pass        # returning 0 lets the next handler in the chain handle the signal        return 0",
        "labels_text": "A callback function for handling CTRL event cleanly on Windows When called this function will terminate all running win subprocesses the current process started in new process group"
    },
    {
        "input_text": "summarize: def _reduce_model(model: BaseModel):        return (        _unreduce_model,        (            to_qualified_name(type(model)),            model.model_dump_json(**getattr(model, \"__reduce_kwargs__\", {})),        ),    )",
        "labels_text": "Helper for serializing a cythonized model with cloudpickle Keyword argument can provide additional setting to the json call Since reduce take no argument these are set on the reducekwargs attr"
    },
    {
        "input_text": "summarize: def _unreduce_model(model_name, json):        model = from_qualified_name(model_name)    return model.model_validate_json(json)",
        "labels_text": "Helper for restoring model after serialization"
    },
    {
        "input_text": "summarize: def add_cloudpickle_reduction(__model_cls: Optional[Type[M]] = None, **kwargs: Any):        if __model_cls:        __model_cls.__reduce__ = _reduce_model        __model_cls.__reduce_kwargs__ = kwargs        return __model_cls    else:        return cast(            Callable[[Type[M]], Type[M]],            partial(                add_cloudpickle_reduction,                **kwargs,            ),        )",
        "labels_text": "Adds a reducer to the given class that ensures it is cloudpickle compatible Workaround for issue with cloudpickle when using cythonized pydantic which throw exception when attempting to pickle the class which ha compiled validator method dynamically attached to it We cannot define this utility in the model class itself because the class is the type that contains unserializable method Any model using some feature of Pydantic eg Path validation with a Cython compiled Pydantic installation may encounter pickling issue See related issue at httpsgithubcomcloudpipecloudpickleissues"
    },
    {
        "input_text": "summarize: def get_class_fields_only(model: Type[BaseModel]) -> set:        subclass_class_fields = set(model.__annotations__.keys())    parent_class_fields = set()    for base in model.__class__.__bases__:        if issubclass(base, BaseModel):            parent_class_fields.update(base.__annotations__.keys())    return (subclass_class_fields - parent_class_fields) | (        subclass_class_fields & parent_class_fields    )",
        "labels_text": "Gets all the field name defined on the model class but not any parent class Any field that are on the parent but redefined on the subclass are included"
    },
    {
        "input_text": "summarize: def parse_obj_as(    type_: type[T],    data: Any,    mode: Literal[\"python\", \"json\", \"strings\"] = \"python\",) -> T:        adapter = TypeAdapter(type_)    if get_origin(type_) is list and isinstance(data, dict):        data = next(iter(data.values()))    parser: Callable[[Any], T] = getattr(adapter, f\"validate_{mode}\")    return parser(data)",
        "labels_text": "Parse a given data structure a a Pydantic model via TypeAdapter Read more about TypeAdapter herehttpsdocspydanticdevlatestconceptstypeadapter Args type The type to parse the data a data The data to be parsed mode The mode to use for parsing either python json or string Defaults to python where data should be a Python object eg dict Returns The parsed data a the given type Example Basic Usage of parseas python from prefectutilitiespydantic import parseas from pydantic import BaseModel class ExampleModelBaseModel name str parsing python object parsed parseasExampleModel name Marvin assert isinstanceparsed ExampleModel assert parsedname Marvin parsing json string parsed parseas listExampleModel name Marvin name Arthur modejson assert allisinstanceitem ExampleModel for item in parsed assert parsedname Marvin assert parsedname Arthur parsing raw string parsed parseasint modestrings assert isinstanceparsed int assert parsed"
    },
    {
        "input_text": "summarize: def start_client_metrics_server():        if not PREFECT_CLIENT_ENABLE_METRICS:        return    global _metrics_server    if _metrics_server:        return    from prometheus_client import start_http_server    _metrics_server = start_http_server(port=PREFECT_CLIENT_METRICS_PORT.value())",
        "labels_text": "Start the processwide Prometheus metric server for client metric if enabled with PREFECTCLIENTENABLEMETRICS on the port PREFECTCLIENTMETRICSPORT"
    },
    {
        "input_text": "summarize: def stop_client_metrics_server():        global _metrics_server    if _metrics_server:        server, thread = _metrics_server        server.shutdown()        thread.join()        _metrics_server = None",
        "labels_text": "Start the processwide Prometheus metric server for client metric if it ha previously been started"
    },
    {
        "input_text": "summarize: def determine_placeholder_type(name: str) -> PlaceholderType:        if name.startswith(BLOCK_DOCUMENT_PLACEHOLDER_PREFIX):        return PlaceholderType.BLOCK_DOCUMENT    elif name.startswith(VARIABLE_PLACEHOLDER_PREFIX):        return PlaceholderType.VARIABLE    elif name.startswith(ENV_VAR_PLACEHOLDER_PREFIX):        return PlaceholderType.ENV_VAR    else:        return PlaceholderType.STANDARD",
        "labels_text": "Determines the type of a placeholder based on it name Args name The name of the placeholder Returns The type of the placeholder"
    },
    {
        "input_text": "summarize: def convert_class_to_name(obj: Any) -> str:        cls = obj if inspect.isclass(obj) else obj.__class__    name = cls.__name__    return \"\".join([\"-\" + i.lower() if i.isupper() else i for i in name]).lstrip(\"-\")",
        "labels_text": "Convert CamelCase class name to dashseparated lowercase name"
    },
    {
        "input_text": "summarize: def track_viz_task(    is_async: bool,    task_name: str,    parameters: dict,    viz_return_value: Optional[Any] = None,):        if is_async:        return from_async.wait_for_call_in_loop_thread(            partial(_track_viz_task, task_name, parameters, viz_return_value)        )    else:        return _track_viz_task(task_name, parameters, viz_return_value)",
        "labels_text": "Return a result if sync otherwise return a coroutine that return the result"
    },
    {
        "input_text": "summarize: def _fix_null_typing(    key: str,    schema: Dict,    required_fields: List[str],    allow_none_with_default: bool = False,):        if (        key not in required_fields        and \"type\" in schema        and schema.get(\"type\") != \"null\"        and (\"default\" not in schema or allow_none_with_default)    ):        schema[\"anyOf\"] = [{\"type\": schema[\"type\"]}, {\"type\": \"null\"}]        del schema[\"type\"]",
        "labels_text": "Pydantic V doe not generate a valid Draft schema for null type"
    },
    {
        "input_text": "summarize: def _fix_tuple_items(schema: Dict):        if (        schema.get(\"items\")        and isinstance(schema[\"items\"], list)        and not schema.get(\"prefixItems\")    ):        schema[\"prefixItems\"] = deepcopy(schema[\"items\"])        del schema[\"items\"]",
        "labels_text": "Pydantic V doe not generate a valid Draft schema for tuples"
    },
    {
        "input_text": "summarize: def json_template(cls) -> dict:                configuration = {}        properties = cls.model_json_schema()[\"properties\"]        for k, v in properties.items():            if v.get(\"template\"):                template = v[\"template\"]            else:                template = \"{{ \" + k + \" }}\"            configuration[k] = template        return configuration",
        "labels_text": "Returns a dict with job configuration a key and the corresponding template a value Defaults to using the job configuration parameter name a the template variable name eg key key default variable template key template template specifically provide a template"
    },
    {
        "input_text": "summarize: def _base_flow_run_command() -> str:                return \"prefect flow-run execute\"",
        "labels_text": "Generate a command for a flow run job"
    },
    {
        "input_text": "summarize: def _base_flow_run_labels(flow_run: \"FlowRun\") -> Dict[str, str]:                return {            \"prefect.io/flow-run-id\": str(flow_run.id),            \"prefect.io/flow-run-name\": flow_run.name,            \"prefect.io/version\": prefect.__version__,        }",
        "labels_text": "Generate a dictionary of label for a flow run job"
    },
    {
        "input_text": "summarize: def _base_environment(cls) -> Dict[str, str]:                return get_current_settings().to_environment_variables(exclude_unset=True)",
        "labels_text": "Environment variable that should be passed to all created infrastructure These value should be overridable with the env field"
    },
    {
        "input_text": "summarize: def _base_flow_run_environment(flow_run: \"FlowRun\") -> Dict[str, str]:                return {\"PREFECT__FLOW_RUN_ID\": str(flow_run.id)}",
        "labels_text": "Generate a dictionary of environment variable for a flow run job"
    },
    {
        "input_text": "summarize: def get_worker_class_from_type(type: str) -> Optional[Type[\"BaseWorker\"]]:                load_prefect_collections()        worker_registry = get_registry_for_type(BaseWorker)        if worker_registry is not None:            return worker_registry.get(type)",
        "labels_text": "Returns the worker class for a given worker type If the worker type is not recognized return None"
    },
    {
        "input_text": "summarize: def get_all_available_worker_types() -> List[str]:                load_prefect_collections()        worker_registry = get_registry_for_type(BaseWorker)        if worker_registry is not None:            return list(worker_registry.keys())        return []",
        "labels_text": "Returns all worker type available in the local registry"
    },
    {
        "input_text": "summarize: def get_status(self):                return {            \"name\": self.name,            \"work_pool\": (                self._work_pool.model_dump(mode=\"json\")                if self._work_pool is not None                else None            ),            \"settings\": {                \"prefetch_seconds\": self._prefetch_seconds,            },        }",
        "labels_text": "Retrieves the status of the current worker including it name current worker pool the work pool queue it is polling and it local setting"
    },
    {
        "input_text": "summarize: def _base_flow_run_command(self) -> str:                return \"python -m prefect.engine\"",
        "labels_text": "Override the base flow run command because enhanced cancellation doesnt work with the process worker"
    },
    {
        "input_text": "summarize: def start_healthcheck_server(    worker: Union[BaseWorker, ProcessWorker],    query_interval_seconds: float,    log_level: str = \"error\",) -> None:        server = build_healthcheck_server(worker, query_interval_seconds, log_level)    server.run()",
        "labels_text": "Run a healthcheck FastAPI server for a worker Args worker BaseWorker ProcessWorker the worker whose health we will check loglevel str the log level to use for the server"
    },
    {
        "input_text": "summarize: def inject_renamed_module_alias_finder():        sys.meta_path.insert(0, AliasedModuleFinder(DEPRECATED_MODULE_ALIASES))",
        "labels_text": "Insert an aliased module finder into Pythons import machinery Required for registerrenamedmodule to work"
    },
    {
        "input_text": "summarize: def enabled_experiments() -> Set[str]:        return {        name[len(\"PREFECT_EXPERIMENTAL_ENABLE_\") :].lower()        for name, setting in SETTING_VARIABLES.items()        if name.startswith(\"PREFECT_EXPERIMENTAL_ENABLE_\") and setting.value()    }",
        "labels_text": "Return the set of all enabled experiment"
    },
    {
        "input_text": "summarize: def import_string_class_method(new_location: str) -> Callable:        from pydantic._internal._validators import import_string    class_name, method_name = new_location.rsplit(\".\", 1)    cls = import_string(class_name)    method = getattr(cls, method_name, None)    if method is not None and callable(method):        return method    raise PrefectImportError(f\"Unable to import {new_location!r}\")",
        "labels_text": "Handle moved class method importstring doe not account for moved class method This function handle case where a method ha been moved to a class For example if newlocation is prefectvariablesVariableget importstringnewlocation will raise an error because it doe not handle class method This function will import the class and get the method from the class Args newlocation str The new location of the method Returns method The resolved method from the class Raises PrefectImportError If the method is not found in the class"
    },
    {
        "input_text": "summarize: def wait_for_call_in_loop_thread(        __call: Union[Callable[[], T], Call[T]],        timeout: Optional[float] = None,        done_callbacks: Optional[Iterable[Call]] = None,    ) -> T:                raise NotImplementedError()",
        "labels_text": "Schedule a function in the global worker thread and wait for completion Returns the result of the call"
    },
    {
        "input_text": "summarize: def wait_for_call_in_new_thread(        __call: Union[Callable[[], T], Call[T]],        timeout: Optional[float] = None,        done_callbacks: Optional[Iterable[Call]] = None,    ) -> T:                raise NotImplementedError()",
        "labels_text": "Schedule a function in a new worker thread Returns the result of the call"
    },
    {
        "input_text": "summarize: def call_soon_in_new_thread(        __call: Union[Callable[[], T], Call[T]], timeout: Optional[float] = None    ) -> Call[T]:                call = _cast_to_call(__call)        runner = WorkerThread(run_once=True)        call.set_timeout(timeout)        runner.submit(call)        return call",
        "labels_text": "Schedule a call for execution in a new worker thread Returns the submitted call"
    },
    {
        "input_text": "summarize: def call_soon_in_loop_thread(        __call: Union[Callable[[], Awaitable[T]], Call[Awaitable[T]]],        timeout: Optional[float] = None,    ) -> Call[T]:                call = _cast_to_call(__call)        runner = get_global_loop()        call.set_timeout(timeout)        runner.submit(call)        return call",
        "labels_text": "Schedule a call for execution in the global event loop thread Returns the submitted call"
    },
    {
        "input_text": "summarize: def call_in_new_thread(        __call: Union[Callable[[], T], Call[T]], timeout: Optional[float] = None    ) -> T:                raise NotImplementedError()",
        "labels_text": "Run a call in a new worker thread Returns the result of the call"
    },
    {
        "input_text": "summarize: def call_in_loop_thread(        __call: Union[Callable[[], Awaitable[T]], Call[Awaitable[T]]],        timeout: Optional[float] = None,    ) -> T:                raise NotImplementedError()",
        "labels_text": "Run a call in the global event loop thread Returns the result of the call"
    },
    {
        "input_text": "summarize: def add_cancel_callback(self, callback: Callable[[], None]):                # If we were to invoke cancel callbacks the same as \"done\" callbacks, we        # would not propagate chained cancellation in waiters in time to actually        # interrupt calls.        if self._cancel_scope:            # Add callback to current cancel scope if it exists            self._cancel_scope.add_cancel_callback(callback)        # Also add callbacks to tracking list        self._cancel_callbacks.append(callback)",
        "labels_text": "Add a callback to be enforced on cancellation Unlike done callback this callback will be invoked before the future is cancelled If added after the future is cancelled nothing will happen"
    },
    {
        "input_text": "summarize: def _invoke_callbacks(self):                if self._done_callbacks:            done_callbacks = self._done_callbacks[:]            self._done_callbacks[:] = []            for callback in done_callbacks:                try:                    callback(self)                except Exception:                    logger.exception(\"exception calling callback for %r\", self)        self._cancel_callbacks = []        if self._cancel_scope:            self._cancel_scope._callbacks = []            self._cancel_scope = None",
        "labels_text": "Invoke our done callback and clean up cancel scope and cancel callback Fixes a memory leak that hung on to Call object preventing garbage collection of Futures A fix for"
    },
    {
        "input_text": "summarize: def set_timeout(self, timeout: Optional[float] = None) -> None:                if self.future.done() or self.future.running():            raise RuntimeError(\"Timeouts cannot be added when the call has started.\")        self.timeout = timeout",
        "labels_text": "Set the timeout for the call The timeout begin when the call start"
    },
    {
        "input_text": "summarize: def set_runner(self, portal: \"Portal\") -> None:                if self.runner is not None:            raise RuntimeError(\"The portal is already set for this call.\")        self.runner = portal",
        "labels_text": "Update the portal used to run this call"
    },
    {
        "input_text": "summarize: def result(self, timeout: Optional[float] = None) -> T:                return self.future.result(timeout=timeout)",
        "labels_text": "Wait for the result of the call Not safe for use from asynchronous context"
    },
    {
        "input_text": "summarize: def cancelled(self) -> bool:                return self.future.cancelled()",
        "labels_text": "Check if the call wa cancelled"
    },
    {
        "input_text": "summarize: def timedout(self) -> bool:                return self.future.timedout()",
        "labels_text": "Check if the call timed out"
    },
    {
        "input_text": "summarize: def __call__(self) -> T:                coro = self.run()        # Return an awaitable if in an async context        if coro is not None:            async def run_and_return_result():                await coro                return self.result()            return run_and_return_result()        else:            return self.result()",
        "labels_text": "Execute the call and return it result All execution during execution of the call are reraised"
    },
    {
        "input_text": "summarize: def submit(self, call: \"Call\") -> \"Call\":",
        "labels_text": "Submit a call to execute elsewhere The call result can be retrieved with callresult Returns the call for convenience"
    },
    {
        "input_text": "summarize: def active(self) -> bool:                return self._lock._count > 0",
        "labels_text": "Returns true if the shield is active"
    },
    {
        "input_text": "summarize: def shield():        with (        anyio.CancelScope(shield=True)        if get_running_loop()        else contextlib.nullcontext()    ):        with _get_thread_shield(threading.current_thread()):            yield",
        "labels_text": "Prevent code from within the scope from being cancelled This guard against cancellation from alarm signal and injected exception a used in this module If an event loop is running in the thread where this is called it will be shielded from asynchronous cancellation a well"
    },
    {
        "input_text": "summarize: def cancel(self, throw: bool = True) -> bool:                with self._lock:            if not self.started:                raise RuntimeError(\"Scope has not been entered.\")            if self._completed:                return False            if self._cancelled:                return True            self._cancelled = True        logger.info(\"%r cancelling\", self)        for callback in self._callbacks:            callback()        return True",
        "labels_text": "Cancel this scope If throw is not set this will only mark the scope a cancelled and will not throw the cancelled error"
    },
    {
        "input_text": "summarize: def add_cancel_callback(self, callback: Callable[[], None]):                self._callbacks.append(callback)",
        "labels_text": "Add a callback to execute on cancellation"
    },
    {
        "input_text": "summarize: def _send_cancelled_error(self):                if self._supervised_thread.is_alive():            logger.debug(                \"%r sending exception to supervised thread %r\",                self,                self._supervised_thread,            )            with _get_thread_shield(self._supervised_thread):                try:                    _send_exception_to_thread(self._supervised_thread, CancelledError)                except ValueError:                    # If the thread is gone; just move on without error                    logger.debug(\"Thread missing!\")",
        "labels_text": "Send a cancelled error to the supervised thread"
    },
    {
        "input_text": "summarize: def _timeout_enforcer(self):                if not self._event.wait(self.timeout):            logger.debug(\"%r enforcer detected timeout!\", self)            if self.cancel(throw=False):                with _get_thread_shield(self._supervised_thread):                    self._send_cancelled_error()        # Wait for the supervised thread to exit its context        logger.debug(\"%r waiting for supervised thread to exit\", self)        self._event.wait()",
        "labels_text": "Target for a thread that enforces a timeout"
    },
    {
        "input_text": "summarize: def get_deadline(timeout: Optional[float]):        if timeout is None:        return None    return time.monotonic() + timeout",
        "labels_text": "Compute an deadline given a timeout Uses a monotonic clock"
    },
    {
        "input_text": "summarize: def get_timeout(deadline: Optional[float]):        if deadline is None:        return None    return max(0, deadline - time.monotonic())",
        "labels_text": "Compute an timeout given a deadline Uses a monotonic clock"
    },
    {
        "input_text": "summarize: def cancel_async_at(deadline: Optional[float], name: Optional[str] = None):        with cancel_async_after(get_timeout(deadline), name=name) as ctx:        yield ctx",
        "labels_text": "Cancel any async call within the context if it doe not exit by the given deadline Deadlines must be computed with the monotonic clock See getdeadline A timeout error will be raised on the next await when the timeout expires Yields a CancelContext"
    },
    {
        "input_text": "summarize: def cancel_async_after(timeout: Optional[float], name: Optional[str] = None):        with AsyncCancelScope(timeout=timeout, name=name) as ctx:        yield ctx",
        "labels_text": "Cancel any async call within the context if it doe not exit after the given timeout A timeout error will be raised on the next await when the timeout expires Yields a CancelContext"
    },
    {
        "input_text": "summarize: def cancel_sync_at(deadline: Optional[float], name: Optional[str] = None):        timeout = max(0, deadline - time.monotonic()) if deadline is not None else None    with cancel_sync_after(timeout, name=name) as ctx:        yield ctx",
        "labels_text": "Cancel any sync call within the context if it doe not exit by the given deadline Deadlines must be computed with the monotonic clock See getdeadline The cancel method varies depending on if this is called in the main thread or not See cancelsyncafter for detail Yields a CancelContext"
    },
    {
        "input_text": "summarize: def _send_exception_to_thread(thread: threading.Thread, exc_type: Type[BaseException]):        if not thread.ident:        raise ValueError(\"Thread is not started.\")    ret = ctypes.pythonapi.PyThreadState_SetAsyncExc(        ctypes.c_long(thread.ident), ctypes.py_object(exc_type)    )    if ret == 0:        raise ValueError(\"Thread not found.\")",
        "labels_text": "Raise an exception in a thread This will not interrupt longrunning system call like sleep or wait"
    },
    {
        "input_text": "summarize: def get_running_loop() -> Optional[asyncio.BaseEventLoop]:        try:        return asyncio.get_running_loop()    except RuntimeError:        return None",
        "labels_text": "Get the current running loop Returns None if there is no running loop"
    },
    {
        "input_text": "summarize: def _f_lineno(frame: FrameType) -> int:        f_lineno = frame.f_lineno    if f_lineno is not None:        return f_lineno    f_lasti = frame.f_lasti    code = frame.f_code    prev_line = code.co_firstlineno    for start, next_line in dis.findlinestarts(code):        if f_lasti < start:            return prev_line        prev_line = next_line    return prev_line",
        "labels_text": "Work around some frame lacking an flineno See httpsbugspythonorgissue"
    },
    {
        "input_text": "summarize: def repr_frame(frame: FrameType) -> str:        co = frame.f_code    f_lineno = _f_lineno(frame)    text = f'  File \"{co.co_filename}\", line {f_lineno}, in {co.co_name}'    line = linecache.getline(co.co_filename, f_lineno, frame.f_globals).lstrip()    return text + \"\\n\\t\" + line",
        "labels_text": "Render a frame a a line for inclusion into a text traceback"
    },
    {
        "input_text": "summarize: def call_stack(frame: FrameType) -> List[str]:        L = []    cur_frame: Optional[FrameType] = frame    while cur_frame:        L.append(repr_frame(cur_frame))        cur_frame = cur_frame.f_back    return L[::-1]",
        "labels_text": "Create a call text stack from a frame"
    },
    {
        "input_text": "summarize: def send(self, item: T):                with self._lock:            if self._stopped:                raise RuntimeError(\"Cannot put items in a stopped service instance.\")            logger.debug(\"Service %r enqueuing item %r\", self, item)            self._queue.put_nowait(self._prepare_item(item))",
        "labels_text": "Send an item to this instance of the service"
    },
    {
        "input_text": "summarize: def _prepare_item(self, item: T) -> T:                return item",
        "labels_text": "Prepare an item for submission to the service This is called before the item is sent to the service The default implementation return the item unchanged"
    },
    {
        "input_text": "summarize: def drain(self, at_exit: bool = False) -> None:                future = self._drain(at_exit=at_exit)        if get_running_loop() is not None:            return asyncio.wrap_future(future)        else:            return future.result()",
        "labels_text": "Stop this instance of the service and wait for remaining work to be completed Returns an awaitable if called from an async context"
    },
    {
        "input_text": "summarize: def wait_until_empty(self):                self._queue.join()",
        "labels_text": "Wait until the queue is empty and all item have been processed"
    },
    {
        "input_text": "summarize: def instance(cls: Type[Self], *args) -> Self:                with cls._instance_lock:            key = hash(args)            if key not in cls._instances:                cls._instances[key] = cls._new_instance(*args)            return cls._instances[key]",
        "labels_text": "Get an instance of the service If an instance already exists with the given argument it will be returned"
    },
    {
        "input_text": "summarize: def _new_instance(cls, *args):                instance = cls(*args)        # If already on the global loop, just start it here to avoid deadlock        if threading.get_ident() == get_global_loop().thread.ident:            instance.start()        # Otherwise, bind the service to the global loop        else:            from_sync.call_soon_in_loop_thread(create_call(instance.start)).result()        return instance",
        "labels_text": "Create and start a new instance of the service"
    },
    {
        "input_text": "summarize: def _get_size(self, item: T) -> int:                # By default, batch size is just the number of items        return 1",
        "labels_text": "Calculate the size of a single item"
    },
    {
        "input_text": "summarize: def start(self):                with self._lock:            if not self._started:                self._started = True                self.thread.start()",
        "labels_text": "Start the worker thread"
    },
    {
        "input_text": "summarize: def shutdown(self) -> None:                self._queue.put_nowait(None)",
        "labels_text": "Shutdown the worker thread Does not wait for the thread to stop"
    },
    {
        "input_text": "summarize: def _entrypoint(self):                try:            self._run_until_shutdown()        except CancelledError:            logger.exception(\"%s was cancelled\", self.name)        except BaseException:            # Log exceptions that crash the thread            logger.exception(\"%s encountered exception\", self.name)            raise",
        "labels_text": "Entrypoint for the thread"
    },
    {
        "input_text": "summarize: def start(self):                with self._lock:            if self._loop is None:                self.thread.start()                self._ready_future.result()",
        "labels_text": "Start the worker thread raise any exception encountered during startup"
    },
    {
        "input_text": "summarize: def shutdown(self) -> None:                with self._lock:            if self._shutdown_event is None:                return            self._shutdown_event.set()",
        "labels_text": "Shutdown the worker thread Does not wait for the thread to stop"
    },
    {
        "input_text": "summarize: def _entrypoint(self):                try:            asyncio.run(self._run_until_shutdown())        except BaseException:            # Log exceptions that crash the thread            logger.exception(\"%s encountered exception\", self.name)            raise",
        "labels_text": "Entrypoint for the thread Immediately create a new event loop and pas control to rununtilshutdown"
    },
    {
        "input_text": "summarize: def get_global_loop() -> EventLoopThread:        global GLOBAL_LOOP    # Create a new worker on first call or if the existing worker is dead    if (        GLOBAL_LOOP is None        or not GLOBAL_LOOP.thread.is_alive()        or GLOBAL_LOOP._shutdown_event.is_set()    ):        GLOBAL_LOOP = EventLoopThread(daemon=True, name=\"GlobalEventLoopThread\")        GLOBAL_LOOP.start()    return GLOBAL_LOOP",
        "labels_text": "Get the global loop thread Creates a new one if there is not one available"
    },
    {
        "input_text": "summarize: def in_global_loop() -> bool:        if GLOBAL_LOOP is None:        # Avoid creating a global loop if there isn't one        return False    return get_global_loop()._loop == get_running_loop()",
        "labels_text": "Check if called from the global loop"
    },
    {
        "input_text": "summarize: def get_run_sync_loop() -> EventLoopThread:        global RUN_SYNC_LOOP    # Create a new worker on first call or if the existing worker is dead    if (        RUN_SYNC_LOOP is None        or not RUN_SYNC_LOOP.thread.is_alive()        or RUN_SYNC_LOOP._shutdown_event.is_set()    ):        RUN_SYNC_LOOP = EventLoopThread(daemon=True, name=\"RunSyncEventLoopThread\")        RUN_SYNC_LOOP.start()    return RUN_SYNC_LOOP",
        "labels_text": "Get the runsync loop thread Creates a new one if there is not one available"
    },
    {
        "input_text": "summarize: def in_run_sync_loop() -> bool:        if RUN_SYNC_LOOP is None:        # Avoid creating a global loop if there isn't one        return False    return get_run_sync_loop()._loop == get_running_loop()",
        "labels_text": "Check if called from the global loop"
    },
    {
        "input_text": "summarize: def wait_for_global_loop_exit(timeout: Optional[float] = None) -> None:        loop_thread = get_global_loop()    loop_thread.shutdown()    if threading.get_ident() == loop_thread.thread.ident:        raise RuntimeError(\"Cannot wait for the loop thread from inside itself.\")    loop_thread.thread.join(timeout)",
        "labels_text": "Shutdown the global loop and wait for it to exit"
    },
    {
        "input_text": "summarize: def add_waiter_for_thread(waiter: \"Waiter\", thread: threading.Thread):        if thread not in _WAITERS_BY_THREAD:        _WAITERS_BY_THREAD[thread] = deque()    _WAITERS_BY_THREAD[thread].append(waiter)",
        "labels_text": "Add a waiter for a thread"
    },
    {
        "input_text": "summarize: def wait(self) -> Union[Awaitable[None], None]:                raise NotImplementedError()",
        "labels_text": "Wait for the call to finish Watch for and execute any waiting callback"
    },
    {
        "input_text": "summarize: def add_done_callback(self, callback: Call) -> Call:                raise NotImplementedError()",
        "labels_text": "Schedule a call to run when the waiter is done waiting If the waiter is already done a RuntimeError error will be thrown"
    },
    {
        "input_text": "summarize: def submit(self, call: Call):                if self.call_is_done():            raise RuntimeError(f\"The call {self._call} is already done.\")        self._queue.put_nowait(call)        call.set_runner(self)        return call",
        "labels_text": "Submit a callback to execute while waiting"
    },
    {
        "input_text": "summarize: def __eq__(self, other: Any) -> bool:                copy_dict = self.model_dump(exclude=self._reset_fields)        if isinstance(other, PrefectBaseModel):            return copy_dict == other.model_dump(exclude=other._reset_fields)        if isinstance(other, BaseModel):            return copy_dict == other.model_dump()        else:            return copy_dict == other",
        "labels_text": "Equaltiy operator that ignores the resettable field of the PrefectBaseModel NOTE this equality operator will only be applied if the PrefectBaseModel is the lefthand operand This is a limitation of Python"
    },
    {
        "input_text": "summarize: def reset_fields(self: Self) -> Self:                return self.model_copy(            update={                field: self.model_fields[field].get_default(call_default_factory=True)                for field in self._reset_fields            }        )",
        "labels_text": "Reset the field of the model that are in the resetfields set Returns PrefectBaseModel A new instance of the model with the reset field"
    },
    {
        "input_text": "summarize: def orjson_dumps(v: Any, *, default: Any) -> str:        return orjson.dumps(v, default=default).decode()",
        "labels_text": "Utility for dumping a value to JSON using orjson orjsondumps return byte to match standard jsondumps we need to decode"
    },
    {
        "input_text": "summarize: def orjson_dumps_extra_compatible(v: Any, *, default: Any) -> str:        return orjson.dumps(        v, default=default, option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY    ).decode()",
        "labels_text": "Utility for dumping a value to JSON using orjson but allows for nonstring key this is helpful for situation like panda dataframes which can result in nonstring key numpy type for serializing numpy array orjsondumps return byte to match standard jsondumps we need to decode"
    },
    {
        "input_text": "summarize: def validate_schema(schema: dict):        try:        if schema is not None:            # Most closely matches the schemas generated by pydantic            jsonschema.Draft4Validator.check_schema(schema)    except jsonschema.SchemaError as exc:        raise ValueError(            \"The provided schema is not a valid json schema. Schema error:\"            f\" {exc.message}\"        ) from exc",
        "labels_text": "Validate that the provided schema is a valid json schema Args schema The schema to validate Raises ValueError If the provided schema is not a valid json schema"
    },
    {
        "input_text": "summarize: def handle_openapi_schema(value: Optional[\"ParameterSchema\"]) -> \"ParameterSchema\":        from prefect.utilities.callables import ParameterSchema    if value is None:        return ParameterSchema()    return value",
        "labels_text": "This method ensures setting a value of None is handled gracefully"
    },
    {
        "input_text": "summarize: def validate_parameters_conform_to_schema(value: dict, values: dict) -> dict:        if values.get(\"enforce_parameter_schema\"):        validate_values_conform_to_schema(            value, values.get(\"parameter_openapi_schema\"), ignore_required=True        )    return value",
        "labels_text": "Validate that the parameter conform to the parameter schema"
    },
    {
        "input_text": "summarize: def validate_parameter_openapi_schema(value: dict, values: dict) -> dict:        if values.get(\"enforce_parameter_schema\"):        validate_schema(value)    return value",
        "labels_text": "Validate that the parameteropenapischema is a valid json schema"
    },
    {
        "input_text": "summarize: def validate_k8s_job_required_components(cls, value: Dict[str, Any]):        from prefect.utilities.pydantic import JsonPatch    patch = JsonPatch.from_diff(value, cls.base_job_manifest())    missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])    if missing_paths:        raise ValueError(            \"Job is missing required attributes at the following paths: \"            f\"{', '.join(missing_paths)}\"        )    return value",
        "labels_text": "Validate that a Kubernetes job manifest ha all required component"
    },
    {
        "input_text": "summarize: def set_default_namespace(values: dict) -> dict:        job = values.get(\"job\")    namespace = values.get(\"namespace\")    job_namespace = job[\"metadata\"].get(\"namespace\") if job else None    if not namespace and not job_namespace:        values[\"namespace\"] = \"default\"    return values",
        "labels_text": "Set the default namespace for a Kubernetes job if not provided"
    },
    {
        "input_text": "summarize: def set_default_image(values: dict) -> dict:        job = values.get(\"job\")    image = values.get(\"image\")    job_image = (        job[\"spec\"][\"template\"][\"spec\"][\"containers\"][0].get(\"image\") if job else None    )    if not image and not job_image:        values[\"image\"] = get_prefect_image_name()    return values",
        "labels_text": "Set the default image for a Kubernetes job if not provided"
    },
    {
        "input_text": "summarize: def get_or_create_state_name(v: str, values: dict) -> str:        # if `type` is not in `values` it means the `type` didn't pass its own    # validation check and an error will be raised after this function is called    if v is None and values.get(\"type\"):        v = \" \".join([v.capitalize() for v in values.get(\"type\").value.split(\"_\")])    return v",
        "labels_text": "If a name is not provided use the type"
    },
    {
        "input_text": "summarize: def validate_picklelib_and_modules(values: dict) -> dict:        if values.get(\"picklelib\") != \"cloudpickle\" and values.get(\"pickle_modules\"):        raise ValueError(            \"`pickle_modules` cannot be used without 'cloudpickle'. Got\"            f\" {values.get('picklelib')!r}.\"        )    return values",
        "labels_text": "Prevents module from being specified if picklelib is not cloudpickle"
    },
    {
        "input_text": "summarize: def set_run_policy_deprecated_fields(values: dict) -> dict:        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:        values[\"retries\"] = values[\"max_retries\"]    if (        not values.get(\"retry_delay\", None)        and values.get(\"retry_delay_seconds\", 0) != 0    ):        values[\"retry_delay\"] = values[\"retry_delay_seconds\"]    return values",
        "labels_text": "If deprecated field are provided populate the corresponding new field to preserve orchestration behavior"
    },
    {
        "input_text": "summarize: def return_v_or_none(v: Optional[str]) -> Optional[str]:        if not v:        return None    return v",
        "labels_text": "Make sure that empty string are treated a None"
    },
    {
        "input_text": "summarize: def validate_command(v: str) -> Path:        if v:        return relative_path_to_current_platform(v)    return v",
        "labels_text": "Make sure that the working directory is formatted for the current platform"
    },
    {
        "input_text": "summarize: def pytest_runtest_call(item):        yield    assert_lifespan_is_not_left_open()",
        "labels_text": "This hook will be called within the test run Allowing u to raise error or add assertion to every test On error the test will be marked a failed If we used a fixture instead the test teardown would report an error instead"
    },
    {
        "input_text": "summarize: def test_database_connection_url(generate_test_database_connection_url):        url = generate_test_database_connection_url    if url is None:        yield None    else:        with temporary_settings({PREFECT_API_DATABASE_CONNECTION_URL: url}):            yield url",
        "labels_text": "Update the setting for the database connection url to the generated value from generatetestdatabaseconnectionurl This must be separate from the generation of the test url because async fixture are run in a separate context from the test suite"
    },
    {
        "input_text": "summarize: def reset_registered_blocks():        registry = get_registry_for_type(Block)    before = registry.copy()    yield    registry.clear()    registry.update(before)",
        "labels_text": "Ensures each test only ha type that were registered at module initialization"
    },
    {
        "input_text": "summarize: def caplog(caplog):        from prefect.logging.configuration import PROCESS_LOGGING_CONFIG    for name, logger_config in PROCESS_LOGGING_CONFIG[\"loggers\"].items():        if not logger_config.get(\"propagate\", True):            logger = logging.getLogger(name)            if caplog.handler not in logger.handlers:                logger.handlers.append(caplog.handler)    yield caplog",
        "labels_text": "Overrides caplog to apply to all of our logger that do not propagate and consequently would not be captured by caplog"
    },
    {
        "input_text": "summarize: def test_version_none_if_source_file_cannot_be_determined(        self, monkeypatch, sourcefile    ):                monkeypatch.setattr(            \"prefect.flows.inspect.getsourcefile\", MagicMock(return_value=sourcefile)        )        f = Flow(name=\"test\", fn=lambda **kwargs: 42)        assert f.version is None",
        "labels_text": "getsourcefile will return None when function are defined interactively or other value on Windows"
    },
    {
        "input_text": "summarize: def test_serve_starts_a_runner(self, mock_runner_start):                self.flow.serve(\"test\")        mock_runner_start.assert_awaited_once()",
        "labels_text": "This test only make sure Runnerstart is called The actual functionality of the runner is tested in testrunnerpy"
    },
    {
        "input_text": "summarize: def test_redact_substr_from_collections(        self, caplog, raw_log_record, expected_log_record    ):                @flow()        def test_log_list():            logger = get_run_logger()            logger.info(raw_log_record)        with temporary_settings({PREFECT_API_KEY: \"super-mega-admin-key\"}):            test_log_list()        assert str(expected_log_record) in caplog.text",
        "labels_text": "This is a regression test for httpsgithubcomPrefectHQprefectissues"
    },
    {
        "input_text": "summarize: def test_without_disable_logger(caplog):        logger = logging.getLogger(\"griffe.agents.nodes\")    def function_with_logging(logger):        assert not logger.disabled        logger.critical(\"it's enabled!\")        return 42    function_with_logging(logger)    assert not logger.disabled    assert (\"griffe.agents.nodes\", 50, \"it's enabled!\") in caplog.record_tuples",
        "labels_text": "Sanity test to double check whether caplog actually work so can be more confident in the asserts in testdisablelogger"
    },
    {
        "input_text": "summarize: def test_save_profiles_does_not_include_default(self, temporary_profiles_path):                save_profiles(ProfilesCollection(active=None, profiles=[]))        assert \"profiles.default\" not in temporary_profiles_path.read_text()",
        "labels_text": "Including the default ha a tendency to bake in setting the user may not want and can prevent them from gaining new default"
    },
    {
        "input_text": "summarize: def test_validate_settings_ignores_environment_variables(self, monkeypatch):                monkeypatch.setenv(\"PREFECT_SERVER_API_PORT\", \"1234\")        profile = Profile(name=\"test\", settings={PREFECT_SERVER_API_PORT: \"foo\"})        with pytest.raises(pydantic.ValidationError):            profile.validate_settings()",
        "labels_text": "If using contextuseprofile to validate setting environment variable may override the setting and hide validation error"
    },
    {
        "input_text": "summarize: def test_logs_message_when_submitted_tasks_end_in_pending(self, caplog):                @task        def find_palindromes():                        num = 10            while True:                _ = str(num) == str(num)[::-1]                num += 1        @flow        def test_flow():            find_palindromes.submit()        test_flow()        assert (            \"Please wait for all submitted tasks to complete before exiting your flow\"            in caplog.text        )",
        "labels_text": "If submitted task arent waited on before a flow exit they may fail to run because theyre transition from PENDING to RUNNING is denied This test ensures that a message is logged when this happens"
    },
    {
        "input_text": "summarize: def test_handles_recursively_submitted_tasks(self):                @task        def recursive_task(n):            if n == 0:                return n            time.sleep(0.1)            future = recursive_task.submit(n - 1)            return future.result()        @flow        def test_flow():            return recursive_task.submit(33)        assert test_flow().result() == 0",
        "labels_text": "Regression test for httpsgithubcomPrefectHQprefectissues This test ensures that the ThreadPoolTaskRunner doesnt place an upper limit on the number of submitted task active at once The highest default max worker on a ThreadPoolExecutor is so this test submits task recursively which will deadlock without the ThreadPoolTaskRunner setting the maxworkers to sysmaxsize"
    },
    {
        "input_text": "summarize: def not_enough_open_files() -> bool:        try:        import resource    except ImportError:        # resource limits is not a concept on all systems, notably Windows        return False    soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE)    return soft_limit < 512 or hard_limit < 512",
        "labels_text": "The current process doe not currently allow enough open file for this test You can increase the number of open file with ulimit n"
    },
    {
        "input_text": "summarize: def test_all_triggers_represented():        assert (        set(CLIENT_TRIGGER_TYPES)        == set(type(t) for t in EXAMPLE_TRIGGERS) | CLOUD_ONLY_TRIGGER_TYPES    )",
        "labels_text": "Ensures that we have an example for all clientside trigger type"
    },
    {
        "input_text": "summarize: def test_all_deployment_triggers_represented():        assert (        set(DEPLOYMENT_TRIGGER_TYPES)        == set(type(t) for t in EXAMPLE_DEPLOYMENT_TRIGGERS)        | CLOUD_ONLY_DEPLOYMENT_TRIGGER_TYPES    )",
        "labels_text": "Ensures that we have an example for all deploymnet trigger type"
    },
    {
        "input_text": "summarize: def test_all_actions_represented():        assert (        set(CLIENT_ACTION_TYPES)        == set(type(a) for a in EXAMPLE_ACTIONS) | CLOUD_ONLY_ACTION_TYPES    )",
        "labels_text": "Ensures that we have an example for all clientside action type"
    },
    {
        "input_text": "summarize: def test_worker_instance_ephemeral_prefect_events_client(enable_ephemeral_server):        worker = EventsWorker.instance()    assert worker.client_type == PrefectEventsClient",
        "labels_text": "Getting an instance of the worker with ephemeral server mode enabled should return a PrefectEventsClient pointing to the subprocess server"
    },
    {
        "input_text": "summarize: def assert_message_represents_event(message: Message, event: ReceivedEvent):        assert message.data    assert ReceivedEvent.model_validate_json(message.data) == event    assert message.attributes    assert message.attributes[\"id\"] == str(event.id)    assert message.attributes[\"event\"] == event.event",
        "labels_text": "Confirms that the message adequately represents the event"
    },
    {
        "input_text": "summarize: def triggers_disabled():        with temporary_settings({PREFECT_API_SERVICES_TRIGGERS_ENABLED: False}):        yield",
        "labels_text": "Because this test will modify automation we want to suppress the async notification they make to avoid a nondeterministic event loop issue in the test suite where the aftercommit callback is invoked during teardown Disabling the trigger service for the duration of this test suite will accomplish this"
    },
    {
        "input_text": "summarize: def ignore_prefect_deprecation_warnings():        with warnings.catch_warnings():        warnings.simplefilter(\"ignore\", category=PrefectDeprecationWarning)        yield",
        "labels_text": "Ignore deprecation warning from the agent module to avoid test failure"
    },
    {
        "input_text": "summarize: def registry() -> Generator[str, None, None]:        registry_url = \"http://localhost:5555\"    if not _wait_for_registry(registry_url):        raise RuntimeError(            \"Docker registry did not become ready in time. If you're running \"            \"the tests locally, make sure you have the registry running with \"            \"`docker compose up -d`.\"        )    yield registry_url",
        "labels_text": "Return the URL for the local Docker registry"
    },
    {
        "input_text": "summarize: def reset_api_log_handler():        yield    APILogHandler.flush()    APILogHandler.workers = {}",
        "labels_text": "Since we have a singleton worker for the runtime of the process we must reset it to None between test and we should flush log before each test exit to stop the logging thread"
    },
    {
        "input_text": "summarize: def enable_api_log_handler_if_marked(request):        marker = request.node.get_closest_marker(\"enable_api_log_handler\")    if marker is not None:        with temporary_settings(updates={PREFECT_LOGGING_TO_API_ENABLED: True}):            yield True    else:        yield False",
        "labels_text": "The APILogHandler is disabled during testing by default to reduce overhead Test function or class can be marked with pytestmarkenableapiloghandler to indicate that they need the handler to be reenabled because they are testing it functionality"
    },
    {
        "input_text": "summarize: def test_flow_resultlike_result_is_retained(    persist_result: bool, resultlike, tmp_path: Path):        @flow(persist_result=persist_result)    def my_flow():        return resultlike    result = my_flow()    assert result == resultlike",
        "labels_text": "Since Pydantic will coerce dictionary into BaseResult type we need to be sure that user dicts that look like a bit like result do not cause problem"
    },
    {
        "input_text": "summarize: def disable_fetch_by_default():        with temporary_settings({PREFECT_ASYNC_FETCH_STATE_RESULT: False}):        yield",
        "labels_text": "The test suite default to the future behavior For these test we enable the default user behavior"
    },
    {
        "input_text": "summarize: def default_persistence_off():        with temporary_settings({PREFECT_RESULTS_PERSIST_BY_DEFAULT: False}):        yield",
        "labels_text": "Many test return result factory which arent serialiable When we switched the default persistence setting to True this caused test to fail"
    },
    {
        "input_text": "summarize: def test_task_resultlike_result_is_retained(persist_result, resultlike):        @task(persist_result=persist_result)    def my_task():        return resultlike    @flow    def my_flow():        return quote(my_task())    result = my_flow().unquote()    assert result == resultlike",
        "labels_text": "Since Pydantic will coerce dictionary into BaseResult type we need to be sure that user dicts that look like a bit like result do not cause problem"
    },
    {
        "input_text": "summarize: def dummy_flow_1():        pass",
        "labels_text": "Im just here for test"
    },
    {
        "input_text": "summarize: def generate_lower_bounds(script_path):        globals = runpy.run_path(str(script_path))    return globals[\"generate_lower_bounds\"]",
        "labels_text": "Retrieves the function that generates lower bound"
    },
    {
        "input_text": "summarize: def test_app_generates_correct_api_openapi_schema():        schema = create_app(ephemeral=True).openapi()    assert len(schema[\"paths\"].keys()) > 1    assert all([p.startswith(\"/api/\") for p in schema[\"paths\"].keys()])",
        "labels_text": "Test that help detect situation in which our REST API reference doc fail to render properly"
    },
    {
        "input_text": "summarize: def template_optional_field_with_default():        return {        \"job_configuration\": {            \"laptop_choice\": \"{{ laptop_choice }}\",        },        \"variables\": {            \"properties\": {                \"laptop_choice\": {                    \"type\": \"string\",                    \"title\": \"Laptop Choice\",                    \"default\": \"macbook pro\",                },            },            \"required\": [],        },    }",
        "labels_text": "This is the broken JSON schema that Pydantic v generates for both of these field type laptopchoice str None macbook pro laptopchoice str macbook pro The first field can accept None but the second field cannot yet the JSON schema doesnt let u differentiate between the two"
    },
    {
        "input_text": "summarize: def test_url_encoded_variables(self, client):                x = \"| ; \ud83d\udc4d\"        response = client.get(f\"/{x}\")        quoted_response = client.get(urllib.parse.quote(f\"/{x}\"))        assert x == response.json() == quoted_response.json()",
        "labels_text": "FastAPI automatically handle urlencoded variable"
    },
    {
        "input_text": "summarize: def test_context_carries_to_async_frame(self):                class MyVar(ContextModel):            __var__ = ContextVar(\"my_var\")            x: int = 1        async def load_var():            return MyVar.get().x        async def parent():            with MyVar(x=42):                return run_coro_as_sync(load_var())        # this has to be run via asyncio.run because        # otherwise the context is maintained automatically        assert asyncio.run(parent()) == 42",
        "labels_text": "Ensures that ContextVars set in a parent scope of runsync are automatically carried over to the async frame"
    },
    {
        "input_text": "summarize: def test_run_coro_as_sync_runs_in_run_sync_thread(self):                run_sync_loop = get_run_sync_loop()        async def foo():            return (threading.current_thread(), asyncio.get_running_loop())        assert run_coro_as_sync(foo()) == (run_sync_loop.thread, run_sync_loop._loop)",
        "labels_text": "runcoroassync should always submit coros to the same runsync loop thread"
    },
    {
        "input_text": "summarize: def test_visit_collection_with_private_pydantic_attributes(self):                input = PrivatePydantic(x=2)        input._y = 3        input._z = 4        result = visit_collection(input, visit_fn=visit_even_numbers, return_data=False)        assert EVEN == {2}, \"Only the public field should be visited\"        assert result is None, \"Data should not be returned\"        # The original model should not be mutated        assert input._y == 3        assert input._z == 4",
        "labels_text": "We should not visit private field on Pydantic model"
    },
    {
        "input_text": "summarize: def test_visit_collection_simple_identity(self, val):                result = visit_collection(val, lambda x: x, return_data=True)        # same object, unmodified        assert result is val",
        "labels_text": "test that visit collection doe not modify an object at all in the identity case"
    },
    {
        "input_text": "summarize: def test_get_open_file_limit():        limit = get_open_file_limit()    # The functions that check the open file limit on either Windows or Unix    # have an 'Any' return type, so this assertion ensures any changes to the    # function don't break its contract.    assert isinstance(limit, int)    # It shouldn't be possible to have a negative open file limit.    assert limit >= 0    # The open file limit should not equal the default value of 200    # returned if an error occurs.    assert limit != 200",
        "labels_text": "Test that we can get the open file limit Although this is a simple test it ensures that the function work a expected on both Windows and Unix"
    },
    {
        "input_text": "summarize: def test_safe_load_namespace_does_not_execute_function_body():        source_code = dedent(            )    # should not raise any errors    namespace = safe_load_namespace(source_code)    assert not namespace[\"you_done_goofed\"]",
        "labels_text": "Regression test for httpsgithubcomPrefectHQprefectissues"
    },
    {
        "input_text": "summarize: def test_hydrate_unregistered_prefect_kind(self):                assert (            hydrate({\"__prefect_kind\": \"never-heard-of-it\", \"value\": \"hello\"})            == \"hello\"        )",
        "labels_text": "We pas through the value key of any unrecognized prefectkind"
    },
    {
        "input_text": "summarize: def test_pydantic_v1_required_int(self):                schema = {            \"title\": \"Parameters\",            \"type\": \"object\",            \"properties\": {                \"required_int\": {                    \"title\": \"required_int\",                    \"position\": 0,                    \"type\": \"integer\",                }            },            \"required\": [\"required_int\"],        }        # no change        preprocessed_schema = preprocess_schema(schema)        assert schema == preprocessed_schema",
        "labels_text": "requiredint int"
    },
    {
        "input_text": "summarize: def test_pydantic_v1_optional_int(self):                schema = {            \"title\": \"Parameters\",            \"type\": \"object\",            \"properties\": {                \"optional_int\": {                    \"title\": \"optional_int\",                    \"default\": 10,                    \"position\": 0,                    \"type\": \"integer\",                }            },        }        # no change        preprocessed_schema = preprocess_schema(schema)        assert preprocessed_schema == schema",
        "labels_text": "optionalint int"
    },
    {
        "input_text": "summarize: def test_pydantic_v2_required_int(self):                schema = {            \"title\": \"Parameters\",            \"type\": \"object\",            \"properties\": {                \"required_int\": {                    \"position\": 0,                    \"title\": \"required_int\",                    \"type\": \"integer\",                }            },            \"required\": [\"required_int\"],        }        # no change        preprocessed_schema = preprocess_schema(schema)        assert preprocessed_schema == schema",
        "labels_text": "requiredint int"
    },
    {
        "input_text": "summarize: def test_pydantic_v2_optional_int(self):                schema = {            \"title\": \"Parameters\",            \"type\": \"object\",            \"properties\": {                \"optional_int\": {                    \"default\": 10,                    \"position\": 0,                    \"title\": \"optional_int\",                    \"type\": \"integer\",                }            },        }        # no change        preprocessed_schema = preprocess_schema(schema)        assert preprocessed_schema == schema",
        "labels_text": "optionalint int"
    },
    {
        "input_text": "summarize: def test_pydantic_v2_optional_int_default_none(self):                schema = {            \"title\": \"Parameters\",            \"type\": \"object\",            \"properties\": {                \"optional_int_default_none\": {                    \"default\": None,                    \"position\": 0,                    \"title\": \"optional_int_default_none\",                    \"type\": \"integer\",                }            },        }        # no change        preprocessed_schema = preprocess_schema(schema)        assert preprocessed_schema == schema",
        "labels_text": "optionalintdefaultnoneOptionalint None"
    },
    {
        "input_text": "summarize: def read_pid_file(pid_file):        if os.path.isfile(pid_file):        with open(pid_file, \"r\") as fp:            pid = fp.read().strip()        if pid:            pid = int(pid)            return pid    else:        return 0",
        "labels_text": "return pid a int or"
    },
    {
        "input_text": "summarize: def main(cmd_args=sys.argv[1:]):        args = _parse_args(cmd_args)    core_args = (args.userdir, args.tempdir, args.storagedir, args.debug, args.reset, args.dry_run)    if args.quit:        quit_instance(args.pidfile)    else:        run(core_args, args.daemon, args.pidfile)",
        "labels_text": "Entry point for consolescripts"
    },
    {
        "input_text": "summarize: def remove_job(self, d):                index = -1        for i, j in enumerate(self.queue):            if j[1].deferred == d:                index = i        if index >= 0:            del self.queue[index]            return True        return False",
        "labels_text": "param d deferred object return if job wa deleted"
    },
    {
        "input_text": "summarize: def get(self):                try:            el = heappop(self.queue)            return el        except IndexError:            return None, None",
        "labels_text": "return element or None"
    },
    {
        "input_text": "summarize: def get_config_value(self, category, option, section=\"core\"):                if section == \"core\":            value = self.pyload.config[category][option]        else:            value = self.pyload.config.get_plugin(category, option)        return value",
        "labels_text": "Retrieve config value param category name of category or plugin param option config option param section plugin or core return config value"
    },
    {
        "input_text": "summarize: def get_config(self):                return self._convert_config_format(self.pyload.config.config)",
        "labels_text": "Retrieves complete config of core return list of ConfigSection"
    },
    {
        "input_text": "summarize: def get_config_dict(self):                return self.pyload.config.config",
        "labels_text": "Retrieves complete config in dict format not for RPC return dict"
    },
    {
        "input_text": "summarize: def get_plugin_config(self):                return self._convert_config_format(self.pyload.config.plugin)",
        "labels_text": "Retrieves complete config for all plugins return list of ConfigSection"
    },
    {
        "input_text": "summarize: def get_plugin_config_dict(self):                return self.pyload.config.plugin",
        "labels_text": "Plugin config a dict not for RPC return dict"
    },
    {
        "input_text": "summarize: def pause_server(self):                self.pyload.thread_manager.pause = True",
        "labels_text": "Pause server It wont start any new downloads but nothing get aborted"
    },
    {
        "input_text": "summarize: def unpause_server(self):                self.pyload.thread_manager.pause = False",
        "labels_text": "Unpause server New Downloads will be started"
    },
    {
        "input_text": "summarize: def toggle_pause(self):                self.pyload.thread_manager.pause ^= True        return self.pyload.thread_manager.pause",
        "labels_text": "Toggle pause state return new pause state"
    },
    {
        "input_text": "summarize: def toggle_reconnect(self):                self.pyload.config.toggle(\"reconnect\", \"enabled\")        return self.pyload.config.get(\"reconnect\", \"enabled\")",
        "labels_text": "Toggle reconnect activation return new reconnect state"
    },
    {
        "input_text": "summarize: def free_space(self):                return fs.free_space(self.pyload.config.get(\"general\", \"storage_folder\"))",
        "labels_text": "Available free space at download directory in byte"
    },
    {
        "input_text": "summarize: def get_server_version(self):                return self.pyload.version",
        "labels_text": "pyLoad Core version"
    },
    {
        "input_text": "summarize: def kill(self):                self.pyload._do_exit = True",
        "labels_text": "Clean way to quit pyLoad"
    },
    {
        "input_text": "summarize: def restart(self):                self.pyload._do_restart = True",
        "labels_text": "Restart pyload core"
    },
    {
        "input_text": "summarize: def is_time_download(self):                start = self.pyload.config.get(\"download\", \"start_time\").split(\":\")        end = self.pyload.config.get(\"download\", \"end_time\").split(\":\")        return seconds.compare(start, end)",
        "labels_text": "Checks if pyload will start new downloads according to time in config return bool"
    },
    {
        "input_text": "summarize: def is_time_reconnect(self):                start = self.pyload.config.get(\"reconnect\", \"start_time\").split(\":\")        end = self.pyload.config.get(\"reconnect\", \"end_time\").split(\":\")        return seconds.compare(start, end) and self.pyload.config.get(            \"reconnect\", \"enabled\"        )",
        "labels_text": "Checks if pyload will try to make a reconnect return bool"
    },
    {
        "input_text": "summarize: def parse_urls(self, html=None, url=None):                urls = []        if html:            urls += urlmatcher.findall(html)        if url:            page = get_url(url)            urls += urlmatcher.findall(page)        # remove duplicates        return self.check_urls(set(urls))",
        "labels_text": "Parses html content or any arbitrary text for link and return result of checkurls param html html source param url url to load html source from return"
    },
    {
        "input_text": "summarize: def check_urls(self, urls):                data = self.pyload.plugin_manager.parse_urls(urls)        plugins = {}        for url, plugin in data:            if plugin in plugins:                plugins[plugin].append(url)            else:                plugins[plugin] = [url]        return plugins",
        "labels_text": "Gets url and return pluginname mapped to list of match url param url return plugin url"
    },
    {
        "input_text": "summarize: def check_online_status_container(self, urls, container, data):                with open(            os.path.join(                self.pyload.config.get(\"general\", \"storage_folder\"), \"tmp_\" + container            ),            \"wb\",        ) as th:            th.write(data)        return self.check_online_status(urls + [th.name])",
        "labels_text": "check online status of url and a submitted container file param url list of url param container container file name param data file content return online check"
    },
    {
        "input_text": "summarize: def poll_results(self, rid):                result = self.pyload.thread_manager.get_info_result(rid)        if \"ALL_INFO_FETCHED\" in result:            del result[\"ALL_INFO_FETCHED\"]            return OnlineCheck(-1, result)        else:            return OnlineCheck(rid, result)",
        "labels_text": "Polls the result available for ResultID param rid ResultID return OnlineCheck if rid is then no more data available"
    },
    {
        "input_text": "summarize: def generate_packages(self, links):                result = parse_names((x, x) for x in links)        return result",
        "labels_text": "Parses link generates package name from url param link list of url return package name mapped to url"
    },
    {
        "input_text": "summarize: def generate_and_add_packages(self, links, dest=Destination.COLLECTOR):                return [            self.add_package(name, urls, dest)            for name, urls in self.generate_packages(links).items()        ]",
        "labels_text": "Generates and add package param link list of url param dest Destination return list of package id"
    },
    {
        "input_text": "summarize: def check_and_add_packages(self, links, dest=Destination.COLLECTOR):                data = self.pyload.plugin_manager.parse_urls(links)        self.pyload.thread_manager.create_result_thread(data, True)",
        "labels_text": "Checks online status retrieves name and will add package Because of these package are not added immediately only for internal use param link list of url param dest Destination return None"
    },
    {
        "input_text": "summarize: def get_package_info(self, package_id):                data = self.pyload.files.get_package_data(int(package_id))        if not data:            raise PackageDoesNotExists(package_id)        pdata = PackageData(            data[\"id\"],            data[\"name\"],            data[\"folder\"],            data[\"site\"],            data[\"password\"],            data[\"queue\"],            data[\"order\"],            fids=[int(x) for x in data[\"links\"]],        )        return pdata",
        "labels_text": "Returns information about package without detailed information about containing file param packageid package id return PackageData with fid attribute"
    },
    {
        "input_text": "summarize: def get_file_data(self, file_id):                info = self.pyload.files.get_file_data(int(file_id))        if not info:            raise FileDoesNotExists(file_id)        fileinfo = list(info.values())[0]        fdata = self._convert_py_file(fileinfo)        return fdata",
        "labels_text": "Get complete information about a specific file param fileid file id return FileData"
    },
    {
        "input_text": "summarize: def delete_files(self, file_ids):                for id in file_ids:            self.pyload.files.delete_link(int(id))        self.pyload.files.save()",
        "labels_text": "Deletes several file entry from pyload param fileids list of file id"
    },
    {
        "input_text": "summarize: def delete_packages(self, package_ids):                for id in package_ids:            self.pyload.files.delete_package(int(id))        self.pyload.files.save()",
        "labels_text": "Deletes package and containing link param packageids list of package id"
    },
    {
        "input_text": "summarize: def get_queue(self):                return [            PackageData(                pack[\"id\"],                pack[\"name\"],                pack[\"folder\"],                pack[\"site\"],                pack[\"password\"],                pack[\"queue\"],                pack[\"order\"],                pack[\"linksdone\"],                pack[\"sizedone\"],                pack[\"sizetotal\"],                pack[\"linkstotal\"],            )            for pack in self.pyload.files.get_info_data(Destination.QUEUE).values()        ]",
        "labels_text": "Returns info about queue and package not about file see getqueuedata or getpackagedata instead return list of PackageInfo"
    },
    {
        "input_text": "summarize: def get_collector(self):                return [            PackageData(                pack[\"id\"],                pack[\"name\"],                pack[\"folder\"],                pack[\"site\"],                pack[\"password\"],                pack[\"queue\"],                pack[\"order\"],                pack[\"linksdone\"],                pack[\"sizedone\"],                pack[\"sizetotal\"],                pack[\"linkstotal\"],            )            for pack in self.pyload.files.get_info_data(Destination.COLLECTOR).values()        ]",
        "labels_text": "same a getqueue for collector return list of PackageInfo"
    },
    {
        "input_text": "summarize: def add_files(self, package_id, links):                self.pyload.files.add_links(links, int(package_id))        self.pyload.log.info(            self._(\"Added {count:d} links to package #{package:d} \").format(                count=len(links), package=package_id            )        )        self.pyload.files.save()",
        "labels_text": "Adds file to specific package param packageid package id param link list of url"
    },
    {
        "input_text": "summarize: def push_to_queue(self, package_id):                self.pyload.files.set_package_location(package_id, Destination.QUEUE)",
        "labels_text": "Moves package from Collector to Queue param packageid package id"
    },
    {
        "input_text": "summarize: def pull_from_queue(self, package_id):                self.pyload.files.set_package_location(package_id, Destination.COLLECTOR)",
        "labels_text": "Moves package from Queue to Collector param packageid package id"
    },
    {
        "input_text": "summarize: def restart_package(self, package_id):                self.pyload.files.restart_package(int(package_id))",
        "labels_text": "Restarts a package reset every containing file param packageid package id"
    },
    {
        "input_text": "summarize: def restart_file(self, file_id):                self.pyload.files.restart_file(int(file_id))",
        "labels_text": "Resets file status so it will be downloaded again param fileid file id"
    },
    {
        "input_text": "summarize: def recheck_package(self, package_id):                self.pyload.files.recheck_package(int(package_id))",
        "labels_text": "Probes online status of all file in a package also a default action when package is added param packageid return"
    },
    {
        "input_text": "summarize: def stop_all_downloads(self):                pyfiles = list(self.pyload.files.cache.values())        for pyfile in pyfiles:            pyfile.abort_download()",
        "labels_text": "Aborts all running downloads"
    },
    {
        "input_text": "summarize: def stop_downloads(self, file_ids):                pyfiles = list(self.pyload.files.cache.values())        for pyfile in pyfiles:            if pyfile.id in file_ids:                pyfile.abort_download()",
        "labels_text": "Aborts specific downloads param fileids list of file id return"
    },
    {
        "input_text": "summarize: def set_package_name(self, package_id, name):                pack = self.pyload.files.get_package(package_id)        pack.name = name        pack.sync()",
        "labels_text": "Renames a package param packageid package id param name new package name"
    },
    {
        "input_text": "summarize: def move_package(self, destination, package_id):                try:            dest = Destination(destination)        except ValueError:            pass        else:            self.pyload.files.set_package_location(package_id, dest)",
        "labels_text": "Set a new package location param destination Destination param packageid package id"
    },
    {
        "input_text": "summarize: def move_files(self, file_ids, package_id):                # TODO: implement        pass",
        "labels_text": "Move multiple file to another package param fileids list of file id param packageid destination package return"
    },
    {
        "input_text": "summarize: def upload_container(self, filename, data):                with open(            os.path.join(                self.pyload.config.get(\"general\", \"storage_folder\"), \"tmp_\" + filename            ),            \"wb\",        ) as th:            th.write(data)        self.add_package(th.name, [th.name], Destination.COLLECTOR)",
        "labels_text": "Uploads and add a container file to pyLoad param filename file name extension is important so it can correctly decrypt param data file content"
    },
    {
        "input_text": "summarize: def order_package(self, package_id, position):                self.pyload.files.reorder_package(package_id, position)",
        "labels_text": "Gives a package a new position param packageid package id param position"
    },
    {
        "input_text": "summarize: def order_file(self, file_id, position):                self.pyload.files.reorder_file(file_id, position)",
        "labels_text": "Gives a new position to a file within it package param fileid file id param position"
    },
    {
        "input_text": "summarize: def set_package_data(self, package_id, data):                p = self.pyload.files.get_package(package_id)        if not p:            raise PackageDoesNotExists(package_id)        for key, value in data.items():            if key == \"id\":                continue            setattr(p, key, value)        p.sync()        self.pyload.files.save()",
        "labels_text": "Allows to modify several package attribute param packageid package id param data dict that map attribute to desired value"
    },
    {
        "input_text": "summarize: def delete_finished(self):                return self.pyload.files.delete_finished_links()",
        "labels_text": "Deletes all finished file and completely finished package return list of deleted package id"
    },
    {
        "input_text": "summarize: def restart_failed(self):                self.pyload.files.restart_failed()",
        "labels_text": "Restarts all failed failes"
    },
    {
        "input_text": "summarize: def get_package_order(self, destination):                packages = self.pyload.files.get_info_data(Destination(destination))        order = {}        for package_id in packages:            pack = self.pyload.files.get_package_data(int(package_id))            while pack[\"order\"] in order.keys():  #: just in case                pack[\"order\"] += 1            order[pack[\"order\"]] = pack[\"id\"]        return order",
        "labels_text": "Returns information about package order param destination Destination return dict mapping order to package id"
    },
    {
        "input_text": "summarize: def get_file_order(self, package_id):                raw_data = self.pyload.files.get_package_data(int(package_id))        order = {}        for id, pyfile in raw_data[\"links\"].items():            while pyfile[\"order\"] in order.keys():  #: just in case                pyfile[\"order\"] += 1            order[pyfile[\"order\"]] = pyfile[\"id\"]        return order",
        "labels_text": "Information about file order within package param packageid return dict mapping order to file id"
    },
    {
        "input_text": "summarize: def is_captcha_waiting(self):                self.pyload.last_client_connected = time.time()        task = self.pyload.captcha_manager.get_task()        return task is not None",
        "labels_text": "Indicates wether a captcha task is available return bool"
    },
    {
        "input_text": "summarize: def get_captcha_task(self, exclusive=False):                self.pyload.last_client_connected = time.time()        task = self.pyload.captcha_manager.get_task()        if task:            task.set_waiting_for_user(exclusive=exclusive)            data, type, result = task.get_captcha()            t = CaptchaTask(int(task.id), json.dumps(data), type, result)            return t        else:            return CaptchaTask(-1)",
        "labels_text": "Returns a captcha task param exclusive unused return CaptchaTask"
    },
    {
        "input_text": "summarize: def get_captcha_task_status(self, tid):                self.pyload.last_client_connected = time.time()        t = self.pyload.captcha_manager.get_task_by_id(tid)        return t.get_status() if t else \"\"",
        "labels_text": "Get information about captcha task param tid task id return string"
    },
    {
        "input_text": "summarize: def set_captcha_result(self, tid, result):                self.pyload.last_client_connected = time.time()        task = self.pyload.captcha_manager.get_task_by_id(tid)        if task:            task.set_result(result)            self.pyload.captcha_manager.remove_task(task)",
        "labels_text": "Set result for a captcha task param tid task id param result captcha result"
    },
    {
        "input_text": "summarize: def get_accounts(self, refresh):                accs = self.pyload.account_manager.get_account_infos(False, refresh)        accounts = []        for group in accs.values():            accounts.extend(                [                    AccountInfo(                        acc[\"validuntil\"],                        acc[\"login\"],                        acc[\"options\"],                        acc[\"valid\"],                        acc[\"trafficleft\"],                        acc[\"premium\"],                        acc[\"type\"],                    )                    for acc in group                ]            )        return accounts",
        "labels_text": "Get information about all entered account param refresh reload account info return list of AccountInfo"
    },
    {
        "input_text": "summarize: def get_account_types(self):                return list(self.pyload.account_manager.accounts.keys())",
        "labels_text": "All available account type return list"
    },
    {
        "input_text": "summarize: def update_account(self, plugin, account, password=None, options={}):                self.pyload.account_manager.update_account(plugin, account, password, options)",
        "labels_text": "Changes pwoptions for specific account"
    },
    {
        "input_text": "summarize: def remove_account(self, plugin, account):                self.pyload.account_manager.remove_account(plugin, account)",
        "labels_text": "Remove account from pyload param plugin pluginname param account accountname"
    },
    {
        "input_text": "summarize: def login(self, username, password):                return True if self.check_auth(username, password) else False",
        "labels_text": "Login into pyLoad this must be called when using rpc before any method can be used param username param password return bool indicating login wa successful"
    },
    {
        "input_text": "summarize: def check_auth(self, username, password):                return self.pyload.db.check_auth(username, password)",
        "labels_text": "Check authentication and return detail param username param password return dict with info empty when login is incorrect"
    },
    {
        "input_text": "summarize: def user_exists(self, username):                return self.pyload.db.user_exists(username)",
        "labels_text": "Check if a user actually exists in the database param username return boolean"
    },
    {
        "input_text": "summarize: def is_authorized(self, func, userdata):                if userdata[\"role\"] == Role.ADMIN:            return True        elif func in perm_map and has_permission(            userdata[\"permission\"], perm_map[func]        ):            return True        else:            return False",
        "labels_text": "check if the user is authorized for specific method param func function name param userdata dictionary of user data return boolean"
    },
    {
        "input_text": "summarize: def getUserData(self, username, password):                user = self.check_auth(username, password)        if user:            return OldUserData(                user[\"name\"],                user[\"email\"],                user[\"role\"],                user[\"permission\"],                user[\"template\"],            )        else:            return OldUserData()",
        "labels_text": "similar to checkauth but return UserData thrift type"
    },
    {
        "input_text": "summarize: def get_userdata(self, username, password):                user = self.check_auth(username, password)        if user:            return UserData(                user[\"id\"],                user[\"name\"],                user[\"email\"],                user[\"role\"],                user[\"permission\"],                user[\"template\"],            )        else:            return UserData()",
        "labels_text": "similar to checkauth but return UserData thrift type"
    },
    {
        "input_text": "summarize: def getAllUserData(self):                res = {}        for id, data in self.pyload.db.get_all_user_data().items():            res[data[\"name\"]] = OldUserData(                data[\"name\"],                data[\"email\"],                data[\"role\"],                data[\"permission\"],                data[\"template\"],            )        return res",
        "labels_text": "return all known user and info"
    },
    {
        "input_text": "summarize: def get_all_userdata(self):                res = {}        for id, data in self.pyload.db.get_all_user_data().items():            res[id] = UserData(                id,                data[\"name\"],                data[\"email\"],                data[\"role\"],                data[\"permission\"],                data[\"template\"],            )        return res",
        "labels_text": "return all known user and info"
    },
    {
        "input_text": "summarize: def get_services(self):                data = {}        for plugin, funcs in self.pyload.addon_manager.methods.items():            data[plugin] = funcs        return data",
        "labels_text": "A dict of available service these can be defined by addon plugins return dict with this style plugin method description"
    },
    {
        "input_text": "summarize: def has_service(self, plugin, func):                cont = self.pyload.addon_manager.methods        return plugin in cont and func in cont[plugin]",
        "labels_text": "Checks whether a service is available param plugin param func return bool"
    },
    {
        "input_text": "summarize: def call(self, info):                plugin = info.plugin        func = info.func        args = info.arguments        parse = info.parse_arguments        if not self.has_service(plugin, func):            raise ServiceDoesNotExists(plugin, func)        try:            ret = self.pyload.addon_manager.call_rpc(plugin, func, args, parse)            return str(ret)        except Exception as exc:            raise ServiceException(exc)",
        "labels_text": "Calls a service a method in addon plugin param info ServiceCall return result raise ServiceDoesNotExists when it not available raise ServiceException when an exception wa raised"
    },
    {
        "input_text": "summarize: def get_all_info(self):                return self.pyload.addon_manager.get_all_info()",
        "labels_text": "Returns all information stored by addon plugins Values are always string return plugin name value"
    },
    {
        "input_text": "summarize: def get_info_by_plugin(self, plugin):                return self.pyload.addon_manager.get_info(plugin)",
        "labels_text": "Returns information stored by a specific plugin param plugin pluginname return dict of attr name mapped to value name value"
    },
    {
        "input_text": "summarize: def add_user(self, user, newpw, role=0, perms=0):                return self.pyload.db.add_user(user, newpw, role, perms)",
        "labels_text": "creates new user login"
    },
    {
        "input_text": "summarize: def remove_user(self, user):                return self.pyload.db.remove_user(user)",
        "labels_text": "deletes a user login"
    },
    {
        "input_text": "summarize: def change_password(self, user, oldpw, newpw):                return self.pyload.db.change_password(user, oldpw, newpw)",
        "labels_text": "change password for specific user"
    },
    {
        "input_text": "summarize: def read_default_config(self):                self.config = self.parse_config(            os.path.join(PKGDIR, \"core\", \"config\", \"default.cfg\")        )        self.plugin = self.parse_config(self.pluginpath)        try:            homeconf = self.parse_config(self.configpath)            self.update_values(homeconf, self.config)        except Exception as exc:            exc_logger.exception(exc)",
        "labels_text": "read the config file"
    },
    {
        "input_text": "summarize: def update_values(self, config, dest):                for section in config.keys():            if section in dest:                for option in config[section].keys():                    if option in (\"desc\", \"outline\"):                        continue                    if option in dest[section]:                        dest[section][option][\"value\"] = config[section][option][                            \"value\"                        ]",
        "labels_text": "set the config value from a parsed config file to value in destination"
    },
    {
        "input_text": "summarize: def save(self):                self.save_config(self.config, self.configpath)        self.save_config(self.plugin, self.pluginpath)",
        "labels_text": "save the configs to disk"
    },
    {
        "input_text": "summarize: def __getitem__(self, section):                return Section(self, section)",
        "labels_text": "provides dictonary like access csectionoption"
    },
    {
        "input_text": "summarize: def get(self, section, option):                return self.config[section][option][\"value\"]",
        "labels_text": "get value"
    },
    {
        "input_text": "summarize: def set(self, section, option, value):                value = self.cast(self.config[section][option][\"type\"], value)        self.config[section][option][\"value\"] = value        self.save()",
        "labels_text": "set value"
    },
    {
        "input_text": "summarize: def get_plugin(self, plugin, option):                return self.plugin[plugin][option][\"value\"]",
        "labels_text": "get a value for a plugin"
    },
    {
        "input_text": "summarize: def set_plugin(self, plugin, option, value):                value = self.cast(self.plugin[plugin][option][\"type\"], value)        # TODO: check if callable        if self.plugin_cb:            self.plugin_cb(plugin, option, value)        self.plugin[plugin][option][\"value\"] = value        self.save()",
        "labels_text": "set a value for a plugin"
    },
    {
        "input_text": "summarize: def get_meta_data(self, section, option):                return self.config[section][option]",
        "labels_text": "get all config data for an option"
    },
    {
        "input_text": "summarize: def delete_config(self, name):                if name in self.plugin:            del self.plugin[name]",
        "labels_text": "Removes a plugin config"
    },
    {
        "input_text": "summarize: def __init__(self, parser, section):                self.parser = parser        self.section = section",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def __getitem__(self, item):                return self.parser.get(self.section, item)",
        "labels_text": "getitem"
    },
    {
        "input_text": "summarize: def __setitem__(self, item, value):                self.parser.set(self.section, item, value)",
        "labels_text": "setitem"
    },
    {
        "input_text": "summarize: def filecount(self, queue):                self.c.execute(            \"SELECT COUNT(*) FROM links as l INNER JOIN packages as p ON l.package=p.id WHERE p.queue=?\",            (queue,),        )        return self.c.fetchone()[0]",
        "labels_text": "return number of file in queue"
    },
    {
        "input_text": "summarize: def queuecount(self, queue):                self.c.execute(            \"SELECT COUNT(*) FROM links as l INNER JOIN packages as p ON l.package=p.id WHERE p.queue=? AND l.status NOT IN (0,4)\",            (queue,),        )        return self.c.fetchone()[0]",
        "labels_text": "number of file in queue not finished yet"
    },
    {
        "input_text": "summarize: def processcount(self, queue, fid):                self.c.execute(            \"SELECT COUNT(*) FROM links as l INNER JOIN packages as p ON l.package=p.id WHERE p.queue=? AND l.status IN (2,3,5,7,12) AND l.id != ?\",            (queue, str(fid)),        )        return self.c.fetchone()[0]",
        "labels_text": "number of file which have to be processed"
    },
    {
        "input_text": "summarize: def add_links(self, links, package):                order = self._next_file_order(package)        orders = [order + x for x in range(len(links))]        links = [(x[0], parse.name(x[0]), x[1], package, o) for x, o in zip(links, orders)]        self.c.executemany(            \"INSERT INTO links(url, name, plugin, package, linkorder) VALUES(?,?,?,?,?)\",            links,        )",
        "labels_text": "link is a list of tuples urlplugin"
    },
    {
        "input_text": "summarize: def update_link_info(self, data):                self.c.executemany(            \"UPDATE links SET name=?, size=?, status=? WHERE url=? AND status IN (1,2,3,14)\",            data,        )        ids = []        statuses = \"','\".join(x[3] for x in data)        self.c.execute(f\"SELECT id FROM links WHERE url IN ('{statuses}')\")        for r in self.c:            ids.append(int(r[0]))        return ids",
        "labels_text": "data is list of tuples name size status url"
    },
    {
        "input_text": "summarize: def get_package(self, id):                self.c.execute(            \"SELECT name,folder,site,password,queue,packageorder FROM packages WHERE id=?\",            (str(id),),        )        r = self.c.fetchone()        if not r:            return None        return PyPackage(self.pyload.files, id, *r)",
        "labels_text": "return package instance from id"
    },
    {
        "input_text": "summarize: def get_file(self, id):                self.c.execute(            \"SELECT url, name, size, status, error, plugin, package, linkorder FROM links WHERE id=?\",            (str(id),),        )        r = self.c.fetchone()        if not r:            return None        return PyFile(self.pyload.files, id, *r)",
        "labels_text": "return link instance from id"
    },
    {
        "input_text": "summarize: def get_job(self, occupied):                # TODO: improve this hardcoded method        # plugins which are processed in collector        pre = (\"DLC\", \"TXT\", \"CCF\", \"RSDF\")        self.c.execute(            f,            occupied + pre,        )        return [x[0] for x in self.c]",
        "labels_text": "return pyfile id which are suitable for download and dont use an occupied plugin"
    },
    {
        "input_text": "summarize: def get_plugin_job(self, plugins):                self.c.execute(            f,            plugins,        )        return [x[0] for x in self.c]",
        "labels_text": "return pyfile id with suited plugins"
    },
    {
        "input_text": "summarize: def get_unfinished(self, pid):                self.c.execute(            \"SELECT id FROM links WHERE package=? AND status NOT IN (0, 4, 13) LIMIT 3\",            (str(pid),),        )        return [r[0] for r in self.c]",
        "labels_text": "return list of max length id with pyfiles in package not finished or processed"
    },
    {
        "input_text": "summarize: def find_duplicates(self, id, folder, filename):                self.c.execute(            \"SELECT l.plugin FROM links as l INNER JOIN packages as p ON l.package=p.id AND p.folder=? WHERE l.id!=? AND l.status=0 AND l.name=?\",            (folder, id, filename),        )        return self.c.fetchone()",
        "labels_text": "check if filename exists with different id and same package"
    },
    {
        "input_text": "summarize: def init_plugin(self):                if not self.plugin:            self.pluginmodule = self.m.pyload.plugin_manager.get_plugin(self.pluginname)            self.pluginclass = getattr(                self.pluginmodule,                self.m.pyload.plugin_manager.get_plugin_name(self.pluginname),            )            self.plugin = self.pluginclass(self)",
        "labels_text": "inits plugin instance"
    },
    {
        "input_text": "summarize: def has_plugin(self):                return hasattr(self, \"plugin\") and self.plugin",
        "labels_text": "Thread safe way to determine this file ha initialized plugin attribute return"
    },
    {
        "input_text": "summarize: def package(self):                return self.m.get_package(self.packageid)",
        "labels_text": "return package instance"
    },
    {
        "input_text": "summarize: def sync(self):                self.m.update_link(self)",
        "labels_text": "sync PyFile instance with database"
    },
    {
        "input_text": "summarize: def release(self):                # file has valid package        if self.packageid > 0:            self.sync()        if hasattr(self, \"plugin\") and self.plugin:            self.plugin.clean()            del self.plugin        self.m.release_link(self.id)",
        "labels_text": "sync and remove from cache"
    },
    {
        "input_text": "summarize: def delete(self):                self.m.delete_link(self.id)",
        "labels_text": "delete pyfile from database"
    },
    {
        "input_text": "summarize: def to_dict(self):                return self.to_db_dict()",
        "labels_text": "return dict with all information for interface"
    },
    {
        "input_text": "summarize: def to_db_dict(self):                return {            self.id: {                \"id\": self.id,                \"url\": self.url,                \"name\": self.name,                \"plugin\": self.pluginname,                \"size\": self.get_size(),                \"format_size\": self.format_size(),                \"status\": self.status,                \"statusmsg\": self.get_status_name(),                \"package\": self.packageid,                \"error\": self.error,                \"order\": self.order,            }        }",
        "labels_text": "return data a dict for databse format id url url name name"
    },
    {
        "input_text": "summarize: def abort_download(self):                while self.id in self.m.pyload.thread_manager.processing_ids():            self.abort = True            if self.plugin and self.plugin.req:                self.plugin.req.abort_downloads()            time.sleep(0.1)        self.abort = False        if self.has_plugin() and self.plugin.req:            self.plugin.req.abort_downloads()        self.release()",
        "labels_text": "abort pyfile if possible"
    },
    {
        "input_text": "summarize: def finish_if_done(self):                if self.id in self.m.pyload.thread_manager.processing_ids():            return False        self.set_status(\"finished\")        self.release()        self.m.check_all_links_finished()        return True",
        "labels_text": "set status to finish and release file if every thread is finished with it"
    },
    {
        "input_text": "summarize: def format_wait(self):                seconds = int(self.wait_until - time.time())        return format.time(seconds, literally=False)",
        "labels_text": "format and return wait time in human readable format"
    },
    {
        "input_text": "summarize: def format_size(self):                return format.size(self.get_size())",
        "labels_text": "format size to readable format"
    },
    {
        "input_text": "summarize: def format_eta(self):                seconds = self.get_eta()        return format.time(seconds, literally=False)",
        "labels_text": "format eta to readable format"
    },
    {
        "input_text": "summarize: def get_speed(self):                try:            return self.plugin.req.speed        except Exception:            return 0",
        "labels_text": "calculates speed"
    },
    {
        "input_text": "summarize: def get_eta(self):                try:            return int(self.get_bytes_left() // self.get_speed())        except ZeroDivisionError:            return 0",
        "labels_text": "get established time of arrival"
    },
    {
        "input_text": "summarize: def get_bytes_left(self):                try:            return max(self.get_size() - self.plugin.req.arrived, 0)        except Exception:            return 0",
        "labels_text": "get byte left"
    },
    {
        "input_text": "summarize: def get_percent(self):                if self.status == 12:            try:                return self.plugin.req.percent            except Exception:                return 0        else:            return self.progress",
        "labels_text": "get of download"
    },
    {
        "input_text": "summarize: def get_size(self):                try:            if self.plugin.req.size:                return self.plugin.req.size            else:                return self.size        except Exception:            return self.size",
        "labels_text": "get size of download"
    },
    {
        "input_text": "summarize: def to_dict(self):                return {            self.id: {                \"id\": self.id,                \"name\": self.name,                \"folder\": self.folder,                \"site\": self.site,                \"password\": self.password,                \"queue\": self.queue,                \"order\": self.order,                \"links\": {},            }        }",
        "labels_text": "Returns a dictionary representation of the data return dict id attr value"
    },
    {
        "input_text": "summarize: def get_children(self):                return self.m.get_package_data(self.id)[\"links\"]",
        "labels_text": "get information about contained link"
    },
    {
        "input_text": "summarize: def sync(self):                self.m.update_package(self)",
        "labels_text": "sync with db"
    },
    {
        "input_text": "summarize: def release(self):                self.sync()        self.m.release_package(self.id)",
        "labels_text": "sync and delete from cache"
    },
    {
        "input_text": "summarize: def __init__(self, core):                self.pyload = core        self._ = core._        self.lock = Lock()        # TODO: Recheck        configdir = os.path.join(core.userdir, \"settings\")        os.makedirs(configdir, exist_ok=True)        self.configpath = os.path.join(configdir, \"accounts.cfg\")        self.init_plugins()        self.save_accounts()",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def get_account_plugin(self, plugin):                if plugin in self.accounts:            if plugin not in self.plugins:                klass = self.pyload.plugin_manager.load_class(\"account\", plugin)                if klass:                    self.plugins[plugin] = klass(self, self.accounts[plugin])                else:                    return None            return self.plugins[plugin]        else:            return None",
        "labels_text": "get account instance for plugin or None if anonymous"
    },
    {
        "input_text": "summarize: def get_account_plugins(self):                plugins = []        for plugin in self.accounts.keys():            plugins.append(self.get_account_plugin(plugin))        return plugins",
        "labels_text": "get all account instance"
    },
    {
        "input_text": "summarize: def init_account_plugins(self):                for name in self.pyload.plugin_manager.get_account_plugins():            self.accounts[name] = {}",
        "labels_text": "init name"
    },
    {
        "input_text": "summarize: def update_account(self, plugin, user, password=None, options={}):                if plugin in self.accounts and user:            p = self.get_account_plugin(plugin)            if p is not None:                updated = p.update_accounts(user, password, options)                # since accounts is a ref in plugin self.accounts doesn't need to be                # updated here                self.save_accounts()                if updated:                    p.schedule_refresh(user, force=False)",
        "labels_text": "add or update an account"
    },
    {
        "input_text": "summarize: def remove_account(self, plugin, user):                if plugin in self.accounts:            p = self.get_account_plugin(plugin)            p.remove_account(user)            self.save_accounts()",
        "labels_text": "remove account"
    },
    {
        "input_text": "summarize: def active_plugins(self):                return [x for x in self.plugins if x.is_activated()]",
        "labels_text": "return all active plugins"
    },
    {
        "input_text": "summarize: def get_all_info(self):                info = {}        for name, plugin in self.plugin_map.items():            if plugin.info:                # copy and convert so str                info[name] = {x: str(y) for x, y in plugin.info.items()}        return info",
        "labels_text": "return info stored by addon plugins"
    },
    {
        "input_text": "summarize: def add_event(self, event, func):                if event in self.events:            if func not in self.events[event]:                self.events[event].append(func)        else:            self.events[event] = [func]",
        "labels_text": "Adds an event listener for event name"
    },
    {
        "input_text": "summarize: def remove_event(self, event, func):                if event in self.events:            if func in self.events[event]:                self.events[event].remove(func)",
        "labels_text": "remove previously added event listener"
    },
    {
        "input_text": "summarize: def dispatch_event(self, event, *args):                if event in self.events:            for f in self.events[event]:                try:                    f(*args)                except Exception as exc:                    self.pyload.log.warning(                        self._(\"Error calling event handler {}: {}, {}, {}\").format(                            event, f, args, exc                        ),                        exc_info=self.pyload.debug > 1,                        stack_info=self.pyload.debug > 2,                    )",
        "labels_text": "dispatch event with args"
    },
    {
        "input_text": "summarize: def set_waiting(self, sec):                self.wait_until = max(time.time() + sec, self.wait_until)        self.status = \"waiting\"",
        "labels_text": "let the captcha wait sec for the solution"
    },
    {
        "input_text": "summarize: def is_textual(self):                return self.captcha_result_type == \"textual\"",
        "labels_text": "return if text is written on the captcha"
    },
    {
        "input_text": "summarize: def is_positional(self):                return self.captcha_result_type == \"positional\"",
        "labels_text": "return if user have to click a specific region on the captcha"
    },
    {
        "input_text": "summarize: def is_interactive(self):                return self.captcha_result_type == \"interactive\"",
        "labels_text": "return if user ha to solve the captcha in an interactive iframe"
    },
    {
        "input_text": "summarize: def is_invisible(self):                return self.captcha_result_type == \"invisible\"",
        "labels_text": "return if invisible browser only no user interaction captcha"
    },
    {
        "input_text": "summarize: def invalid(self):                [x.captcha_invalid(self) for x in self.handler]",
        "labels_text": "indicates the captcha wa not correct"
    },
    {
        "input_text": "summarize: def save(self):                self.pyload.db.commit()",
        "labels_text": "save all data to backend"
    },
    {
        "input_text": "summarize: def sync_save(self):                pyfiles = self.cache.values()        for pyfile in pyfiles:            pyfile.sync()        pypacks = self.package_cache.values()        for pypack in pypacks:            pypack.sync()        self.pyload.db.sync_save()",
        "labels_text": "save all data to backend and wait until all the data is written"
    },
    {
        "input_text": "summarize: def get_info_data(self, queue=Destination.QUEUE):                queue = queue.value        packs = self.pyload.db.get_all_packages(queue)        for x in self.package_cache.values():            if x.queue != queue or x.id not in packs:                continue            packs[x.id].update(x.to_dict()[x.id])        return packs",
        "labels_text": "get a data representation without link"
    },
    {
        "input_text": "summarize: def add_links(self, urls, package):                self.pyload.addon_manager.dispatch_event(\"links_added\", urls, package)        data = self.pyload.plugin_manager.parse_urls(urls)        self.pyload.db.add_links(data, package)        self.pyload.thread_manager.create_info_thread(data, package)        # TODO: change from reload_all event to package update event        self.pyload.event_manager.add_event(ReloadAllEvent(\"collector\"))",
        "labels_text": "add link"
    },
    {
        "input_text": "summarize: def add_package(self, name, folder, queue=Destination.QUEUE):                last_id = self.pyload.db.add_package(name, folder, queue.value)        p = self.pyload.db.get_package(last_id)        e = InsertEvent(            \"pack\",            last_id,            p.order,            \"collector\" if queue is Destination.COLLECTOR else \"queue\",        )        self.pyload.event_manager.add_event(e)        return last_id",
        "labels_text": "add a package default to link collector"
    },
    {
        "input_text": "summarize: def release_link(self, id):                if id in self.cache:            del self.cache[id]",
        "labels_text": "remove pyfile from cache"
    },
    {
        "input_text": "summarize: def release_package(self, id):                if id in self.package_cache:            del self.package_cache[id]",
        "labels_text": "remove package from cache"
    },
    {
        "input_text": "summarize: def update_link(self, pyfile):                self.pyload.db.update_link(pyfile)        e = UpdateEvent(            \"file\", pyfile.id, \"collector\" if not pyfile.package().queue else \"queue\"        )        self.pyload.event_manager.add_event(e)",
        "labels_text": "update link"
    },
    {
        "input_text": "summarize: def update_package(self, pypack):                self.pyload.db.update_package(pypack)        e = UpdateEvent(\"pack\", pypack.id, \"collector\" if not pypack.queue else \"queue\")        self.pyload.event_manager.add_event(e)",
        "labels_text": "update a package"
    },
    {
        "input_text": "summarize: def get_package(self, id):                if id in self.package_cache:            return self.package_cache[id]        else:            return self.pyload.db.get_package(id)",
        "labels_text": "return package instance"
    },
    {
        "input_text": "summarize: def get_file_data(self, id):                if id in self.cache:            return self.cache[id].to_db_dict()        return self.pyload.db.get_link_data(id)",
        "labels_text": "return dict with file information"
    },
    {
        "input_text": "summarize: def get_file(self, id):                if id in self.cache:            return self.cache[id]        else:            return self.pyload.db.get_file(id)",
        "labels_text": "return pyfile instance"
    },
    {
        "input_text": "summarize: def get_decrypt_job(self):                if \"decrypt\" in self.job_cache:            return None        plugins = tuple(            chain(                self.pyload.plugin_manager.decrypter_plugins.keys(),                self.pyload.plugin_manager.container_plugins.keys(),            )        )        jobs = self.pyload.db.get_plugin_job(plugins)        if jobs:            return self.get_file(jobs[0])        else:            self.job_cache[\"decrypt\"] = \"empty\"            return None",
        "labels_text": "return job for decrypting"
    },
    {
        "input_text": "summarize: def get_file_count(self):                if self.filecount == -1:            self.filecount = self.pyload.db.filecount(1)        return self.filecount",
        "labels_text": "return number of file"
    },
    {
        "input_text": "summarize: def get_queue_count(self, force=False):                if self.queuecount == -1 or force:            self.queuecount = self.pyload.db.queuecount(1)        return self.queuecount",
        "labels_text": "number of file that have to be processed"
    },
    {
        "input_text": "summarize: def check_all_links_finished(self):                if not self.get_queue_count(True):            self.pyload.addon_manager.dispatch_event(\"all_downloads_finished\")            self.pyload.log.debug(\"All downloads finished\")            return True        return False",
        "labels_text": "check if all file are finished and dispatch event"
    },
    {
        "input_text": "summarize: def check_all_links_processed(self, fid):                # reset count so statistic will update (this is called when dl was processed)        self.reset_count()        if not self.pyload.db.processcount(1, fid):            self.pyload.addon_manager.dispatch_event(\"all_downloads_processed\")            self.pyload.log.debug(\"All downloads processed\")            return True        return False",
        "labels_text": "check if all file wa processed and pyload would idle now need fid which will be ignored when counting"
    },
    {
        "input_text": "summarize: def update_file_info(self, data, pid):                self.pyload.db.update_link_info(data)        e = UpdateEvent(            \"pack\", pid, \"collector\" if not self.get_package(pid).queue else \"queue\"        )        self.pyload.event_manager.add_event(e)",
        "labels_text": "update file info name size status url"
    },
    {
        "input_text": "summarize: def recheck_package(self, pid):                data = self.pyload.db.get_package_data(pid)        urls = []        for pyfile in data.values():            if pyfile[\"status\"] not in (0, 12, 13):                urls.append((pyfile[\"url\"], pyfile[\"plugin\"]))        self.pyload.thread_manager.create_info_thread(urls, pid)",
        "labels_text": "recheck link in package"
    },
    {
        "input_text": "summarize: def restart_failed(self):                self.pyload.db.restart_failed()",
        "labels_text": "restart all failed link"
    },
    {
        "input_text": "summarize: def get_plugin(self, plugin_name, original=False):                plugin, plugin_type = self.find_plugin(plugin_name)        if not plugin:            self.pyload.log.warning(self._(\"Plugin {} not found\").format(plugin_name))            plugin = self.downloader_plugins[\"DefaultPlugin\"]        if \"new_module\" in plugin and not original:            return plugin[\"new_module\"]        return self.load_module(plugin_type, plugin_name)",
        "labels_text": "return plugin module from downloaderdecryptercontainer"
    },
    {
        "input_text": "summarize: def get_plugin_name(self, name):                plugin, plugin_type = self.find_plugin(name)        if \"new_name\" in plugin:            return plugin[\"new_name\"]        return name",
        "labels_text": "used to obtain new name if other plugin wa injected"
    },
    {
        "input_text": "summarize: def load_class(self, module_type, module_name):                module = self.load_module(module_type, module_name)        if module:            return getattr(module, module_name)",
        "labels_text": "Returns the class of a plugin with the same name"
    },
    {
        "input_text": "summarize: def get_account_plugins(self):                return list(self.account_plugins.keys())",
        "labels_text": "return list of account plugin name"
    },
    {
        "input_text": "summarize: def create_download_thread(self):                thread = DownloadThread(self)        self.threads.append(thread)",
        "labels_text": "create a download thread"
    },
    {
        "input_text": "summarize: def create_info_thread(self, data, pid):                self.timestamp = time.time() + timedelta(minutes=5).total_seconds()        InfoThread(self, data, pid)",
        "labels_text": "start a thread whichs fetch online status and other info data"
    },
    {
        "input_text": "summarize: def create_result_thread(self, data, add=False):                self.timestamp = time.time() + timedelta(minutes=5).total_seconds()        rid = self.result_ids        self.result_ids += 1        InfoThread(self, data, rid=rid, add=add)        return rid",
        "labels_text": "creates a thread to fetch online status return result id"
    },
    {
        "input_text": "summarize: def get_info_result(self, rid):                self.timestamp = time.time() + timedelta(minutes=5).total_seconds()        if rid in self.info_results:            data = self.info_results[rid]            self.info_results[rid] = {}            return data        else:            return {}",
        "labels_text": "return result and clear it"
    },
    {
        "input_text": "summarize: def processing_ids(self):                return [x.id for x in self.get_active_files()]",
        "labels_text": "get an id list of all pyfiles processed"
    },
    {
        "input_text": "summarize: def check_thread_count(self):                if len(self.threads) == self.pyload.config.get(\"download\", \"max_downloads\"):            return True        elif len(self.threads) < self.pyload.config.get(\"download\", \"max_downloads\"):            self.create_download_thread()        else:            free = [x for x in self.threads if not x.active]            if free:                free[0].put(\"quit\")",
        "labels_text": "check if there are need for increasing or reducing thread count"
    },
    {
        "input_text": "summarize: def load(self, *args, **kwargs):                return self.http.load(*args, **kwargs)",
        "labels_text": "retrieves page"
    },
    {
        "input_text": "summarize: def put_header(self, name, value):                self.http.put_header(name, value)",
        "labels_text": "add a header to the request"
    },
    {
        "input_text": "summarize: def add_auth(self, pwd):                self.options[\"auth\"] = pwd        self.renew_http_request()",
        "labels_text": "Adds user and pw for http auth param pwd string userpassword"
    },
    {
        "input_text": "summarize: def set_option(self, name, value):                self.options[name] = value",
        "labels_text": "Adds an option to the request see HTTPRequest for existing one"
    },
    {
        "input_text": "summarize: def close(self):                if hasattr(self, \"http\"):            self.http.close()            del self.http        if hasattr(self, \"dl\"):            del self.dl        if hasattr(self, \"cj\"):            del self.cj",
        "labels_text": "cleanup"
    },
    {
        "input_text": "summarize: def consumed(self, amount):                if self.rate < self.MIN_RATE:            return 0  # NOTE: May become unresponsive otherwise        self._calc_token()        self.token -= amount        consumed = -self.token // self._rate if self.token < 0 else 0        return consumed",
        "labels_text": "Return time the process have to sleep after consumed specified amount"
    },
    {
        "input_text": "summarize: def get_http_request(self, **kwargs):                options = self.get_options()        options.update(kwargs)  #: submit kwargs as additional options        return HTTPRequest(CookieJar(None), options)",
        "labels_text": "return a http request dont forget to close it"
    },
    {
        "input_text": "summarize: def get_url(self, *args, **kwargs):                with HTTPRequest(None, self.get_options()) as h:            rep = h.load(*args, **kwargs)        return rep",
        "labels_text": "see HTTPRequest for argument list"
    },
    {
        "input_text": "summarize: def get_options(self):                return {            \"interface\": self.iface(),            \"proxies\": self.get_proxies(),            \"ipv6\": self.pyload.config.get(\"download\", \"ipv6\"),            \"ssl_verify\": self.pyload.config.get(\"general\", \"ssl_verify\"),        }",
        "labels_text": "return option needed for pycurl"
    },
    {
        "input_text": "summarize: def update_bucket(self):                if not self.pyload.config.get(\"download\", \"limit_speed\"):            self.bucket.set_rate(-1)        else:            self.bucket.set_rate(self.pyload.config.get(\"download\", \"max_speed\") << 10)",
        "labels_text": "set value in the bucket according to setting"
    },
    {
        "input_text": "summarize: def stop(self):                self.range = [0, 0]        self.size = 0",
        "labels_text": "The download will not proceed after next call of writebody"
    },
    {
        "input_text": "summarize: def reset_range(self):                self.range = None",
        "labels_text": "Reset the range so the download will load all data available"
    },
    {
        "input_text": "summarize: def flush_file(self):                self.fp.flush()        os.fsync(self.fp.fileno())  #: make sure everything was written to disk        self.fp.close()",
        "labels_text": "flush and close file"
    },
    {
        "input_text": "summarize: def close(self):                if self.fp:            self.fp.close()        self.c.close()        if hasattr(self, \"p\"):            del self.p",
        "labels_text": "close everything unusable after this"
    },
    {
        "input_text": "summarize: def find_chunk(self, handle):                for chunk in self.chunks:            if chunk.c == handle:                return chunk",
        "labels_text": "linear search to find a chunk should be ok since chunk size is usually low"
    },
    {
        "input_text": "summarize: def close(self):                for chunk in self.chunks:            self.close_chunk(chunk)        self.chunks = []        if hasattr(self, \"m\"):            self.m.close()            del self.m        if hasattr(self, \"cj\"):            del self.cj        if hasattr(self, \"info\"):            del self.info",
        "labels_text": "cleanup"
    },
    {
        "input_text": "summarize: def add_cookies(self):                if self.cj:            self.cj.add_cookies(self.c.getinfo(pycurl.INFO_COOKIELIST))",
        "labels_text": "put cooky from curl handle to cj"
    },
    {
        "input_text": "summarize: def get_cookies(self):                if self.cj:            for c in self.cj.get_cookies():                self.c.setopt(pycurl.COOKIELIST, c)        return",
        "labels_text": "add cooky from cj to curl handle"
    },
    {
        "input_text": "summarize: def check_header(self):                return int(self.c.getinfo(pycurl.RESPONSE_CODE)) not in BAD_STATUS_CODES",
        "labels_text": "check if header indicates failure"
    },
    {
        "input_text": "summarize: def get_response(self):                if self.rep is None:            return b\"\"        else:            return self.rep.getvalue()",
        "labels_text": "retrieve response from byte io"
    },
    {
        "input_text": "summarize: def write_body(self, buf):                if self.abort:            self.exception = Abort()            return pycurl.E_WRITE_ERROR        elif self.limit and self.rep.tell() > self.limit:            rep = self.get_response()            with open(\"response.dump\", mode=\"wb\") as fp:                fp.write(rep)            self.exception = Exception(f\"Loaded URL exceeded limit ({self.limit})\")            return pycurl.E_WRITE_ERROR        self.rep.write(buf)        return None",
        "labels_text": "writes response"
    },
    {
        "input_text": "summarize: def write_header(self, buf):                self.response_header += buf",
        "labels_text": "writes header"
    },
    {
        "input_text": "summarize: def close(self):                if self.rep:            self.rep.close()            del self.rep        if hasattr(self, \"cj\"):            del self.cj        if hasattr(self, \"c\"):            self.c.close()            del self.c",
        "labels_text": "cleanup unusable after this"
    },
    {
        "input_text": "summarize: def __init__(self, manager, function, args, kwargs):                super().__init__(manager)        self.f = function        self.args = args        self.kwargs = kwargs        self.active = []        manager.local_threads.append(self)        self.start()",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def add_active(self, pyfile):                if pyfile not in self.active:            self.active.append(pyfile)",
        "labels_text": "Adds a pyfile to active list and thus will be displayed on overview"
    },
    {
        "input_text": "summarize: def __init__(self, manager, pyfile):                super().__init__(manager)        self.active = pyfile        manager.local_threads.append(self)        pyfile.set_status(\"decrypting\")        self.start()",
        "labels_text": "constructor"
    },
    {
        "input_text": "summarize: def __init__(self, manager):                super().__init__(manager)        self.queue = Queue()  #: job queue        self.active = False        self.start()",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def put(self, job):                self.queue.put(job)",
        "labels_text": "assing job to thread"
    },
    {
        "input_text": "summarize: def stop(self):                self.put(\"quit\")",
        "labels_text": "stop the thread"
    },
    {
        "input_text": "summarize: def __init__(self, manager, data, pid=-1, rid=-1, add=False):                super().__init__(manager)        self.data = data        self.pid = pid  #: package id        # [ .. (name, plugin) .. ]        self.rid = rid  #: result id        self.add = add  #: add packages instead of return result        self.cache = []  #: accumulated data        self.start()",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def __init__(self, manager):                super().__init__()        self.active = False        self.daemon = True        self.pyload = manager.pyload        self._ = manager._        self.m = self.manager = manager",
        "labels_text": "Constructor"
    },
    {
        "input_text": "summarize: def clean(self, pyfile):                self.active = False        pyfile.release()",
        "labels_text": "set thread a inactive and release pyfile"
    },
    {
        "input_text": "summarize: def is_bits_set(value, bits):        return bits == (value & bits)",
        "labels_text": "Checks if all bit are set in value or some bit are zero"
    },
    {
        "input_text": "summarize: def cmp(x, y):        return (x > y) - (x < y)",
        "labels_text": "Compare the two object x and y and return an integer according to the outcome"
    },
    {
        "input_text": "summarize: def has_method(obj, name):        return callable(getattr(obj, name, None))",
        "labels_text": "Check if method name wa defined in obj"
    },
    {
        "input_text": "summarize: def has_propriety(obj, name):        attr = getattr(obj, name, None)    return attr and not callable(attr)",
        "labels_text": "Check if propriety name wa defined in obj"
    },
    {
        "input_text": "summarize: def methods(obj):        return [name for name in dir(obj) if has_method(obj, name)]",
        "labels_text": "List all the method declared in obj"
    },
    {
        "input_text": "summarize: def proprieties(obj):        return [name for name in dir(obj) if has_propriety(obj, name)]",
        "labels_text": "List all the propriety attribute declared in obj"
    },
    {
        "input_text": "summarize: def is_iterable(obj, strict=False):        return isinstance(obj, Iterable) and (        strict or not isinstance(obj, str) or not isinstance(obj, bytes)    )",
        "labels_text": "Check if object is iterable type str excluded if strictFalse"
    },
    {
        "input_text": "summarize: def is_sequence(obj):        return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))",
        "labels_text": "Check if object is sequence"
    },
    {
        "input_text": "summarize: def is_mapping(obj):        return isinstance(obj, Mapping)",
        "labels_text": "Check if object is mapping"
    },
    {
        "input_text": "summarize: def is_module(name):        return importlib.util.find_spec(name) is not None",
        "labels_text": "Check if exists a module with given name"
    },
    {
        "input_text": "summarize: def missing(iterable, start=None, end=None):        iter_seq = set(map(int, iterable))    min_val = start or min(iter_seq)    max_val = end or max(iter_seq)    full_seq = set(range(min_val, max_val + 1))    return sorted(full_seq - iter_seq)",
        "labels_text": "List all the value between start and stop that are missing from iterable"
    },
    {
        "input_text": "summarize: def to_list(obj):        if isinstance(obj, list):        pass    elif is_mapping(obj):        return list(obj.items())    elif is_iterable(obj, strict=False):        return list(obj)    elif obj is not None:        return [obj]    else:        return list(obj)",
        "labels_text": "Convert value to a list with value inside"
    },
    {
        "input_text": "summarize: def size(value):        return bitmath.Byte(value).best_prefix().format(\"{value:.2f} {unit}\")",
        "labels_text": "format size of byte"
    },
    {
        "input_text": "summarize: def exists(path, strict=False):        if not strict:        return os.path.exists(path)    if os.path.exists(path):        dirpath, name = os.path.split(path)        return name in os.listdir(dirpath)    return False",
        "labels_text": "Casesensitive ospathexists"
    },
    {
        "input_text": "summarize: def monkey_patch():                from js2py.constructors.jsobject import Object        fn = Object.own[\"getOwnPropertyNames\"][\"value\"].code        def wraps(*args, **kwargs):            result = fn(*args, **kwargs)            return list(result)        Object.own[\"getOwnPropertyNames\"][\"value\"].code = wraps",
        "labels_text": "Patching jspy for CVE"
    },
    {
        "input_text": "summarize: def accumulate(iterable, to_map=None):        if to_map is None:        to_map = {}    for key, value in iterable:        to_map.setdefault(value, []).append(key)    return to_map",
        "labels_text": "Accumulate key value data to value key dictionary"
    },
    {
        "input_text": "summarize: def reversemap(obj):        return obj.__class__(reversed(item) for item in obj.items())",
        "labels_text": "Invert mapping object preserving type and ordering"
    },
    {
        "input_text": "summarize: def chars(text, chars, repl=\"\"):        chars = chars.replace(\"\\\\\", \"\\\\\\\\\")    return re.sub(rf\"[{chars}]\", repl, text)",
        "labels_text": "Removes all char in repl from text"
    },
    {
        "input_text": "summarize: def name(text, sep=\"_\", allow_whitespaces=True):        bc = uniquify(_WINBADCHARS + _MACBADCHARS + _UNIXBADCHARS)    repl = r\"\".join(bc)    if not allow_whitespaces:        repl += \" \"    res = chars(text, repl, sep).strip()    if res.lower() in _WINBADWORDS:        res = (sep or \"_\") + res    return res",
        "labels_text": "Remove invalid character"
    },
    {
        "input_text": "summarize: def uniquify(seq):        seen = set()    seen_add = seen.add    return type(seq)(x for x in seq if x not in seen and not seen_add(x))",
        "labels_text": "Remove duplicate from list preserving order"
    },
    {
        "input_text": "summarize: def match_first(string, *args):        for patternlist in args:        for pattern in patternlist:            r = pattern.search(string)            if r is not None:                name = r.group(1)                return name    return string",
        "labels_text": "match against list of regexp and return first match"
    },
    {
        "input_text": "summarize: def safejoin(*args):        return safepath(os.path.join(*args))",
        "labels_text": "ospathjoin safepath"
    },
    {
        "input_text": "summarize: def safename(value):        # repl = '<>:\"/\\\\|?*' if os.name == \"nt\" else '\\0/\\\\\"'    repl = '<>:\"/\\\\|?*\\0'    name = purge.chars(value, repl)    return name",
        "labels_text": "Remove invalid character"
    },
    {
        "input_text": "summarize: def lowerkeys(self):                return self.__dict__.keys()",
        "labels_text": "Like key but with all lowercase key"
    },
    {
        "input_text": "summarize: def loweritems(self):                return ((lowerkey, val) for lowerkey, (key, val) in self.__dict__.items())",
        "labels_text": "Like item but with all lowercase key"
    },
    {
        "input_text": "summarize: def __init__(self):                # Condition variable, used to signal waiters of a change in object        # state.        self.__condition = Condition(Lock())        # Initialize with no writers.        self.__writer = None        self.__writercount = 0        self.__upgradewritercount = 0        self.__pendingwriters = []        # Initialize with no readers.        self.__readers = {}",
        "labels_text": "Initialize this readwrite lock"
    },
    {
        "input_text": "summarize: def comments(value):        return _RE_COMMENTS.sub(\"\", value).strip()",
        "labels_text": "Removes HTML comment from a text string"
    },
    {
        "input_text": "summarize: def unescape(value):        return html.unescape(value)",
        "labels_text": "Translates HTML or XML escape character reference and entity from a text string"
    },
    {
        "input_text": "summarize: def tags(value):        return _RE_TAGS.sub(\"\", value).strip()",
        "labels_text": "Removes HTML tag from a text string"
    },
    {
        "input_text": "summarize: def rfc2047(value):        def decode_chunk(m):        data, encoding = decode_rfc2047_header(m.group(0))[0]        try:            res = data.decode(encoding)        except (LookupError, UnicodeEncodeError):            res = m.group(0)        return res    return _RE_RFC2047.sub(decode_chunk, value, re.I)",
        "labels_text": "Decodes RFC email message header value"
    },
    {
        "input_text": "summarize: def text(value):        return tags(unescape(value)).strip(\"'\\\" \")",
        "labels_text": "Removes HTML tag translate HTML escape character and remove surrounding quotation mark"
    },
    {
        "input_text": "summarize: def set(self, option, value, plugin=None):                self.plugin.pyload.api.set_config_value(            plugin or self.plugin.classname, option, value, section=\"plugin\"        )",
        "labels_text": "Set config value for current plugin param option param value return"
    },
    {
        "input_text": "summarize: def get(self, option, default=None, plugin=None):                try:            return self.plugin.pyload.config.get_plugin(                plugin or self.plugin.classname, option            )        except KeyError:            self.plugin.log_debug(                \"Config option `{}` not found, use default `{}`\".format(option, default)            )  # TODO: Restore to `log_warning` in 0.6.x            return default",
        "labels_text": "Returns config value for current plugin param option return"
    },
    {
        "input_text": "summarize: def store(self, key, value):                # NOTE: value must not be <bytes> otherwise BOOM! and moreover our sqlite db always return strings as <str>        entry = b85encode(json.dumps(value, ensure_ascii=False).encode()).decode()        self.plugin.pyload.db.set_storage(self.plugin.classname, key, entry)",
        "labels_text": "Saves a value persistently to the database"
    },
    {
        "input_text": "summarize: def retrieve(self, key=None, default=None):                entry = self.plugin.pyload.db.get_storage(self.plugin.classname, key)        if key:            if entry is None:                value = default            else:                value = json.loads(b85decode(entry).decode())        else:            if not entry:                value = default            else:                value = {k: json.loads(b85decode(v).decode()) for k, v in entry.items()}        return value",
        "labels_text": "Retrieves saved value or dict of all saved entry if key is None"
    },
    {
        "input_text": "summarize: def delete(self, key):                self.plugin.pyload.db.del_storage(self.plugin.classname, key)",
        "labels_text": "Delete entry in db"
    },
    {
        "input_text": "summarize: def which(filename):        dirname, basename = os.path.split(filename)    if dirname:        return filename if is_executable(filename) else None    else:        for path in os.environ[\"PATH\"].split(os.pathsep):            filename = os.path.join(path.strip('\"'), filename)            if is_executable(filename):                return filename",
        "labels_text": "Works exactly like the unix command which Courtesy of httpstackoverflowcoma"
    },
    {
        "input_text": "summarize: def forward(source, destination, recv_timeout=None, buffering=1024):        timeout = source.gettimeout()    source.settimeout(recv_timeout)    try:        raw_data = source.recv(buffering)    except socket.timeout:        pass    else:        while raw_data:            destination.sendall(raw_data)            try:                raw_data = source.recv(buffering)            except socket.timeout:                break    source.settimeout(timeout)",
        "labels_text": "Forward data from one socket to another"
    },
    {
        "input_text": "summarize: def mpi_to_int(self, s):                return int(\"\".join(\"{:02x}\".format(s[2:][x]) for x in range(len(s[2:]))), 16)",
        "labels_text": "Convert GCRYMPIFMTPGP bignum format to integer"
    },
    {
        "input_text": "summarize: def logged(self):                if not self.user:            return False        self.sync()        if (            self.info[\"login\"][\"timestamp\"] == 0            or self.timeout != -1            and self.info[\"login\"][\"timestamp\"] + self.timeout < time.time()            or self.req            and not self.req.cj.parse_cookie(\"fs_secure\")        ):            self.log_debug(\"Reached login timeout for user `%s`\" % self.user)            return False        else:            return True",
        "labels_text": "Checks if user is still logged in"
    },
    {
        "input_text": "summarize: def _find_owner_plugin(self):                f = frame = inspect.currentframe()        try:            while True:                if f is None:                    return None                elif \"self\" in f.f_locals and is_simple_plugin(f.f_locals[\"self\"]):                    return f.f_locals[\"self\"]                else:                    f = f.f_back        finally:            del frame",
        "labels_text": "Walk the callstack until we find SimpleDownloader or SimpleDecrypter class Dirty but work"
    },
    {
        "input_text": "summarize: def config_changed(self, category, option, value, section):                if (            category == self.__name__            and option == \"timetable\"            and value != self.last_timetable        ):            self.update_schedule(schedule=value)",
        "labels_text": "Listen for config change to trigger a schedule update"
    },
    {
        "input_text": "summarize: def pyload_updated(self, etag):                self.call_script(\"pyload_updated\", etag)",
        "labels_text": "plugins were updated by UpdateManager"
    },
    {
        "input_text": "summarize: def pyload_start(self):                self.call_script(\"pyload_start\")",
        "labels_text": "pyload wa just started"
    },
    {
        "input_text": "summarize: def exit(self):                event = \"restart\" if self.pyload._do_restart else \"stop\"        self.call_script(\"pyload_\" + event, lock=True)",
        "labels_text": "deprecated method use pyloadstop or pyloadrestart instead"
    },
    {
        "input_text": "summarize: def before_reconnect(self, ip):                self.call_script(\"before_reconnect\", ip)",
        "labels_text": "called before reconnecting"
    },
    {
        "input_text": "summarize: def after_reconnect(self, ip, oldip):                self.call_script(\"after_reconnect\", ip, oldip)",
        "labels_text": "called after reconnecting"
    },
    {
        "input_text": "summarize: def download_preparing(self, pyfile):                args = [pyfile.id, pyfile.name, None, pyfile.pluginname, pyfile.url]        self.call_script(\"download_preparing\", *args)",
        "labels_text": "a download wa just queued and will be prepared now"
    },
    {
        "input_text": "summarize: def download_failed(self, pyfile):                file = pyfile.plugin.last_download        args = [pyfile.id, pyfile.name, file, pyfile.pluginname, pyfile.url]        self.call_script(\"download_failed\", *args)",
        "labels_text": "download ha failed"
    },
    {
        "input_text": "summarize: def download_finished(self, pyfile):                file = pyfile.plugin.last_download        args = [pyfile.id, pyfile.name, file, pyfile.pluginname, pyfile.url, pyfile.package().name]        self.call_script(\"download_finished\", *args)",
        "labels_text": "download successfully finished"
    },
    {
        "input_text": "summarize: def download_processed(self, pyfile):                file = pyfile.plugin.last_download        args = [pyfile.id, pyfile.name, file, pyfile.pluginname, pyfile.url]        self.call_script(\"download_processed\", *args)",
        "labels_text": "download wa precessed"
    },
    {
        "input_text": "summarize: def archive_extract_failed(self, pyfile, archive):                args = [pyfile.id, pyfile.name, archive.filename, archive.out, archive.files]        self.call_script(\"archive_extract_failed\", *args)",
        "labels_text": "archive extraction failed"
    },
    {
        "input_text": "summarize: def archive_extracted(self, pyfile, archive):                args = [pyfile.id, pyfile.name, archive.filename, archive.out, archive.files]        self.call_script(\"archive_extracted\", *args)",
        "labels_text": "archive wa successfully extracted"
    },
    {
        "input_text": "summarize: def archive_processed(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"archive_processed\", *args)",
        "labels_text": "package wa either extracted successfully or not or ignored because not an archive"
    },
    {
        "input_text": "summarize: def package_finished(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"package_finished\", *args)",
        "labels_text": "package finished successfully"
    },
    {
        "input_text": "summarize: def package_processed(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"package_processed\", *args)",
        "labels_text": "package wa processed"
    },
    {
        "input_text": "summarize: def package_deleted(self, pid):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        pdata = self.pyload.api.get_package_info(pid)        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pdata.folder)        args = [pdata.pid, pdata.name, dl_folder, pdata.password]        self.call_script(\"package_deleted\", *args)",
        "labels_text": "package wad deleted from the queue"
    },
    {
        "input_text": "summarize: def package_failed(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"package_failed\", *args)",
        "labels_text": "package failed somehow"
    },
    {
        "input_text": "summarize: def package_extract_failed(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder, pypack.password]        self.call_script(\"package_extract_failed\", *args)",
        "labels_text": "package extraction failed"
    },
    {
        "input_text": "summarize: def package_extracted(self, pypack):                dl_folder = self.pyload.config.get(\"general\", \"storage_folder\")        if self.pyload.config.get(\"general\", \"folder_per_package\"):            dl_folder = os.path.join(dl_folder, pypack.folder)        args = [pypack.id, pypack.name, dl_folder]        self.call_script(\"package_extracted\", *args)",
        "labels_text": "package wa successfully extracted"
    },
    {
        "input_text": "summarize: def all_downloads_finished(self):                self.call_script(\"all_downloads_finished\")",
        "labels_text": "every download in queue is finished successfully"
    },
    {
        "input_text": "summarize: def all_downloads_processed(self):                self.call_script(\"all_downloads_processed\")",
        "labels_text": "every download wa handled successfully or not pyload would idle afterwards"
    },
    {
        "input_text": "summarize: def all_archives_extracted(self):                self.call_script(\"all_archives_extracted\")",
        "labels_text": "all archive were extracted"
    },
    {
        "input_text": "summarize: def all_archives_processed(self):                self.call_script(\"all_archives_processed\")",
        "labels_text": "every archive wa handled successfully or not"
    },
    {
        "input_text": "summarize: def extract_package(self, *ids):                for id in ids:            self.queue.add(id)        if not self.config.get(\"waitall\") and not self.extracting:            self.extract_queued()",
        "labels_text": "Extract package with given id"
    },
    {
        "input_text": "summarize: def get_passwords(self, reload=True):                if reload:            self.reload_passwords()        return self.passwords",
        "labels_text": "List of saved password"
    },
    {
        "input_text": "summarize: def add_password(self, password):                try:            self.passwords = uniquify([password] + self.passwords)            file = os.fsdecode(self.config.get(\"passwordfile\"))            with open(file, mode=\"w\") as fp:                for pw in self.passwords:                    fp.write(pw + \"\\n\")        except IOError as exc:            self.log_error(exc)",
        "labels_text": "Adds a password to saved list"
    },
    {
        "input_text": "summarize: def update(self):                if self._update() != 2 or not self.config.get(\"autorestart\"):            return        if not self.pyload.api.status_downloads():            self.pyload.api.restart()        else:            self.log_warning(                self._(\"pyLoad restart scheduled\"),                self._(                    \"Downloads are active, pyLoad restart postponed once the download is done\"                ),            )            self.pyload.api.pause_server()            self.do_restart = True",
        "labels_text": "Check for update"
    },
    {
        "input_text": "summarize: def announce(self, message):                self.log_debug(\"Announce, message:\", message)        for user in self.config.get(\"owners\").split(\";\"):            self.log_debug(\"Send message to\", user)            to_jid = slixmpp.jid.JID(user)            self.xmpp.sendMessage(                mfrom=self.jid, mto=to_jid, mtype=\"chat\", mbody=str(message)            )",
        "labels_text": "Send message to all owner"
    },
    {
        "input_text": "summarize: def logged(self):                if not self.user:            return False        self.sync()        if (            self.info[\"login\"][\"timestamp\"] == 0            or self.timeout != -1            and self.info[\"login\"][\"timestamp\"] + self.timeout < time.time()        ):            self.log_debug(f\"Reached login timeout for user `{self.user}`\")            return False        else:            return True",
        "labels_text": "Checks if user is still logged in"
    },
    {
        "input_text": "summarize: def setup(self):                pass",
        "labels_text": "Setup for environment and other thing called before logging possibly more than one time"
    },
    {
        "input_text": "summarize: def signin(self, user, password, data):                raise NotImplementedError",
        "labels_text": "Login into account the cooky will be saved so user can be recognized"
    },
    {
        "input_text": "summarize: def logout(self):                self.sync()        self.info[\"login\"][\"timestamp\"] = 0        self.syncback()",
        "labels_text": "Invalidate the account timestamp so relogin will be forced next time"
    },
    {
        "input_text": "summarize: def syncback(self):                return self.sync(reverse=True)",
        "labels_text": "Wrapper to directly sync selfinfo selfaccountsselfuser"
    },
    {
        "input_text": "summarize: def grab_info(self, user, password, data):                raise NotImplementedError",
        "labels_text": "This should be overwritten in account plugin and retrieving account information for user param user param password param data return"
    },
    {
        "input_text": "summarize: def update_accounts(self, user, password=None, options={}):                if user in self.accounts:            self.log_info(self._(\"Updating account info for user `{}`...\").format(user))            u = self.accounts[user]            if password:                u[\"password\"] = password            if options:                u[\"options\"].update(options)            u[\"plugin\"].relogin()        else:            self.add(user, password, options)",
        "labels_text": "Updates account and return true if anything changed"
    },
    {
        "input_text": "summarize: def expose(func):        @wraps(func)    def wrapper(self, *args, **kwargs):        if not wrapper._exposed:            self.pyload.adm.add_rpc(func.__module__, func.__name__, func.__doc__)            wrapper._exposed = True        return func(self, *args, **kwargs)    wrapper._exposed = False    return wrapper",
        "labels_text": "Used for decoration to declare rpc service"
    },
    {
        "input_text": "summarize: def activated(self):                return self.config.get(\"enabled\")",
        "labels_text": "Checks if addon is activated"
    },
    {
        "input_text": "summarize: def deactivate(self):                pass",
        "labels_text": "Called when addon wa deactivated"
    },
    {
        "input_text": "summarize: def activate(self):                pass",
        "labels_text": "Called when addon wa activated"
    },
    {
        "input_text": "summarize: def exit(self):                pass",
        "labels_text": "Called by coreshutdown just before pyLoad exit"
    },
    {
        "input_text": "summarize: def captcha_task(self, task):                pass",
        "labels_text": "New captcha task for the plugin it MUST set the handler and timeout or will be ignored"
    },
    {
        "input_text": "summarize: def recognize(self, image):                pass",
        "labels_text": "Extend to build your custom anticaptcha ocr"
    },
    {
        "input_text": "summarize: def _cmd_captcha(self, args):                if not args:            return [self._(\"ERROR: Captcha ID missing.\")]        task = self.pyload.captcha_manager.get_task_by_id(args[0])        if not task:            return [self._(\"ERROR: Captcha Task with ID {} does not exists.\").format(args[0])]        task.set_result(\" \".join(args[1:]))        return [self._(\"INFO: Result {} saved.\").format(\" \".join(args[1:]))]",
        "labels_text": "Captcha answer"
    },
    {
        "input_text": "summarize: def _get_package_by_name_or_id(self, id_or_name):                if id_or_name.isdigit():            try:                package_id = int(id_or_name)                pack = self.pyload.api.get_package_data(package_id)            except PackageDoesNotExists:                pack = self._get_package_by_name(id_or_name)        else:            pack = self._get_package_by_name(id_or_name)        return pack",
        "labels_text": "Return the first PackageData found or None"
    },
    {
        "input_text": "summarize: def _get_package_by_name(self, name):                pq = self.pyload.api.get_queue_data()        for pack in pq:            if pack.name == name:                return pack        pc = self.pyload.api.get_collector()        for pack in pc:            if pack.name == name:                return pack        return None",
        "labels_text": "Return the first PackageData found or None"
    },
    {
        "input_text": "summarize: def process(self, pyfile):                self._make_tmpfile()        self.decrypt(pyfile)        if self.links:            self._generate_packages()        elif not self.packages:            self.error(self._(\"No link grabbed\"), \"decrypt\")        self._delete_tmpfile()        self._create_packages()",
        "labels_text": "Main method"
    },
    {
        "input_text": "summarize: def decrypt(self, pyfile):                raise NotImplementedError",
        "labels_text": "The main method of every decrypter plugin you have to overwrite it"
    },
    {
        "input_text": "summarize: def _generate_packages(self):                name = self.info[\"pattern\"].get(\"N\")        if name is None:            links = [self.fixurl(url) for url in self.links]            pdict = self.pyload.api.generate_packages(links)            packages = [                (name, links, parse.name(name)) for name, links in pdict.items()            ]        else:            packages = [(name, self.links, parse.name(name))]        self.packages.extend(packages)",
        "labels_text": "Generate new package from selflinks"
    },
    {
        "input_text": "summarize: def archivetype(cls, filename):                name = os.path.basename(filename).lower()        for ext in cls.EXTENSIONS:            if isinstance(ext, str):                if name.endswith(\".\" + ext):                    return ext            elif isinstance(ext, tuple):                if re.search(r\"\\.\" + ext[1] + \"$\", name):                    return ext[0]        return None",
        "labels_text": "Get archive default extension from filename param filename file name to test return Extension or None"
    },
    {
        "input_text": "summarize: def find(cls):                pass",
        "labels_text": "Check if system statisfy dependency"
    },
    {
        "input_text": "summarize: def get_targets(cls, files_ids):                targets = []        processed = []        for id, fname, fout in files_ids:            if not cls.isarchive(fname):                continue            if cls.ismultipart(fname):                pname = cls._RE_PART.sub(\"\", fname)            else:                pname = os.path.splitext(fname)[0]            if pname in processed:                continue            processed.append(pname)            targets.append((id, fname, fout))        return targets",
        "labels_text": "Filter suited target from list of filename id tuple list param filesids List of filepathes return List of target id tuple list"
    },
    {
        "input_text": "summarize: def verify(self, password=None):                pass",
        "labels_text": "Testing with Extractors builtin method Raise error if password is needed integrity is questionable or else"
    },
    {
        "input_text": "summarize: def extract(self, password=None):                raise NotImplementedError",
        "labels_text": "Extract the archive Raise specific error in case of failure"
    },
    {
        "input_text": "summarize: def chunks(self):                return [self.filename]",
        "labels_text": "Return list of archive part"
    },
    {
        "input_text": "summarize: def list(self, password=None):                raise NotImplementedError",
        "labels_text": "Return list of archive file"
    },
    {
        "input_text": "summarize: def progress(self, x):                return self.pyfile.set_progress(int(x))",
        "labels_text": "Set extraction progress"
    },
    {
        "input_text": "summarize: def setup(self):                pass",
        "labels_text": "Setup for enviroment and other thing called before downloading possibly more than one time"
    },
    {
        "input_text": "summarize: def process(self, pyfile):                raise NotImplementedError",
        "labels_text": "The main method of every downloader plugin you have to overwrite it"
    },
    {
        "input_text": "summarize: def skip(self, msg=\"\"):                raise Skip(msg or self.pyfile.error or self.pyfile.pluginname)",
        "labels_text": "Skip and give msg"
    },
    {
        "input_text": "summarize: def fail(self, msg=\"\"):                msg = msg.strip()        if msg:            self.pyfile.error = msg        else:            msg = (                self.pyfile.error                or self.info.get(\"error\")                or self.pyfile.get_status_name()            )        raise Fail(msg)",
        "labels_text": "Fail and give msg"
    },
    {
        "input_text": "summarize: def abort(self, msg=\"\"):                if msg:  # TODO: Remove in 0.6.x            self.pyfile.error = msg        raise Abort",
        "labels_text": "Abort and give msg"
    },
    {
        "input_text": "summarize: def offline(self, msg=\"\"):                self.fail(\"offline\")",
        "labels_text": "Fail and indicate file is offline"
    },
    {
        "input_text": "summarize: def temp_offline(self, msg=\"\"):                self.fail(\"temp. offline\")",
        "labels_text": "Fail and indicates file ist temporary offline the core may take consequence"
    },
    {
        "input_text": "summarize: def get_password(self):                return self.pyfile.package().password or \"\"",
        "labels_text": "Get the password the user provided in the package"
    },
    {
        "input_text": "summarize: def clean(self):                super().clean()        for attr in (\"account\", \"html\", \"pyfile\", \"thread\"):            if hasattr(self, attr):                setattr(self, attr, None)",
        "labels_text": "Clean everything and remove reference"
    },
    {
        "input_text": "summarize: def grab_hosters(self, user, password, data):                raise NotImplementedError",
        "labels_text": "Load list of supported downloaders return List of domain name"
    },
    {
        "input_text": "summarize: def deactivate(self):                pass",
        "labels_text": "Delete all tmp image"
    },
    {
        "input_text": "summarize: def call_cmd(self, command, *args, **kwargs):                call = [command] + args        self.log_debug(\"EXECUTE \" + \" \".join(call))        popen = subprocess.Popen(call)        popen.wait()        output = popen.stdout.read() + \" | \" + popen.stderr.read()        popen.stdout.close()        popen.stderr.close()        self.log_debug(f\"Tesseract ReturnCode {popen.returncode}\", f\"Output: {output}\")",
        "labels_text": "Run a command"
    },
    {
        "input_text": "summarize: def init(self):                pass",
        "labels_text": "Initialize the plugin in addition to init"
    },
    {
        "input_text": "summarize: def skip(self, msg):                raise Skip(msg)",
        "labels_text": "Skip and give msg"
    },
    {
        "input_text": "summarize: def fail(self, msg):                raise Fail(msg)",
        "labels_text": "Fail and give msg"
    },
    {
        "input_text": "summarize: def clean(self):                try:            # self.req.clear_cookies()            self.req.close()        except AttributeError:            pass        else:            self.req = None",
        "labels_text": "Remove reference"
    },
    {
        "input_text": "summarize: def get_links(self):                if self.premium:            self.log_info(self._(\"Decrypting as premium link...\"))            self.handle_premium(self.pyfile)        elif not self.LOGIN_ACCOUNT:            self.log_info(self._(\"Decrypting as free link...\"))            self.handle_free(self.pyfile)        links = self.links        self.links = []        return links",
        "labels_text": "Returns the link extracted from selfdata You should override this only if it impossible to extract link using only the LINKPATTERN"
    },
    {
        "input_text": "summarize: def delete_torrent_from_server(self, torrent_id):                api_data = self.api_request(\"magnet/delete\",                                    get={\"apikey\": self.api_token,                                          \"id\": torrent_id})        if api_data.get(\"error\", False):            self.log_warning(\"{} (code: {})\".format(api_data[\"error\"][\"message\"], api_data[\"error\"][\"code\"]))",
        "labels_text": "Remove the torrent from the server"
    },
    {
        "input_text": "summarize: def sanitize(self, name):                keepcharacters = (\" \", \"\\t\", \".\", \"_\")        replacecharacters = (\" \", \"\\t\")        return \"\".join(            c if c not in replacecharacters else \"_\"            for c in name.strip()            if c.isalnum() or c in keepcharacters        ).strip(\"_\")",
        "labels_text": "Turn Imgur Gallery title into a safe Package and Folder name"
    }
]